{
	"input": "I need to optimize a specific 32-bit hash function with 6 stages, where each stage follows the pattern:\n    a = ((a op1 const1) op2 (a shift_op shift_amount))\n  where op1/op2 are + or ^, and shift_op is << or >>\n\n  This creates a 18-operation sequential dependency chain per hash call. I need to execute 256 hashes per round × 16 rounds = 4096 total\n  hashes. I have 12 ALU slots and 6 VALU slots per cycle.\n\n  Search for the most advanced techniques:\n\n  1. Research on \"SIMD hash function implementation\", \"vectorized hash functions\", \"parallel hashing techniques\", particularly for\n  non-cryptographic hashes like MurmurHash, xxHash, CityHash\n\n  2. Papers on \"bitslicing\" or \"interleaved implementations\" that restructure hash computations to expose more parallelism - especially\n  techniques from cryptographic implementations\n\n  3. Research on \"latency hiding through interleaving\" - computing multiple independent hash instances simultaneously by interleaving their\n  instructions\n\n  4. Forum posts, blog posts, or code from high-performance computing practitioners about hand-optimizing hash functions for specific SIMD\n  architectures\n\n  5. Any research on \"dependency chain breaking\" or \"chain scheduling\" - techniques to restructure long sequential computations\n\n  6. Papers from Intel, ARM, or compiler research on \"strength reduction\" or \"algebraic simplification\" for hash-like computations\n\n  7. Research on \"speculative hash computation\" - could you speculatively compute both branches (val & 1 == 0 vs 1) and select?\n\n  8. Obscure mathematical properties that might simplify this specific hash pattern - any papers on analyzing or optimizing hash functions of\n  the form a = (a+c1) ^ (a<<c2)\n\n  9. GPU shader optimization techniques for hash functions - demoscene, procedural generation, or noise function optimization discussions\n\n  10. Research on \"operation fusion\" or \"macro-op fusion\" that might let you combine multiple stages of the hash into fewer cycles\n\n  11. Papers on \"software pipelining for reduction operations\" or hash-like computations with loop-carried dependencies\n\n  The hash must be computed exactly (bit-identical results), but any mathematical transformation that produces the same output is valid. I'm\n  looking for any way to expose parallelism in what appears to be an inherently sequential computation.",
	"output": {
		"optimization_summary": "The most effective strategies for optimizing the sequential hash function revolve around exploiting the embarrassingly parallel nature of the 4096-hash workload. The primary approach is to combine wide SIMD vectorization (preferably AVX-512 to process 16 hashes per instruction) with deep instruction interleaving (processing 2-4 independent SIMD vectors concurrently). This combination transforms the latency-bound problem of a single hash's 18-operation dependency chain into a throughput-bound problem, effectively hiding the latency by keeping the CPU's execution units saturated with independent work. This core strategy must be supported by a critical data layout transformation to an Array of Structures of Arrays (AoSoA), which enables efficient, contiguous memory access for the SIMD units and avoids slow gather operations. Further performance can be gained through microarchitectural optimizations like using the x86 LEA instruction for fused arithmetic, understanding AVX-512 frequency scaling on the target CPU, and ensuring a register allocation plan that avoids spills. For ultimate throughput, the entire parallel workload can be offloaded to a GPU, leveraging its thousands of cores for massive parallel execution.",
		"primary_recommendation": "The single most impactful recommendation for achieving maximum throughput is to implement a vector-interleaved kernel. This involves using wide SIMD instructions, preferably AVX-512, to process 16 independent 32-bit hash states in parallel. To hide the 18-operation dependency chain latency inherent in each hash, the computation should be interleaved by processing multiple (e.g., 2 to 4) independent SIMD vectors concurrently. This strategy transforms the problem from being latency-bound to throughput-bound by ensuring the CPU's execution units are always saturated with independent instructions from different hash states. This approach, which requires a one-time data layout transformation to an AoSoA (Array of Structures of Arrays) format for efficient memory access, will yield the most significant performance gains, far outweighing other techniques like algebraic simplification or speculative execution.",
		"simd_vectorization_strategy": {
			"recommended_isa": "AVX-512",
			"supported_isas": "AVX2, SSE2, ARM NEON, ARM SVE, POWER VSX",
			"intrinsics_pattern": "For a stage defined as `a = ((a + const1) ^ (a << shift_amount))`, the AVX-512 intrinsics pattern is: `_mm512_add_epi32` to perform the addition with a broadcasted constant, `_mm512_slli_epi32` to perform the logical left shift, and `_mm512_xor_si512` to combine the results. The full sequence for one stage on a vector of 16 hash states (`zmm_a`) would be: `tmp_add = _mm512_add_epi32(zmm_a, zmm_const1); tmp_shl = _mm512_slli_epi32(zmm_a, shift_amount); zmm_a = _mm512_xor_si512(tmp_add, tmp_shl);`.",
			"lane_width_choice": 16,
			"estimated_cycles_per_hash": 1.125
		},
		"interleaving_and_software_pipelining": {
			"technique_description": "Interleaving independent hash instances is a latency-hiding technique where instructions from multiple, separate hash computations are mixed together in the instruction stream. Instead of completing all 6 stages for one hash (or one SIMD vector of hashes) before starting the next, the processor executes stage 1 for instance A, then stage 1 for instance B, and so on. This provides the out-of-order execution engine with a large pool of independent instructions, allowing it to find and execute work from one hash instance while another instance is stalled waiting for the result of a dependent operation. This keeps the processor's functional units (ALUs, VALUs) continuously busy, effectively hiding the 18-operation dependency chain latency and maximizing instruction throughput.",
			"dependency_chain_latency_to_hide": 18,
			"recommended_interleave_factor": 4,
			"register_pressure_analysis": "On x86-64 architectures, scalar interleaving is severely limited by the ~14 available general-purpose registers (GPRs), making it impossible to hide an 18-cycle latency without spilling registers to memory. In contrast, vector interleaving with AVX-512 is highly efficient due to the 32 available 512-bit ZMM registers. Interleaving 4 vectors requires only 4 registers for the hash states, plus a few for temporaries, easily fitting within the available register file and avoiding spills. On AArch64, the larger GPR file (31 registers) makes scalar interleaving more feasible, but the vector approach remains superior for throughput.",
			"scalar_vs_vector_interleaving": "Vector-interleaved (SIMD) scheduling is unequivocally superior to scalar interleaving for this workload. Vector interleaving processes multiple hashes (e.g., 16 with AVX-512) per instruction, leading to massively higher throughput and more efficient use of wide execution units. Performance estimates show vector interleaving achieving ~1.125 cycles/hash, a potential 4x improvement over a scalar-interleaved approach (~4.5 cycles/hash), which is further degraded by register spilling on x86-64 architectures. Vector interleaving also results in lower register pressure per hash processed."
		},
		"data_layout_optimization": {
			"recommended_layout": "AoSoA (Array of Structures of Arrays)",
			"layout_rationale": "The AoSoA layout is superior because it arranges the 32-bit hash states into contiguous blocks, with each block sized to match the SIMD vector width (e.g., 16 states for AVX-512). This allows a full vector of independent hash states to be loaded with a single, efficient, contiguous memory instruction (e.g., `vmovdqa32`). This contrasts sharply with an Array of Structures (AoS) layout, which would require slow, high-latency `gather` instructions to collect states from disparate memory locations. AoSoA ensures 100% cache line utilization for field-wise operations and creates a simple, linear access pattern that is easily detected by hardware prefetchers, maximizing memory bandwidth and hiding latency.",
			"transposition_strategy": "If the initial 4096 hash states are in a non-optimal layout like AoS, a one-time transposition into the AoSoA format is required. This is performed using highly optimized SIMD shuffle and permute instructions. For example, an 8x8 matrix of 32-bit integers can be transposed using AVX2 instructions, and a 16x16 matrix can be transposed even more efficiently with AVX-512 instructions like `vshufi32x4` and `vpermi2d`. Research shows that an optimized AVX-512 transpose can be over 37 times faster than a scalar equivalent, making the upfront cost of this transformation minimal and easily amortized by the massive performance gains in the main computational kernel.",
			"cache_and_prefetching_impact": "The AoSoA layout dramatically improves memory subsystem performance. By processing data in tiles that fit within the L1 cache (e.g., a tile of 256 hashes is only 1KB), it ensures high data locality and minimizes cache misses. The unit-stride, linear access pattern through the array of tiles is ideal for the CPU's hardware prefetcher, which can accurately predict and fetch subsequent cache lines into the L1 and L2 caches before they are requested by the program, effectively hiding memory access latency and ensuring the execution units are not starved for data.",
			"gather_instruction_avoidance": true
		},
		"microarchitectural_scheduling_model": {
			"target_architectures": "The analysis covers a wide range of modern and recent high-performance CPU microarchitectures, including: Intel architectures from Skylake through Sapphire Rapids (specifically Skylake, Skylake-SP/SKL-X, Sunny Cove/Ice Lake Client, Golden Cove/Sapphire Rapids/Alder Lake, Rocket Lake, and Tiger Lake); AMD architectures from Zen 2 through Zen 5 (specifically Zen 2, Zen 3, Zen 4, and Zen 5); and ARM architectures including Neoverse cores (N1, V1, V2) and Cortex cores (A76, A78). This selection provides a comprehensive view of instruction characteristics and scheduling opportunities across the major server and client CPU families.",
			"bottleneck_resource": "Across all analyzed modern microarchitectures (Intel, AMD, ARM), the primary execution resource that limits the throughput of the vectorized hash function is consistently identified as the **vector shift units**. While these CPUs have multiple powerful vector ALU pipes for handling additions and logical operations, the number of pipes or ports capable of executing vector shifts is comparatively lower. For example, on AMD Zen 4, there are four pipes for vector logical/add operations but only two for vector shifts, creating a 2:1 throughput ratio. Similarly, on Intel Skylake, there are three vector logical ports but only two vector shift ports. This disparity makes the vector shift capability the main bottleneck, dictating the maximum sustainable rate of the hash computation.",
			"port_pressure_analysis": "Contention for specific execution ports is a critical factor. On Intel Skylake cores, vector logical operations (`VPXOR`, `VPADDD`) can use ports p0, p1, and p5, while vector shifts (`VPSLLD`) are restricted to ports p0 and p5. This creates a potential bottleneck on port p5 if shifts and logicals are heavily mixed. On newer Intel cores like Sapphire Rapids, the number of units is increased, but a disparity remains: there are more units for logical operations than for shifts, especially at 512-bit width. On AMD Zen 4, vector logical/add operations can execute on four pipes (FP0-FP3), but vector shifts are restricted to just two pipes (FP0-FP1), making shifts the clear bottleneck. On ARM Neoverse cores, a similar pattern of pipeline specialization exists, where vector shifts are often restricted to a subset of the available vector pipelines (e.g., on Neoverse N1, there are two vector logical pipes but only one vector shift pipe). This consistent pattern of shift units being a more constrained resource than general ALU units is the primary source of port pressure for this workload.",
			"alu_valu_slot_utilization_plan": "For the user's hypothetical machine with 12 ALU and 6 VALU slots per cycle, the optimal plan is to map the core hash computations to the VALU (Vector ALU) slots and use the ALU slots for auxiliary scalar tasks. The 6-stage hash, composed of add/xor/shift operations, is implemented using SIMD vector instructions. To saturate the 6 VALU slots, the long 18-operation dependency chain must be broken. This is achieved by interleaving multiple independent SIMD vectors. For instance, using AVX-512 (16 hashes per vector) with a 4-way interleave means the CPU is actively working on 64 hashes concurrently. This provides a deep pool of independent vector instructions, allowing the scheduler to dispatch up to 6 VALU instructions per cycle. The theoretical throughput is `(4096 hashes * 18 ops/hash) / (6 VALU ops/cycle) = 12288` total cycles, or 3 cycles per hash. The 12 ALU slots are utilized in parallel for all scalar and address-generation tasks, such as: 1) Loop control, including incrementing counters and performing comparisons. 2) Address calculations for loading the initial hash states and storing the final results, especially when using an AoSoA (Array of Structures of Arrays) data layout which requires pointer arithmetic. 3) Handling any control flow for partitioning the workload across threads. This division of labor ensures both the vector and scalar execution resources of the core are kept busy, maximizing overall throughput."
		},
		"algebraic_simplification_techniques": {
			"x86_lea_optimization": "The x86 Load Effective Address (`LEA`) instruction can be repurposed for general-purpose arithmetic to fuse multiple operations into a single micro-op (uop). Specifically, for the hash pattern where `op2` is addition and `shift_op` is a left shift, the expression `a + (a << k) + const1` can be computed in a single `LEA` instruction, such as `LEA reg, [reg + reg*scale + const]`. This is only possible when the shift amount `k` corresponds to a valid `scale` factor, limiting it to `k` values of 0, 1, 2, or 3 (for scales of 1, 2, 4, or 8, respectively). On modern Intel CPUs (Ice Lake and newer) and AMD Zen 4, this complex `LEA` has a latency of just 1 cycle. However, on older architectures like Skylake and Haswell, it has a 3-cycle latency, which could make a sequence of simpler `ADD` instructions preferable if it's on the critical path. A key benefit of `LEA` is that it does not modify the CPU's flags register, which breaks data dependencies and gives the out-of-order execution engine more scheduling flexibility. On some architectures, it also uses dedicated Address Generation Units (AGUs), offloading work from the main ALUs.",
			"aarch64_shifted_operand_optimization": "The AArch64 instruction set provides a highly flexible and powerful mechanism for fusing operations through its shifted-register operand format. Instructions like `ADD` and `EOR` can take a second source operand that is shifted before the operation is performed. For example, the expression `a + (a << k)` can be computed in a single instruction: `ADD wD, wN, wM, LSL #k`. This feature is more general than x86's `LEA` as it supports a wider range of operations (including `EOR` for XOR), a full range of shift types (`LSL`, `LSR`, `ASR`, and `ROR` for logical ops), and a much wider range of immediate shift amounts (0-31 for 32-bit registers). These instructions typically have a 1-cycle latency and execute as a single micro-op. While the `const1` part of the hash pattern would still require a separate `ADD` instruction, this fusion significantly reduces the instruction count and latency of the core `a op2 (a shift_op ...)` part of the computation.",
			"reassociation_opportunities": "The ability to reorder operations depends on the properties of `op1` and `op2`. If both operators are the same and are associative and commutative (like `+` or `^`), reordering is possible. For a pure addition pattern `a = (a + const1) + (a shift_op amount)`, it can be safely reordered to `a = (a + (a shift_op amount)) + const1`. This form is particularly advantageous as it groups the `a`-dependent terms, making it a candidate for fusion with `LEA` on x86 or shifted-register `ADD` on AArch64. Similarly, a pure XOR pattern `a = (a ^ const1) ^ (a shift_op amount)` can be reordered to `a = (a ^ (a shift_op amount)) ^ const1`. However, for mixed-operator patterns (ARX - Add-Rotate-XOR), such as `(a + const1) ^ (a << amount)`, the operations do not distribute over each other, and the expression is non-linear, preventing simple reassociation while preserving bit-exactness. These mixed forms are common in cryptography precisely because they resist algebraic simplification.",
			"safety_and_bit_exactness": "Ensuring bit-identical results across different platforms and optimization levels requires strict adherence to well-defined semantics. For 32-bit unsigned integers (`uint32_t`), arithmetic operations are defined to wrap around (operate modulo 2^32), a behavior that is consistent across C/C++ and modern ISAs, making algebraic transformations based on modular arithmetic safe. The primary area of concern is shift operations. The C/C++ standard defines shifts by an amount greater than or equal to the bit-width of the operand as undefined behavior. In contrast, the x86 ISA guarantees that the shift count is masked to 5 bits (i.e., `shift_amount & 31`), meaning a shift by 32 is a no-op. ARM NEON and AVX2/AVX-512 variable shift instructions have their own defined behavior, often zeroing the result for out-of-range shift counts. A transformation is only proven-safe if it correctly models these specific target semantics. Validation must involve rigorous differential testing against a golden scalar reference, using a wide range of random inputs and exhaustive testing of edge cases, particularly for shift amounts from 0 to 31 and beyond."
		},
		"bitslicing_and_carry_save_analysis": {
			"technique_description": "Bitslicing, also known as data orthogonalization or SIMD Within A Register (SWAR), is a technique that restructures data to enable massive parallelism at the bit level. Instead of storing a 32-bit word in a single register, the data is transposed. To process 'm' independent hashes in parallel, the state is represented by 32 variables (typically SIMD registers). The first register holds bit 0 from all 'm' hash states, the second register holds bit 1 from all 'm' states, and so on, up to the 31st bit. This transforms the problem from operating on 'm' words to operating on 32 bit-planes, where each bit-plane contains 'm' bits.",
			"advantages": "The primary advantage of bitslicing is its extreme efficiency for certain operations. All bitwise logical operations (AND, OR, XOR, NOT) on the original data words translate into a single corresponding SIMD instruction on the bit-plane registers, executing the operation for all 'm' instances simultaneously. Furthermore, constant-amount shifts and rotations become effectively 'free' at runtime. These operations are resolved at compile time by simply relabeling or re-routing the variables that represent the bit slices, requiring no actual machine instructions. This makes bitslicing exceptionally powerful for algorithms dominated by logical and shift operations.",
			"challenges": "The main challenge of bitslicing is the implementation of arithmetic operations, particularly addition. Since native CPU `ADD` instructions cannot be used on the bit-plane representation, addition must be simulated using a sequence of bitwise logical instructions that implement a Boolean adder circuit. A simple ripple-carry adder creates a long dependency chain as the calculation for each bit depends on the carry from the previous bit, limiting parallelism. While faster parallel-prefix adders (e.g., Kogge-Stone) can reduce this dependency to logarithmic time, they significantly increase the total instruction count. This makes bitsliced addition 2 to 5 times slower than a native `ADD`. Other significant challenges include the high overhead of converting data into and out of the bitsliced format (a matrix transposition operation) and the very high register pressure, as a single 32-bit variable requires 32 registers to hold its state.",
			"feasibility_for_workload": "For the specified workload of 4096 independent hashes, a fully bitsliced implementation is a highly promising, albeit complex, strategy. The massive parallelism of 4096 instances is sufficient to amortize the one-time overhead of data conversion (transposition). While the addition operations within the hash will be slower to simulate, the 'free' shifts and highly efficient XORs can provide a significant performance benefit that outweighs this cost. To manage the latency of carry propagation from the addition stages, carry-save addition (CSA) techniques can be adapted. This involves deferring the final carry-propagate step by representing intermediate results as a pair of sum and carry vectors, which can be combined later. Given the scale of the workload, the benefits of bitslicing are very likely to outweigh its drawbacks, making it a viable and potentially superior approach compared to more straightforward vectorization."
		},
		"dependency_chain_scheduling_theory": {
			"key_concepts": "The core theoretical concepts for scheduling this computation are rooted in compiler optimization for loops with recurrences. The primary concepts are: 1) **Recurrence-constrained Minimum Initiation Interval (RecMII):** This is the fundamental analytical limit on the performance of a loop with a data dependency that carries across iterations. It is calculated as the total latency of all operations in the dependency cycle. For the 18-operation chain, the RecMII is the sum of the latencies of those 18 operations, which dictates the minimum number of cycles between the start of successive hash computations if the chain is not broken. 2) **Software Pipelining and Modulo Scheduling:** These are techniques for overlapping the execution of multiple loop iterations to hide latency and keep hardware resources busy. Modulo Scheduling is a specific family of algorithms (like Swing Modulo Scheduling) that schedules each iteration at a constant rate, known as the Initiation Interval (II), aiming to achieve the MII. 3) **Reassociation and Tree Height Reduction:** These techniques reorder expressions using associative and commutative properties to create a more balanced computation tree, reducing the length of the critical path and exposing more instruction-level parallelism.",
			"primary_limitation_identified": "The fundamental performance limitation is the **loop-carried true data dependency** inherent in the 6-stage hash function. Each stage's output `a` is the input to the next stage, creating a sequential chain of 18 dependent operations. This dependency prevents a single hash from being parallelized internally. In theoretical terms, this creates a high Recurrence-constrained Minimum Initiation Interval (RecMII). The RecMII is at least 18 cycles (assuming 1-cycle latency per operation), meaning a new hash computation for the *same* state cannot begin until the previous one is complete 18 cycles later. This serial dependency chain will leave the processor's wide execution resources (12 ALU / 6 VALU slots) severely underutilized if only a single hash is processed at a time.",
			"most_effective_technique": "The most effective and widely recommended technique for breaking the dependency chain is **N-way Interleaving**, also known as using multiple accumulators. Since the 4096 hashes are independent, instead of computing one hash from start to finish, the instructions from multiple independent hash computations are interleaved. This approach transforms the problem from being latency-bound by the 18-op chain to being throughput-bound by the CPU's execution units. The optimal implementation of this is a **vector-interleaved schedule**, where SIMD instructions (like AVX-512) process a vector of hashes (e.g., 16) at once, and the code is structured to interleave operations from multiple *vectors* of hashes. This provides a massive number of independent instructions to the CPU's out-of-order engine, completely hiding the latency of the dependency chain and allowing for the saturation of all available execution ports."
		},
		"compiler_auto_vectorization_guidance": {
			"compiler": "Clang/LLVM, GCC, Intel ICC/ICX, MSVC",
			"key_flags": "To enable and control vectorization, specific compiler flags are essential. For Clang/LLVM, use `-O3` (which enables vectorization by default), `-fvectorize`, and target-specific flags like `-mavx2` or `-mavx512f`. For GCC, use `-O2` or `-O3` (which enable `-ftree-vectorize` by default) and target architecture flags like `-march=native` or `-mavx2`. For Intel's ICX, `-O3` is not sufficient; you must also use `-xCORE-AVX2` or `-xCORE-AVX512` to enable vectorization for those ISAs, and `-qopt-zmm-usage=high` can encourage more aggressive use of 512-bit registers. For MSVC, `/O2` enables vectorization, and `/arch:AVX2` or `/arch:AVX512` specifies the target instruction set. To analyze the compiler's decisions, use diagnostic flags: `-Rpass=loop-vectorize` (Clang), `-fopt-info-vec` (GCC), `-qopt-report` (Intel), and `/Qvec-report:2` (MSVC).",
			"key_pragmas": "Source-level pragmas are critical for guiding the compiler and overriding its cost models. The most important and portable directive is `#pragma omp simd`, which is supported by all major compilers and asserts that a loop is safe to vectorize without loop-carried dependencies. Another widely supported pragma is `#pragma ivdep` (GCC, Intel, MSVC), which similarly informs the compiler to ignore potential vector dependencies. For more fine-grained control, Clang offers `#pragma clang loop vectorize(enable)` to force vectorization and `#pragma clang loop vectorize_width(N)` to specify the number of lanes. Intel's compilers have a similar `#pragma vector always` to force vectorization, which can be necessary for the newer LLVM-based ICX compiler.",
			"refactoring_tips": "To maximize the effectiveness of auto-vectorization, several code refactoring techniques are crucial. First, ensure bit-exact semantics by using `unsigned` integer types (e.g., `uint32_t`) for all hash state calculations; this guarantees well-defined wraparound arithmetic and logical right shifts. Second, explicitly communicate data independence to the compiler by using the `restrict` keyword (in C) or `__restrict__` (in C++) on pointers to assert they do not alias. Third, the most critical transformation is to change the data layout from a naive Array-of-Structures (AoS) to a SIMD-friendly Structure-of-Arrays (SoA) or, ideally, an Array-of-Structures-of-Arrays (AoSoA). This enables the compiler to use efficient, contiguous vector loads (`vmovdqa`) instead of extremely slow `gather` instructions. Fourth, for specific patterns like `a = (a + const) + (a << k)`, be aware that compilers can leverage specialized instructions like x86's `LEA` (for k=1,2,3) or AArch64's shifted-operand `ADD` to fuse multiple operations into a single, faster instruction. Finally, ensure all shift amounts are within the valid range for the data type to avoid C/C++ undefined behavior, which can prevent vectorization."
		},
		"speculative_computation_applicability": {
			"core_pattern_branchless_nature": true,
			"applicable_areas": "Since the core 6-stage hash pattern is inherently branchless, speculative and branchless selection techniques are not applicable to the main loop itself. Their value lies in optimizing adjacent code paths that do contain conditional logic. The most significant applicable area is in the 'tail processing' or 'finalizer' stage of a hash function, which handles the last few bytes of input data (e.g., 1-15 bytes) that do not form a full processing block. This logic often involves a `switch` statement or a series of `if` statements based on the input length, which can lead to a cascade of difficult-to-predict branches. Converting this logic to a branchless implementation using masks and shifts is a common and effective optimization. Other potential areas include conditional mixing logic that changes based on input characteristics, data-dependent loop termination conditions, and handling collisions in a hash table lookup.",
			"cost_benefit_analysis": "The decision to use a branchless approach is a trade-off between the fixed overhead of executing extra instructions versus the potentially severe penalty of a branch misprediction. A mispredicted branch forces the CPU to flush its pipeline and restart execution, costing 10 to over 30 clock cycles on modern CPUs (e.g., ~15-20 cycles on Intel Skylake, ~18 cycles on AMD Zen 2/3). A branchless implementation computes the results for both potential paths and then selects the correct one. This is advantageous if the cost of the misprediction outweighs the overhead of the branchless code. The breakeven point can be modeled as: `Overhead_Branchless < Penalty_Mispredict * Rate_Mispredict`. For example, if a misprediction costs 20 cycles and the branchless alternative adds 5 cycles of work, the branchless code is faster if the branch mispredicts more than 25% of the time. For highly unpredictable branches, such as those dependent on random input data where the misprediction rate approaches 50%, branchless techniques are almost always superior.",
			"selection_instructions": "Modern CPUs provide specific instructions for performing branchless selection. The primary scalar instruction is `CMOVcc` (Conditional Move), which reads the CPU's flag register (set by a preceding `CMP` or `TEST` instruction) and moves a source operand to a destination only if the condition is met. It is not a branch but an ALU select operation that avoids pipeline flushes. On modern CPUs, it is a fast, 1-cycle latency instruction. The SIMD equivalent involves vector blend instructions, such as `vblendps` (AVX) or `vpblendd` (AVX2). The typical pattern is to use a vector compare instruction (e.g., `vpcmpgtd`) to generate a mask register where each element is all ones (true) or all zeros (false). This mask is then used by the blend instruction to select elements from one of two source vectors, lane by lane, enabling efficient parallel conditional logic."
		},
		"gpu_implementation_strategy": {
			"kernel_mapping_strategy": "The most efficient strategy for the 4096 independent hash evaluations is to assign one GPU thread per hash. This creates a total grid of 4096 threads. This grid can be structured in various ways, with a common and effective approach being to use thread blocks that are multiples of the warp size (32 threads). Recommended configurations include launching 128 blocks of 32 threads each (1 warp per block), 64 blocks of 64 threads each (2 warps per block), or 32 blocks of 128 threads each (4 warps per block). A good starting point for performance tuning is to use 128 or 256 threads per block (4 or 8 warps), as this provides a sufficient number of warps per block for the Streaming Multiprocessor (SM) scheduler to effectively hide latency.",
			"memory_optimization_techniques": "Two primary memory optimization techniques are crucial. First, the `const1` and `shift_amount` values for the hash function should be stored in `__constant__` memory. This is a 64 KB, device-wide, read-only memory space that is cached on the SM. When all 32 threads in a warp access the same address in constant memory—which is expected as they will be at the same stage of the hash—the value is broadcast to all threads in a single cycle, making the access as fast as a register read. Second, if the initial states for the 4096 hashes are read from global memory, these reads must be coalesced. By mapping contiguous thread indices to contiguous array indices, the GPU's memory controller will automatically merge the individual 32-bit reads from a warp into a single, efficient 128-byte memory transaction, maximizing memory bandwidth and minimizing latency.",
			"latency_hiding_mechanism": "The primary mechanism for hiding instruction and memory latency on the GPU is high occupancy. Occupancy is the ratio of active warps to the maximum number of warps an SM can support. The hash function's 18-operation sequential dependency chain limits instruction-level parallelism (ILP) within a single thread. Therefore, the GPU relies on thread-level parallelism. By having many active warps resident on an SM (e.g., an occupancy of ~80% is achievable with 40 registers per thread on an NVIDIA A100), the SM's warp scheduler can switch to another ready-to-execute warp whenever one warp stalls waiting for a memory access or a long-latency instruction (like a `__shfl_sync` which can take 20-24 cycles) to complete. This keeps the execution units constantly busy and effectively hides the latency of the individual operations.",
			"comparative_throughput_analysis": "A GPU implementation offers orders of magnitude higher raw throughput for this workload compared to an optimized multi-core CPU. The GPU's strength lies in its massive parallelism. For example, an NVIDIA Ampere A100 GPU has 108 Streaming Multiprocessors (SMs), each with 64 dedicated 32-bit integer (INT32) cores. This allows the chip to execute `108 * 64 = 6912` integer operations per clock cycle. At a boost clock of 1.41 GHz, this translates to approximately 9.7 Trillion Integer Operations per Second (TIOPS). In contrast, a high-end CPU with 12 ALU and 6 VALU slots has a theoretical peak of 18 scalar operations per cycle. Even with AVX-512 vectorization processing 16 hashes at once, a single CPU core's throughput is vastly lower than the entire GPU's. The 'embarrassingly parallel' nature of the 4096 independent hashes is perfectly suited to the GPU's architecture, which can leverage thousands of threads and tens of thousands of integer cores concurrently."
		},
		"practitioner_codebases_and_libraries": {
			"library_name": "xxHash (specifically XXH3) and Google's HighwayHash",
			"key_techniques": "These libraries demonstrate several key techniques that are directly transferable. First, they rely on **manual vectorization using intrinsics** for their core loops, as compilers often fail to effectively auto-vectorize the complex logic. Second, they employ a **runtime dispatch mechanism** to detect the host CPU's capabilities and select the most optimal code path at runtime (e.g., choosing between SSE2, AVX2, and AVX-512). Third, to hide instruction latency and maximize instruction-level parallelism, they use **significant loop unrolling and interleaving**, processing multiple independent hash states (accumulators) concurrently. Fourth, they feature **highly optimized memory access patterns** and careful use of shuffle/permute instructions to ensure good bit distribution and high throughput. On ARM NEON, xxHash even uses a hybrid scalar/vector approach to balance pipeline usage on certain cores.",
			"supported_isas": "These libraries offer exceptionally broad support for SIMD instruction sets to ensure performance portability. This includes x86/x64 ISAs such as SSE2, SSE4.1, AVX2, and AVX-512. For ARM architectures, they support both NEON (for ARMv7 and ARMv8) and the newer Scalable Vector Extension (SVE). They also include support for POWER architecture's VSX instructions. This wide coverage is a hallmark of production-grade, high-performance libraries.",
			"reported_performance": "The performance of these libraries is state-of-the-art and often CPU-bound, exceeding main memory bandwidth when data is in cache. For large data, XXH3 can achieve approximately **59 GB/s using AVX2**, with AVX-512 providing a further **+50% performance boost**. On an Apple M1 Pro, its NEON implementation reaches **30-36 GB/s**. HighwayHash reports throughput of over **10 GB/s on a single Intel core**, achieving **0.24 cycles/byte** with AVX2 on 1KB inputs. These metrics highlight the massive performance gains achievable with hand-tuned SIMD.",
			"licensing": "The licensing for these libraries is permissive, making them suitable for reuse in both open-source and commercial projects. The core xxHash library is available under the **BSD 2-Clause License**. Google's HighwayHash is available under the **Apache License 2.0**."
		},
		"avx512_performance_analysis": {
			"frequency_throttling_by_generation": "The behavior of AVX-512 frequency downclocking has evolved dramatically, making it progressively more advantageous on newer CPUs. \n- **Skylake-SP/Cascade Lake-SP (1st/2nd Gen Xeon):** Severe throttling. These CPUs use a three-tiered 'power license' system (L0, L1, L2). Even 'light' AVX-512 integer instructions trigger a frequency drop (L1), and transitions between levels incur costly stalls of 10-20 microseconds. Performance can be unpredictable and significantly degraded.\n- **Cooper Lake/Ice Lake Server (3rd Gen Xeon):** Low impact. The frequency drop for AVX-512 is significantly reduced, and the costly transition stalls are virtually eliminated, making AVX-512 much more stable and performant.\n- **Ice Lake Client/Rocket Lake (10th/11th Gen Core):** Negligible to zero impact. These architectures have a much flatter frequency profile. Ice Lake Client sees only a minor 100 MHz drop on a single core and zero license-based downclocking on multiple cores. Rocket Lake has no license-based downclocking at all; AVX-512 runs at the same frequency as other code for a given core count.\n- **AMD Zen 4/Zen 5:** These CPUs do not use Intel's licensing scheme. Zen 4 implements AVX-512 by 'double-pumping' its 256-bit units. Zen 5 introduces native 512-bit units and is reported to have no fixed clock speed offsets for AVX-512, using adaptive clocking to run near peak frequency, making it a pure performance win.",
			"register_file_advantage": "AVX-512 provides a crucial advantage over AVX2 through its significantly larger register file, which is essential for the high-depth interleaving required to optimize this hash workload. AVX-512 offers **32 512-bit ZMM registers**, whereas AVX2 provides only **16 256-bit YMM registers**. For the 32-bit hash function, this means AVX-512 can hold the state for `32 registers * 16 hashes/register = 512` independent hashes simultaneously. In contrast, AVX2 can only hold `16 registers * 8 hashes/register = 128` states. To effectively hide the 18-operation dependency chain latency, many independent computations must be kept in flight. With AVX-512, a deep interleaving factor can be used without exhausting the register file. With AVX2, the limited number of registers makes it highly probable that the compiler will be forced to 'spill' registers to the stack or L1 cache to manage the states, incurring massive performance penalties from the extra load/store operations. Therefore, the larger register file of AVX-512 is a key enabler for achieving maximum throughput by avoiding these spills.",
			"energy_performance_tradeoff": "The trade-off between energy consumption (Joules) and performance (time) for AVX-512 is highly dependent on the CPU generation. Energy is the product of power and time (`E = P * t`). AVX-512 increases power draw but aims to drastically reduce execution time.\n- **On older architectures (Skylake-SP/Cascade Lake-SP):** The trade-off is often poor. The severe frequency throttling can erode the 2x work-per-cycle benefit, leading to only minor performance gains (or even losses) while operating at a high power level for a longer duration. This results in poor energy efficiency (higher Joules/hash) compared to a stable AVX2 implementation.\n- **On modern architectures (Ice Lake, Rocket Lake, Zen 5):** The trade-off is excellent. With minimal or no frequency penalty, AVX-512 nearly doubles the throughput. The task finishes much faster, and while the instantaneous power is higher, the significantly reduced execution time leads to a net win in both performance (hashes/second) and energy efficiency (lower Joules/hash).",
			"decision_guidelines": "The choice between AVX-512 and AVX2 for this 'light' integer hash workload depends directly on the target microarchitecture:\n- **Skylake-X / Cascade Lake-SP (1st/2nd Gen Xeon):** **USE WITH CAUTION.** The risk of severe frequency throttling and transition stalls is high. Only consider if the workload can be isolated to dedicated cores and benchmark extensively against a stable AVX2 version. AVX2 is often the safer, more predictable choice.\n- **Cooper Lake / Ice Lake Server (3rd Gen Xeon):** **RECOMMENDED.** The architectural improvements in these generations largely fix the throttling issues. The 2x vector width and 2x register count provide a significant performance and efficiency advantage.\n- **Ice Lake Client / Rocket Lake / Alder Lake (10th Gen+ Core):** **STRONGLY RECOMMENDED.** These platforms have minimal to zero frequency penalties for AVX-512. It is a nearly 'free' performance upgrade, and the larger register file is critical for this workload. AVX-512 is the clear winner.\n- **AMD Zen 4:** **CONSIDER AND BENCHMARK.** AVX-512 is implemented via double-pumped 256-bit operations, so it won't be twice as fast as AVX2. However, the larger register file can still provide a net benefit by avoiding register spills. Performance must be validated against a tuned AVX2 implementation.\n- **AMD Zen 5:** **RECOMMENDED.** With native 512-bit units and favorable frequency behavior, AVX-512 is expected to be the optimal choice. The larger register file remains a key advantage."
		},
		"multi_core_scheduling_plan": {
			"numa_and_core_partitioning": "The workload of 4096 hashes should be partitioned at the NUMA (Non-Uniform Memory Access) node level to avoid costly cross-socket memory access. For a dual-socket server, the hashes would be split, with each NUMA node handling 2048 hashes. A process should be launched for each node and pinned to it using tools like `numactl --cpunodebind=0 --membind=0`. Within each NUMA node, the workload should be distributed evenly among all available physical cores. For Simultaneous Multi-Threading (SMT/Hyper-Threading), the recommended starting point is to disable it and pin one software thread to each physical core. This avoids contention for shared execution units (ALUs, VALUs) and cache resources. SMT should only be tested as a secondary optimization if profiling reveals that execution ports are significantly underutilized.",
			"memory_placement_strategy": "To ensure data is allocated in memory local to the processing core, a 'first-touch' policy is essential. The thread that will ultimately process a given set of hashes must be the one to initialize their state. This action forces the operating system to allocate the corresponding memory pages on the NUMA node where the thread is running, preventing remote memory access latency. The entire state for all 4096 hashes is 16 KB (4096 * 4 bytes), which is small enough to fit entirely within a single modern CPU core's L1 data cache (typically 32 KB or 48 KB). This minimizes memory latency once the data is loaded.",
			"tiling_strategy": "To maximize L1 data cache locality and keep the working set hot, a tiling strategy is recommended. Instead of processing all 4096 hashes through one stage before moving to the next, a small 'tile' of hashes should be processed through all 16 rounds of the computation. An ideal tile size corresponds to the data processed by a few SIMD vectors, such as 32 or 64 hashes (which would occupy 2 or 4 AVX-512 ZMM registers, respectively). A core's thread would load the state for one tile, process it completely through all rounds, write the final results, and only then move to the next tile. This ensures the active working set remains extremely small and resident in the L1 cache and registers, minimizing cache misses.",
			"scaling_and_saturation_analysis": "Performance is expected to scale almost linearly with the number of physical cores used, due to the embarrassingly parallel nature of the task combined with the NUMA-aware partitioning strategy. The primary saturation point, or bottleneck, will be the execution throughput of each core's vector units. With an effective interleaving strategy to hide latency, performance will be limited by the number of vector micro-operations the core can issue per cycle. For example, on an AMD Zen 4 CPU, the bottleneck is the ability to execute two 512-bit operations per cycle. On more advanced architectures like Intel Sapphire Rapids or AMD Zen 5, which can sustain 3-4 vector operations per cycle, the bottleneck will be the available execution ports for vector ALU and shift operations. The frontend (instruction decode) could also become a bottleneck if the inner loop is not structured efficiently."
		},
		"portable_simd_frameworks_evaluation": {
			"framework_name": "Google Highway",
			"api_expressiveness": "Highway's API is highly expressive for the required operations. It provides platform-agnostic functions like `Add(a, b)`, `Xor(a, b)`, `ShiftLeftSame(v, bits)`, and `ShiftRightSame(v, bits)`. Using explicit functions over operator overloading is recommended for maximum portability, especially on SVE. Broadcasting constants is straightforward using `Set(D d, T val)`, where `d` is a vector type descriptor. The API is designed to guide developers toward performance-portable code by providing clear functions and documenting operations that may be expensive on certain architectures.",
			"architecture_support": "Highway offers comprehensive support for all target architectures, including AVX2, AVX-512, and NEON. Its standout feature is its robust support for the **dynamic vector lengths of ARM SVE**. This is a critical advantage over competitors like xsimd and EVE, which only support SVE with fixed-size vectors (e.g., 128 or 256 bits) and thus cannot fully leverage the hardware's length-agnostic capabilities. The Vc library lacks support for AVX-512, NEON, and SVE altogether, making it unsuitable.",
			"generated_code_quality": "Highway is designed to generate code with performance comparable to hand-written intrinsics. It achieves this by using platform-specific intrinsics in its backend and providing a robust dynamic dispatch system. A function can be exported for multiple targets using `HWY_EXPORT`, and `HWY_DYNAMIC_DISPATCH` creates a dispatcher that automatically calls the best available version for the host CPU at runtime. The library also provides an `instruction_matrix.pdf` document that details the instruction cost of operations on various targets, allowing developers to reason about performance and make informed implementation choices.",
			"overall_recommendation": "Google Highway is the most suitable and strongly recommended framework for this optimization task. Its combination of a performance-oriented API, comprehensive support for all target ISAs, and its unique and crucial support for dynamic-length SVE make it the best choice for writing high-performance, portable, and future-proof SIMD code. While xsimd and EVE are also modern and capable libraries, their lack of dynamic SVE support is a significant drawback for targeting modern ARM server architectures. Highway provides the necessary tools to achieve near hand-tuned performance without the maintenance burden of writing raw intrinsics for multiple platforms."
		},
		"verification_and_benchmarking_methodology": {
			"verification_strategy": "The core of the verification strategy is differential testing against a golden scalar reference implementation. This reference will be written in standard C (e.g., C17) using the exact-width integer type `uint32_t` to guarantee 32-bit width and well-defined unsigned arithmetic (wraparound modulo 2^32). A test harness will generate millions of test vectors, each with a unique input seed. Crucially, this test suite must exhaustively cover all edge-case shift amounts from 0 to 31. The output from the optimized SIMD and GPU implementations will then be compared bit-for-bit against the output from the scalar reference for every test vector. Any discrepancy will immediately flag a verification failure, ensuring all implementations are semantically identical.",
			"portability_and_semantics": "Achieving bit-identical results across platforms requires careful handling of differing arithmetic and shift semantics. While using `uint32_t` ensures consistent wraparound arithmetic, shift operations are a major portability hazard. The C++ standard defines shifts by an amount greater than or equal to the bit-width (32) as undefined behavior. However, hardware ISAs have specific, differing behaviors: Intel SIMD (AVX2/AVX-512) and ARM NEON zero out the result for such shifts, while the OpenCL C specification mandates that the shift amount is effectively masked by the log2 of the bit-width. A portable, bit-exact implementation must account for these differences, either by ensuring shift amounts are always within the valid range [0, 31] or by emulating the most restrictive behavior (e.g., zeroing) across all platforms.",
			"profiling_tools": "A suite of platform-specific profiling tools is recommended for in-depth analysis. For CPU performance on Intel, AMD, and ARM, the primary tools are Intel VTune Profiler, AMD uProf, and ARM Streamline, respectively, complemented by the Linux `perf` tool for raw hardware counter collection. For GPU profiling, the recommended tools are NVIDIA Nsight Systems (for system-level traces) and Nsight Compute (for detailed kernel analysis), and AMD's `rocprof` command-line profiler for ROCm-based applications. These tools are essential for measuring port pressure, micro-op throughput, stalls, and other microarchitectural events.",
			"benchmarking_best_practices": "To achieve reproducible and reliable performance results, a strict set of best practices must be followed. Benchmarks should be run on an otherwise idle system to minimize interference. Before measurements begin, the workload should be run several times as a warm-up to populate instruction/data caches and allow CPU frequency to stabilize at its boost clock. The entire 4096-hash workload should be processed in a single batch to amortize function call or kernel launch overhead. For CPU benchmarks, thread affinity must be used to pin the process to a specific physical core, preventing noise from thread migration. Finally, many measurement runs should be performed to collect statistical data (mean, median, standard deviation) to account for system noise.",
			"key_metrics_to_report": "To allow for clear and standardized comparisons across different implementations and platforms, two key performance metrics should be reported. The first is 'Cycles/Hash', calculated as the total number of CPU cycles consumed divided by the number of hashes computed (4096). This can be measured directly using hardware performance counters like `CPU_CLK_UNHALTED.THREAD`. The second metric is 'Hashes/Sec', calculated as the total number of hashes computed divided by the total elapsed wall-clock time in seconds. The elapsed time must be measured using high-resolution timers appropriate for the platform, such as `clock_gettime` on Linux, `QueryPerformanceCounter` on Windows, or the native event APIs for CUDA and HIP."
		},
		"synthesized_kernel_blueprint": {
			"simd_and_interleaving_plan": "The core kernel will be implemented using the x86 AVX-512 instruction set to process 16 independent 32-bit hash states per instruction. To hide the 18-operation dependency chain latency and saturate the CPU's execution units, an interleaving factor of 4 will be used. This means the kernel will operate on 4 independent ZMM (512-bit) vectors concurrently, processing a total of 64 hash states in a pipelined fashion within the main loop.",
			"data_layout_specification": "The 4096 hash states must be organized in an Array of Structures of Arrays (AoSoA) layout. The data will be structured as an array of 256 blocks, where each block is a structure containing an array of 16 32-bit unsigned integers (`struct { uint32_t a[16]; }`). Each of these 256 blocks must be aligned to a 64-byte boundary to enable efficient, contiguous memory access using `_mm512_load_si512` and `_mm512_store_si512` instructions. If the initial data is not in this format, a one-time transposition pre-processing step is required.",
			"register_allocation_plan": "To avoid register spills, the 32 available AVX-512 ZMM registers will be allocated as follows for the 4-way interleaved kernel: \n- `zmm0` - `zmm3`: 4 registers to hold the current state (`a`) for the four independent, interleaved SIMD streams. \n- `zmm4`: 1 register to hold the `const1` value, pre-broadcasted across all 16 lanes. \n- `zmm5` - `zmm12`: 8 registers to serve as temporaries for intermediate calculations. Within each of the 6 stages, each of the 4 streams requires two temporaries (for `(a op1 const1)` and `(a shift_op amount)`), totaling 8 temporary registers. \nThis plan utilizes approximately 13 of the 32 available ZMM registers, leaving ample room for compiler-managed values and ensuring the entire working set remains in registers.",
			"performance_model": "Based on a realistic microarchitectural model for a modern Intel CPU (e.g., Ice Lake) with approximately 3 vector execution ports, the estimated performance is **0.375 cycles per hash**. This is derived as follows: The workload of 4096 hashes requires processing 256 AVX-512 vectors. Each vector requires 18 instructions (3 ops/stage * 6 stages). This results in a total of `256 * 18 = 4608` vector micro-operations. With a sustained throughput of 3 vector operations per cycle, the total execution time is `4608 / 3 = 1536` cycles. The performance per hash is therefore `1536 cycles / 4096 hashes`.",
			"implementation_pseudocode": "```c\n#include <immintrin.h>\n\n// Assumes data is in AoSoA format: struct { uint32_t a[16]; } blocks[256];\n// Assumes op1=ADD, op2=XOR, shift_op=left shift\n\nvoid hash_kernel_avx512_interleaved(AoSoA_Block* data, uint32_t const1, int shift_amount) {\n    const size_t num_total_blocks = 256;\n    const int interleaving_factor = 4;\n\n    // Pre-broadcast constant to a vector register\n    __m512i zmm_c1 = _mm512_set1_epi32(const1);\n\n    // Main loop processes 4 blocks (64 states) per iteration\n    for (size_t i = 0; i < num_total_blocks; i += interleaving_factor) {\n        // 1. Load states for 4 independent streams\n        __m512i zmm_a0 = _mm512_load_si512(&data[i + 0].a);\n        __m512i zmm_a1 = _mm512_load_si512(&data[i + 1].a);\n        __m512i zmm_a2 = _mm512_load_si512(&data[i + 2].a);\n        __m512i zmm_a3 = _mm512_load_si512(&data[i + 3].a);\n\n        // 2. Unroll all 6 hash stages with interleaved instructions\n        #pragma unroll\n        for (int stage = 0; stage < 6; ++stage) {\n            // Compute independent intermediate results for all 4 streams first\n            __m512i tmp_add0 = _mm512_add_epi32(zmm_a0, zmm_c1);\n            __m512i tmp_shl0 = _mm512_slli_epi32(zmm_a0, shift_amount);\n\n            __m512i tmp_add1 = _mm512_add_epi32(zmm_a1, zmm_c1);\n            __m512i tmp_shl1 = _mm512_slli_epi32(zmm_a1, shift_amount);\n\n            __m512i tmp_add2 = _mm512_add_epi32(zmm_a2, zmm_c1);\n            __m512i tmp_shl2 = _mm512_slli_epi32(zmm_a2, shift_amount);\n\n            __m512i tmp_add3 = _mm512_add_epi32(zmm_a3, zmm_c1);\n            __m512i tmp_shl3 = _mm512_slli_epi32(zmm_a3, shift_amount);\n\n            // Final XOR to update state registers, breaking the dependency chain\n            zmm_a0 = _mm512_xor_si512(tmp_add0, tmp_shl0);\n            zmm_a1 = _mm512_xor_si512(tmp_add1, tmp_shl1);\n            zmm_a2 = _mm512_xor_si512(tmp_add2, tmp_shl2);\n            zmm_a3 = _mm512_xor_si512(tmp_add3, tmp_shl3);\n        }\n\n        // 3. Store results back to memory\n        _mm512_store_si512(&data[i + 0].a, zmm_a0);\n        _mm512_store_si512(&data[i + 1].a, zmm_a1);\n        _mm512_store_si512(&data[i + 2].a, zmm_a2);\n        _mm512_store_si512(&data[i + 3].a, zmm_a3);\n    }\n}\n```"
		}
	},
	"outputBasis": [
		{
			"field": "simd_vectorization_strategy",
			"citations": [
				{
					"title": "Intel® Intrinsics Guide",
					"url": "https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html",
					"excerpts": [
						"__m256i _mm256_add_epi32 ( __m256i a , __m256i b )",
						"Instruction: vpaddd ymm, ymm, ymm",
						"Section Title: Description\nContent:\nAdd packed 32-bit integers in a and b , and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set).\n ... \nSection Title: Description\nContent:\nAdd packed 32-bit integers in a and b , and store the results in dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set)."
					]
				},
				{
					"title": "_mm256_xor_si256",
					"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-D5D92979-BBC2-4252-947F-1CC054712CBA.htm",
					"excerpts": [
						"Performs bitwise logical XOR operation on signed integer vectors. The corresponding Intel® AVX2 instruction is VPXOR .\n",
						"| extern __m256i _mm256_xor_si256(__m256i s1, __m256i s2); |",
						"Performs a bitwise logical XOR operation of the signed integer elements of source vector s2 and the corresponding elements in source vector s1 and stores the result in the destination vector. If the corresponding bits of the first and second vectors differ, each bit of the result is set to 1, otherwise it is set to 0."
					]
				},
				{
					"title": "intrinsics - Difference between _mm256_xor_si256() and _mm256_xor_ps() - Stack Overflow",
					"url": "https://stackoverflow.com/questions/27804476/difference-between-mm256-xor-si256-and-mm256-xor-ps",
					"excerpts": [
						"There is no difference in the effects, both do a bitwise XOR of 256 bits. But that doesn't mean there aren't differences, the differences only are less visible.",
						"`vxorps` can, on Haswell, only go to port port 5 (and therefore has a throughput of 1), but `vpxor` can go to ports 0, 1 and 5, and has a throughput of 3/cycle.",
						"Also, there is a bypass delay when a result generated in floating point domain is used by an instruction that executes in the integer domain, and vice versa. So using the \"wrong\" instruction can have a slightly higher latency, which is why `vxorps` may be better in some contexts (but it's not so simple as \"always when using floats\")."
					]
				},
				{
					"title": "Improving xxHash",
					"url": "https://encode.su/threads/2556-Improving-xxHash",
					"excerpts": [
						"And for fast AVX2/AVX512 implementations we should process 2-4 times more data, i.e. split data stream into as much as 16 lanes, each 128-bit ...Read more"
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` ).",
						"| **XXH3** (SSE2) | 64 | 31.5 GB/s | 133.1 | 10 |  |"
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"`x86` / `x64` systems can enjoy a new unit, `xxh_x86dispatch` , which can detect at runtime the best vector instruction set present on host system (none, `sse2` , `avx2` or `avx512` ), thanks to a cpu feature detector designed by [@easyaspi314](https://github.com/easyaspi314) . It then automatically runs the appropriate vector code.",
						"It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` .",
						"`xxhsum` automatically detects and employs the best available vector extension (SSE, AVX, etc.) on `x86` / `x64` cpus. Previously, this required an explicit build macro; it is now enabled by default. Maintainers can still control it manually, and can disable it with `DISPATCH=0` .",
						"On `wasm` , speed fo `xxh3` is improved by a large factor x2 to x3 (depending on underlying hardware) through the use of `simd128` ( [@easyaspi314](https://github.com/easyaspi314) ). This is especially efficient under the `v8` js engine, notably used by `chrome` and `node.js` .",
						"This makes it safer to deploy a single binary with advanced vector instruction sets, such as `AVX2` , since there is no hard requirement for *all* target systems to actually support it : the binary can automatically switch to `SSE2` instead."
					]
				},
				{
					"title": "xxHash: xxhash.h Source File",
					"url": "https://xxhash.com/doc/v0.8.2/xxhash_8h_source.html",
					"excerpts": [
						"_mm256_set1_epi32",
						"4722 const __m256i prime32 = _mm256_set1_epi32(( int ) [XXH_PRIME32_1](group___x_x_h32__impl.html) );",
						"4714 XXH_FORCE_INLINE [XXH_TARGET_AVX2](group__dispatch.html) void",
						"4715 XXH3_scrambleAcc_avx2( void * XXH_RESTRICT acc, const void * XXH_RESTRICT secret)",
						"3630 # define XXH_ACC_ALIGN 64 /* for compatibility with avx512 */",
						"XXH3_scrambleAcc_avx512( void * XXH_RESTRICT acc, const void * XXH_RESTRICT secret)",
						"const __m512i prime32 = _mm512_set1_epi32(( int ) [XXH_PRIME32_1](group___x_x_h32__impl.html) );",
						"4635 __m512i const acc_vec = *xacc;",
						"__m512i const key_vec = _mm512_loadu_si512 (secret);",
						"__m512i const acc_vec = *xacc;",
						"__m512i const acc_vec = *xacc;",
						"4634 /* xacc[0] ^= (xacc[0] >> 47) */",
						"4634 /* xacc[0] ^= (xacc[0] >> 47) */",
						"_mm512_srli_epi64 (acc_vec, 47);",
						"4636 __m512i const shifted = _mm512_srli_epi64 (acc_vec, 47);",
						"4638 __m512i const key_vec = _mm512_loadu_si512 (secret);",
						"4638 __m512i const key_vec = _mm512_loadu_si512 (secret);",
						"4626 XXH_FORCE_INLINE [XXH_TARGET_AVX512](group__dispatch.html) void",
						"4626 XXH_FORCE_INLINE [XXH_TARGET_AVX512](group__dispatch.html) void",
						"4627 XXH3_scrambleAcc_avx512( void * XXH_RESTRICT acc, const void * XXH_RESTRICT secret)",
						"4627 XXH3_scrambleAcc_avx512( void * XXH_RESTRICT acc, const void * XXH_RESTRICT secret)",
						"4631 { __m512i* const xacc = (__m512i*) acc;",
						"XXH3_scrambleAcc_avx2",
						"This isn't enough for the 32 byte aligned loads in AVX2. 5756 * or on 32-bit, the 16 byte aligned loads in SSE2 and NEON. ... Controls the NEON to scalar ratio ...Read more"
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": [
						" - SIMD acceleration",
						"  - SIMD acceleration",
						"XXH3 has a fast scalar implementation, but it also includes accelerated SIMD",
						" implementations for many common platforms:",
						"   - AVX512",
						"   - AVX2",
						"   - SSE2",
						"   - ARM NEON",
						"   - WebAssembly SIMD128",
						"   - POWER8 VSX",
						"   - s390x ZVector",
						"enum XXH_VECTOR_TYPE /* fake enum */ {\n    XXH_SCALAR = 0,  /*!< Portable scalar version */\n    XXH_SSE2   = 1,  /*!<\n                      * SSE2 for Pentium 4, Opteron, all x86_64. *\n                      * @note SSE2 is also guaranteed on Windows 10, macOS, and\n                      * Android x86. */\n    XXH_AVX2   = 2,  /*!< AVX2 for Haswell and Bulldozer */\n    XXH_AVX512 = 3,  /*!< AVX512 for Skylake and Icelake */\n    XXH_NEON   = 4,  /*!<\n                       * NEON for most ARMv7-A, all AArch64, and WASM SIMD128\n                       * via the SIMDeverywhere polyfill provided with the\n                       * Emscripten SDK. */\n    XXH_VSX    = 5,  /*!< VSX and ZVector for POWER8/z13 (64-bit) */\n    XXH_SVE    = 6,  /*!< SVE for some ARMv8-A and ARMv9-A */\n};",
						"t: Auto detected.\n ... \n*/\n#ifndef XXH_ACC_ALIGN\n#  if defined(XXH_X86DISPATCH)\n#     define XXH_ACC_ALIGN 64  /* for compatibility with avx512 */\n#  elif XXH_VECTOR == XXH_SCALAR  /* scalar */\n#     define XXH_ACC_ALIGN 8\n#  elif XXH_VECTOR == XXH_SSE2  /* sse2 */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX2  /* avx2 */\n#     define XXH_ACC_ALIGN 32\n#  elif XXH_VECTOR == XXH_NEON  /* neon */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_VSX   /* vsx */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX512  /* avx512 */\n#     define XXH_ACC_ALIGN 64\n#  elif XXH_VECTOR == XXH_SVE   /* sve */\n#     define XXH_ACC_ALIGN 64\n#  endif\n#endif\n\n#if",
						"\n#if defined(XXH_X86DISPATCH) || XXH_VECTOR == XXH_SSE2 \\\n    || XXH_VECTOR == XXH_AVX2 || XXH_VECTOR == XXH_AVX512\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#elif XXH_VECTOR == XXH_SVE\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#else\n#  define XXH_SEC_ALIGN 8\n#endif",
						" *\n * @see XXH3_accumulate_512_neon()\n */\n# ifndef XXH3_NEON_LANES\n#  if (defined(__aarch64__) || defined(__arm64__) || defined(_M_ARM64) || defined(_M_ARM64EC)) \\\n   && !defined(__APPLE__) && XXH_SIZE_OPT <= 0\n#   define XXH3_NEON_LANES 6\n#  else\n#   define XXH3_NEON_LANES XXH_ACC_NB\n#  endif\n# endif\n#endif  /* XXH_VECTOR == XXH_NEON */"
					]
				},
				{
					"title": "Computing multiple hash values in parallel with AVX2",
					"url": "https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/",
					"excerpts": [
						"After a couple of false starts, I ended up with a version of [xxHash32](https://github.com/Cyan4973/xxHash) that computes hash values for 8 keys at the same time using AVX2. The code\nis at [parallel-xxhash](https://github.com/jsnell/parallel-xxhash) .",
						"Section Title: [Computing multiple hash values in parallel with AVX2](https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/) > Benchmarks",
						"Before heading off into the weeds with the details, below are a\ncouple of pretty graphs showing performance with different key\nsizes for a few different implementations: CityHash64 since it's\nbeen my default hash function for years, xxHash64 since my\nparallel implementation was based on xxHash32, and MetroHash64\nsince I saw people suggesting it was the fastest option for small\nkeys. I did not include FarmHash since it was consistently\nslower than CityHash for all key sizes.",
						"All implementations computed hashes for the same number of keys;\nthe parallel implementations did it 8 keys at a time, the others\ndid them sequentially.",
						"All implementations computed hashes for the same number of keys;\nthe parallel implementations did it 8 keys at a time, the others\ndid them sequentially. The tests were run on a i7-6700 and GCC\n6.3.0, with `-O3 -march=native -fno-strict-aliasing` . The benchmark code is in the\nrepository, but you'll need to bring your own copies of the\nexternal hash table libraries.",
						"CityHash64 since it's\nbeen my default hash function for years, xxHash64 since my\nparallel implementation was based on xxHash32, and MetroHash64\nsince I saw people suggesting it was the fastest option for small\nkeys. I did not include FarmHash since it was consistently\nslower than CityHash for all key sizes.",
						"This is most likely because x86 has a\ndedicated rotate instruction, while most other methods require two\ninstructions, e.g. shift+xor. For AVX that's not the case, and you\nneed to synthesize the rotate from two shifts and a xor/or.",
						"These days new hash functions are mostly built with 64*64->64\nmultiplies. We won't have that in SIMD until AVX-512 (and given the way\nthings are going, I wonder if a general purpose CPU using AVX-512\nwill actually ever launch). Synthesizing a 64-bit multiply from\n32-bit multiplies doesn't seem viable for this use case. So for\nthis use case, we really want to look at the hash functions\ndefined a few years ago rather than the latest hotness.",
						"After having written the version using intrinsics, it occurred to\nme that I really should have started off with just writing out\nplain C++ with the same semantics, and see if it auto-vectorizes.",
						"And while the transformation from a scalar version to using\nintrinsics is not too bad, the transformation to standard C++ expressing\nthe same order of operations on the same memory layout is\neasier yet.",
						"GCC 6.3 seemed to vectorize everything\nperfectly. GCC 4.9 [ [1]() ] missed something (I didn't track down exactly\nwhat) that cost about 25% performance. And Clang 3.8 did nothing\nat all, with the plain-C++ version being 150% slower than the\nversion using intrinsics."
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"| XOR | 32-bit | _mm512_xor_epi32(x, y) |",
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family.",
						"In this article, we propose an LSH-512 implementation utilizing the AVX-512 SIMD Intel instruction sets.",
						"| 64-bit | _mm512_xor_epi64(x, y) |  |",
						"| 512-bit | _mm512_xor_si512(x, y) |  |",
						"| Left Shift | 32-bit | _mm512_slli_epi32(x, y) |",
						"| Rotation | 32-bit | _mm512_rol_epi32(x, imm) |"
					]
				},
				{
					"title": "Intrinsics for Intel® Advanced Vector Extensions 512 ...",
					"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.htm",
					"excerpts": [
						"Intel® AVX-512 intrinsics are supported on IA-32 and Intel® 64 architectures built from 32nm process technology. They map directly to the new Intel® AVX-512 ...Read more"
					]
				},
				{
					"title": "Shift Intrinsics",
					"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-63F9164B-0E69-4C86-8DE1-EDC72EFE5CDC.htm",
					"excerpts": [
						"The count argument is one shift count that applies to all elements of the operand being shifted. It is not a vector shift count that shifts each element by a different amount.",
						"__m128i _mm_slli_epi32(__m128i a, int count);\nShifts the four signed or unsigned 32-bit integers in a left by count bits while shifting in zeros.",
						"__m128i _mm_sll_epi32(__m128i a, __m128i count);\nShifts the four signed or unsigned 32-bit integers in a left by count bits while shifting in zeros."
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Intel-x86-Docs/64-ia-32-architectures-instruction-set-extensions-reference-manual.pdf",
					"excerpts": [
						"VPXORD zmm1 {k1}{z}, zmm2,\nzmm3/m512/m32bcst",
						"AVX-512 Foundation instructions can be divided in three different groups:\n•\nInstructions which support “zeroing-masking”.\n•\nAlso allow merging-masking",
						"operands. The first source operand is a ZMM/YMM/XMM registers. The second source operand can be an\nZMM/YMM/XMM register, a 512/256/128-bit memory location or a 512/256/128-bit vector broadcasted from a 64-\nbit memory location. The destination is a ZMM/YMM/XMM register, and updated according to the writemask at 64-\nbit granularity.\nOper",
						" The opmask is a set of eight architectural registers of size MAX_KL (64-bit). Note that from this set of 8\narchitectural registers, only k1 through k7 can be addressed as predicate operand.",
						"AVX-512 Foundation instructions provide a collection of opmask instructions that allow programmers to set, copy,\nor operate on the contents of a given opmask register.",
						"An opmask serving as a predicate operand in AVX-512 obeys the following properties:",
						"The instruction’s operation is not performed for an element if the corresponding opmask bit is not set. This\nimplies that no exception or violation can be caused by an operation on a masked-off element.",
						"For a given vector length, each\ninstruction accesses only the number of least significant mask bits that are needed based on its data type.",
						"e vpaddd instruction performs 32-bit integer additions on each data element conditionally based on the corre-\nsponding bit value in the predicate operand k3.",
						"An opmask register serving as a predicate operand is expressed as a curly-braces-enclosed decorator following the\nfirst operand in the Intel assembly syntax.",
						"VPADDW__m128i _mm_mask_add_epi16 (__m128i s, __mmask8 m, __m128i a, __m128i b)",
						"VPADDW__m128i _mm_maskz_add_epi16 (__mmask8 m, __m128i a, __m128i b)",
						"VPADDD __m512i _mm512_add_epi32( __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_mask_add_epi32(__m512i s, __mmask16 k, __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_maskz_add_epi32( __mmask16 k, __m512i a, __m512i b);"
					]
				},
				{
					"title": "Capabilities of Intel® AVX-512 in Intel® Xeon® Scalable Processors (Skylake) | Colfax Research",
					"url": "https://colfaxresearch.com/skl-avx512/",
					"excerpts": [
						"Masking in AVX-512 is supported by a set of eight 64-bit opmask registers. Masking registers conditionally control computational operations and updates of the destination vector with per-element granularity.",
						" AVX-512 masking uses dedicated bitmask registers, which do not occupy ZMM space. Most AVX-512 instructions accept a mask explicitly, so following up with bitwise “and” is not neccessary. Addit",
						"\nThe figure below demonstrates the difference between the implementation of masking in AVX2 and AVX-512 for the kernel “ if (B[i] < 0.0) B[i]*=0.1"
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/319433-024-697869.pdf",
					"excerpts": [
						"VPSLLVD xmm1, xmm2, xmm3/m128",
						"VPSLLVQ xmm1, xmm2, xmm3/m128",
						"VPSRLVD xmm1, xmm2, xmm3/m128",
						"VPSRLVQ xmm1, xmm2, xmm3/m128",
						"VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical",
						"The count operand can be either an XMM register or\na 128-bit memory location or an 8-bit immediate."
					]
				},
				{
					"title": "XXH3 performance issue on ARM & WASM with >128byte inputs · Issue #211 · Cyan4973/xxHash",
					"url": "https://github.com/Cyan4973/xxHash/issues/211",
					"excerpts": [
						"The \"large input\" mode highly depends on performance of vectorial instructions.",
						"This works better for `SSE2` and `AVX2` , as it was designed for these ones.",
						"An alternative could be to force the implementation to use a specific code path, using the `XXH_VECTOR` build macro. For `NEON` , the code is `3` , which gives for `gcc` : `-DXXH_VECTOR=3` .",
						"| XXH_VECTOR | Apple llvm 10.0 | gcc v8.3.0 |",
						"| 0 \"scalar\" | 24 GB/s | 11 GB/s |",
						"| 1 \"sse2\" | 30 GB/s | 27 GB/s |",
						"| 2 \"avx2\" | 55 GB/s | 45 GB/s |",
						"Performance dip : The 128-bit final mixer has a larger fixed cost than the 64-bit one, expectably. Therefore, it takes more time to dilute that cost, meaning the mid-size performance dip is larger. But the new mid-size mode, introduced for `XXH3_64bit()` and re-employed for `XXH3_128bit()` , still uses the same size threshold as 64-bit. The goal was to make it compatible with existing `XXH3_state_t` object used in streaming mode, and to share most of the streaming code. This can be discussed. If this is considered a sub-optimal choice, it could be updated.",
						"Comparison with AES-based algorithm : `SSE2` is not enough to face special-purpose hardware. `XXH3` is designed to leverage `AVX2` , when available, in order to reach similar performance territory. For example, on my laptop, it reaches ~45 GB/s on \"large\" data. Yet, while `SSE2` is a guaranteed capability of any `x64` cpu, later SIMD versions are not. I suspect that's the reason why `AVX2` wasn't benchmarked.",
						"128-bytes is the limit between the \"small input\" and \"large input\" modes. The \"large input\" mode highly depends on performance of vectorial instructions.Read more"
					]
				}
			],
			"reasoning": "The user is asking about the `simd_vectorization_strategy` for a specific hash function. The provided value includes details about recommended and supported ISAs, an intrinsics pattern, lane width, and estimated cycles per hash. The excerpts discuss various SIMD instruction sets like AVX2 and AVX-512, and their corresponding intrinsics. For instance, excerpts mention `_mm512_add_epi32`, `_mm512_slli_epi32`, and `_mm512_xor_si512` which directly correspond to the operations described in the `intrinsics_pattern` field. Several excerpts also mention AVX-512 and AVX2 as supported ISAs, and some discuss performance metrics like cycles per hash or throughput, which relate to `estimated_cycles_per_hash`. The `lane_width_choice` of 16.0 is supported by discussions of 512-bit registers (which can hold 16x 32-bit integers) and 256-bit registers (holding 8x 32-bit integers). The `recommended_isa` of AVX-512 is supported by multiple excerpts detailing its capabilities and performance benefits, especially for packet processing and hash functions. The `supported_isas` are also well-represented in the excerpts, covering AVX2, SSE2, and ARM NEON/SVE.",
			"confidence": "high"
		},
		{
			"field": "data_layout_optimization",
			"citations": [
				{
					"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
					"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
					"excerpts": [
						"When storing collections of multi-field data structures (like particles, vertices, or database records), we face a fundamental choice in memory layout: **Array of Structures (AoS)** groups fields by object, while **Structure of Arrays (SoA)** groups fields by type. This seemingly simple decision can result in **10-100x performance differences** in modern computing systems.\nThe ch",
						" Structure of Arrays (SoA)",
						"SIMD Note:** Data not contiguous, requires gather/scatter operatio",
						"For a warp of 32 threads accessing particle positions:\n• **AoS:** 32 transactions × 32 bytes = 1024 bytes transferred, 96 bytes used (9.4% efficiency)\n• **SoA:** 1 transaction × 128 bytes = 128 bytes transferred, 128 bytes used (100% efficiency)\n• **Result:** SoA is 10.7x more bandwidth efficient on GPU"
					]
				},
				{
					"title": "Intel® AVX-512 Instructions",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/intel-avx-512-instructions.html",
					"excerpts": [
						"Intel AVX-512 features include 32 vector registers each 512 bits wide, eight dedicated mask registers, 512-bit operations on packed floating point data or packed integer data, embedded rounding controls (override global settings), embedded broadcast, embedded floating-point fault suppression, embedded memory fault suppression, new operations, additional gather/scatter support, high speed math instructions, compact representation of large displacement value, and the ability to have optional capabilities beyond the foundational capabilities.",
						"Intel AVX-512 instructions use the EVEX prefix which is one byte longer. The EVEX prefix enables the additional functionality of Intel AVX-512. In general, if the extra capabilities of the EVEX prefix are not needed then the AVX2 instructions can be used, coded using the VEX prefix saving a byte in certain cases."
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"| XOR | 32-bit | _mm512_xor_epi32(x, y) |",
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family.",
						"In this article, we propose an LSH-512 implementation utilizing the AVX-512 SIMD Intel instruction sets.",
						"| 64-bit | _mm512_xor_epi64(x, y) |  |",
						"| 512-bit | _mm512_xor_si512(x, y) |  |",
						"| Left Shift | 32-bit | _mm512_slli_epi32(x, y) |",
						"| Rotation | 32-bit | _mm512_rol_epi32(x, imm) |"
					]
				},
				{
					"title": "Intrinsics for Intel® Advanced Vector Extensions 512 ...",
					"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.htm",
					"excerpts": [
						"Intel® AVX-512 intrinsics are supported on IA-32 and Intel® 64 architectures built from 32nm process technology. They map directly to the new Intel® AVX-512 ...Read more"
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Intel-x86-Docs/64-ia-32-architectures-instruction-set-extensions-reference-manual.pdf",
					"excerpts": [
						"VPXORD zmm1 {k1}{z}, zmm2,\nzmm3/m512/m32bcst",
						"AVX-512 Foundation instructions can be divided in three different groups:\n•\nInstructions which support “zeroing-masking”.\n•\nAlso allow merging-masking",
						"operands. The first source operand is a ZMM/YMM/XMM registers. The second source operand can be an\nZMM/YMM/XMM register, a 512/256/128-bit memory location or a 512/256/128-bit vector broadcasted from a 64-\nbit memory location. The destination is a ZMM/YMM/XMM register, and updated according to the writemask at 64-\nbit granularity.\nOper",
						" The opmask is a set of eight architectural registers of size MAX_KL (64-bit). Note that from this set of 8\narchitectural registers, only k1 through k7 can be addressed as predicate operand.",
						"AVX-512 Foundation instructions provide a collection of opmask instructions that allow programmers to set, copy,\nor operate on the contents of a given opmask register.",
						"An opmask serving as a predicate operand in AVX-512 obeys the following properties:",
						"The instruction’s operation is not performed for an element if the corresponding opmask bit is not set. This\nimplies that no exception or violation can be caused by an operation on a masked-off element.",
						"For a given vector length, each\ninstruction accesses only the number of least significant mask bits that are needed based on its data type.",
						"e vpaddd instruction performs 32-bit integer additions on each data element conditionally based on the corre-\nsponding bit value in the predicate operand k3.",
						"An opmask register serving as a predicate operand is expressed as a curly-braces-enclosed decorator following the\nfirst operand in the Intel assembly syntax.",
						"VPADDW__m128i _mm_mask_add_epi16 (__m128i s, __mmask8 m, __m128i a, __m128i b)",
						"VPADDW__m128i _mm_maskz_add_epi16 (__mmask8 m, __m128i a, __m128i b)",
						"VPADDD __m512i _mm512_add_epi32( __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_mask_add_epi32(__m512i s, __mmask16 k, __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_maskz_add_epi32( __mmask16 k, __m512i a, __m512i b);"
					]
				},
				{
					"title": "Capabilities of Intel® AVX-512 in Intel® Xeon® Scalable Processors (Skylake) | Colfax Research",
					"url": "https://colfaxresearch.com/skl-avx512/",
					"excerpts": [
						"Masking in AVX-512 is supported by a set of eight 64-bit opmask registers. Masking registers conditionally control computational operations and updates of the destination vector with per-element granularity.",
						" AVX-512 masking uses dedicated bitmask registers, which do not occupy ZMM space. Most AVX-512 instructions accept a mask explicitly, so following up with bitwise “and” is not neccessary. Addit",
						"\nThe figure below demonstrates the difference between the implementation of masking in AVX2 and AVX-512 for the kernel “ if (B[i] < 0.0) B[i]*=0.1"
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/319433-024-697869.pdf",
					"excerpts": [
						"VPSLLVD xmm1, xmm2, xmm3/m128",
						"VPSLLVQ xmm1, xmm2, xmm3/m128",
						"VPSRLVD xmm1, xmm2, xmm3/m128",
						"VPSRLVQ xmm1, xmm2, xmm3/m128",
						"VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical",
						"The count operand can be either an XMM register or\na 128-bit memory location or an 8-bit immediate."
					]
				},
				{
					"title": "4. Instruction tables",
					"url": "https://www.agner.org/optimize/instruction_tables.pdf",
					"excerpts": [
						"VPGATHERDD\nx{k},[r+s*x]\n24\n5\nAVX512VL",
						"VPGATHERDD\ny{k},[r+s*y]\n41\n9",
						"VPGATHERDD\nz{k},[r+s*z]\n81\n17",
						"VPGATHERDQ\nx{k},[r+s*x]\n17\n4",
						"VPGATHERDQ\ny{k},[r+s*x]\n24\n5",
						"VPGATHERDQ\nz{k},[r+s*y]\n46\n9",
						"VPGATHERQD\nx{k},[r+s*x]\n18\n4",
						"VPGATHERQD\nx{k},[r+s*y]\n24\n5",
						"VPGATHERQD\ny{k},[r+s*z]\n46\n9",
						"VPGATHERQQ\nx{k},[r+s*x]\n17\n4",
						"VPGATHERQQ\ny{k},[r+s*y]\n24\n5",
						"VPSCATTERDD\n[r+s*x]{k},x\n27\n6",
						"VPSCATTERDD\n[r+s*y]{k},y\n48\n13",
						"VPSCATTERDD\n[r+s*z]{k},z\n89\n24",
						"VPSCATTERDQ\n[r+s*x]{k},x\n17\n4",
						"VPSCATTERDQ\n[r+s*x]{k},y\n28\n7",
						"VPSCATTERDQ\n[r+s*y]{k},z\n48\n9",
						"VPSCATTERQD\n[r+s*x]{k},x\n17\n4",
						"VPSCATTERQD\n[r+s*y]{k},x\n27\n6",
						"VPSCATTERQD\n[r+s*z]{k},y\n48\n12",
						"VPSCATTERQQ\n[r+s*x]{k},x\n17\n4",
						"VPSCATTERQQ\n[r+s*y]{k},y\n27\n7",
						"VPSCATTERQQ\n[r+s*z]{k},z\n48\n12",
						"LEA\nr32,[m]\n1\n2\n1/3\nAGU\nAny address size",
						"LEA\nr64,[m]\n1\n2\n1/3\nAGU\nAny address size",
						"LEA\nr32/64,[m]\n1\n2\n0.5\nEX01",
						"Port 5: Integer and vector ALU",
						"Port 6: Integer ALU, branch",
						"r16,m",
						"p1 p05",
						"2-4",
						"p15",
						"0.5",
						"r32/64,m",
						"r32/64,m",
						"p1",
						"lea",
						"lea",
						"lea",
						"lea",
						"3",
						"1",
						"1",
						"1",
						"1",
						"1",
						"1",
						"1",
						"2",
						"2"
					]
				},
				{
					"title": "Cortex-A72 Microarchitecture",
					"url": "https://wordpress-courses2425.wolfware.ncsu.edu/ece-785-sprg-2025/wp-content/uploads/sites/80/2025/03/Cortex-A72-Microarchitecture.pdf",
					"excerpts": ["2-4", "3", "1", "1", "2"]
				},
				{
					"title": "Table",
					"url": "https://uops.info/table.html?search=vpternlogd%20ymm&cb_lat=on&cb_tp=on&cb_uops=on&cb_ports=on&cb_ADLP=on&cb_ZEN4=on&cb_measurements=on&cb_base=on&cb_avx512=on",
					"excerpts": [
						"Latency Throughput Number of Uops Port Usage. Conroe Wolfdale Nehalem Westmere Sandy Bridge Ivy Bridge Haswell Broadwell Skylake Skylake-X Kaby LakeRead more",
						"Latency Throughput Number of Uops Port Usage\nConroe Wolfdale Nehalem Westmere Sandy Bridge Ivy Bridge Haswell Broadwell Skylake Skylake-X Kaby Lake Coffee Lake Cannon Lake Cascade Lake Ice Lake Tiger Lake Rocket Lake Alder Lake-P Bonnell Airmont Goldmont Goldmont Plus Tremont Alder Lake-E Zen+ Zen 2 Zen 3 Zen 4\nMeasurements IACA 2.1 IACA 2.2 IACA 2.3 IACA 3.0 Documentation\nBase AES AVX AVX2 AVX512 BMI FMA MMX SSE X87 Others"
					]
				},
				{
					"title": "Question about Skylake Execution Unit Ports : r/simd",
					"url": "https://www.reddit.com/r/simd/comments/9ztifk/question_about_skylake_execution_unit_ports/",
					"excerpts": [
						"Largely correct. It actually uses Port 0 only, but it does also use the vector unit from Port 1. This means that you can actually execute a non ...Read more"
					]
				},
				{
					"title": "Optimization Reference Manual",
					"url": "https://cdrdv2-public.intel.com/821613/355308-Optimization-Reference-Manual-050-Changes-Doc.pdf",
					"excerpts": [
						"**Table 2** **-** **13. Dispatch Port and Execution Stacks of the Skylake Client Microarchitecture**",
						"**Port 0**",
						"**Port 1**",
						"**Port 2, 3**",
						"**Port 4**",
						"**Port 5**",
						"**Port 6**",
						"**Port 7**",
						"Vec ALU",
						"Vec ALU",
						"LD",
						"STD",
						"Fast LEA,",
						"Fast LEA,",
						"Vec ALU,",
						"ALU,",
						"ALU,",
						"ALU,",
						"ALU,",
						"Shft,",
						"Sta",
						"Sta",
						"Vec Shft,",
						"Vec Shft,",
						"Vec Add,",
						"Vec Add,",
						"Vec Shuffle,",
						"Branch1",
						"FMA,",
						"Vec Mul,",
						"Vec Mul,",
						"FMA",
						"DIV,",
						"Slow Int",
						"Branch2",
						"Slow LEA",
						"**Table 2** **-** **14. Skylake Client Microarchitecture Execution Units and Representative Instructions** **1**",
						"# of",
						"unit",
						"unit",
						"instructions",
						"ALU",
						"add, and, cmp, or, test, xor, movzx, movsx, mov, (v)movdqu, (v)movdqa, (v)movap*,",
						"(v)movup*",
						"Document #: 248966-050US",
						"2-35",
						"INTEL® 64 AND IA-32 PROCESSOR ARCHITECTURES",
						"A significant portion of the Intel SSE, Intel AVX and general-purpose instructions also have latency improvements.",
						"\n**TWO SOURCE PERMUTE EXAMPLE**\nIn this example we will show the use of the two source permute instructions in a matrix transpose operation. The\nmatrix we want to transpose is square 8x8 matrix of word elements.\nThe corresponding C code is as follows (assuming each matrix occupies a continuous block of 8*8*2 = 128 bytes):\n**for(int iY = 0; iY < 8; iY++)**\n{\n**for(int iX = 0; iX < 8; iX++)**\n**{**\n**trasposedMatrix[iY*8+iX] = originalMatrix[iX*8+iY];",
						"This document is an update to the optimization recommendations contained in the Intel® 64 and IA-32 Architectures. Optimization Reference Manual, also known as:.",
						"•\nLEAs without a scaled index and with only two sources among base, index, and displacement execute as a normal\nALU operation on any port (00 through 03).",
						"\nLEAs with three sources fracture into two operations and take an additional cycle of latency.\n",
						"LEAs with a scaled index without a displacement execute as a single operation but are statically bound to port 02.",
						"]\nlea r12,[rdi+r10*8] ; using LEA to avoid",
						"EAs with three sources fracture into two operations and take an additional cycle of latency. Index consuming por -\ntion, regardless of scale value, will bind to port 02 while second operation binds to either port 00 or 0",
						"The ALU ports are optimized to execute standard two-source arithmetic/logical operations, while the AGUs are\noptimized to handle the complexities of x86 memory addressing.",
						"SLOW LEA represents a lea with two registers and displacement, all other lea versions considered as FAST LEA.",
						". LEAs without a scaled index and only two sources (among base, index, and displacement inputs) execute as one\noperation on any ALU port (00, 01, or 02).\nAL",
						"LEAs with a scaled index but without a displacement execute as one operation on port 02.",
						"The Out of Order and execution engines changes in Ice Lake client microarchitecture include:\n•\nA significant increase in size of reorder buffer, load buffer, store buffer, and reservation stations enable deeper\nOOO execution and higher cache bandwidth.",
						"Table 2 - 6 summarizes the OOO engine's capability to dispatch different types of operations to ports.",
						"Table 2 - 7 lists execution units and common representative instructions that rely on these units.",
						"4",
						"execution"
					]
				},
				{
					"title": "Skylake (server) - Microarchitectures - Intel - WikiChip",
					"url": "https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)",
					"excerpts": [
						"|Execution Unit |# of Units |Instructions |",
						"|ALU |4 |add, and, cmp, or, test, xor, movzx, movsx, mov, (v)movdqu, (v)movdqa, (v)movap*, (v)movup* |",
						"|Shift |2 |sal, shl, rol, adc, sarx, adcx, adox, etc... |",
						"|Vec ALU |3 |(v)pand, (v)por, (v)pxor, (v)movq, (v)movq, (v)movap*, (v)movup*, (v)andp*, (v)orp*, (v)paddb/w/d/q, (v)blendv*, (v)blendp*, (v)pblendd |",
						"|Vec Shift |2 |(v)psllv*, (v)psrlv*, vector shift count in imm8 |",
						"|Vec Add |2 |(v)addp*, (v)cmpp*, (v)max*, (v)min*, (v)padds*, (v)paddus*, (v)psign, (v)pabs, (v)pavgb, (v)pcmpeq*, (v)pmax, (v)cvtps2dq, (v)cvtdq2ps, (v)cvtsd2si, (v)cvtss2si |",
						"|Vec Mul |2 |(v)mul*, (v)pmul*, (v)pmadd* |"
					]
				},
				{
					"title": "uops.info",
					"url": "https://www.uops.info/",
					"excerpts": [
						"This website provides more than 700,000 pages with detailed latency, throughput, and port usage data for most instructions on many recent x86 microarchitectures.",
						"A machine-readable XML file with latency, throughput, and port usage data. Contains also information on the operands of each instruction.",
						"For Instructions on Recent x86 Microarchitectures",
						"While such data is important for understanding, predicting, and optimizing the performance of software running on these microarchitectures, most of it is not documented in the official processor manuals.",
						"[Learn more](background.html)",
						"This website provides more than 700,000 pages with detailed latency, throughput, and port usage data for most instructions on many recent x86 ...Read more",
						"This website provides more than 700,000 pages with detailed latency, throughput, and port usage data for most instructions on many recent x86 microarchitectures ...Read more",
						"uops .info"
					]
				},
				{
					"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
					"url": "https://www.agner.org/optimize/microarchitecture.pdf",
					"excerpts": [
						"*\nSimple integer vector instructions such as addition, shift, and boolean operations, have a\nlatency of only one clock cycle, versus two clock cycles on previous AMD processors.",
						"The throughput is two vector additions and two multiplications per clock cycle. The FMA",
						"The Zen 5 has no vector instructions with a 1 clock\nlatency (but vector move instructions have zero latency).",
						"Floating point addition has a latency of 3. Multiplication latency is 3 in Zen 2. Zen 1 has a",
						"multiplication latency of 3 for single precision and 4 for double precision. Fused multiply-\nand-add (FMA) has a latency of 5 clocks.",
						"instructions use the same pipes as multiplication instructions and they also partially occupy",
						"the addition units.",
						"The execution units have 128 bit width on Zen 1 and 256 bits in Zen 2. This means that a",
						"typical 256-bit vector instruction generates two µops on Zen 1, but only one µop on Zen 2.",
						"Eight integer µops and six floating point/vector µops can be scheduled per clock cycle.\n**2",
						"The Zen 3 can do three memory operations per clock cycle, with at most two memory\nwrites, i.e. three reads, or two reads and one write, or one read and two writes.",
						"**22.11 AVX instructions**",
						"256-bit vector instructions are implemented differently on Zen 1 and Zen 2:",
						"Zen 2: The Zen 2 has full 256-bit execution units and data paths. This effectively doubles\nthe throughput for 256-bit vector instructions on Zen 2 over Zen 1.",
						"ns from\n ... \nThe Zen 4 is the first AMD processor to support the AVX512 instruction set. The following\nextensions are also supported: AVX512F, BW, DQ, VL, CD, VBMI, VBMI2, IFMA, VNNI,\nBITALG, POPCNTDQ, BF16, GFNI.\n",
						"The Zen 4 has little or no transition penalty when mixing VEX and non-VEX code, unlike\nprevious AMD processors (see page 134).",
						"**25.9 Floating point execution units**",
						"There are six floating point/vector execution units: two multiply units, two add units, and two\nmemory read/write units.",
						"The latency of simple integer vector instructions is 2 clock cycles, where previous Zen\nprocessors have a 1 clock latency.",
						"The AVX512 vector compare instructions have longer latencies than the legacy SSE and\nAVX2 compare instructions.",
						"The maximum throughput for 512-bit vector instructions is one floating\npoint vector addition and one floating point vector multiplication per clock cycle, or two\ninteger vector instructions per clock cycle. This throughput is doubled for vectors of 256 bits\nor less.",
						"The high throughput of the execution units means that the performance is likely to be limited\nby the latencies rather than the throughputs of the execution units.",
						"The gather instructions are efficient for gathering non-contiguous data into vectors, and for\nvectorizing table-based lookup functions.",
						"The fused multiply-and-add (FMA) instructions are useful for improving the performance of\nfloating point code. Floating point vector additions, multiplications and FMA instructions all\nhave a throughput of one or two instructions per clock cycle and a latency of 4 clock cycles.",
						"Always avoid memory intermediates in dependency chains, as explained on page 65.",
						"Execution latencies are generally low. Most integer ALU operations have a latency of only\none clock cycle, even for 512-bit vector operations, and floating point arithmetic operations\nhave a latency of 4.",
						"The LEA instruction uses the address generation unit (AGU) rather than the ALU. This\ncauses a latency of 4 clock cycles when dependent on a pointer register or index register\nbecause of the distance between the AGU and the ALU. It is therefore faster to use addition\nand shift instructions than to use the LEA instruction in most cases.",
						"Simple integer instructions have a latency of one clock cycle.",
						"Details about µops, execution units, subunits, throughput and latencies are listed in manual. 4: \"Instruction tables\". The following examples ...Read more",
						"The Zen 2 has three levels of branch target buffer (BTB). The three levels of BTB have 16,\n512, and 7168 entries, respectively, with a latency of 0, 1, and 4 clock cycles, respectively.",
						"The misprediction penalty is approximately 13 clock cycles in the PM and 15 clock cycles in\nthe Core2, corresponding to the length of the pipeline.",
						"The measured throughput for jumps and branches varies between one branch per clock\ncycle and one branch per two clock cycles for jumps and predicted taken branches.",
						"The misprediction penalty was measured to 15 clock cycles or more\nfor branches inside the µop cache and slightly more for branches in the level-1 code cache.",
						"The branch predictor appears to have been redesigned in the Haswell and later Intel\nprocessors, but the design is undocumented.",
						"The throughput is reduced to one jump per two clock cycles if there is more than one branch\ninstruction per 16 bytes of code. If there are more than 128 branches in the critical part of\nthe code, and if they are spaced by at least 16 bytes, then apparently the first 128 branches\nhave the high throughput and the remaining have the low throughput.",
						"The branch misprediction penalty varies a lot. It was measured to 15 - 20 clock cycles.",
						"Pattern recognition for conditional jumps",
						"The processor is able to predict very long repetitive jump patterns with few or no\nmispredictions.",
						"Prediction of function returns",
						"The return stack buffer has 16 entries for near returns.",
						"3.9 Branch prediction in Intel Atom, Silvermont, Goldmont, and Knights Landing",
						" 30\n3.9 Branch prediction in Intel Atom, Silvermont, Goldmont, and Knights Landing",
						"The branch prediction mechanism in the Intel Atom and Silvermont processors is a two-level\nadaptive predictor with a global history table, following the principles described on page 16.",
						"The branch history register has 12 bits. The pattern history table on the Atom has 4096\nentries and is shared between threads.",
						"The branch target buffer has 128 entries, organized\nas 4 ways by 32 sets.",
						"The Silvermont and Goldmont have branch prediction both at the fetch stage and at the\nlater decode stage in the pipeline, where the latter can correct errors in the former,\naccording to the article by David Kanter: \" Silvermont, Intel’s Low Power Architecture \", May\n6, 2013, Realworldtech .",
						"The penalty for mispredicting a branch is 11-13 clock cycles.",
						"\n3.12 Branch prediction in AMD Bulldozer, Piledriver, Steamroller, and Excavator ..",
						"The return stack buffer has 24 entries.",
						"Misprediction penalty",
						"The misprediction penalty is specified as minimum 20 clock cycles for conditional and\nindirect branches and 15 clock cycles for unconditional jumps and returns.",
						"return stack buffer",
						"**3.13 Branch prediction in AMD Zen**",
						"Branch prediction in Zen 1-4 is based on perceptrons.",
						"Zen 3 and 4 can predict\nloops with a count of up to 64.",
						"The Zen 3 has a level-1 BTB with 1024 entries and a level-2 BTB with 6656 entries.",
						"The Zen 5 provides a major improvement with a predictor that looks two branches ahead\nand decodes both sides of a 2-way branch simultaneously.",
						"The average misprediction penalty was measured to approximately 18 clock cycles for Zen\n1-3.",
						"The return stack buffer has 32 entries in Zen 1-4, and 52 entries in Zen5.",
						"Most 512-bit vector instructions have the same latency as similar instructions with 256 bits",
						"**Table 12.2. Execution units in Ice Lake**",
						"or less. Exceptions are division and square root.",
						"vector arithmetic, logic, shift. 256 bits",
						"integer arithmetic, logic, shift",
						"0",
						"0",
						"**Operations**",
						"**Port**",
						"**9.5 Micro-op fusion**",
						"The processor uses µop fusion in the same way as previous processors. Some instructions\nthat need two µops in the execution units can use the µop fusion technique to keep these\ntwo µops together as one through most of the pipeline in order to save pipeline bandwidth.",
						"See page 93 and 109.",
						"**9.6 Macro-op fusion**",
						"The Sandy Bridge and Ivy Bridge can fuse two instructions into one µop in more cases than\nprevious processors can (see page 110).",
						"The decoders will fuse an arithmetic or logic instruction and a subsequent conditional jump\ninstruction into a single compute-and-branch µop in certain cases. The compute-and-branch\nµop is not split in two at the execution units but executed as a single µop by the branch unit\nat execution port 5.",
						"INC and DEC can fuse with signed branch instructions, and TEST and AND instructions can\nfuse with all branch instructions (including useless combinations), as indicated in the\nfollowing table:",
						"**10.5 Micro-op fusion**",
						"ses µop fusion in the same way as previous processors. Some instructions\nthat need two µops in the execution units can use the µop fusion technique to keep these\ntwo µops together as one through most of the pipeline in order to save pipeline bandwidth.\nSee",
						"The decoders can handle four µop-fused instructions per clock cycle.",
						"You can see which\ninstructions use µop fusion by looking at the tables in manual 4: \"Instruction tables\".\nInstructions with µop fusion have a higher number of µops listed under \"unfused domain\"\nthan under \"fused domain\".",
						"**10.6 Macro-op fusion**",
						"The Haswell and Broadwell can fuse two instructions into one µop in the same way that\nprevious processors can (see page 110 and 127).",
						"The decoders will fuse an arithmetic or logic instruction and a subsequent conditional jump\ninstruction into a single compute-and-branch µop in certain cases. The compute-and-branch\nµop is not split in two at the execution units but executed as a single µop by the branch unit\nat execution port 0 or 6.",
						"The CMP , ADD and SUB instructions can fuse with signed and unsigned branch instructions.",
						"The CMP , ADD and SUB instructions can fuse with signed and unsigned branch instructions.",
						"1",
						"**Latency**"
					]
				},
				{
					"title": "3. The microarchitecture of Intel, AMD and VIA CPUs",
					"url": "https://www.cs.utexas.edu/~hunt/class/2018-spring/cs340d/documents/Agner-Fog/microarchitecture.pdf",
					"excerpts": [
						"The misprediction penalty was measured to 15 clock cycles or more\nfor branches inside the µop cache and slightly more for branches in the level-1 code cache.",
						"Pattern recognition for conditional jumps",
						"The return stack buffer has 16 entries for near returns.",
						"Misprediction penalty",
						"Misprediction penalty"
					]
				},
				{
					"title": "Intel® 64 and IA-32 Architectures Optimization Reference ...",
					"url": "https://kib.kiev.ua/x86docs/Intel/Intel-OptimGuide/355308-001.pdf",
					"excerpts": ["0", "1", "2"]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"0",
						"4",
						"4",
						"4",
						"4",
						"4",
						"1",
						"1",
						"1",
						"1",
						"1",
						"1",
						"2",
						"2",
						"2",
						"2"
					]
				},
				{
					"title": "A78 Core - Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/60a5413bd63d3c31550c391e/",
					"excerpts": [
						"Table 3-4 AArch64 Arithmetic and logical instructions",
						"ADD, ADC, AND,",
						"BIC, EOR,",
						"ORN, ORR, SUB,",
						"SBC",
						"ALU, basic, flagset",
						"ADDS, ADCS,",
						"ANDS, BICS,",
						"SUBS, SBCS",
						"Arithmetic, LSL shift, shift <= 4",
						"ADD, SUB",
						"4",
						"4",
						"Arithmetic, LSR/ASR/ROR shift\nor LSL shift > 4",
						"ADD{S}, SUB{S}",
						"Conditional compare",
						"CCMN, CCMP",
						"® Cortex ® -A78 Core Software Optimization\n",
						"Guide",
						"3",
						"3",
						"1",
						"1",
						"1",
						"1",
						"M",
						"Instructions",
						"throughput",
						"utilized",
						"pipelines",
						"latency",
						"i",
						"i",
						"i",
						"i",
						"AArch64",
						"ALU, basic",
						"Instruction group",
						"execution",
						"execution",
						"-",
						"-",
						"-",
						"-",
						"-",
						"2",
						"2"
					]
				},
				{
					"title": "Software Pipelining",
					"url": "https://people.iith.ac.in/ramakrishna/fc5264/soft-pipe.pdf",
					"excerpts": ["4", "3", "1", "2"]
				},
				{
					"title": "Arm® Neoverse™ N1 Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/6687d73a69e89f01e39c4507?token=",
					"excerpts": [
						"Arithmetic, basic\nADD{S},\nADC{S},\nSUB{S}, SBC{S}\n1",
						"Arithmetic, extend and\nshift",
						"Arithmetic, LSL shift,\nshift <= 4",
						"3",
						"3",
						"I",
						"I",
						"Arithmetic,\nLSR/ASR/ROR shift or\nLSL shift > 4",
						"ADD{S},",
						"ADD{S},",
						"ADD{S},",
						"SUB{S}",
						"SUB{S}",
						"SUB{S}",
						"1",
						"1",
						"1",
						"1",
						"M",
						"M",
						"Instructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cycle.",
						"Table 3: Neoverse N1 pipeline",
						"The execution pipelines support different types of operations, as shown in the following table.",
						"e following diagram describes the high-level Neoverse N1 instruction processing pipeline.\nInstructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cycle",
						" This document provides high-level information about the Neoverse N1 pipeline, instruction\nperformance characteristics, and special performance considerations. This information is\nintended to aid people who are optimizing software and compilers for Neoverse N1. For a\nmore complete description of the Neoverse N1 processor, please refer to the Arm®\nNeoverse™ N1 Technical Reference Manual.",
						"Up to 2 uops utilizing B pipeline",
						"Up to 4 uops utilizing S pipelines",
						"Up to 2 uops utilizing M pipeline",
						"Up to 2 uops utilizing each of the V pipelines.",
						"Up to 2 uops utilizing each of the L pipelines",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in\n ..",
						"Arithmetic, LSL shift by\nimmed, shift <= 4,\nunconditional",
						"Arithmetic, basic\nADD{S},\nADC{S},\nSUB{S}, SBC{S}\n1\n3\nI",
						"ADC{S},",
						"RSB{S},",
						"RSC{S},",
						"SUB{S},",
						"SBC{S}",
						"ALU, shift by register,\nunconditional\n(same as\nALU, basic)",
						"unconditional",
						"ALU, shift by register,\nconditional\n(same as\nALU, basic)",
						"ALU, shift by register,\nconditional\n(same as\nALU, basic)",
						"m",
						"ALU, shift by register,",
						"conditional",
						"(same as",
						"ALU, basic)",
						"I, M",
						"PJDOC-466751330-9707",
						"The following diagram describes the high-level Neoverse N1 instruction processing pipeline.",
						"Instructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cyc",
						"Table 2: Neoverse N1 operations",
						"Instruction groups",
						"Instructions",
						"Branch forms",
						"Branch µOPs",
						"Integer Single-Cycle\n0/1",
						"Integer ALU µOPs",
						"nteger Single/Multi-\ncycle 0/1",
						"nteger shift-ALU, multiply, divide, CRC and sum-of-absolute-\ndifferences µOPs",
						"Load/Store Address\nGeneration 0/1",
						"Load, Store address generation and special memory µOPs",
						"FP/ASIMD-0",
						"ASIMD ALU, ASIMD misc, ASIMD integer multiply, FP convert, FP\nmisc, FP add, FP multiply, FP divide, FP sqrt, crypto µOPs, store\ndata µOPs",
						"FP/ASIMD-1",
						"ASIMD ALU, ASIMD misc, FP misc, FP add, FP multiply, ASIMD\nshift µOPs, store data µOPs, crypto µOPs.",
						"Issue 4.0",
						"Issue 4.0",
						"Instruction characteristics",
						"Instruction characteristics",
						"Instruction characteristics",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32\nand A64 instructions. A series of tables summarize the effective execution latency and\nthroughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours\nassociated with each group of instructions. Utilized pipelines correspond to the execution\npipelines described in chapter 2.",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32\nand A64 instructions. A series of tables summarize the effective execution latency and\nthroughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours\nassociated with each group of instructions. Utilized pipelines correspond to the execution\npipelines described in chapter 2.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation\ndependent on an instruction in the described group.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation\ndependent on an instruction in the described group.",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in",
						"Execution Throughput",
						"throughput",
						"utilized",
						"pipelines",
						"Logical, basic\nAND{S},\nBIC{S}, EON,\nEOR, ORN,\nORR\n1\n3\nI",
						"AArch32",
						"exec",
						"latency",
						"s",
						"i",
						"i",
						"+1",
						"b",
						"**3.4** **Arithmetic and logical instructions**",
						"**Table 6: AArch64 Arithmetic and logical instructions**",
						"**Instruction Group**",
						"**AArch64**",
						"**Instructions**",
						"**Exec**",
						"**Latency**",
						"**Execution**",
						"**Throughput**",
						"**Utilized**",
						"**Pipeline**",
						"**s**",
						"**Notes**",
						"Arithmetic, basic",
						"SUB{S}, SBC{S}",
						"s.\nArm® Neoverse™ N1 Software Optimization Guide\nPJDOC-466751330-9707\nIssue 4.0\nInstruction characteristics\nCopyright © 2018-2023 Arm Limited (or its affiliates). All rights reserved.\nNon-Confidential\nPage 11 of 66\nA",
						"AArch64",
						"3.1",
						"Instruction tables",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32",
						"and A64 instructions. A series of tables summarize the effective execution latency and",
						"throughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours",
						"associated with each group of instructions. Utilized pipelines correspond to the execution",
						"pipelines described in chapter 2.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation",
						"dependent on an instruction in the described group.",
						"Note",
						"ALU, basic",
						"ADR,",
						"ORR{S},",
						"SBC{S}, TEQ,",
						"TST",
						"Arithmetic, LSL shift by",
						"Arithmetic, LSL shift by",
						"immed, shift <= 4,",
						"immed, shift <= 4,",
						"Arithmetic,",
						"LSR/ASR/ROR shift by",
						"immed or LSL shift by",
						"immed > 4",
						"noflagset",
						"AND, BIC,",
						"EOR, ORN,",
						"orr",
						"Logical, shift by immed,",
						"Logical, shift by immed,",
						"flagset",
						"AND{S},",
						"AND{S},",
						"BIC{S},",
						"BIC{S},",
						"EOR{S},",
						"EOR{S},",
						"ORN{S},",
						"ORN{S},",
						"ORR{S}",
						"Test/Compare, shift by",
						"immed",
						"CMN, CMP,",
						"CMN, CMP,",
						"TEQ, TST",
						"+B",
						"\nNote:\nBranch forms are possible when the instruction destination register is the PC. For those cases,\nan additional branch µOP is required. This adds 1 cycle to the latency.",
						" ... ",
						" ... ",
						" ... ",
						"s\nCopyright © 2018-2023 Arm Limited (or its affiliates). All rights reserved.",
						"Non-Confidential",
						"Page 58 of 66",
						"Instruction group",
						"Instruction group",
						"Instruction group",
						"execution",
						"execution",
						"execution",
						"Notes",
						"Notes",
						"Notes",
						"Crypto SHA1 hash",
						"acceleration op",
						"SHA1H",
						"Crypto SHA1 schedule",
						"acceleration ops",
						"SHA1SU0,",
						"SHA1SU1",
						"V0",
						"V0",
						"-",
						"-",
						"2",
						"2"
					]
				},
				{
					"title": "Arm Neoverse V2 Core Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/668bc0a369e89f01e39c4668?token=",
					"excerpts": [
						"Instructions",
						"Branch µOPs",
						"Integer ALU µOPs",
						"Load, Store address generation and special memory µOPs",
						"FP/ASIMD-0",
						"FP/ASIMD-1"
					]
				},
				{
					"title": "Cortex-A76 Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/61c1f57d2183326f217720de",
					"excerpts": [
						"Arithmetic, basic\nADD{S}, ADC{S}, SUB{S},\nSBC{S}\n1\n3\nI",
						"Arithmetic, LSL shift, shift <= 4\nADD{S}, SUB{S}\n1\n3\nI",
						"Arithmetic, LSR/ASR/ROR shift\nor LSL shift > 4\nADD{S}, SUB{S}\n2\n1\nM",
						"Logical, basic\nAND{S}, BIC{S}, EON, EOR, ORN,\nORR\n1\n3\nI",
						"The following diagram describes the high-level Cortex-A76 instruction processing pipeline. Instructions are first fetched, then\ndecoded into internal macro-operations (Mops). From there, the Mops proceed through register renaming and dispatch stages.",
						"A Mop can be split further into two Uops at dispatch stage. Once dispatched, uops wait for their operands and issue out-of-order\nto one of eight execution pipelines. Each execution pipeline can accept and complete one uop per cycle.",
						"Unroll the loop to include multiple load and store operations per iteration, minimizing the overheads of looping.",
						"Align stores on 16B boundary wherever possible.",
						"The Cortex-A76 processor includes two load/store pipelines, which allow it to execute two 128 bit load uops and one 128 bit\nstore uop every cycle",
						"*\nThis chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32 and A64 instructions. A series of\ntables summarize the effective execution latency and throughput (instruction bandwidth per cycle), pipelines utilized, and\nspecial behaviours associated with each group of instructions. U",
						".\nIn the tables below, Exec Latency is defined as the minimum latency seen by an operation dependent on an instruction in the\ndescribed group.",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in instructions per cycle) of the specified\n ... ",
						"\nNotes:\n1. Branch forms are possible when the instruction destination register is the PC. For those cases, an additional branch uop\nis required. This adds 1 cycle to the latency.\n",
						"Software Optimization Guide**"
					]
				},
				{
					"title": "Arm Neoverse N1 Core: Performance Analysis Methodology",
					"url": "https://armkeil.blob.core.windows.net/developer/Files/pdf/white-paper/neoverse-n1-core-performance-v2.pdf",
					"excerpts": [
						"Neoverse N1 can fetch up to 4 instructions per cycle and dispatch a maximum 8 micro- operations per cycle. The Back End of the CPU handles the ..."
					]
				},
				{
					"title": "ROL (R32, 1)",
					"url": "https://uops.info/html-instr/ROL_R32_1.html",
					"excerpts": [
						"Section Title: ROL (R32, 1) > Skylake",
						"[Latency operand 1 → 1:](.",
						"[Latency operand 1 → 3:](.",
						"[Number of μops]",
						"Executed: 2",
						"Retire slots: 2",
						"Decoded (MITE): 2",
						"Requires the complex decoder",
						"(3 other instructions can be decoded with simple decoders in the same cycle)",
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"[Number of μops:]",
						"[Number of μops:]",
						"2",
						"2",
						"Measurements",
						"Latencies",
						"Microcode Sequencer (MS): 0",
						"IACA 2.3",
						"IACA 3.0",
						"Throughput",
						"Throughput",
						"Throughput"
					]
				},
				{
					"title": "CMOVBE (R32, R32)",
					"url": "https://uops.info/html-instr/CMOVBE_R32_R32.html",
					"excerpts": [
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"Throughput",
						"Throughput"
					]
				},
				{
					"title": "C6.2.5 ADD (shifted register) Assembler symbols",
					"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos217/precepts/18overrun/precept18.pdf",
					"excerpts": ["2"]
				},
				{
					"title": "ADD (R32, 0)",
					"url": "https://uops.info/html-instr/ADD_R32_0.html",
					"excerpts": [
						"Section Title: ADD (R32, 0) > Skylake",
						"Measurements",
						"Latencies",
						"[Latency operand 1 → 1:](../html-lat/SKL/ADD_R32_0-Measurements.html%3E1) 1",
						"[Latency operand 1 → 3:](../html-lat/SKL/ADD_R32_0-Measurements.html%3E3) 1",
						"[Measured (loop):](../html-tp/SKL/ADD_R32_0-Measurements.html) 0.25",
						"[Measured (unrolled):](../html-tp/SKL/ADD_R32_0-Measurements.html) 0.25",
						"[Number of μops](../html-tp/SKL/ADD_R32_0-Measurements.html)",
						"Executed: 1",
						"Retire slots: 1",
						"Decoded (MITE): 1",
						"Microcode Sequencer (MS): 0",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-Measurements.html) 1*p0156",
						"[Macro-fusible with:](../html-tp/SKL/ADD_R32_0-Measurements.html) JB, JBE, JL, JLE, JNB, JNBE, JNL, JNLE, JNZ, JZ",
						"IACA 2.3",
						"[IACA:](../html-tp/SKL/ADD_R32_0-IACA2.3.html) 0.25",
						"[Number of μops:](../html-tp/SKL/ADD_R32_0-IACA2.3.html) 1",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-IACA2.3.html) 1*p0156",
						"IACA 3.0",
						"Throughput",
						"Throughput",
						"Throughput",
						"Computed from the port usage: 0.25",
						"Computed from the port usage: 0.25",
						"Computed from the port usage: 0.25",
						"[IACA:](../html-tp/SKL/ADD_R32_0-IACA3.0.html) 0.25",
						"[Number of μops:](../html-tp/SKL/ADD_R32_0-IACA3.0.html) 1",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-IACA3.0.html) 1*p0156"
					]
				},
				{
					"title": "ADD (R64, I8)",
					"url": "https://uops.info/html-instr/ADD_R64_I8.html",
					"excerpts": ["Measurements", "Latencies", "Executed: 1", "Throughput"]
				},
				{
					"title": "4. Instruction tables",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Agner-Fog/instruction_tables.pdf",
					"excerpts": [
						"ADC, SBB\nr,r/i\n1\n1\n1/3\nALU",
						"AND, OR, XOR\nr,r\n1\n1\n1/3\nALU",
						"AND, OR, XOR\nr,m\n1\n1\n1/2\nALU, AGU",
						"SHL, SHR, SAR\nr,i/CL\n1\n1\n1/3\nALU",
						"tables of instruction latencies, throughputs and micro-operation\nbreakdown and other tables for x86 family microprocessors from Intel, AMD and VIA",
						" figures in the instruction tables represent the results of my measurements rather than the offi-\ncial values published by microprocessor vendors.",
						"Copyright © 1996 – 2016. Last updated 2016-01-09."
					]
				},
				{
					"title": "CS107 Guide to x86-64",
					"url": "https://web.stanford.edu/class/cs107/guide/x86-64.html",
					"excerpts": [
						"Registers. The table below lists the commonly used registers (sixteen general-purpose plus two special). Each register is 64 bits wide; the lower 32-, 16 ...Read more"
					]
				},
				{
					"title": "Pipeline of Intel Core CPUs",
					"url": "https://www.uops.info/background.html",
					"excerpts": [
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these ...",
						"The latency of an instruction is [commonly](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) defined as the *\"number of clock cycles that are required for the execution\ncore to complete the execution of all of the μops that form\nan instruction\"* (assuming that there are no other instructions\nthat compete for execution resources). Thus",
						"Thus, it denotes the\ntime from when the operands of the instruction are ready and\nthe instruction can begin execution to when the results of the\ninstruction are ready.",
						"*To take this into account, we therefore provide separate latency numbers for each pair of input and output operands.*",
						"nt:\nWe use the following notation for the port usage: *3*p015+1*p23* , for example, denotes an instruction with four μops;\nthree of these μops can each be executed on ports 0, 1, and 5, and one μop can be executed on ports 2 and 3.\n ",
						"Section Title: Supported Microarchitectures\nContent:\n**Conroe:** Measurements (Core 2 Duo E6750)\n**Wolfdale:** Measurements (Core 2 Duo E8400)\n**Nehalem:** Measurements (Core i5-750), IACA 2.1, IACA 2.2\n**Westmere:** Measurements (Core i5-650), IACA 2.1, IACA2.2\n**Sandy Bridge:** Measurements (Core i7-2600), IACA 2.1, IACA 2.2, IACA 2.3\n**Ivy Bridge:** Measurements (Core i5-3470), IACA 2.1, IACA 2.2, IACA 2.3\n**Haswell:** Measurements (Xeon E3-1225 v3), IACA 2.1, IACA 2.2, IACA 2.3, IACA 3.0\n**Broadwell:** Measurements (Core i3-5005U), IACA 2.2, IACA 2.3, IACA 3.0\n**Skylake:** Measurements (Core i5-6500), IACA 2.3, IACA 3.0\n**Skylake-X:** Measurements (Core i9-9900X), IACA 2.3, IACA 3.0\n**Kaby Lake:** Measurements (Core i7-7700)\n**Coffee Lake:** Measurements (Core i7-8700K)\n**Cannon Lake:** Measurements (Core i3-8121U)\n**Cascade Lake:** Measurements (Core i9-10980XE)\n**Ice Lake:** Measurements (Core i5-1035G1), [Documentation](https://software.intel.com/en-us/download/10th-generation-intel-core-processor-instruction-throughput-and-latency-docs)\n**Tiger Lake:** Measurements (Core i7-1165G7)\n**Rocket Lake:** Measurements (Core i9-11900)\n**Alder Lake:** Measurements (Core i5-12600K)\n**Bonnell:** Measurements (Atom D525)\n**Airmont:** Measurements (Celeron N3000)\n**Goldmont:** Measurements (Celeron J3455)\n**Goldmont Plus:** Measurements (Celeron J4125)\n**Tremont:** Measurements (Pentium Silver N6005)\n**AMD Zen+:** Measurements (Ryzen 5 2600),",
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these ...Read more",
						"Knowledge of the latency, throughput and port usage is necessary for building performance-analysis tools, cycle-accurate simulators, and optimzing compilers.\nIt is also helpful when manually fine-tuning a piece of code for a specific processor.",
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these microbenchmarks.",
						"he latency of an instruction is [commonly](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) defined as the *\"number of clock cycles that are required for the execution\ncore to complete the execution of all of the μops that form\nan instruction\"* (assuming that there are no other instructions\nthat compete for execution resources). Thus, it denotes the\ntime from when the operands of the instruction are ready and\nthe instruction can begin execution to when the results of the\ninstruction are ready",
						"When comparing throughput data from different publications,\nit is important to note that these publications do not all use the\nsame definition of throughput. Intel defines throughput in its [manual](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) as follows:\n*\"The number of clock cycles\nrequired to wait before the issue ports are free to accept\nthe same instruction again.\"*\nOn the other hand, [Agner Fog](https://www.agner.org/optimize/instruction_tables.pdf) uses the following definition\nfor (reciprocal) throughput:\n*\"The average number of\ncore clock cycles per instruction for a series of independent\ninstructions of the same kind in the same thread.\"*\nThese two definitions are not equivalent, as there can ",
						"**Conroe:** Measurements (Core 2 Duo E6750)\n**Wolfdale:** Measurements (Core 2 Duo E8400)\n**Nehalem:** Measurements (Core i5-750), IACA 2.1, IACA 2.2\n**Westmere:** Measurements (Core i5-650), IACA 2.1, IACA2.2\n**Sandy Bridge:** Measurements (Core i7-2600), IACA 2.1, IACA 2.2, IACA 2.3\n**Ivy Bridge:** Measurements (Core i5-3470), IACA 2.1, IACA 2.2, IACA 2.3\n**Haswell:** Measurements (Xeon E3-1225 v3), IACA 2.1, IACA 2.2, IACA 2.3, IACA 3.0\n**Broadwell:** Measurements (Core i3-5005U), IACA 2.2, IACA 2.3, IACA 3.0\n**Skylake:** Measurements (Core i5-6500), IACA 2.3, IACA 3.0\n**Skylake-X:** Measurements (Core i9-9900X), IACA 2.3, IACA 3.0\n**Kaby Lake:** Measurements (Core i7-7700)\n**Coffee Lake:** Measurements (Core i7-8700K)\n**Cannon Lake:** Measurements (Core i3-8121U)\n**Cascade Lake:** Measurements (Core i9-10980XE)\n**Ice Lake:** Measurements (Core i5-1035G1), [Documentation](https://software.intel.com/en-us/download/10th-generation-intel-core-processor-instruction-throughput-and-latency-docs)\n**Tiger Lake:** Measurements (Core i7-1165G7)\n**Rocket Lake:** Measurements (Core i9-11900)\n**Alder Lake:** Measurements (Core i5-12600K)\n**Bonnell:** Measurements (Atom D525)\n**Airmont:** Measurements (Celeron N3000)\n**Goldmont:** Measurements (Celeron J3455)\n**Goldmont Plus:** Measurements (Celeron J4125)\n**Tremont:** Measurements (Pentium Silver N6005)",
						"ntent:\n[Documentation](https://developer.amd.com/wordpress/media/2013/12/55723_3_00.ZIP)\n**AMD Zen 2:** Measurements (Ryzen 7 3700X), [Documentation](https://developer.amd.com/wp-content/resources/56305.zip)\n**AMD Zen 3:** Measurements (Ryzen 5 5600X), [Documentation](https://www.amd.com/system/files/TechDocs/56665.zip)\n**AMD Zen 4:** Measurements (Ryzen 5 7600X)"
					]
				},
				{
					"title": "AMD Ryzen 5800 - Agner's CPU blog",
					"url": "https://www.agner.org/forum/viewtopic.php?t=56",
					"excerpts": [
						"The throughput of the Zen 3 is now as high as six instructions per clock cycle. This may be six integer instructions or six floating point/ ...Read more"
					]
				},
				{
					"title": "x86-64 - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/X86-64",
					"excerpts": [
						"In addition to increasing the size of the general-purpose registers, the number of named general-purpose registers is increased from eight (i.e. eax , ebx , ecx , edx , esi , edi , esp , ebp ) in x86 to 16 (i.e. rax , rbx , rcx , rdx , rsi , rdi , rsp , rbp , r8 , r9 , r10 , r11 , r12 , r13 , r14 , r15 ). It is therefore possible to keep more local variables in registers rather than on the stack, and to let registers hold frequently accessed constants; arguments for small and fast subroutines may also be passed in registers to a greater extent.",
						"64-bit integer capability",
						"All [general-purpose registers](/wiki/General-purpose_register \"General-purpose register\") (GPRs) are expanded from 32 [bits](/wiki/Bit \"Bit\") to 64 bits, and all arithmetic and logical operations, memory-to-register and register-to-memory operations, etc., can operate directly on 64-bit integers."
					]
				},
				{
					"title": "Exploring 32-bit vs 64-bit Architectures: A Deep Dive into CPU Registers, Performance, and Memory in AMD and ARM | by Gitesh Wadhwa | Medium",
					"url": "https://medium.com/@GiteshWadhwa/exploring-32-bit-vs-64-bit-architectures-a-deep-dive-into-cpu-registers-performance-and-memory-fd853deaebb4",
					"excerpts": [
						"ARM64 operates with 64-bit wide registers, which means that each register can hold 64 bits of data (8 bytes).",
						"**31 general-purpose registers** : `X0-X30` , each 64-bit wide. These registers can be used for storing variables, addresses, or intermediate values during computation.",
						"In 32-bit ARM architecture (AArch32), these registers were 32-bit wide, significantly limiting data processing capacity.",
						"The x86 CPU has **general-purpose registers** , like `EAX` , `EBX` , `ECX` , etc., which are all 32-bit registers.",
						"In 64-bit mode, these general-purpose registers expand to 64 bits. The names change: `EAX` becomes `RAX` , `EBX` becomes `RBX` , and so on. These registers can hold larger values and memory addresses.",
						"In 64-bit mode, 16 general-purpose registers are available compared to 8 in 32-bit mode."
					]
				},
				{
					"title": "ADD (R32, M32)",
					"url": "https://www.uops.info/html-instr/ADD_R32_M32.html",
					"excerpts": [
						"Section Title: ADD (R32, M32) > Skylake",
						"Throughput\nComputed from the port usage: 0.50"
					]
				},
				{
					"title": "Transpose bits in byte using SIMD instructions",
					"url": "http://0x80.pl/notesen/2010-03-31-simd-transpose-bits.html",
					"excerpts": [
						"Method presented here allows to get any bit permutation, transposition is just one of possible operations. Lookup-based approach would be ...Read more"
					]
				},
				{
					"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
					"excerpts": [
						"AVX2 has gathers (not scatters), but they're only fast on Skylake and newer*",
						"Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.",
						"So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather.",
						"32-bit elements would be an obvious choice.",
						"Conflict reduction with narrower loads/stores"
					]
				},
				{
					"title": "Programming Reference",
					"url": "https://cdrdv2-public.intel.com/836496/architecture-instruction-set-extensions-programming-reference.pdf",
					"excerpts": [
						"ages. The 512-bit\nSIMD vector SIMD extensions, referred to as Intel ® Advanced Vector Extensions 512 (Intel ® AVX-512) instruc-\ntions, deliver comprehensive set of functionality and higher performance than Intel ® Advanced Vector Extensions\n(Intel ® AVX) and Intel ® Advanced Vector Extensions 2 (Intel ® AVX2) instructions. Intel AVX, Intel AVX2, and many\nIntel AVX-512 instructions are covered in the Intel *®* 64 and IA-32 Architectures Software Developer’s Manual *.* The\nreader can refer to them for basic and more background information related to various features referenced in this\ndocument.\nThe base",
						"The base of the 512-bit SIMD instruction extensions are referred to as Intel AVX-512 Foundation instructions. They\ninclude extensions of the Intel AVX and Intel AVX2 family of SIMD instructions but are encoded using a new\nencoding scheme with support for 512-bit vector registers, up to 32 vector registers in 64-bit mode, and condi-\ntional processing using opmask registers."
					]
				},
				{
					"title": "Parsing Integers with SIMD - HackMD",
					"url": "https://hackmd.io/@BFuTCPqJS16n8Njn0LhJ_Q/HyzSeH9cn?print-pdf",
					"excerpts": [
						"`_mm256_broadcast_sd` : copy a `double` from memory to 4 elements of a vector",
						"Most SIMD intrinsics follow a naming convention similar to `_mm<size>_<action>_<type>` and correspond to a single similarly named assembly instruction.",
						"`_mm256_cmpeq_epi32` : compare 8+8 packed `int` s and return a 256-bit mask",
						"_mm256_blendv_ps` : pick elements from one of two vectors according to a mask",
						"`_mm256_permutevar8x32_epi32` : pick elements by indices from a vector",
						"The naming can be a bit confusing. I've spent hundreds of hours writing SIMD code with intrinsics and I still can't remember whether it is underscore-m-m (used for intrinsics) or underscore-underscore-m (used for types).",
						"[https://db.in.tum.de/~finis/x86 intrinsics cheat sheet v1.0.pdf](https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf)",
						"[software.intel.com/sites/landingpage/IntrinsicsGuide/](http://software.intel.com/sites/landingpage/IntrinsicsGuide/)",
						"Intel being Intel doesn't include timings for AMD CPUs, but you can look them up in other instruciton tables"
					]
				},
				{
					"title": "c - SOA (Structure of array) vs AOS (Array of Structure) - Stack Overflow",
					"url": "https://stackoverflow.com/questions/72840933/soa-structure-of-array-vs-aos-array-of-structure",
					"excerpts": [
						"The core idea is to use structures matching the access patterns in the application, ideally you want a sequential memory access aligned to cache lines.Read more"
					]
				},
				{
					"title": "AVX-512",
					"url": "https://www.chessprogramming.org/AVX-512",
					"excerpts": [
						"AVX-512, an expansion of Intel's AVX and AVX2 instructions using the EVEX prefix, featuring 32 512-bit wide vector SIMD registers zmm0 through zmm31."
					]
				},
				{
					"title": "AVX-512",
					"url": "https://grokipedia.com/page/AVX-512",
					"excerpts": [
						"AMD enhanced its implementation with native 512-bit wide vector units in the Zen 5 microarchitecture (2024), improving throughput in Ryzen 9000 and EPYC Turin ...Read more"
					]
				},
				{
					"title": "AoS and SoA - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
					"excerpts": [
						"For example, to store *N* points in 3D space using a structure of arrays:",
						"If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line.",
						"rray of structures** ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved",
						"rray of structures** ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved",
						". This is often more intuitive, and supported directly by most [programming languages]",
						"Array of structures of arrays** ( **AoSoA** ) or **tiled array of structs** is a hybrid approach between the previous layouts, in which data for different fields is interleaved using tiles or blocks with size equal to the SIMD vector siz",
						"Array of structures of arrays** ( **AoSoA** ) or **tiled array of structs** is a hybrid approach between the previous layouts, in which data for different fields is interleaved using tiles or blocks with size equal to the SIMD vector siz",
						"icular, memory requests in modern processors have to be fulfilled in fixed width (e.g., size of a cacheline [[ 3 ]]() ). The tiled storage of AoSoA aligns the memory access pattern to the requests' fixed width, leading to fewer access operations to complete a memory request and thus increasing the efficien",
						"icular, memory requests in modern processors have to be fulfilled in fixed width (e.g., size of a cacheline [[ 3 ]]() ). The tiled storage of AoSoA aligns the memory access pattern to the requests' fixed width, leading to fewer access operations to complete a memory request and thus increasing the efficien",
						"The Julia package StructArrays.jl allows for accessing SoA as AoS to combine the performance of SoA with the intuitiveness of AoS.",
						"**Array of structures of arrays** ( **AoSoA** ) or **tiled array of structs** is a hybrid approach between the previous layouts, in which data for different fields is interleaved using tiles or blocks with size equal to the SIMD vector size. This is often less intuitive, but can achieve the memory throughput of the SoA approach, while being more friendly to the cache locality and load port architectures of modern processors. [[ 2 ]]() In particular, memory requests in modern processors have to be fulfilled in fixed width (e.g., size of a cacheline [[ 3 ]]() ). The tiled storage of AoSoA aligns the memory access pattern to the requests' fixed width, leading to fewer access operations to complete a memory request and thus increasing the efficiency. [[ 4 ]]()",
						" ,"
					]
				},
				{
					"title": "CPU Caches: From Theory to Optimization with ECS Examples in Rust | by Jordangrilly | Jan, 2026 | Medium",
					"url": "https://medium.com/@jordangrilly/cpu-caches-from-theory-to-optimization-with-ecs-example-in-rust-c3d52ff99e36",
					"excerpts": [
						"AoS vs SoA Cache Miss Example on sum position calculation:\nContent:\n***1. Run the “AoS” simulation.****The yellow frame represents the cache line (64 bytes). When the CPU (the bright cursor) wants to read a position (green block), it has to pull the entire line from RAM.**-* ***The problem*** *: Look at all the orange (Velocity) and blue (HP) blocks that turn gray and dull.\n- They are loaded into the cache, consuming electricity and bandwidth… only to be immediately discarded because they are not used.\n-* ***Result*** *: Look at the “Efficiency” meter. It stagnates at around 37.5%. You are throwing 60% of your performance out the wind"
					]
				},
				{
					"title": "AVX-512 - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/AVX-512",
					"excerpts": [
						". AVX-512 consists of multiple extensions that may be implemented independently. [[ 2 ]]() This policy is a departure from the historical requirement of implementing the entire instruction block. Only the core extension AVX-512F (AVX-512 Foundation) is required by all AVX-512 implementations.\nB"
					]
				},
				{
					"title": "How do you learn to write SIMD code and optimize it?",
					"url": "https://www.reddit.com/r/gamedev/comments/ulzczy/how_do_you_learn_to_write_simd_code_and_optimize/",
					"excerpts": [
						"Before vectorizing your code, your data will have to be transposed into a cache-aligned \"struct of arrays\" format for optimal transfer into the ...Read more"
					]
				},
				{
					"title": "How to transpose a 16×16 byte matrix using SSE",
					"url": "https://pzemtsov.github.io/2014/10/01/how-to-transpose-a-16x16-matrix.html",
					"excerpts": [
						"The usual way to transpose this matrix is to divide it into small blocks that fit into available registers, and transpose each block separately.Read more"
					]
				},
				{
					"title": "Intel DOWNFALL: New Vulnerability Affecting AVX2/AVX-512 With Big Performance Implications : r/hardware",
					"url": "https://www.reddit.com/r/hardware/comments/15lnwre/intel_downfall_new_vulnerability_affecting/",
					"excerpts": [
						"GATHER is slow (microcoded even?) on all current and past normal AMD and Intel CPUs. I'd be surprised if anyone is using it in performance ...Read more"
					]
				},
				{
					"title": "assembly - transpose 8x16 matrix on AVX 512 - Stack Overflow",
					"url": "https://stackoverflow.com/questions/70559645/transpose-8x16-matrix-on-avx-512",
					"excerpts": [
						"I have a 8x16 `uint32_t` matrix already loaded in 8 `zmm` registers. They have the layout\nwhere `i` goes from 0 to 7 and `ai_j` are 32 bit integers for each `j` from 0 to 15. I am looking for a fast way of transforming the layout of these registers so that I obtain something that is not strictly a transpose of the matrix, but end result I want is to have a buffer in memory containing the 128 = 8 x 16 integers `ai_j` in the following layout",
						"I can do this \"transposition\" on registers by 8 `vshufps` (throughput 1 and latency 1 on Xeon KNL) and 16 vpermi2d (throughput 1 latency 3) together with their index loading. A quick napking computation convinced me that this was the fastest way of doing this on registers before storing to memory and also that this was faster than trading some `vpermi2d` for scatter store operations to memory.",
						"I am aware of the 16x16 transpositions [here](https://stackoverflow.com/questions/29519222/how-to-transpose-a-16x16-matrix-using-simd-instructions) and in [Intel's implementation](https://github.com/intel/intel-ipsec-mb/blob/main/lib/include/transpose_avx512.asm) but both rely on a trick of loading the matrix from memory in a precise layout to avoid 128 lane crossings. I cannot change the fact that I already have the matrix loaded in that particular way in 8 registers as described above. I can do this \"transposition\" on registers by 8 `vshufps` (throughput 1 and latency 1 on Xeon KNL) and 16 vpermi2d (throughput 1 latency 3) together with their index loading. A quick napking computation convinced me that this was the fastest way of doing this on registers before storing to memory and also that this was faster than trading some `vpermi2d` for scatter store operations to memory.",
						"The handwaving way of convincing myself of this is that I start with registers containing 1 letter and 16 indices (the macro below clarifies this notation) and I want to end up with a register containing 8 letters and 2 indices. At each `vshufps` and `vpermi2d` I can only divide by two the number of indices and multiply by 2 the number of letters. But I am forced to cross 128 bit lanes at least twice to separate contiguous indices; thus I am forced to have two rounds of non simple shuffles."
					]
				},
				{
					"title": "What it takes to transpose a matrix - Andrei Gudkov",
					"url": "http://gudok.xyz/transpose/",
					"excerpts": [
						"For example, with 256 bit registers we could create efficient 32×32 transposer\nby using log 2 (32)=5 levels of 2×2 block decomposition.",
						"In the fifth stage of the algorithm we are transposing 2x2 block matrices where each block is 16x16,\neffectively meaning that we have to swap lanes.",
						"AVX-512 ( AVX-512F, AVX-512VL, AVX-512BW and a dozen of others )\nfamily further expands the width of the registers, now to 512 bits. However, the availability of CPUs with AVX-512 is still not universal as of 2023.",
						" is another highly useful instruction.\nIt accepts two source registers and a mask, and constructs result where every `i` -th element\nis selected from either `src1[i]` or `src2[i]` based on the mas",
						"ombination of shuffle/permute and blend allows us to build a value consisting of almost\narbitrary selection of elements (remember cross-lane restriction) from two input registers, `a` and `b` ,\nin the following wa",
						"Code-generated 32x32 transposer must be plugged into the overall algorithm, again with carefully\nchosen strategy for preloading.",
						"As before, we will work level by level, row by row.",
						"The same header also provides the intrinsics to work with these registers.",
						"We do not need to call them manually since compilers can insert such instructions automatically\non encountering `_mm256i r = *ptr;` and `*ptr = r;` respectively.",
						"I will assume that we have access to AVX2 but not the newer extensions.",
						"For example, replacing AVX2 32-byte shuffling and blending instructions with their respective\n64-byte counterparts from AVX-512 allows to transpose 64x64 submatrix with a single invocation\nof block algorithm that is constructed of log 2 (64)=6 levels.",
						"For example, replacing AVX2 32-byte shuffling and blending instructions with their respective\n64-byte counterparts from AVX-512 allows to transpose 64x64 submatrix with a single invocation\nof block algorithm that is constructed of log 2 (64)=6 levels.",
						"Section Title: What it takes to transpose a matrix > 256-bit SIMD [0.49]",
						"Content:\nI will assume that we have access to AVX2 but not the newer extensions.",
						"AVX2 gives us access to 256 bit registers and integer instructions.",
						"AVX2 gives us access to 256 bit registers and integer instructions.",
						"This means that we will be able to create efficient 32x32 transposer by using log 2 (32)=5 levels of block decomposition.",
						"This means that we will be able to create efficient 32x32 transposer by using log 2 (32)=5 levels of block decomposition.",
						"Semantics of the other three instructions is more complicated and is provided below.",
						"Hence during each invocation we need to preload 16 rows of the next block.",
						"There are 393 instructions in total, so we insert preload each 393/16=24 instructions into generated code.",
						"Section Title: What it takes to transpose a matrix > Conclusion",
						"It should be obvious that Vec256Buf is not the ultimate solution.",
						"It should be obvious that Vec256Buf is not the ultimate solution.",
						"There is plenty of room for further improvement by using newer instruction extensions.",
						"There is plenty of room for further improvement by using newer instruction extensions.",
						"Third direction may be the application of AMX extension.",
						"It specifically targets matrix operations and in particular is capable of loading and storing submatrices\ninto tile registers using strided memory access."
					]
				},
				{
					"title": "AoS and SoA",
					"url": "https://grokipedia.com/page/AoS_and_SoA",
					"excerpts": [
						"AoSoA) is a hybrid memory layout that combines the benefits of Array of Structures (AoS) and Structure of Arrays (SoA) by partitioning an array into fixed-size blocks—typically 4 or 8 elements per block—and organizing the fields within each block in a contiguous, SoA manner",
						"This structure supports efficient construction by transposing data from an initial AoS array into the hybrid format, often using strided copies.",
						"Structure of Arrays (SoA) layout facilitates vectorized updates across multiple particles or rays, such as adding velocity components to position arrays using Single Instruction, Multiple Data (SIMD) instructions, enabling parallel computation of updates like p i ​ = p i ​ + v i ​ Δ t for all i without interleaving data."
					]
				},
				{
					"title": "Why You Shouldn't Forget to Optimize the Data Layout | CedarDB",
					"url": "https://cedardb.com/blog/optimizing_data_layouts/",
					"excerpts": [
						"Additionally, the SoA approach can greatly benefit of auto vectorization within the compiler.",
						"With the optimization level O3, g++ produces SIMD code for this data layout.",
						"For the AoS approach, the compiler still loops over the data given an array of employees, as shown [here at godbolt](https://godbolt.org/z/sPeqaM8zM) .",
						"This auto vectorization gives us another 1.5x performance improvement, so in total SoA is more than 4x faster than AoS.",
						"| Store | Execution Time |",
						"| Array of Structs | 137.22 ms |",
						"| Struct of Arrays | 45.67 ms |",
						"| Struct of Arrays (auto-vectorized) | 33.13 ms |"
					]
				},
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code.",
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor.",
						".\nHere is a simple example that shows the difference between these forms:\n ... \nSection Title: Memory Layout Transformations > Take Aways\nContent:\nAlthough it may seem more logical to organize data in Array of Structures (AoS), this organization makes it extremely difficult to access the memory for reads (gather) and writes (scatter). This prevents many optimizations by the compiler, most notably it often prevents efficient vectorization. Algorithms with this characteristic are often very poor performers on Intel® Xeon Architecture."
					]
				}
			],
			"reasoning": "The provided value describes an optimization strategy for data layout, specifically recommending the Array of Structures of Arrays (AoSoA) layout. This is justified by its superior performance characteristics for SIMD operations, particularly in hash functions. The rationale highlights how AoSoA arranges data into contiguous blocks that align with SIMD vector widths, enabling efficient single memory instructions for loading data. This contrasts with the Array of Structures (AoS) layout, which would necessitate slower gather instructions. The document also emphasizes the benefits of AoSoA for cache utilization, hardware prefetcher efficiency, and the avoidance of gather/scatter instructions, all of which are crucial for high-performance computing and SIMD-based operations like hashing.",
			"confidence": "high"
		},
		{
			"field": "simd_vectorization_strategy.estimated_cycles_per_hash",
			"citations": [
				{
					"title": "Computing multiple hash values in parallel with AVX2",
					"url": "https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/",
					"excerpts": [
						"After a couple of false starts, I ended up with a version of [xxHash32](https://github.com/Cyan4973/xxHash) that computes hash values for 8 keys at the same time using AVX2. The code\nis at [parallel-xxhash](https://github.com/jsnell/parallel-xxhash) ."
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"| XOR | 32-bit | _mm512_xor_epi32(x, y) |",
						"In this article, we propose an LSH-512 implementation utilizing the AVX-512 SIMD Intel instruction sets.",
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family."
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` )."
					]
				}
			],
			"reasoning": "To support a low per-hash cycle estimate like 1.125, the most relevant information points to aggressive parallelization of hash computations using wide SIMD lanes and hardware-specific intrinsics. A concrete example shows implementing a hash function for 8 keys in parallel with AVX2, illustrating a path to sub-1-cycle per-hash performance when data parallelism is effectively exploited across multiple lanes. Complementary material notes that AVX-512 is a notable SIMD instruction set with documented capabilities and that AVX-512 enables higher throughput and masking features, which are critical for keeping per-hash latency low in a highly pipelined, multi-hash workflow. Additional excerpts discuss AVX-512 acceleration for hash-like workloads, including details on XOR, shift, and combination operations that map well to the a = ((a op1 const1) op2 (a shift_op shift_amount)) pattern, enabling fused or fused-like implementations that reduce cycles per hash. Practical observations from parallel hashing experiments (e.g., processing multiple keys in parallel, and the discussion of vectorized vs scalar paths) provide empirical grounding for estimating small cycle counts per hash when leveraging wide vectors, interleaving, and macro-op fusion opportunities. Together, these excerpts collectively support the notion that substantial parallelism via SIMD and careful scheduling can approach, and in some contexts exceed, a per-hash cycle target around 1.125, especially when multiple hashes are computed concurrently across many lanes and when hardware features such as AVX-512 masking and wide vector operations are effectively utilized.",
			"confidence": "medium"
		},
		{
			"field": "simd_vectorization_strategy.lane_width_choice",
			"citations": [
				{
					"title": "Improving xxHash",
					"url": "https://encode.su/threads/2556-Improving-xxHash",
					"excerpts": [
						"And for fast AVX2/AVX512 implementations we should process 2-4 times more data, i.e. split data stream into as much as 16 lanes, each 128-bit ...Read more"
					]
				},
				{
					"title": "Computing multiple hash values in parallel with AVX2",
					"url": "https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/",
					"excerpts": [
						"After a couple of false starts, I ended up with a version of [xxHash32](https://github.com/Cyan4973/xxHash) that computes hash values for 8 keys at the same time using AVX2. The code\nis at [parallel-xxhash](https://github.com/jsnell/parallel-xxhash) .",
						"Section Title: [Computing multiple hash values in parallel with AVX2](https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/) > Benchmarks",
						"Before heading off into the weeds with the details, below are a\ncouple of pretty graphs showing performance with different key\nsizes for a few different implementations: CityHash64 since it's\nbeen my default hash function for years, xxHash64 since my\nparallel implementation was based on xxHash32, and MetroHash64\nsince I saw people suggesting it was the fastest option for small\nkeys. I did not include FarmHash since it was consistently\nslower than CityHash for all key sizes.",
						"All implementations computed hashes for the same number of keys;\nthe parallel implementations did it 8 keys at a time, the others\ndid them sequentially.",
						"All implementations computed hashes for the same number of keys;\nthe parallel implementations did it 8 keys at a time, the others\ndid them sequentially. The tests were run on a i7-6700 and GCC\n6.3.0, with `-O3 -march=native -fno-strict-aliasing` . The benchmark code is in the\nrepository, but you'll need to bring your own copies of the\nexternal hash table libraries.",
						"CityHash64 since it's\nbeen my default hash function for years, xxHash64 since my\nparallel implementation was based on xxHash32, and MetroHash64\nsince I saw people suggesting it was the fastest option for small\nkeys. I did not include FarmHash since it was consistently\nslower than CityHash for all key sizes."
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family.",
						"In this article, we propose an LSH-512 implementation utilizing the AVX-512 SIMD Intel instruction sets.",
						"| 64-bit | _mm512_xor_epi64(x, y) |  |",
						"| 512-bit | _mm512_xor_si512(x, y) |  |",
						"| Left Shift | 32-bit | _mm512_slli_epi32(x, y) |",
						"| Rotation | 32-bit | _mm512_rol_epi32(x, imm) |"
					]
				},
				{
					"title": "Intrinsics for Intel® Advanced Vector Extensions 512 ...",
					"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.htm",
					"excerpts": [
						"Intel® AVX-512 intrinsics are supported on IA-32 and Intel® 64 architectures built from 32nm process technology. They map directly to the new Intel® AVX-512 ...Read more"
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` )."
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"`x86` / `x64` systems can enjoy a new unit, `xxh_x86dispatch` , which can detect at runtime the best vector instruction set present on host system (none, `sse2` , `avx2` or `avx512` ), thanks to a cpu feature detector designed by [@easyaspi314](https://github.com/easyaspi314) . It then automatically runs the appropriate vector code.",
						"It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` "
					]
				}
			],
			"reasoning": "The most directly supportive information comes from a source that explicitly mentions exposing parallelism by dividing the data stream into as many as 16 lanes, each 128-bit wide. This directly aligns with the field value describing a 16-wide lane choice. Additional items discuss parallel hashing and SIMD implementations (including AVX-512 and AVX2), which provides context on achievable lane widths and the hardware capabilities that enable wider parallelism. Other excerpts describe 8-key/8-lane parallelism as a comparison point, illustrating that 16-lane width is a design option among others. Together, these excerpts demonstrate that 16-wide SIMD parallelism is a valid and discussed strategy in the literature, and they contrast it with narrower widths such as 8 lanes, helping justify why 16 lanes could be chosen or evaluated in a vectorized hash implementation.",
			"confidence": "high"
		},
		{
			"field": "simd_vectorization_strategy.intrinsics_pattern",
			"citations": [
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"| Left Shift | 32-bit | _mm512_slli_epi32(x, y) |",
						"| XOR | 32-bit | _mm512_xor_epi32(x, y) |",
						"| 512-bit | _mm512_xor_si512(x, y) |  |",
						"| Rotation | 32-bit | _mm512_rol_epi32(x, imm) |",
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family."
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Intel-x86-Docs/64-ia-32-architectures-instruction-set-extensions-reference-manual.pdf",
					"excerpts": [
						"VPADDD __m512i _mm512_mask_add_epi32(__m512i s, __mmask16 k, __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_add_epi32( __m512i a, __m512i b);",
						"VPADDD __m512i _mm512_maskz_add_epi32( __mmask16 k, __m512i a, __m512i b);"
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/319433-024-697869.pdf",
					"excerpts": [
						"VPSRLVD xmm1, xmm2, xmm3/m128",
						"The count operand can be either an XMM register or\na 128-bit memory location or an 8-bit immediate.",
						"VPSLLVD xmm1, xmm2, xmm3/m128",
						"VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical"
					]
				},
				{
					"title": "XXH3 performance issue on ARM & WASM with >128byte inputs · Issue #211 · Cyan4973/xxHash",
					"url": "https://github.com/Cyan4973/xxHash/issues/211",
					"excerpts": [
						"| 0 \"scalar\" | 24 GB/s | 11 GB/s |",
						"Comparison with AES-based algorithm : `SSE2` is not enough to face special-purpose hardware. `XXH3` is designed to leverage `AVX2` , when available, in order to reach similar performance territory. For example, on my laptop, it reaches ~45 GB/s on \"large\" data. Yet, while `SSE2` is a guaranteed capability of any `x64` cpu, later SIMD versions are not. I suspect that's the reason why `AVX2` wasn't benchmarked."
					]
				}
			],
			"reasoning": "The field value describes a single hash stage implemented as an addition of a constant to the current vector, a left shift of the original vector, and a bitwise XOR of the two results, all performed with 512-bit wide lanes. The most directly relevant excerpts explicitly show the specific AVX-512 intrinsics for these operations on 32-bit lanes, which aligns with the described sequence. The evidence demonstrates adding 32-bit integers with a vector add intrinsic, followed by a left-shift intrinsic for 32-bit elements within a 512-bit vector, and then XOR-ing the results with a vector XOR intrinsic. The presence of an explicit 512-bit add intrinsic, together with corresponding 512-bit shift and 512-bit XOR intrinsics, directly mirrors the sequence in the field value and confirms that this combination is a valid path to realize the pattern in a vectorized, lane-parallel manner. Additional excerpts discuss related patterns (masked adds, alternative intrinsics, and broader AVX-512 context) that reinforce how such a stage could be scheduled and executed across 16 lanes concurrently, further supporting the proposed design. Some excerpts provide exact function prototypes (_mm512_add_epi32, _mm512_slli_epi32, _mm512_xor_si512) or disclose how these intrinsics are mapped in AVX-512, which strengthens the claim that this pattern can be implemented as described. Collectively, these excerpts corroborate the mapping of the stage’s arithmetic (add with const, shift, xor) to the AVX-512 intrinsics across a 16-wide vector of hash states, matching the field value’s proposed sequence. This support is consistent across multiple sources, including explicit mentions of add and shift intrinsics, as well as the dedicated AVX-512 XOR path, which together provide a coherent and practical implementation approach. Overall, the evidence set aligns well with the field value, indicating a high level of agreement among the sources about the viability and exact nature of the intrinsics in this context.",
			"confidence": "high"
		},
		{
			"field": "simd_vectorization_strategy.recommended_isa",
			"citations": [
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Intel-x86-Docs/64-ia-32-architectures-instruction-set-extensions-reference-manual.pdf",
					"excerpts": [
						"VPADDD __m512i _mm512_add_epi32( __m512i a, __m512i b);"
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"In this article, we propose an LSH-512 implementation utilizing the AVX-512 SIMD Intel instruction sets.",
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family."
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` )."
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ."
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": ["   - AVX512"]
				},
				{
					"title": "Capabilities of Intel® AVX-512 in Intel® Xeon® Scalable Processors (Skylake) | Colfax Research",
					"url": "https://colfaxresearch.com/skl-avx512/",
					"excerpts": [
						"Masking in AVX-512 is supported by a set of eight 64-bit opmask registers. Masking registers conditionally control computational operations and updates of the destination vector with per-element granularity.",
						" AVX-512 masking uses dedicated bitmask registers, which do not occupy ZMM space. Most AVX-512 instructions accept a mask explicitly, so following up with bitwise “and” is not neccessary. Addit",
						"\nThe figure below demonstrates the difference between the implementation of masking in AVX2 and AVX-512 for the kernel “ if (B[i] < 0.0) B[i]*=0.1"
					]
				},
				{
					"title": "Intel® Architecture Instruction Set Extensions Programming ...",
					"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/319433-024-697869.pdf",
					"excerpts": [
						"VPSLLVD xmm1, xmm2, xmm3/m128",
						"VPSRLVD xmm1, xmm2, xmm3/m128",
						"VPSRLVQ xmm1, xmm2, xmm3/m128",
						"VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical",
						"The count operand can be either an XMM register or\na 128-bit memory location or an 8-bit immediate."
					]
				}
			],
			"reasoning": "The most relevant excerpts directly assert AVX-512 as a noteworthy or recommended SIMD path for hash-like workloads. One excerpt explicitly states that AVX-512 is a notable SIMD instruction set and links it to performance benefits in the context of hash-like computations. Another excerpt highlights an LSH-512 implementation that utilizes AVX-512, illustrating a concrete, practical use case for AVX-512 in hashing pipelines. Additional excerpts present AVX-512 intrinsic usage examples (such as 512-bit add operations) and discuss masking capabilities and architectural features that enable high-parallelism, data-level parallelism, and lane-wise computations essential for a vectorized hash function.\n\nThe remaining excerpts reinforce the AVX-512 narrative by outlining compiler/intrinsic support, vector-dispatch mechanisms that include AVX-512 as a target, and architectural documentation about masking, 512-bit registers, and how AVX-512-enabled pathways can outperform AVX2 in similar workloads. Several entries explicitly mention AVX-512 in the context of product releases and vectorization support, underscoring AVX-512 as a viable and strategic choice for achieving the desired parallelism in a hash-like computation with long dependency chains by leveraging wide data paths and masking-based computation. Collectively, these excerpts provide a cohesive set of evidence that AVX-512 is the recommended ISA for achieving higher parallelism in the described hash-style pipeline, through wide-vector operations, masking, and 512-bit intrinsics.",
			"confidence": "high"
		},
		{
			"field": "simd_vectorization_strategy.supported_isas",
			"citations": [
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` )."
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": [
						" - SIMD acceleration",
						"t: Auto detected.\n ... \n*/\n#ifndef XXH_ACC_ALIGN\n#  if defined(XXH_X86DISPATCH)\n#     define XXH_ACC_ALIGN 64  /* for compatibility with avx512 */\n#  elif XXH_VECTOR == XXH_SCALAR  /* scalar */\n#     define XXH_ACC_ALIGN 8\n#  elif XXH_VECTOR == XXH_SSE2  /* sse2 */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX2  /* avx2 */\n#     define XXH_ACC_ALIGN 32\n#  elif XXH_VECTOR == XXH_NEON  /* neon */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_VSX   /* vsx */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX512  /* avx512 */\n#     define XXH_ACC_ALIGN 64\n#  elif XXH_VECTOR == XXH_SVE   /* sve */\n#     define XXH_ACC_ALIGN 64\n#  endif\n#endif\n\n#if"
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"`x86` / `x64` systems can enjoy a new unit, `xxh_x86dispatch` , which can detect at runtime the best vector instruction set present on host system (none, `sse2` , `avx2` or `avx512` ), thanks to a cpu feature detector designed by [@easyaspi314](https://github.com/easyaspi314) . It then automatically runs the appropriate vector code."
					]
				},
				{
					"title": "Computing multiple hash values in parallel with AVX2",
					"url": "https://www.snellman.net/blog/archive/2017-03-19-parallel-hashing-with-avx2/",
					"excerpts": [
						"After a couple of false starts, I ended up with a version of [xxHash32](https://github.com/Cyan4973/xxHash) that computes hash values for 8 keys at the same time using AVX2. The code\nis at [parallel-xxhash](https://github.com/jsnell/parallel-xxhash) ."
					]
				},
				{
					"title": "Parallel Implementation of Lightweight Secure Hash Algorithm on CPU and GPU Environments",
					"url": "https://www.mdpi.com/2079-9292/13/5/896",
					"excerpts": [
						"AVX-512 is a notable SIMD instruction set, initially integrated into the Xeon Scalable Processor family and Intel Skylake-X family."
					]
				}
			],
			"reasoning": "Several excerpts clearly enumerate or imply a broad, cross-ISA SIMD support set that matches the requested field value. One excerpt states that the project uses a vector implementation framework that can target multiple ISA families (SSE2, AVX2, AVX512, NEON, and VSX) and that vectorization is chosen at runtime or compile time, which directly aligns with the requested ISAs. Another excerpt explicitly lists common vector targets in xxHash’s dispatch mechanism, including AVX2, AVX512, SSE2, NEON, and VSX, indicating concrete support for those architectures. A third excerpt discusses SIMD acceleration across multiple architectures and explicitly references NEON, AVX2, AVX512, SSE2, and provides notes about SIMD acceleration that map to those ISAs. A fourth excerpt highlights automatic selection of vector dispatch paths, including AVX2 and AVX512, which reinforces cross-ISA support, including the ones requested. A fifth excerpt provides a practical demonstration of parallel hashing using AVX2, which confirms real-world use of that ISA for hashing workloads. A sixth excerpt reiterates AVX-512 capabilities and related masking features, contributing to the higher-level picture that AVX-512 is an actively supported path in SIMD hash implementations. Collectively, these excerpts directly support the presence of the listed ISAs (AVX2, SSE2, ARM NEON, ARM SVE, POWER VSX) in vectorized hash workflows and suggest that implementations can dispatch to these targets depending on the platform.",
			"confidence": "high"
		},
		{
			"field": "data_layout_optimization.layout_rationale",
			"citations": [
				{
					"title": "AoS and SoA",
					"url": "https://grokipedia.com/page/AoS_and_SoA",
					"excerpts": [
						"AoSoA) is a hybrid memory layout that combines the benefits of Array of Structures (AoS) and Structure of Arrays (SoA) by partitioning an array into fixed-size blocks—typically 4 or 8 elements per block—and organizing the fields within each block in a contiguous, SoA manner",
						"This structure supports efficient construction by transposing data from an initial AoS array into the hybrid format, often using strided copies."
					]
				},
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code.",
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor.",
						".\nHere is a simple example that shows the difference between these forms:\n ... \nSection Title: Memory Layout Transformations > Take Aways\nContent:\nAlthough it may seem more logical to organize data in Array of Structures (AoS), this organization makes it extremely difficult to access the memory for reads (gather) and writes (scatter). This prevents many optimizations by the compiler, most notably it often prevents efficient vectorization. Algorithms with this characteristic are often very poor performers on Intel® Xeon Architecture."
					]
				}
			],
			"reasoning": "The field value centers on the superiority of AoSoA for exposing SIMD-level parallelism in a hash-like workload. The initial description of AoSoA explicitly states that it is a hybrid layout that partitions an array into fixed-size blocks (often 4 or 8 elements) and places fields within each block in a contiguous, SoA-like fashion. This directly supports the claim that grouping 32-bit hash state components into blocks that match vector widths enables loading a full vector of independent states with a single, efficient memory instruction, while avoiding expensive gathers characteristic of AoS. Additional excerpts describe using a transposition step from AoS to a hybrid AoSoA format, which reinforces the notion that such structuring is a deliberate strategy to regain vectorizable locality via strided-copy operations. Other excerpts discuss broader SOA/AoS transformations and the practical benefits of OS-to-SOA/conversion patterns for vectorization, as well as general guidance on memory layout transformations to reduce irregular memory accesses. Taken together, these excerpts substantiate the value that AoSoA provides 100% cache-line utilization for field-wise operations and aligns data layout with SIMD-friendly access patterns, which aligns with the claim that AoSoA is superior in the described hash-context. ",
			"confidence": "high"
		},
		{
			"field": "data_layout_optimization.recommended_layout",
			"citations": [
				{
					"title": "AoS and SoA",
					"url": "https://grokipedia.com/page/AoS_and_SoA",
					"excerpts": [
						"AoSoA) is a hybrid memory layout that combines the benefits of Array of Structures (AoS) and Structure of Arrays (SoA) by partitioning an array into fixed-size blocks—typically 4 or 8 elements per block—and organizing the fields within each block in a contiguous, SoA manner"
					]
				},
				{
					"title": "AoS and SoA - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
					"excerpts": [
						"**Array of structures of arrays** ( **AoSoA** ) or **tiled array of structs** is a hybrid approach between the previous layouts, in which data for different fields is interleaved using tiles or blocks with size equal to the SIMD vector size. This is often less intuitive, but can achieve the memory throughput of the SoA approach, while being more friendly to the cache locality and load port architectures of modern processors. [[ 2 ]]() In particular, memory requests in modern processors have to be fulfilled in fixed width (e.g., size of a cacheline [[ 3 ]]() ). The tiled storage of AoSoA aligns the memory access pattern to the requests' fixed width, leading to fewer access operations to complete a memory request and thus increasing the efficiency. [[ 4 ]]()"
					]
				},
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code.",
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor.",
						".\nHere is a simple example that shows the difference between these forms:\n ... \nSection Title: Memory Layout Transformations > Take Aways\nContent:\nAlthough it may seem more logical to organize data in Array of Structures (AoS), this organization makes it extremely difficult to access the memory for reads (gather) and writes (scatter). This prevents many optimizations by the compiler, most notably it often prevents efficient vectorization. Algorithms with this characteristic are often very poor performers on Intel® Xeon Architecture."
					]
				}
			],
			"reasoning": "AoSoA is described as a hybrid memory layout that combines the benefits of AoS and SoA by partitioning an array into fixed-size blocks and organizing fields within each block in a contiguous, SoA manner. This directly aligns with the target field value, which identifies AoSoA as the recommended layout. The first excerpt establishes AoSoA as a hybrid that seeks to capture the performance advantages of both AoS and SoA, which is precisely what the field value indicates as the preferred approach. Supplementary excerpts define AoSoA in the context of transposing data while preserving locality and enabling vectorization, reinforcing why AoSoA is considered advantageous for exposure of parallelism in data-oriented workloads. Additional references discuss the broader trade-offs of AoS vs SoA, and argue for the AoSoA approach in terms of locality, cache behavior, and vectorization friendliness. The later excerpts on memory-layout transformations elaborate how transforming to AoS, SoA, or AoSoA can unlock compiler/vector unit optimizations and reduce irregular memory access patterns, which further supports AoSoA as a recommended layout in performance-critical code. In short, the strongest support comes from explicit definitions of AoSoA as a hybrid layout and its stated benefits for vectorization and data locality, with corroborating context on when and why such a hybrid layout is advantageous.",
			"confidence": "high"
		},
		{
			"field": "data_layout_optimization.cache_and_prefetching_impact",
			"citations": [
				{
					"title": "AoS and SoA",
					"url": "https://grokipedia.com/page/AoS_and_SoA",
					"excerpts": [
						"AoSoA) is a hybrid memory layout that combines the benefits of Array of Structures (AoS) and Structure of Arrays (SoA) by partitioning an array into fixed-size blocks—typically 4 or 8 elements per block—and organizing the fields within each block in a contiguous, SoA manner",
						"This structure supports efficient construction by transposing data from an initial AoS array into the hybrid format, often using strided copies.",
						"Structure of Arrays (SoA) layout facilitates vectorized updates across multiple particles or rays, such as adding velocity components to position arrays using Single Instruction, Multiple Data (SIMD) instructions, enabling parallel computation of updates like p i ​ = p i ​ + v i ​ Δ t for all i without interleaving data."
					]
				},
				{
					"title": "CPU Caches: From Theory to Optimization with ECS Examples in Rust | by Jordangrilly | Jan, 2026 | Medium",
					"url": "https://medium.com/@jordangrilly/cpu-caches-from-theory-to-optimization-with-ecs-example-in-rust-c3d52ff99e36",
					"excerpts": [
						"AoS vs SoA Cache Miss Example on sum position calculation:\nContent:\n***1. Run the “AoS” simulation.****The yellow frame represents the cache line (64 bytes). When the CPU (the bright cursor) wants to read a position (green block), it has to pull the entire line from RAM.**-* ***The problem*** *: Look at all the orange (Velocity) and blue (HP) blocks that turn gray and dull.\n- They are loaded into the cache, consuming electricity and bandwidth… only to be immediately discarded because they are not used.\n-* ***Result*** *: Look at the “Efficiency” meter. It stagnates at around 37.5%. You are throwing 60% of your performance out the wind"
					]
				},
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code.",
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor."
					]
				}
			],
			"reasoning": "The most directly relevant material describes AoSoA as a hybrid layout that partitions data into fixed-size blocks and places fields within each block in a structure-of-arrays fashion. This directly supports the idea that AoSoA can improve memory locality by organizing data into tile-like blocks, which can be chosen to fit cache characteristics. It also notes that AoSoA can be constructed by transposing data from an initial AoS into a hybrid format via strided copies, which aligns with the concept of tiling data to improve access patterns and locality. The discussion of Structure of Arrays within the AoSoA context emphasizes enabling vectorized/parallel updates across multiple elements, reinforcing the notion of improved data throughput when using a layout that preserves locality and SIMD-friendly access. Complementary material explicitly states that AoSoA can deliver locality advantages and reduce irregular memory accesses, which supports the claim that unit-stride, linear access through tiles benefits prefetching and overall memory subsystem performance. Additional excerpts discuss broader memory-layout transformations (AoS to SoA) and their impact on cache locality and vectorization, which, while not as specific to AoSoA, underpin the rationale that a well-chosen layout can substantially impact memory latency hiding and execution-unit utilization. An example discusses AoS vs SoA cache miss scenarios, illustrating practical performance implications of data layouts. Taken together, these excerpts provide direct and contextually supportive evidence for the field value’s claims about AoSoA improving locality via tile-based processing and aiding prefetching to hide memory latency.",
			"confidence": "medium"
		},
		{
			"field": "data_layout_optimization.transposition_strategy",
			"citations": [
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor.",
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code."
					]
				},
				{
					"title": "AoS and SoA",
					"url": "https://grokipedia.com/page/AoS_and_SoA",
					"excerpts": [
						"AoSoA) is a hybrid memory layout that combines the benefits of Array of Structures (AoS) and Structure of Arrays (SoA) by partitioning an array into fixed-size blocks—typically 4 or 8 elements per block—and organizing the fields within each block in a contiguous, SoA manner",
						"This structure supports efficient construction by transposing data from an initial AoS array into the hybrid format, often using strided copies.",
						"Structure of Arrays (SoA) layout facilitates vectorized updates across multiple particles or rays, such as adding velocity components to position arrays using Single Instruction, Multiple Data (SIMD) instructions, enabling parallel computation of updates like p i ​ = p i ​ + v i ​ Δ t for all i without interleaving data."
					]
				},
				{
					"title": "c - SOA (Structure of array) vs AOS (Array of Structure) - Stack Overflow",
					"url": "https://stackoverflow.com/questions/72840933/soa-structure-of-array-vs-aos-array-of-structure",
					"excerpts": [
						"The core idea is to use structures matching the access patterns in the application, ideally you want a sequential memory access aligned to cache lines.Read more"
					]
				},
				{
					"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
					"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
					"excerpts": [
						"For a warp of 32 threads accessing particle positions:\n• **AoS:** 32 transactions × 32 bytes = 1024 bytes transferred, 96 bytes used (9.4% efficiency)\n• **SoA:** 1 transaction × 128 bytes = 128 bytes transferred, 128 bytes used (100% efficiency)\n• **Result:** SoA is 10.7x more bandwidth efficient on GPU"
					]
				},
				{
					"title": "AoS and SoA - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
					"excerpts": [" ,"]
				}
			],
			"reasoning": "The field value centers on a one-time data layout transformation from an initial AoS arrangement to AoSoA to unlock parallelism, specifically via SIMD-enabled transposition. The most directly relevant excerpts explicitly discuss converting data from AoS to AoSoA and performing the transposition in a way that preserves locality and enables vectorization. One excerpt states that converting AoS to SoA (and specifically to AoSoA) can yield locality and unit-stride gains, which is the essence of the proposed transposition strategy. Another excerpt notes that AoSoA is a hybrid layout designed to combine the benefits of AoS and SoA, and emphasizes fixed blocks and contiguous field storage to improve vectorization, which aligns with the proposed transposition approach. A third excerpt explicitly discusses “transposing data from an initial AoS array into the hybrid format,” which maps directly to the one-time AoS-to-AoSoA step in the field value. A fourth excerpt reinforces the same concept by describing AoS-to-SOA/AoSoA transformations as a means to improve vectorization and performance. A fifth excerpt provides a broader high-level justification for such layout changes, referencing the gains from transforming from AoS to AoSoA. A sixth excerpt discusses the general performance benefits of SoA layouts with vectorized updates, supporting the idea that reorganizing data by fields aids SIMD usage. A seventh excerpt adds a related perspective on memory layout transformations and the potential need for AoSoA in terms of locality and unit-stride access. A final couple of excerpts give additional context on transposition-related performance considerations in memory-layout optimization literature, which underpin the rationale for the proposed transposition strategy.",
			"confidence": "high"
		},
		{
			"field": "data_layout_optimization.gather_instruction_avoidance",
			"citations": [
				{
					"title": "Memory Layout Transformations  ",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/memory-layout-transformations.html",
					"excerpts": [
						"The SOA form does come at the cost of reducing locality between accesses to multiple fields of the original structure instance (thus increasing TLB (Translation Look-aside Buffer) pressure for example). Depending on the data-access pattern in the original source-code (whether or not they involve accesses to multiple fields of the structure within a loop, whether or not all structure instances are traversed or a subset), it may be preferable to consider a conversion to **Array-Of-Structure-Of-Arrays (AOSOA)** instead. The idea here is to get the benefit of locality at the outer-level and also unit-stride at the innermost-level. The inner-array can be a small multiple of vector-length here to take advantage of unit-stride full-vectors. Each structure instance will have multiple fields laid out this way (in small arrays). And at the outer-level, you can have an array of the (now larger) structures. In such a layout, the field-accesses will still be close enough to get the benefit of page-locality for nearby structure-instance accesses in the original code.",
						"One class of user code changes for efficiency involves eliminating the use of memory gather-scatter operations. Such irregular memory operations can both increase latency and bandwidth usage, as well as limit the scope of compiler vectorization. Some applications may benefit from a data layout change that converts data structures written in an Array of Structures (AOS) representation to a Structure of Arrays (SOA) representation. This helps prevent gathers when accessing one field of the structure across the array elements, and helps the compiler vectorize loops that iterate over the array. Note that such data transformations have to be done at the program-level (by the user) taking into account all the places where those data structures are used. Doing it at just a loop-level will involve costly transformations between the formats before and after the loop.",
						"OS to SOA conversion:** A common optimization that helps prevent gathers and scatters in vectorized code is to convert data structures from Array-Of-Structures (AOS) to Structure-Of-Array (SOA) representation. Keeping separate arrays for each structure-field keeps memory accesses contiguous when vectorization is performed over structure instances. AOS structures require gathers and scatters, which can impact both SIMD efficiency as well as introduce extra bandwidth and latency for memory accesses. The presence of a hardware gather-scatter mechanism does not eliminate the need for this transformation - gather-scatter accesses commonly need significantly higher bandwidth and latency than contiguous loads",
						"This chapter examines a useful user code transformation: moving from data organized in an Array of Structures (AoS) to an organization of Stucture of Arrays (SoA). This transformation allows the compiler to access data more efficiently on the processor.",
						".\nHere is a simple example that shows the difference between these forms:\n ... \nSection Title: Memory Layout Transformations > Take Aways\nContent:\nAlthough it may seem more logical to organize data in Array of Structures (AoS), this organization makes it extremely difficult to access the memory for reads (gather) and writes (scatter). This prevents many optimizations by the compiler, most notably it often prevents efficient vectorization. Algorithms with this characteristic are often very poor performers on Intel® Xeon Architecture."
					]
				},
				{
					"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
					"excerpts": [
						"AVX2 has gathers (not scatters), but they're only fast on Skylake and newer*",
						"Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.",
						"So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather."
					]
				}
			],
			"reasoning": "The most directly relevant points advocate changing memory layouts to reduce or eliminate gather operations. One set of excerpts describes converting from Array-of-Structures (AoS) to Structure-of-Arrays (SOA) as a common optimization to keep memory accesses contiguous and to prevent the irregular memory accesses that gathers introduce. This directly aligns with the idea of avoiding gather instructions by reformatting data so that memory access patterns become unit-stride or contiguous across a vectorized workload. Additional excerpts articulate that memory-layout transformations are used to avoid gathers and to improve compiler vectorization, underscoring a strategy to expose parallelism by layout changes rather than by forcing gather-based paths. In further related guidance, sections discuss OS-to-SOA conversion and the broader Take-Aways from memory-layout transformations, reinforcing that avoiding irregular memory operations is a key performance tactic when vectorizing workloads. Other excerpts describe the practicalities of avoiding gathers by transforming to SoA and similar layouts, and discuss that avoiding irregular memory operations can improve latency and bandwidth characteristics, which supports the field value that the technique should be True. The later excerpts expand on nuanced aspects of memory-layout transformations (AOSOA hybrids, etc.) and reiterate that preventing gathers is beneficial for vectorization, thus strengthening the case for the True setting. Additional excerpts briefly discuss “what to do without fast gather and scatter” in AVX2 contexts, which, while slightly tangential, still reinforces the overarching theme of minimizing gather-like behavior to preserve performance and parallelism. Overall, the collected evidence consistently supports the notion that avoiding gather instructions through data-layout optimization (AoS vs SoA and related hybrids) is a valid approach to expose parallelism in a seemingly sequential hash-like computation, thereby supporting the field value as True.",
			"confidence": "high"
		},
		{
			"field": "dependency_chain_scheduling_theory",
			"citations": [
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"A dependency chain is a series of instructions where the result of each instruction is an\ninput for the next instruction. Each instruction must wait for the preceding instruction to\nfinish. This prevents the execution of multiple instructions in parallel or out of order.",
						"Loop-carried dependency chains can be broken by using multiple accumulators, as\nexplained on page 64. The maximum useful number of accumulators is the latency of the\nmost critical instruction in the dependency chain divided by the reciprocal throughput for that\ninstruction.",
						"Assuming that floating point addition has a latency of 4 and a reciprocal throughput of 1, the\noptimal implementation will have four accumulators so that we always have four additions in\nthe pipeline of the floating point adder.",
						"The most important way of improving the performance of CPU-intensive code is to do things\nin parallel."
					]
				},
				{
					"title": "Studying Agner Fog 2: Unrolling Loops and Dependency Chains - YouTube",
					"url": "https://www.youtube.com/watch?v=6TRt6_RHhjs",
					"excerpts": [
						"In this video we'll cover techniques to break dependency chains, and how to unroll a loop. We explore two different algorithms; ..."
					]
				},
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/Passes.html",
					"excerpts": [
						" ...",
						" ...",
						"Section Title: LLVM’s Analysis and Transform Passes [¶]( \"Link to this heading\") > ... > [`reassociate` : Reassociate expressions]() [¶]( \"Link to this heading\")\nContent:\nThis pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.\nFor example: 4 + (x + 5) ⇒ x + (4 + 5)\nIn the implementation of this algorithm, constants are assigned rank = 0,\nfunction arguments are rank = 1, and other values are assigned ranks\ncorresponding to the reverse post-order traversal of the current function (starting\nat 2), which effectively gives values in deep loops higher rank than values not\nin loops.",
						"It transforms a loop from:\nWhich can be seen as unrolling the outer loop and “jamming” (fusing) the inner\nloops into one. When variables or loads can be shared in the new inner loop, this\ncan lead to significant performance improvements. It uses [Dependence Analysis]() for proving the transformations are safe.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"Content:\nCombine instructions to form fewer, simpler instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.\nThis pass combines things like:\ninto:\nThis is a simple worklist-driven algorithm.\nThis pass guarantees that the following canonicalizations are performed on the\nprogram:\nIf a binary operator has a constant operand, it is moved to the right-hand\nside.\nBitwise operators with constant operands are always grouped so that shifts\nare performed first, then `or` s, then `and` s, then `xor` s.\nCompare instructions are converted from `<` , `>` , `≤` , or `≥` to `=` or `≠` if possible.\nAll `cmp` instructions on boolean values are replaced with logical\noperations.\n`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\nMultiplies with a constant power-of-two argument are transformed into\nshifts.\n… etc.\nThis "
					]
				},
				{
					"title": "Automated Techniques for Hash Function and Block ...",
					"url": "https://mouha.be/wp-content/uploads/phdthesis.pdf",
					"excerpts": [
						"In Fig. 4, a forward propagation (for *i* = 0 *,* 1 *, . . . ,* 31) is done where edges\nare removed if they start at an impossible input carry."
					]
				},
				{
					"title": "implementation - How is bitslicing faster? - Cryptography Stack Exchange",
					"url": "https://crypto.stackexchange.com/questions/30140/how-is-bitslicing-faster",
					"excerpts": [
						"A general advantage of bitslicing is that bitsliced code tends to run well on heavily [pipelined](https://en.wikipedia.org/wiki/Instruction_pipelining) modern CPU, since it tends to have a low risk of [pipeline stalls](https://en.wikipedia.org/wiki/Pipeline_stall) and plenty of opportunities for optimal instruction reordering. Basically, in normal crypto code, if you calculate something like:\nthen the XOR calculation must finish before the addition can start, preventing the operations from executing in parallel. In a bitsliced implementation, however, we can e.g. calculate the low-order bits of the XOR first, and then move on to the high-order ones, so that when we start doing the addition, the low-order bits that we need first will have had plenty of time to get ready.\nIf we want, we can also e.g. *interleave the XOR and addition calculations together* (or just let the compiler do that for us), only calculating the XOR bits just in time to be ready when we need them for the addition. This can reduce the potential for stalls *within* the addition code, by letting the processor calculate the XORs while it's waiting for the carry bits to be ready, and may also improve register and/or cache locality.\n",
						"Bitslicing is a technique where computation is:\nReduced to elementary operations (called *gates* ) with a single **bit** output (typically NOR, XOR, and similar like OR AND NAND NXOR, often with further restriction to two inputs), rather than operations on words or integers spanning several bits.\nExecuted in parallel, with as many simultaneous instances (on a single CPU) as there are bits in some register kind, performing NOR, XOR.. simultaneously on all bits of such registers.\nDoing 1 generates a large number of gates: XOR of two $n$ -bit words requires $n$ gates; for $n>1$ their addition modulo $2^n$ requires $5n-6$ gates; each 8-bit S-box lookup in AES SubBytes requires about 128 gates; on the other hand rotations, arbitrary bit shuffling, selection or expansion requires no gate.",
						"**Bit shifts, rotations and other bit permutations** become trivial in a bitsliced implementation, since they just correspond to relabeling variables. Thus, such operations effectively *take no time at all* in a bitsliced implementation. Since crypto code tends to use such operations a lot, this alone can be a *huge* win for bitslicing. (However, this is only true for *constant* bit shifts. If the shift amount depends on the data being processed, bitslicing it becomes *much* harder.) **Bitwise logical operations** like XOR tend to be near the break-even point. If you have two sets of 32 numbers, each of them with 32 bits, and you want to XOR them with each other, it doesn't really matter if you do it number by number or bit by bit — you'll need to do 32 XORs either way.Of course, if your CPU has, say, 64-bit or 128-bit registers (like, say, any modern x86 CPU with SSE does), then you can carry out 64 or 128 bitsliced 32-bit XORs, still using only 32 XOR instructions. Thus, bitslicing may still win out for XOR, just because of the extra parallelism. **Addition and subtraction** can also be implemented using bitwise logical operations, essentially by implementing an [adder circuit](https://en.wikipedia.org/wiki/Adder_%28electronics%29) .",
						"The idea is that you encode several bitwise operations within a single register. So, instead of 32 bitwise OR operations in sequence, you could reduce the total number of operations by cramming the data into SIMD registers and executing in parallel.\nSo you would change\ninto\nNow the downside here is that you have to get the data into the format the simd operation wants such that your result will be an easily consumed block of values. Basically you have to \"slice\":\nInto:\nWhich isn't free. However, with careful optimization this can result in a significant speed in many circumstances. As an additional bonus, this technique can be used to avoid table lookups in certain situations ( https://eprint.iacr.org/2015/727.pdf ) helping strengthen the implementation against cache timing attacks.\nThe reason this technique is well suited to crypto, is that cryptography often uses repeated bitwise operations that can easily be paralellized in this way. The others were right to point you toward the original papers for the implementation details of this:\nhttp://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1997/CS/CS0891.pdf\nis a good place to start. I'll give a quote from the original paper which sums up a few of the specific advantages of doing DES with SIMD on a 64bit computer:\n\"The eight S-Boxes can be applied in parallel and thus pipelining can use it without pipeline stalls\""
					]
				},
				{
					"title": "An Implementation of Swing Modulo Scheduling with ...",
					"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
					"excerpts": [
						"MII is defined to be the maximum of the resource constrained II (ResMII), and recurrence\nconstrained II (RecMII) of the loop.",
						"The idea behind Modulo Scheduling can easily be illustrated with the following example. Figure 1.1\nshows a sequence of instructions that represent the body of a loop. Assume that this sequence of\ninstructions reads a value from an array, adds some constant to that value, and stores that value.\nFor this simple example, there are only data dependences within one iteration, since subsequent\niterations access a different element within the same array.",
						"Modulo Scheduling is a family of Software Pipelining\ntechniques that uses a “modulo” technique (instead of maximal unrolling) to place instructions in\nthe schedule such that when iterations are overlapped there are no resource or data conflicts.",
						"Modulo Scheduling is traditionally restricted to single basic block loops without control flow,\nwhich can limit the number of candidate loops. Global Software Pipelining techniques have emerged\nto exploit some of the opportunities for ILP in multiple basic block loops that frequently occur in\ncomputation intensive applications",
						"Modulo Scheduling algorithms aim to create a schedule with\nan Initiation Interval (II) equal to MII, which is the smallest II possible and results in the most\noptimal schedule."
					]
				},
				{
					"title": "Usuba, Optimizing Bitslicing Compiler",
					"url": "https://theses.hal.science/tel-03133456v2/file/MERCADIER_Darius_2020.pdf",
					"excerpts": [
						"Bitslicing prevents from using CPU arithmetic instructions (addition, multiplication *etc.* ),\nsince each *n* -bit number is represented by 1 bit in *n* distinct registers. Instead, bitsliced\nprograms must re-implement binary arithmetic, using solely bitwise instructions."
					]
				},
				{
					"title": "Analysis and Design of Symmetric Cryptographic Algorithms",
					"url": "https://opus4.kobv.de/opus4-uni-passau/files/331/thesis_jovanovic.pdf",
					"excerpts": [
						"ion** H\nAddition of two integers *x* and *y* can be written as\n*x* + *y* = ( *x* *⊕* *y* ) + (( *x* *∧* *y* ) *≪* 1)\naccording to [28, 164]. In NORX we started from the above representation but replaced\ninteger addition with bitwise logical XOR which results in the non-linear operation H of\nthe following form"
					]
				},
				{
					"title": "The GNU C Reference Manual",
					"url": "https://www.gnu.org/software/gnu-c-manual/gnu-c-manual.html",
					"excerpts": [
						"This is a reference manual for the C programming language as implemented by the GNU Compiler Collection (GCC).Read more"
					]
				},
				{
					"title": "Tree Height Reduction - one2bla.me",
					"url": "https://one2bla.me/cs6290/lesson10/tree-height-reduction.html",
					"excerpts": [
						"Using **Tree Height Reduction** , compilers remove dependency chains from the program\nby identifying operations than can be conducted concurrently and separating\nthem. In the example below, our code wants to store the summation of four\noperands into the register `R8` . Without **Tree Height Reduction** , our code would\ncreate a dependency chain as we execute sequential instructions to add to\nregister `R8` . With **Tree Height Reduction** , the compiler groups the operations\nand removes the dependency by using another register, `R7` , to hold the\nintermediate value."
					]
				},
				{
					"title": "scale.score.trans Class TreeHeight",
					"url": "https://www.cs.utexas.edu/~cart/Scale/javadoc/scale/score/trans/TreeHeight.html",
					"excerpts": [
						"The tree height reduction optimization attempts to improve the\nperformance of individual expressions by allowing more operations\nto be performed in parallel. For eample, consider the expression `(a + (b + (c + d)))` . Using this order of precedence,\nthe three addition operations must be performed sequentially. If\nthe expression is transformed to `((a + b) + (c + d))` then two of the additons may be performed simultaneously. This\nwill benefit superscalar architectures."
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://www.researchgate.net/publication/2941400_Swing_Modulo_Scheduling_A_Lifetime-Sensitive_Approach",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://ieeexplore.ieee.org/document/554030/",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "A Software Pipelining Framework for Simple Processor Cores",
					"url": "https://www.capsl.udel.edu/conferences/open64/2009/Papers/112-SoftwarePipeline.pdf",
					"excerpts": [
						"Modulo scheduling requires a fixed initiation interval (II),\nfor which it tries to find a valid schedule. The lower the II,\nthe shorter is the resulting runtime."
					]
				},
				{
					"title": "Class 12 Software Pipelining - Modulo Scheduling",
					"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
					"excerpts": [
						"-\nMinimum Initiation Interval (MII)\n❖ Remember, II = number of cycles between the start of\nsuccessive iterations\n❖ Modulo scheduling requires a candidate II be selected\nbefore scheduling is attempted\n» Try candidate II, see if it works\n» If not, increase by 1, try again repeating until successful\n❖ MII is a lower bound on the II\n» MII = Max(ResMII, RecMII)\n» ResMII",
						"ResMII\nSimple resource model\nA processor has a set of resources R. For each resource r in R\nthere is count(r) specifying the number of identical copies\nConcept: If there were no dependences between the operations, what\nis the the shortest possible schedule?\nResMII = MAX (uses(r) / count(r))\nfor all r in R\nuses(r) = number of times the resource is used in 1 iteration",
						"RecMII\nApproach: Enumerate all irredundant elementary circuits in the\ndependence graph\nRecMII = MAX (delay(c) / distance(c))\nfor all c in C\ndelay(c) = total latency in dependence cycle c (sum of delays)\ndistance(c) = total iteration distance of cycle c (sum of distances)"
					]
				}
			],
			"reasoning": "The core challenge identified is the sequential dependency chain within each hash computation, which limits parallelism. The research points to several key concepts and techniques for addressing this:\n\n**Dependency Chains and RecMII:** Excerpts like 1464, 1460, 1468, 1430, 1473, 1690, 1699, and 1336 directly discuss dependency chains and how to break them or mitigate their impact. Specifically, the concept of Recurrence-constrained Minimum Initiation Interval (RecMII) is crucial, as it represents the theoretical limit imposed by these chains. Excerpts 1417, 1418, and 1419 explain how RecMII is calculated based on instruction latencies and loop-carried dependencies.\n\n**Software Pipelining and Modulo Scheduling:** Techniques like software pipelining and modulo scheduling are highlighted as methods to overlap loop iterations and hide latency, thereby improving throughput. Excerpts 1330, 1334, 1345, 1347, and 1355 describe these techniques, with 1347 and 1355 emphasizing the goal of achieving the Minimum Initiation Interval (MII) for optimal performance.\n\n**N-way Interleaving and Multiple Accumulators:** To break the dependency chain and process multiple independent hashes concurrently, N-way interleaving or using multiple accumulators is identified as the most effective strategy. Excerpts 1460 and 1468 explain how multiple accumulators can be used to match instruction latency and reciprocal throughput, effectively transforming the problem from latency-bound to throughput-bound.\n\n**Vector-Interleaved Schedules:** The most advanced form of interleaving involves using SIMD instructions (like AVX-512) to process vectors of hashes. This approach, described in excerpts like 1326, 1227, 1231, 1333, 1334, 1335, 1226, 1227, 1232, 1317, 1329, 1330, 1333, and 1336, allows for parallel execution across multiple bits or lanes, saturating execution ports and hiding the dependency chain's latency.\n\n**Reassociation and Tree Height Reduction:** Compiler optimizations like reassociation and tree height reduction are mentioned as ways to reorder expressions, balance computation trees, and expose more Instruction-Level Parallelism (ILP). Excerpts 1430, 1439, 1447, 1448, 1690, and 1699 discuss these techniques, with 1447 and 1690 specifically detailing how reassociation and tree height reduction can break dependency chains by exposing parallelism.",
			"confidence": "high"
		},
		{
			"field": "dependency_chain_scheduling_theory.most_effective_technique",
			"citations": [
				{
					"title": "An Implementation of Swing Modulo Scheduling with ...",
					"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
					"excerpts": [
						"MII is defined to be the maximum of the resource constrained II (ResMII), and recurrence\nconstrained II (RecMII) of the loop.",
						"The idea behind Modulo Scheduling can easily be illustrated with the following example. Figure 1.1\nshows a sequence of instructions that represent the body of a loop. Assume that this sequence of\ninstructions reads a value from an array, adds some constant to that value, and stores that value.\nFor this simple example, there are only data dependences within one iteration, since subsequent\niterations access a different element within the same array.",
						"Modulo Scheduling is a family of Software Pipelining\ntechniques that uses a “modulo” technique (instead of maximal unrolling) to place instructions in\nthe schedule such that when iterations are overlapped there are no resource or data conflicts.",
						"Modulo Scheduling is traditionally restricted to single basic block loops without control flow,\nwhich can limit the number of candidate loops. Global Software Pipelining techniques have emerged\nto exploit some of the opportunities for ILP in multiple basic block loops that frequently occur in\ncomputation intensive applications",
						"Modulo Scheduling algorithms aim to create a schedule with\nan Initiation Interval (II) equal to MII, which is the smallest II possible and results in the most\noptimal schedule."
					]
				},
				{
					"title": "Usuba, Optimizing Bitslicing Compiler",
					"url": "https://theses.hal.science/tel-03133456v2/file/MERCADIER_Darius_2020.pdf",
					"excerpts": [
						"Bitslicing prevents from using CPU arithmetic instructions (addition, multiplication *etc.* ),\nsince each *n* -bit number is represented by 1 bit in *n* distinct registers. Instead, bitsliced\nprograms must re-implement binary arithmetic, using solely bitwise instructions."
					]
				},
				{
					"title": "implementation - How is bitslicing faster? - Cryptography Stack Exchange",
					"url": "https://crypto.stackexchange.com/questions/30140/how-is-bitslicing-faster",
					"excerpts": [
						"**Bit shifts, rotations and other bit permutations** become trivial in a bitsliced implementation, since they just correspond to relabeling variables. Thus, such operations effectively *take no time at all* in a bitsliced implementation. Since crypto code tends to use such operations a lot, this alone can be a *huge* win for bitslicing. (However, this is only true for *constant* bit shifts. If the shift amount depends on the data being processed, bitslicing it becomes *much* harder.) **Bitwise logical operations** like XOR tend to be near the break-even point. If you have two sets of 32 numbers, each of them with 32 bits, and you want to XOR them with each other, it doesn't really matter if you do it number by number or bit by bit — you'll need to do 32 XORs either way.Of course, if your CPU has, say, 64-bit or 128-bit registers (like, say, any modern x86 CPU with SSE does), then you can carry out 64 or 128 bitsliced 32-bit XORs, still using only 32 XOR instructions. Thus, bitslicing may still win out for XOR, just because of the extra parallelism. **Addition and subtraction** can also be implemented using bitwise logical operations, essentially by implementing an [adder circuit](https://en.wikipedia.org/wiki/Adder_%28electronics%29) .",
						"Bitslicing is a technique where computation is:\nReduced to elementary operations (called *gates* ) with a single **bit** output (typically NOR, XOR, and similar like OR AND NAND NXOR, often with further restriction to two inputs), rather than operations on words or integers spanning several bits.\nExecuted in parallel, with as many simultaneous instances (on a single CPU) as there are bits in some register kind, performing NOR, XOR.. simultaneously on all bits of such registers.\nDoing 1 generates a large number of gates: XOR of two $n$ -bit words requires $n$ gates; for $n>1$ their addition modulo $2^n$ requires $5n-6$ gates; each 8-bit S-box lookup in AES SubBytes requires about 128 gates; on the other hand rotations, arbitrary bit shuffling, selection or expansion requires no gate.",
						"The idea is that you encode several bitwise operations within a single register. So, instead of 32 bitwise OR operations in sequence, you could reduce the total number of operations by cramming the data into SIMD registers and executing in parallel.\nSo you would change\ninto\nNow the downside here is that you have to get the data into the format the simd operation wants such that your result will be an easily consumed block of values. Basically you have to \"slice\":\nInto:\nWhich isn't free. However, with careful optimization this can result in a significant speed in many circumstances. As an additional bonus, this technique can be used to avoid table lookups in certain situations ( https://eprint.iacr.org/2015/727.pdf ) helping strengthen the implementation against cache timing attacks.\nThe reason this technique is well suited to crypto, is that cryptography often uses repeated bitwise operations that can easily be paralellized in this way. The others were right to point you toward the original papers for the implementation details of this:\nhttp://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1997/CS/CS0891.pdf\nis a good place to start. I'll give a quote from the original paper which sums up a few of the specific advantages of doing DES with SIMD on a 64bit computer:\n\"The eight S-Boxes can be applied in parallel and thus pipelining can use it without pipeline stalls\""
					]
				},
				{
					"title": "Automated Techniques for Hash Function and Block ...",
					"url": "https://mouha.be/wp-content/uploads/phdthesis.pdf",
					"excerpts": [
						"In Fig. 4, a forward propagation (for *i* = 0 *,* 1 *, . . . ,* 31) is done where edges\nare removed if they start at an impossible input carry."
					]
				},
				{
					"title": "The GNU C Reference Manual",
					"url": "https://www.gnu.org/software/gnu-c-manual/gnu-c-manual.html",
					"excerpts": [
						"This is a reference manual for the C programming language as implemented by the GNU Compiler Collection (GCC).Read more"
					]
				},
				{
					"title": "Tree Height Reduction - one2bla.me",
					"url": "https://one2bla.me/cs6290/lesson10/tree-height-reduction.html",
					"excerpts": [
						"Using **Tree Height Reduction** , compilers remove dependency chains from the program\nby identifying operations than can be conducted concurrently and separating\nthem. In the example below, our code wants to store the summation of four\noperands into the register `R8` . Without **Tree Height Reduction** , our code would\ncreate a dependency chain as we execute sequential instructions to add to\nregister `R8` . With **Tree Height Reduction** , the compiler groups the operations\nand removes the dependency by using another register, `R7` , to hold the\nintermediate value."
					]
				},
				{
					"title": "scale.score.trans Class TreeHeight",
					"url": "https://www.cs.utexas.edu/~cart/Scale/javadoc/scale/score/trans/TreeHeight.html",
					"excerpts": [
						"The tree height reduction optimization attempts to improve the\nperformance of individual expressions by allowing more operations\nto be performed in parallel. For eample, consider the expression `(a + (b + (c + d)))` . Using this order of precedence,\nthe three addition operations must be performed sequentially. If\nthe expression is transformed to `((a + b) + (c + d))` then two of the additons may be performed simultaneously. This\nwill benefit superscalar architectures."
					]
				},
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/Passes.html",
					"excerpts": [
						"It transforms a loop from:\nWhich can be seen as unrolling the outer loop and “jamming” (fusing) the inner\nloops into one. When variables or loads can be shared in the new inner loop, this\ncan lead to significant performance improvements. It uses [Dependence Analysis]() for proving the transformations are safe.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"Content:\nCombine instructions to form fewer, simpler instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.\nThis pass combines things like:\ninto:\nThis is a simple worklist-driven algorithm.\nThis pass guarantees that the following canonicalizations are performed on the\nprogram:\nIf a binary operator has a constant operand, it is moved to the right-hand\nside.\nBitwise operators with constant operands are always grouped so that shifts\nare performed first, then `or` s, then `and` s, then `xor` s.\nCompare instructions are converted from `<` , `>` , `≤` , or `≥` to `=` or `≠` if possible.\nAll `cmp` instructions on boolean values are replaced with logical\noperations.\n`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\nMultiplies with a constant power-of-two argument are transformed into\nshifts.\n… etc.\nThis "
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://www.researchgate.net/publication/2941400_Swing_Modulo_Scheduling_A_Lifetime-Sensitive_Approach",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://ieeexplore.ieee.org/document/554030/",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "A Software Pipelining Framework for Simple Processor Cores",
					"url": "https://www.capsl.udel.edu/conferences/open64/2009/Papers/112-SoftwarePipeline.pdf",
					"excerpts": [
						"Modulo scheduling requires a fixed initiation interval (II),\nfor which it tries to find a valid schedule. The lower the II,\nthe shorter is the resulting runtime."
					]
				},
				{
					"title": "Class 12 Software Pipelining - Modulo Scheduling",
					"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
					"excerpts": [
						"-\nMinimum Initiation Interval (MII)\n❖ Remember, II = number of cycles between the start of\nsuccessive iterations\n❖ Modulo scheduling requires a candidate II be selected\nbefore scheduling is attempted\n» Try candidate II, see if it works\n» If not, increase by 1, try again repeating until successful\n❖ MII is a lower bound on the II\n» MII = Max(ResMII, RecMII)\n» ResMII",
						"ResMII\nSimple resource model\nA processor has a set of resources R. For each resource r in R\nthere is count(r) specifying the number of identical copies\nConcept: If there were no dependences between the operations, what\nis the the shortest possible schedule?\nResMII = MAX (uses(r) / count(r))\nfor all r in R\nuses(r) = number of times the resource is used in 1 iteration",
						"RecMII\nApproach: Enumerate all irredundant elementary circuits in the\ndependence graph\nRecMII = MAX (delay(c) / distance(c))\nfor all c in C\ndelay(c) = total latency in dependence cycle c (sum of delays)\ndistance(c) = total iteration distance of cycle c (sum of distances)"
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"Loop-carried dependency chains can be broken by using multiple accumulators, as\nexplained on page 64. The maximum useful number of accumulators is the latency of the\nmost critical instruction in the dependency chain divided by the reciprocal throughput for that\ninstruction.",
						"Assuming that floating point addition has a latency of 4 and a reciprocal throughput of 1, the\noptimal implementation will have four accumulators so that we always have four additions in\nthe pipeline of the floating point adder.",
						"The most important way of improving the performance of CPU-intensive code is to do things\nin parallel."
					]
				}
			],
			"reasoning": "The most directly applicable information points state that breaking loop-carried dependency chains can be achieved by using multiple accumulators, and that this enables overlapping of dependent operations so that you continuously feed the pipeline with ready data. This aligns with the idea of N-way interleaving where several independent hash instances proceed in parallel, effectively converting a latency-bound pattern into throughput-bound execution by presenting the processor with more simultaneous work. Further, general guidance that the primary way to improve CPU-intensive code performance is to run tasks in parallel reinforces that interleaving several hash computations can saturate available execution units. Additional sources describe software-pipelining techniques and modulo scheduling as formal methods to arrange instructions so that independent iterations are overlapped without resource collisions, which supports building a vectorized interleaved schedule that processes a batch of hashes (e.g., a vector of 16) at once. Details about accumulation and latency/throughput considerations show how many accumulators are needed based on instruction latency and throughput, which directly informs how many parallel streams (hash instances) can be sustained without stalling. In sum, the cited material supports the strategy of using multiple accumulators and interleaving across independent hash instances to transform a long dependency chain into high-throughput execution through parallelism and vectorization.",
			"confidence": "high"
		},
		{
			"field": "dependency_chain_scheduling_theory.key_concepts",
			"citations": [
				{
					"title": "An Implementation of Swing Modulo Scheduling with ...",
					"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
					"excerpts": [
						"MII is defined to be the maximum of the resource constrained II (ResMII), and recurrence\nconstrained II (RecMII) of the loop.",
						"Modulo Scheduling algorithms aim to create a schedule with\nan Initiation Interval (II) equal to MII, which is the smallest II possible and results in the most\noptimal schedule.",
						"Modulo Scheduling is a family of Software Pipelining\ntechniques that uses a “modulo” technique (instead of maximal unrolling) to place instructions in\nthe schedule such that when iterations are overlapped there are no resource or data conflicts.",
						"Modulo Scheduling is traditionally restricted to single basic block loops without control flow,\nwhich can limit the number of candidate loops. Global Software Pipelining techniques have emerged\nto exploit some of the opportunities for ILP in multiple basic block loops that frequently occur in\ncomputation intensive applications"
					]
				},
				{
					"title": "implementation - How is bitslicing faster? - Cryptography Stack Exchange",
					"url": "https://crypto.stackexchange.com/questions/30140/how-is-bitslicing-faster",
					"excerpts": [
						"Bitslicing is a technique where computation is:\nReduced to elementary operations (called *gates* ) with a single **bit** output (typically NOR, XOR, and similar like OR AND NAND NXOR, often with further restriction to two inputs), rather than operations on words or integers spanning several bits.\nExecuted in parallel, with as many simultaneous instances (on a single CPU) as there are bits in some register kind, performing NOR, XOR.. simultaneously on all bits of such registers.\nDoing 1 generates a large number of gates: XOR of two $n$ -bit words requires $n$ gates; for $n>1$ their addition modulo $2^n$ requires $5n-6$ gates; each 8-bit S-box lookup in AES SubBytes requires about 128 gates; on the other hand rotations, arbitrary bit shuffling, selection or expansion requires no gate.",
						"**Bit shifts, rotations and other bit permutations** become trivial in a bitsliced implementation, since they just correspond to relabeling variables. Thus, such operations effectively *take no time at all* in a bitsliced implementation. Since crypto code tends to use such operations a lot, this alone can be a *huge* win for bitslicing. (However, this is only true for *constant* bit shifts. If the shift amount depends on the data being processed, bitslicing it becomes *much* harder.) **Bitwise logical operations** like XOR tend to be near the break-even point. If you have two sets of 32 numbers, each of them with 32 bits, and you want to XOR them with each other, it doesn't really matter if you do it number by number or bit by bit — you'll need to do 32 XORs either way.Of course, if your CPU has, say, 64-bit or 128-bit registers (like, say, any modern x86 CPU with SSE does), then you can carry out 64 or 128 bitsliced 32-bit XORs, still using only 32 XOR instructions. Thus, bitslicing may still win out for XOR, just because of the extra parallelism. **Addition and subtraction** can also be implemented using bitwise logical operations, essentially by implementing an [adder circuit](https://en.wikipedia.org/wiki/Adder_%28electronics%29) .",
						"The idea is that you encode several bitwise operations within a single register. So, instead of 32 bitwise OR operations in sequence, you could reduce the total number of operations by cramming the data into SIMD registers and executing in parallel.\nSo you would change\ninto\nNow the downside here is that you have to get the data into the format the simd operation wants such that your result will be an easily consumed block of values. Basically you have to \"slice\":\nInto:\nWhich isn't free. However, with careful optimization this can result in a significant speed in many circumstances. As an additional bonus, this technique can be used to avoid table lookups in certain situations ( https://eprint.iacr.org/2015/727.pdf ) helping strengthen the implementation against cache timing attacks.\nThe reason this technique is well suited to crypto, is that cryptography often uses repeated bitwise operations that can easily be paralellized in this way. The others were right to point you toward the original papers for the implementation details of this:\nhttp://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1997/CS/CS0891.pdf\nis a good place to start. I'll give a quote from the original paper which sums up a few of the specific advantages of doing DES with SIMD on a 64bit computer:\n\"The eight S-Boxes can be applied in parallel and thus pipelining can use it without pipeline stalls\""
					]
				},
				{
					"title": "A Software Pipelining Framework for Simple Processor Cores",
					"url": "https://www.capsl.udel.edu/conferences/open64/2009/Papers/112-SoftwarePipeline.pdf",
					"excerpts": [
						"Modulo scheduling requires a fixed initiation interval (II),\nfor which it tries to find a valid schedule. The lower the II,\nthe shorter is the resulting runtime."
					]
				},
				{
					"title": "Class 12 Software Pipelining - Modulo Scheduling",
					"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
					"excerpts": [
						"-\nMinimum Initiation Interval (MII)\n❖ Remember, II = number of cycles between the start of\nsuccessive iterations\n❖ Modulo scheduling requires a candidate II be selected\nbefore scheduling is attempted\n» Try candidate II, see if it works\n» If not, increase by 1, try again repeating until successful\n❖ MII is a lower bound on the II\n» MII = Max(ResMII, RecMII)\n» ResMII"
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://www.researchgate.net/publication/2941400_Swing_Modulo_Scheduling_A_Lifetime-Sensitive_Approach",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://ieeexplore.ieee.org/document/554030/",
					"excerpts": [
						"This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS). It generates schedules that are near optimal.Read more"
					]
				},
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/Passes.html",
					"excerpts": [
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"Content:\nCombine instructions to form fewer, simpler instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.\nThis pass combines things like:\ninto:\nThis is a simple worklist-driven algorithm.\nThis pass guarantees that the following canonicalizations are performed on the\nprogram:\nIf a binary operator has a constant operand, it is moved to the right-hand\nside.\nBitwise operators with constant operands are always grouped so that shifts\nare performed first, then `or` s, then `and` s, then `xor` s.\nCompare instructions are converted from `<` , `>` , `≤` , or `≥` to `=` or `≠` if possible.\nAll `cmp` instructions on boolean values are replaced with logical\noperations.\n`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\nMultiplies with a constant power-of-two argument are transformed into\nshifts.\n… etc.\nThis "
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"The most important way of improving the performance of CPU-intensive code is to do things\nin parallel.",
						"Loop-carried dependency chains can be broken by using multiple accumulators, as\nexplained on page 64. The maximum useful number of accumulators is the latency of the\nmost critical instruction in the dependency chain divided by the reciprocal throughput for that\ninstruction."
					]
				},
				{
					"title": "Tree Height Reduction - one2bla.me",
					"url": "https://one2bla.me/cs6290/lesson10/tree-height-reduction.html",
					"excerpts": [
						"Using **Tree Height Reduction** , compilers remove dependency chains from the program\nby identifying operations than can be conducted concurrently and separating\nthem. In the example below, our code wants to store the summation of four\noperands into the register `R8` . Without **Tree Height Reduction** , our code would\ncreate a dependency chain as we execute sequential instructions to add to\nregister `R8` . With **Tree Height Reduction** , the compiler groups the operations\nand removes the dependency by using another register, `R7` , to hold the\nintermediate value."
					]
				}
			],
			"reasoning": "- The concept that RecMII is a computation across a dependency cycle and governs the initiation spacing between iterations is directly supported by passages describing RecMII as the recurrence-constrained initiation interval and its relationship to the loop’s dependence cycle latency. This aligns with the field value’s assertion that RecMII equals the total latency across the dependence cycle and determines the minimum cycle distance between successive hash computations when the chain is not broken. \n- The idea that Modulo Scheduling, Swing Modulo Scheduling, and related software-pipelining techniques are used to overlap iterations and hide latency is reinforced by excerpts detailing how modulo scheduling places instructions in a schedule to keep resources busy and to achieve a desired initiation interval. The field value mentions Swing Modulo Scheduling as a technique; the excerpts explicitly describe this approach as a software-pipelining method that aims for near-optimal schedules and discusses its use to overlap iterations. \n- Reassociation and Tree Height Reduction are cited as methods to reorder expressions and reduce dependency depth, thereby exposing more instruction-level parallelism. These excerpts support the field value’s item about reassociation and tree-height reduction contributing to a more balanced computation tree and shorter critical paths. \n- Additional excerpts cover general principles of breaking dependency chains, software pipelining frameworks, and the role of MII/II in scheduling, which provide supportive background for understanding how RecMII and modulo scheduling interact in practice. \n- Overall, the most direct support comes from definitions of RecMII and its placement in scheduling theory, followed by concrete modulo scheduling techniques (including Swing) and then dependency-reducing strategies like reassociation and tree-height reduction. The collected information coherently backs the field value’s emphasis on recurrence-constrained scheduling, latency hiding via software pipelining, and expression restructuring to improve parallelism.",
			"confidence": "high"
		},
		{
			"field": "dependency_chain_scheduling_theory.primary_limitation_identified",
			"citations": [
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"A dependency chain is a series of instructions where the result of each instruction is an\ninput for the next instruction. Each instruction must wait for the preceding instruction to\nfinish. This prevents the execution of multiple instructions in parallel or out of order.",
						"Loop-carried dependency chains can be broken by using multiple accumulators, as\nexplained on page 64. The maximum useful number of accumulators is the latency of the\nmost critical instruction in the dependency chain divided by the reciprocal throughput for that\ninstruction.",
						"Assuming that floating point addition has a latency of 4 and a reciprocal throughput of 1, the\noptimal implementation will have four accumulators so that we always have four additions in\nthe pipeline of the floating point adder.",
						"The most important way of improving the performance of CPU-intensive code is to do things\nin parallel."
					]
				},
				{
					"title": "An Implementation of Swing Modulo Scheduling with ...",
					"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
					"excerpts": [
						"MII is defined to be the maximum of the resource constrained II (ResMII), and recurrence\nconstrained II (RecMII) of the loop."
					]
				},
				{
					"title": "Class 12 Software Pipelining - Modulo Scheduling",
					"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
					"excerpts": [
						"-\nMinimum Initiation Interval (MII)\n❖ Remember, II = number of cycles between the start of\nsuccessive iterations\n❖ Modulo scheduling requires a candidate II be selected\nbefore scheduling is attempted\n» Try candidate II, see if it works\n» If not, increase by 1, try again repeating until successful\n❖ MII is a lower bound on the II\n» MII = Max(ResMII, RecMII)\n» ResMII",
						"RecMII\nApproach: Enumerate all irredundant elementary circuits in the\ndependence graph\nRecMII = MAX (delay(c) / distance(c))\nfor all c in C\ndelay(c) = total latency in dependence cycle c (sum of delays)\ndistance(c) = total iteration distance of cycle c (sum of distances)"
					]
				}
			],
			"reasoning": "The central claim is that a loop-carried true data dependency in the hash stages prevents internal parallelism and thus drives a fundamental performance limitation. An excerpt describing a dependency chain clarifies that when instruction results feed the next instruction, parallel execution is blocked, because each step must wait for the previous one to finish. This directly supports the idea that the hash’s six-stage, 18-operation chain imposes a sequential bottleneck across iterations that hampers throughput when trying to sustain high utilization of wide execution resources.\n\nTo connect the limitation to a mitigation pathway, consider the notion that loop-carried dependency chains can be broken by using multiple accumulators. This excerpt explains that introducing several accumulators helps hide latency within the dependency chain by allowing independent partial results to be computed simultaneously, effectively increasing the useful parallelism available despite a long chain. This provides a concrete technique for alleviating the sequential bottleneck imposed by the chain.\n\nThe scheduling theory surrounding initiation intervals is then relevant. In this context, the idea that the initiation interval (II) can be constrained by resources (ResMII) and recurrence (RecMII) is critical. The excerpts define MII as the maximum of ResMII and RecMII for a loop, linking hardware resource availability and loop-carried dependencies to the minimal feasible stagger between starting successive iterations. This formalizes how the RecMII metric captures the fundamental limit due to data dependencies and recurrence in a loop.\n\nFurther detail on recurrence-constrained scheduling shows RecMII as a function of the delay in a dependence cycle and the distance between iterations, which directly translates into how many cycles are needed to safely initiate a new hash computation without violating dependencies. This provides a quantitative handle on the previously described limitation and how it governs achievable throughput.\n\nThe remaining excerpts reinforce the connection to practical mitigation strategies: (a) modulo scheduling and related software-pipelining concepts that aim to overlap iterations while respecting dependencies, (b) the role of a well-chosen MII/RecMII in scheduling loops, and (c) the general principle that parallelism can be exposed via restructuring and accumulation strategies rather than by changing the bit-level math of the hash itself. Together, these excerpts map the path from the identified dependency-induced bottleneck to concrete scheduling techniques intended to improve utilization of wide SIMD/ALU resources.",
			"confidence": "high"
		},
		{
			"field": "portable_simd_frameworks_evaluation",
			"citations": [
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0.",
						"Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.",
						" and performance-portable SIMD > []() []() Design philosophy\nContent:\nFor example, we write `ShiftLeft<3>(v)` instead of `v << 3` because MSVC 2017 (ARM64) does not propagate the literal ( https://godbolt.org/g/rKx5Ga ). Highway requires function-specific target attributes, supported by GCC 4.9 / Clang 3.9 / MSVC 2015. Efficient and safe runtime dispatch is important. Modules such as image or video codecs are typically embedded into larger applications such as browsers, so they cannot require separate binaries for each CPU. Libraries also cannot predict whether the application already uses AVX2 (and pays the frequency throttling cost), so this decision must be left to the application. Using only the lowest-common denominator instructions sacrifices too much performance. Therefore, we provide code paths for multiple instruction sets and choose the most suitable at runtime. To reduce overhead, dispatch should be hoisted to higher layers instead of checking inside every low-level function. Highway supports inlining functions in the same file or in *-inl.h headers. We generate all code paths from the same source to reduce implementation- and debugging cost. Not every CPU need be supported. For example, pre-SSE4.1 CPUs are increasingly rare and the AVX instruction set is limited to floating-point operations. To reduce code size and compile time, we provide specializations for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback"
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions.",
						"he argument or return type `V` denotes a vector\nwith `N` lanes, and `M` a mask. Operations limited to certain vector\ntypes begin with a constraint of the form `V` : `{prefixes}[{bits}]` .",
						" that overloaded operators were not supported on `RVV` and `SVE` until recently. Unfortunately, clang’s `SVE` comparison operators\nreturn integer vectors instead of the `svbool_t` type which exists for\nthis purpose. To ensure your code works on all targets, we recommend\ninstead using the corresponding equivalents mentioned in our description\nof each overloaded operator, especially for comparisons, for example `Lt` instead of `operator<",
						"Clang and GCC require opting into SIMD intrinsics, e.g. via `-mavx2` flags. However, the flag enables AVX2 instructions in the entire\ntranslation unit, which may violate the one-definition rule (that all\nversions of a function such as `std::abs` are equivalent, thus the\nlinker may choose any). This can cause crashes if non-SIMD functions are\ndefined outside of a target-specific namespace, and the linker happens\nto choose the AVX2 version, which means it may be called without\nverifying AVX2 is indeed supported."
					]
				},
				{
					"title": "GitHub - google/highway: Performance-portable, length-agnostic SIMD with runtime dispatch",
					"url": "https://github.com/google/highway",
					"excerpts": [
						"Content:\n[]()\nYou can use the `benchmark` inside examples/ as a starting point.\nA [quick-reference page](/google/highway/blob/master/g3doc/quick_reference.md) briefly lists all operations\nand their parameters, and the [instruction_matrix](/google/highway/blob/master/g3doc/instruction_matrix.pdf) indicates the number of instructions per operation.",
						"For static dispatch, `HWY_TARGET` will be the best available target among `HWY_BASELINE_TARGETS` , i.e. those allowed for use by the compiler (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ). Functions inside `HWY_NAMESPACE` can be called using `HWY_STATIC_DISPATCH(func)(args)` within\nthe same module they are defined in. You can call the function from other\nmodules by wrapping it in a regular function and declaring the regular\nfunction in a header.",
						"For dynamic dispatch, a table of function pointers is generated via the `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to\ncall the best function pointer for the current CPU's supported targets. A\nmodule is automatically compiled for each target in `HWY_TARGETS` (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ) if `HWY_TARGET_INCLUDE` is\ndefined and `foreach_target.h` is included. Note that the first invocation\nof `HWY_DYNAMIC_DISPATCH` , or each call to the pointer returned by the first\ninvocation of `HWY_DYNAMIC_POINTER` , involves some CPU detection overhead."
					]
				},
				{
					"title": "GitHub - jfalcou/eve: Expressive Vector Engine - SIMD in C++ Goes Brrrr",
					"url": "https://github.com/jfalcou/eve",
					"excerpts": [
						"In terms of SIMD extension sets, we actively support (ie code is optimised and regularly tested) the following:\nFull support with tests:\n**Intel**\nSSE2, SSSE3, SSE3, SSE4.1, SSE4.2\nAVX, AVX2, FMA3\nAVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)\n**ARM**\nNEON A32 (64 & 128 bits)\nNEON A64 (64 & 128 bits)\nASIMD\nSVE with fixed sizes of 128, 256 and 512 bits registers.\nPartial/In-progress support with minimal checks:\n**PowerPC**\nVMX\nVSX\nWe **do not support** ARM SVE with **dynamic size** or GPGPU; this is the job of another tool.",
						"SSE2, SSSE3, SSE3, SSE4.1, SSE4.2",
						"AVX, AVX2, FMA3",
						"AVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)",
						"NEON A32 (64 & 128 bits)",
						"NEON A64 (64 & 128 bits)",
						"ASIMD"
					]
				},
				{
					"title": "GitHub - xtensor-stack/xsimd: C++ wrappers for SIMD intrinsics and parallelized, optimized mathematical functions (SSE, AVX, AVX512, NEON, SVE, WebAssembly, VSX, RISC-V))",
					"url": "https://github.com/xtensor-stack/xsimd",
					"excerpts": [
						"| ARM | NEON, NEON64, SVE128/256/512 (fixed vector size) |"
					]
				},
				{
					"title": "Vc: Introduction",
					"url": "https://web-docs.gsi.de/~mkretz/Vc-1.0/intro.html",
					"excerpts": [
						"For example, AVX has instructions for 256-bit floating point registers, but only 128-bit integer instructions, which is why the integral [Vc::Vector](classVc_1_1Vector.html \"The main vector class for expressing data parallelism. \") types use the SSE implementation for AVX target systems."
					]
				},
				{
					"title": "Vc: Main Page",
					"url": "https://vcdevel.github.io/Vc-1.4.3/",
					"excerpts": [
						"The 1.x releases ship implementations for x86 SIMD instruction sets: SSE, AVX, AVX2, and the Xeon Phi (MIC)."
					]
				}
			],
			"reasoning": "Google Highway's API is highly expressive, offering platform-agnostic functions like `Add(a, b)`, `Xor(a, b)`, `ShiftLeftSame(v, bits)`, and `Set(D d, T val)` for broadcasting constants. It explicitly recommends using these explicit functions over operator overloading for maximum portability, particularly on SVE, and guides developers towards performance-portable code. The framework provides comprehensive architecture support, including SSE4, AVX2, AVX-512, NEON, and critically, ARM SVE with dynamic vector lengths. This is a significant advantage over other libraries like EVE and xsimd, which only support SVE with fixed-size vectors, and Vc, which lacks support for AVX-512, NEON, and SVE entirely. Highway is designed to generate high-quality code comparable to hand-written intrinsics through its use of platform-specific backends and a robust dynamic dispatch system. It uses macros like `HWY_EXPORT` and `HWY_DYNAMIC_DISPATCH` to automatically select the best available instruction set at runtime, and provides an `instruction_matrix.pdf` to detail the instruction cost of operations. This combination of a performance-oriented API, broad and advanced architecture support (especially for dynamic SVE), and strong code quality makes Google Highway the most suitable and strongly recommended framework for writing high-performance, portable, and future-proof SIMD code.",
			"confidence": "high"
		},
		{
			"field": "verification_and_benchmarking_methodology",
			"citations": [
				{
					"title": "Unit 5: Performance & Benchmarking",
					"url": "https://www.cis.upenn.edu/~cis5710/spring2021/slides/05_performance.pdf",
					"excerpts": [
						"Measure something that runs for at least several seconds",
						"Understand the system being measured",
						"Perform many runs: how many is enough?",
						"Take odd number of samples"
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"We compile the 64-bit C++ implementations with a patched GCC 4.9 and run on a\nsingle idle core of a Xeon E5-2690 v3 clocked at 2.6 GHz.",
						"Note that previous measurements included the initialization of their input,\nwhich dramatically increased timings especially for small inputs.",
						"It uses CPU fences and robust statistics to minimize variability, and also\navoids unrealistic branch prediction effects."
					]
				},
				{
					"title": "Zen 5's AVX-512 Frequency Behavior - by Chester Lam",
					"url": "https://chipsandcheese.com/p/zen-5s-avx-512-frequency-behavior",
					"excerpts": [
						"Quickly switching between scalar integer and heavy AVX-512 code basically spreads out the transition, so that the core eventually converges at a sustainable clock speed for the AVX-512 sequence in question.",
						"If there is a transition period for the increased power draw, it’s so tiny that it can be ignored."
					]
				},
				{
					"title": "\n\tSolved: Re: Poor HPL performance of Icelake Server - Intel Community\n",
					"url": "https://community.intel.com/t5/Software-Tuning-Performance/Poor-HPL-performance-of-Icelake-Server/m-p/1379569",
					"excerpts": [
						"Content:\nComputing the \"Peak GFLOPS\" using the \"Base AVX-512 frequency\" will give a *lower bound* on the peak GFLOPS. If the processor does not sustain that frequency while running HPL, there is probably a problem with the processor or the cooling or the power supply. All of our systems are purchased with an HPL performance requirement that corresponds to a sustained frequency that is slightly higher than the \"Base AVX-512\" frequency (though the details differ for each generation of processors). Computing the \"Peak GFLOPS\" using the \"Max All-core AVX-512 Turbo frequency\" will give an *upper bound* on the peak GFLOPS, but due to power-limited frequency throttling, it is no longer as tight an upper bound as it used to be on earlier processors. Wh",
						"sors. When running HPL, every processor will adjust its frequency to run as close to the TDP as possible. The resulting average frequency will depend on the intrinsic leakage properties of the processor die combined with the specifics of the cooling system. (E.g., in many systems one socket is \"downwind\" of the other, so tends to run hotter. Leakage current increases exponentially with temperature, so these processors will have to reduce the frequency more to keep power consumption under TDP.) The average frequency should be between the \"Base AVX-512 frequency\" and the \"Max All-core AVX-512 Turbo frequency\", but sometimes this is a pretty wide range.\n ",
						"The average frequency should be between the \"Base AVX-512 frequency\" and the \"Max All-core AVX-512 Turbo frequency\", but sometimes this is a pretty wide range.\n ... "
					]
				},
				{
					"title": "AVX-512: when and how to use these new instructions",
					"url": "https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/",
					"excerpts": [
						"The processor does not immediately move to a higher license when encountering heavy instructions: it will first execute these instructions with reduced performance (say 4x slower) and only when there are many of them will the processor change its frequency.",
						"However, some of these instructions use a lot of power and generate a lot of heat. To keep power usage within bounds, Intel reduces the frequency of the cores dynamically.",
						"el cores can run in one of three modes: license 0 (L0) is the fastest (and is associated with the turbo frequencies *written on the box* ), license 1 (L1) is slower and license 2 (L2) is the slowest.",
						"o get into license 2, you need sustained use of heavy 512-bit instructions, where *sustained* means approximately one such instruction every cycle",
						"the worst case, you might be running at half the advertised frequency and thus your whole application could run slower.",
						"Engineers should probably use tools to monitor the frequency of their cores to ensure they are running in the expected license."
					]
				},
				{
					"title": "On the dangers of Intel's frequency scaling",
					"url": "https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/",
					"excerpts": [
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"The scaling gets worse when more cores execute AVX-512 and when multiplication is used.",
						"The scaling gets worse when more cores execute AVX-512 and when multiplication is used."
					]
				},
				{
					"title": "Intel® AVX-512 - Instruction Set for Packet Processing",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-instruction-set-for-packet-processing-technology-guide-1645717553.pdf",
					"excerpts": [
						"ple of Power Level transitions is shown in Figure 13 . In the figure, before time interval **t,** a given core is executing Intel®\nAVX2 integer instructions. At interval **t** an Intel® AVX-512 integer instruction runs and consequently a new *AVX-512 Light* power\nlevel is required. The core then blocks for 10-20 µsec until a new frequency is applied. Intel® AVX-512 integer instructions are then\nrun intermittently until **t+2000** µsec, after which the workload returns to executing *AVX2 Light* instructions exclusively. After an\nadditional 2000 µsec, without any additional Intel® AVX-512 instructions executing, the core returns to the *AVX2 Light* power le",
						"en the CPU detects that a\nnew power level is required, it decides which new core frequency to apply. For example, if the Intel® AVX-512 Light instructions run\non a given core with a current power level of *AVX2 Light (Level 0)* *,* and if that core now requires the *AVX-512 Light* *(Level 1)* power\nlevel, a frequency change may occur. When a frequency change occurs on the 1st Generation Intel® Xeon® Scalable processors for\nservers, the core frequency transition has a block time of 10-20 µsec to move between frequencies",
						"s**\nFrequency transitions are significantly improved in the 3rd Generation Intel® Xeon® Scalable processors where the block time is\nreduced to ~0 µsec, allowing for transitions without a latency cost."
					]
				},
				{
					"title": "Trying harder to make AVX-512 look bad: my quantified and reproducible results – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/",
					"excerpts": [
						"Intel documents that the use of AVX-512 instructions can lower the frequency of the processor. How big the effect is depends on the processor.",
						"So at the extreme, doing everything I can do make Intel AVX-512 look bad using a few AVX-512 instructions in a larger program, I get a 15-20% increase in the running time."
					]
				},
				{
					"title": "Ice Lake AVX-512 Downclocking | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html",
					"excerpts": [
						"The Ice Lake CPU has an all-core 512-bit turbo frequency of 3.3 GHz is 89% of the maximum single-core scalar frequency of 3.7 GHz, so within power and thermal limits this chip has a very “flat” frequency profile. The Rocket Lake 11900K is even flatter with an all-eight-cores frequency of 4.8 GHz clocking in at 94% of the 5.1 GHz single-core speed.",
						"The results paint a very promising picture of Rocket Lake’s AVX-512 frequency behavior: there is *no* license-based downclocking evident at any combination of core count and frequency [6](:rklcaveats) . Even heavy AVX-512 instructions can execute at the same frequency as lightweight scalar code."
					]
				},
				{
					"title": "Understanding Latency Hiding on GPUs - UC Berkeley EECS",
					"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf",
					"excerpts": [
						"The total execution time is then found as (Eq. 1, 16 and 17 in the paper):"
					]
				},
				{
					"title": "CUDA on WSL User Guide — CUDA C++ Best Practices Guide 13.1 documentation",
					"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/",
					"excerpts": [
						"The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing.\nThe dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. The multidimensional aspect of these parameters allows easier mapping of multidimensional problems to CUDA and does not play a role in performance. As a result, this section discusses size but not dimension.\nLatency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource (register and shared memory) constraints. Choosing execution parameters is a matter of striking a balance between latency hiding (occupancy) and resource utilization.",
						"Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. This metric is *occupancy"
					]
				},
				{
					"title": "Using CUDA Warp-Level Primitives | NVIDIA Technical Blog",
					"url": "https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/",
					"excerpts": [
						"the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others."
					]
				},
				{
					"title": "cuda - When is shfl.sync.idx fast? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/78496636/when-is-shfl-sync-idx-fast",
					"excerpts": [
						"You should really try to interleave multiple independent pieces of work if latency is something you worry about. The GPU will schedule other warps whilst waiting in between these 2 statements (if the multiprocessor has > 64 threads running on it)."
					]
				},
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"If data is very large (say, a movie), it can’t fit in the cache, so the bottleneck will be at best the main memory, if not I/O system itself. In which case, a faster hash may save cpu time, but will not make the checksumming operation faster.",
						"The main focus is about short keys of random lengths, with a distribution of length roughly in the 20-30 bytes area, featuring occasional outliers, both tiny and large."
					]
				},
				{
					"title": "Optimizing Open Addressing",
					"url": "https://thenumb.at/Hashtables/",
					"excerpts": [
						"Naively, this loop runs one lookup at a time.",
						"However, you might have noticed that we’ve been able to find keys *faster than the CPU can load data from main memory* —so something more complex must be going on."
					]
				},
				{
					"title": "Tuning Thread Affinitization and NUMA - The Talon Manual - Confluence",
					"url": "https://docs.neeveresearch.com:8443/display/TALONDOC/Tuning+Thread+Affinitization+and+NUMA",
					"excerpts": [
						"The goals for NUMA affinitization are to:\nAllocate all of a process's memory on a single NUMA node.\nPin all threads to cores that are connected to that NUMA node.\nWith the above:\nAll threads execute on a core closest to the memory allocated to the process, decreasing memory access time for a thread to access state from main memory.",
						"Pinning a thread to a particular CPU ensures that the OS won’t reschedule the thread to another core and incur a context switch that would force the thread to reload its working state from main memory which results in jitter."
					]
				},
				{
					"title": "Affinity part 2 - System topology and controlling affinity — ROCm Blogs",
					"url": "https://rocm.blogs.amd.com/software-tools-optimization/affinity/part-2/README.html",
					"excerpts": [
						"Placing threads of the same process close together in the same\nL3 cache region would improve cache reuse if the data being read by all those threads\nfits in cache."
					]
				},
				{
					"title": "NUMA, numactl, and taskset: A Practical Guide for AI/ML Engineers | by Tony Seah | Nov, 2025 | Medium",
					"url": "https://tonyseah.medium.com/numa-numactl-and-taskset-a-practical-guide-for-ai-ml-engineers-0fbacb450f22?source=rss------ia-5",
					"excerpts": [
						"**What it does** : Bind a process (or PID) to specific CPU cores.",
						"**What it does** : Control both CPU affinity **and** memory allocation policy.",
						"can interact with `taskset` / `numactl` . For consistent behavior, set:\nA reasonable `OMP_NUM_THREADS` (e.g. number of cores on the node)\n`OMP_PROC_BIND=close` or `spread` depending on your workload"
					]
				}
			],
			"reasoning": "The benchmarking best practices are well-supported by the provided excerpts. Running benchmarks on an idle system to minimize interference is directly supported by the recommendation to run on an unloaded system and on a single idle core. The need for warm-up runs to stabilize CPU frequency and populate caches is indicated by discussions on measuring for several seconds, the convergence of clock speeds for AVX-512, and the dynamic frequency scaling behavior of Intel processors when using AVX2/AVX-512 instructions, which can cause frequency reductions and transition delays. The importance of processing the entire workload in a single batch to amortize overhead is implicitly supported by discussions on latency hiding, the impact of initialization on timings for small inputs, and the observation that efficient lookups often involve more complex mechanisms than naive sequential processing. The necessity of thread affinity for CPU benchmarks to prevent noise from thread migration is explicitly mentioned in the context of pinning threads to cores to decrease memory access time and prevent context switches, and by the use of tools like `taskset` and `numactl` for CPU affinity. Finally, the practice of performing multiple measurement runs to collect statistical data and account for system noise is directly supported by recommendations to perform many runs to remove random error and to use robust statistics to minimize variability.",
			"confidence": "high"
		},
		{
			"field": "portable_simd_frameworks_evaluation.framework_name",
			"citations": [
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions.",
						"he argument or return type `V` denotes a vector\nwith `N` lanes, and `M` a mask. Operations limited to certain vector\ntypes begin with a constraint of the form `V` : `{prefixes}[{bits}]` .",
						"Clang and GCC require opting into SIMD intrinsics, e.g. via `-mavx2` flags. However, the flag enables AVX2 instructions in the entire\ntranslation unit, which may violate the one-definition rule (that all\nversions of a function such as `std::abs` are equivalent, thus the\nlinker may choose any). This can cause crashes if non-SIMD functions are\ndefined outside of a target-specific namespace, and the linker happens\nto choose the AVX2 version, which means it may be called without\nverifying AVX2 is indeed supported."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						" and performance-portable SIMD > []() []() Design philosophy\nContent:\nFor example, we write `ShiftLeft<3>(v)` instead of `v << 3` because MSVC 2017 (ARM64) does not propagate the literal ( https://godbolt.org/g/rKx5Ga ). Highway requires function-specific target attributes, supported by GCC 4.9 / Clang 3.9 / MSVC 2015. Efficient and safe runtime dispatch is important. Modules such as image or video codecs are typically embedded into larger applications such as browsers, so they cannot require separate binaries for each CPU. Libraries also cannot predict whether the application already uses AVX2 (and pays the frequency throttling cost), so this decision must be left to the application. Using only the lowest-common denominator instructions sacrifices too much performance. Therefore, we provide code paths for multiple instruction sets and choose the most suitable at runtime. To reduce overhead, dispatch should be hoisted to higher layers instead of checking inside every low-level function. Highway supports inlining functions in the same file or in *-inl.h headers. We generate all code paths from the same source to reduce implementation- and debugging cost. Not every CPU need be supported. For example, pre-SSE4.1 CPUs are increasingly rare and the AVX instruction set is limited to floating-point operations. To reduce code size and compile time, we provide specializations for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback",
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0.",
						"Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD."
					]
				},
				{
					"title": "GitHub - google/highway: Performance-portable, length-agnostic SIMD with runtime dispatch",
					"url": "https://github.com/google/highway",
					"excerpts": [
						"Content:\n[]()\nYou can use the `benchmark` inside examples/ as a starting point.\nA [quick-reference page](/google/highway/blob/master/g3doc/quick_reference.md) briefly lists all operations\nand their parameters, and the [instruction_matrix](/google/highway/blob/master/g3doc/instruction_matrix.pdf) indicates the number of instructions per operation.",
						"For static dispatch, `HWY_TARGET` will be the best available target among `HWY_BASELINE_TARGETS` , i.e. those allowed for use by the compiler (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ). Functions inside `HWY_NAMESPACE` can be called using `HWY_STATIC_DISPATCH(func)(args)` within\nthe same module they are defined in. You can call the function from other\nmodules by wrapping it in a regular function and declaring the regular\nfunction in a header.",
						"For dynamic dispatch, a table of function pointers is generated via the `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to\ncall the best function pointer for the current CPU's supported targets. A\nmodule is automatically compiled for each target in `HWY_TARGETS` (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ) if `HWY_TARGET_INCLUDE` is\ndefined and `foreach_target.h` is included. Note that the first invocation\nof `HWY_DYNAMIC_DISPATCH` , or each call to the pointer returned by the first\ninvocation of `HWY_DYNAMIC_POINTER` , involves some CPU detection overhead."
					]
				}
			],
			"reasoning": "The finegrained field value refers to a specific portable SIMD framework named Google Highway. Excerpts that introduce Highway as a collection of platform-agnostic operations and describe its API, vector concepts, and runtime-dispatch mechanisms provide direct support for identifying the library as Highway. For example, excerpts describe Highway as a collection of operations on tuples, implemented with platform-specific intrinsics, and discuss concepts like lanes, vector types, and target-specific dispatch. This aligns with the field value by confirming the library’s identity and its focus on portability and SIMD abstractions. Additional excerpts discuss Highway’s design philosophy, how it provides multiple code paths for different instruction sets (SSE4, AVX2, AVX-512, NEON, WASM SIMD), and notes about runtime vs static dispatch, all of which reinforce the association with Google Highway. Excerpts that cover quick-reference documentation and compiler/target considerations further corroborate the Highway framework’s characteristics and its ecosystem. While there are excerpts about other SIMD libraries (e.g., Eve, xtensor/xsimd), they provide contextual contrast and do not directly assert Google Highway, making them less central to the core identification but still useful for disambiguation. Collectively, the strongest connections are explicit references to Highway’s API, portability model, and runtime-dispatch mechanics, which directly support the finegrained field value.”,",
			"confidence": "high"
		},
		{
			"field": "portable_simd_frameworks_evaluation.generated_code_quality",
			"citations": [
				{
					"title": "GitHub - google/highway: Performance-portable, length-agnostic SIMD with runtime dispatch",
					"url": "https://github.com/google/highway",
					"excerpts": [
						"For dynamic dispatch, a table of function pointers is generated via the `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to\ncall the best function pointer for the current CPU's supported targets. A\nmodule is automatically compiled for each target in `HWY_TARGETS` (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ) if `HWY_TARGET_INCLUDE` is\ndefined and `foreach_target.h` is included. Note that the first invocation\nof `HWY_DYNAMIC_DISPATCH` , or each call to the pointer returned by the first\ninvocation of `HWY_DYNAMIC_POINTER` , involves some CPU detection overhead.",
						"Content:\n[]()\nYou can use the `benchmark` inside examples/ as a starting point.\nA [quick-reference page](/google/highway/blob/master/g3doc/quick_reference.md) briefly lists all operations\nand their parameters, and the [instruction_matrix](/google/highway/blob/master/g3doc/instruction_matrix.pdf) indicates the number of instructions per operation."
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions.",
						"he argument or return type `V` denotes a vector\nwith `N` lanes, and `M` a mask. Operations limited to certain vector\ntypes begin with a constraint of the form `V` : `{prefixes}[{bits}]` ."
					]
				},
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				}
			],
			"reasoning": "The most relevant excerpt explicitly describes the dynamic dispatch mechanism: an export macro (HWY_EXPORT) generates a function table and HWY_DYNAMIC_DISPATCH calls the best implementation for the host CPU at runtime. This directly aligns with the field value’s claim that Highway provides a dispatcher that selects the optimal backend implementation for the current hardware. Another excerpt notes that dynamic dispatch relies on a function-pointer table generated by HWY_EXPORT and invoked by HWY_DYNAMIC_DISPATCH, reinforcing the link between export mechanisms and runtime selection. The excerpt describing that Highway is implemented using platform-specific intrinsics, which map to SIMD/vector instructions, directly supports the idea that Highway’s backend targets optimized intrinsics per architecture, contributing to performance parity with hand-written code. Additional excerpts explaining Highway vectors, lanes, and API structure provide necessary context about how Highway exposes vector operations in a portable way, which underpins its ability to generate efficient, architecture-specific code. Finally, the excerpt mentioning the instruction_matrix.pdf confirms that there is a documented cost model for operations across targets, which is essential for reasoning about performance and making implementation choices. Taken together, these excerpts support the finegrained field value’s assertions about dynamic dispatch, backend intrinsic usage, and availability of an instruction-cost reference for performance reasoning.",
			"confidence": "high"
		},
		{
			"field": "portable_simd_frameworks_evaluation.api_expressiveness",
			"citations": [
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions.",
						"he argument or return type `V` denotes a vector\nwith `N` lanes, and `M` a mask. Operations limited to certain vector\ntypes begin with a constraint of the form `V` : `{prefixes}[{bits}]` .",
						"Clang and GCC require opting into SIMD intrinsics, e.g. via `-mavx2` flags. However, the flag enables AVX2 instructions in the entire\ntranslation unit, which may violate the one-definition rule (that all\nversions of a function such as `std::abs` are equivalent, thus the\nlinker may choose any). This can cause crashes if non-SIMD functions are\ndefined outside of a target-specific namespace, and the linker happens\nto choose the AVX2 version, which means it may be called without\nverifying AVX2 is indeed supported."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						" and performance-portable SIMD > []() []() Design philosophy\nContent:\nFor example, we write `ShiftLeft<3>(v)` instead of `v << 3` because MSVC 2017 (ARM64) does not propagate the literal ( https://godbolt.org/g/rKx5Ga ). Highway requires function-specific target attributes, supported by GCC 4.9 / Clang 3.9 / MSVC 2015. Efficient and safe runtime dispatch is important. Modules such as image or video codecs are typically embedded into larger applications such as browsers, so they cannot require separate binaries for each CPU. Libraries also cannot predict whether the application already uses AVX2 (and pays the frequency throttling cost), so this decision must be left to the application. Using only the lowest-common denominator instructions sacrifices too much performance. Therefore, we provide code paths for multiple instruction sets and choose the most suitable at runtime. To reduce overhead, dispatch should be hoisted to higher layers instead of checking inside every low-level function. Highway supports inlining functions in the same file or in *-inl.h headers. We generate all code paths from the same source to reduce implementation- and debugging cost. Not every CPU need be supported. For example, pre-SSE4.1 CPUs are increasingly rare and the AVX instruction set is limited to floating-point operations. To reduce code size and compile time, we provide specializations for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback",
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0.",
						"Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD."
					]
				},
				{
					"title": "GitHub - google/highway: Performance-portable, length-agnostic SIMD with runtime dispatch",
					"url": "https://github.com/google/highway",
					"excerpts": [
						"Content:\n[]()\nYou can use the `benchmark` inside examples/ as a starting point.\nA [quick-reference page](/google/highway/blob/master/g3doc/quick_reference.md) briefly lists all operations\nand their parameters, and the [instruction_matrix](/google/highway/blob/master/g3doc/instruction_matrix.pdf) indicates the number of instructions per operation.",
						"For static dispatch, `HWY_TARGET` will be the best available target among `HWY_BASELINE_TARGETS` , i.e. those allowed for use by the compiler (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ). Functions inside `HWY_NAMESPACE` can be called using `HWY_STATIC_DISPATCH(func)(args)` within\nthe same module they are defined in. You can call the function from other\nmodules by wrapping it in a regular function and declaring the regular\nfunction in a header.",
						"For dynamic dispatch, a table of function pointers is generated via the `HWY_EXPORT` macro that is used by `HWY_DYNAMIC_DISPATCH(func)(args)` to\ncall the best function pointer for the current CPU's supported targets. A\nmodule is automatically compiled for each target in `HWY_TARGETS` (see [quick-reference](/google/highway/blob/master/g3doc/quick_reference.md) ) if `HWY_TARGET_INCLUDE` is\ndefined and `foreach_target.h` is included. Note that the first invocation\nof `HWY_DYNAMIC_DISPATCH` , or each call to the pointer returned by the first\ninvocation of `HWY_DYNAMIC_POINTER` , involves some CPU detection overhead."
					]
				},
				{
					"title": "GitHub - jfalcou/eve: Expressive Vector Engine - SIMD in C++ Goes Brrrr",
					"url": "https://github.com/jfalcou/eve",
					"excerpts": [
						"In terms of SIMD extension sets, we actively support (ie code is optimised and regularly tested) the following:\nFull support with tests:\n**Intel**\nSSE2, SSSE3, SSE3, SSE4.1, SSE4.2\nAVX, AVX2, FMA3\nAVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)\n**ARM**\nNEON A32 (64 & 128 bits)\nNEON A64 (64 & 128 bits)\nASIMD\nSVE with fixed sizes of 128, 256 and 512 bits registers.\nPartial/In-progress support with minimal checks:\n**PowerPC**\nVMX\nVSX\nWe **do not support** ARM SVE with **dynamic size** or GPGPU; this is the job of another tool.",
						"SSE2, SSSE3, SSE3, SSE4.1, SSE4.2",
						"AVX, AVX2, FMA3",
						"AVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)",
						"NEON A32 (64 & 128 bits)",
						"NEON A64 (64 & 128 bits)",
						"ASIMD"
					]
				},
				{
					"title": "GitHub - xtensor-stack/xsimd: C++ wrappers for SIMD intrinsics and parallelized, optimized mathematical functions (SSE, AVX, AVX512, NEON, SVE, WebAssembly, VSX, RISC-V))",
					"url": "https://github.com/xtensor-stack/xsimd",
					"excerpts": [
						"| ARM | NEON, NEON64, SVE128/256/512 (fixed vector size) |"
					]
				},
				{
					"title": "Vc: Introduction",
					"url": "https://web-docs.gsi.de/~mkretz/Vc-1.0/intro.html",
					"excerpts": [
						"For example, AVX has instructions for 256-bit floating point registers, but only 128-bit integer instructions, which is why the integral [Vc::Vector](classVc_1_1Vector.html \"The main vector class for expressing data parallelism. \") types use the SSE implementation for AVX target systems."
					]
				},
				{
					"title": "Vc: Main Page",
					"url": "https://vcdevel.github.io/Vc-1.4.3/",
					"excerpts": [
						"The 1.x releases ship implementations for x86 SIMD instruction sets: SSE, AVX, AVX2, and the Xeon Phi (MIC)."
					]
				}
			],
			"reasoning": "The finegrained field value evaluates the Highway API expressiveness for performing common vector operations (e.g., Add, Xor, ShiftLeft/Right, Set) in a portable, architecture-agnostic way. The most directly relevant excerpts describe Highway’s core design: it is a collection of platform-agnostic operations on vectors (tuples of values) that map to platform-specific intrinsics, and the API uses explicit functions and a vector type descriptor approach (e.g., Set(D d, T val) to broadcast constants; the Set primitive and the notion of V as a vector with N lanes are central to expressing such operations portably). These passages establish that Highway provides explicit, portable operation abstractions rather than relying solely on operator overloading, which aligns with the field value’s emphasis on portability and explicit named operations for performance-portable code. Supporting material also covers the API anatomy (how operations are named and dispatched, and how the API is documented to guide portability across targets), which reinforces the claim that the API is expressive for the needed operations. Additional excerpts reinforce the broader context that Highway exposes a wide set of operations and that the API is designed to be portable across targets, with runtime/compile-time dispatch strategies described to maintain portability and performance. Collectively, these excerpts substantiate that Highway’s API has the expressiveness and design intent described in the finegrained field value, particularly in terms of explicit, portable vector operations and a clear mechanism for broadcasting constants and manipulating vectors across architectures.",
			"confidence": "medium"
		},
		{
			"field": "verification_and_benchmarking_methodology.key_metrics_to_report",
			"citations": [
				{
					"title": "Unit 5: Performance & Benchmarking",
					"url": "https://www.cis.upenn.edu/~cis5710/spring2021/slides/05_performance.pdf",
					"excerpts": [
						"Perform many runs: how many is enough?",
						"Take odd number of samples",
						"Measure something that runs for at least several seconds",
						"Understand the system being measured"
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"It uses CPU fences and robust statistics to minimize variability, and also\navoids unrealistic branch prediction effects.",
						"We compile the 64-bit C++ implementations with a patched GCC 4.9 and run on a\nsingle idle core of a Xeon E5-2690 v3 clocked at 2.6 GHz.",
						"Note that previous measurements included the initialization of their input,\nwhich dramatically increased timings especially for small inputs."
					]
				}
			],
			"reasoning": "The fine-grained field value asks for two concrete benchmarking metrics: Cycles/Hash and Hashes/Sec, along with how to measure them (e.g., CPU counters, high-resolution timers). Excerpts that focus on performance benchmarking practices provide the most direct support for how to report and standardize these metrics. Specifically, guidance on performing multiple benchmark runs, understanding what constitutes an adequate number of samples, and reporting practices directly map to constructing a clear, repeatable benchmarking methodology. For example, guidance emphasizing multiple runs and an appropriate sample count informs how to stabilize Cycles/Hash and Hashes/Sec measurements across implementations and platforms. Additional excerpts discuss the importance of benchmarking context and system understanding, which supports accurately attributing measured cycles and throughput to the hash computation rather than extraneous factors. Content addressing variability and statistical rigor in benchmarking (e.g., robust statistics, reducing measurement noise) reinforces the need for reliable reporting and aligns with the goal of standardized comparisons across implementations and platforms. While some excerpts discuss broader hardware and frequency considerations, which are relevant to performance but not strictly part of the two metrics themselves, they help explain factors that can influence measurements and thus why standardized methodology is necessary. Overall, the most relevant excerpts provide concrete benchmarking practices (number of runs, sampling strategy, and measurement context) directly supporting how to report Cycles/Hash and Hashes/Sec, while less directly relevant excerpts offer contextual factors that can affect measurement outcomes but do not define the primary metrics themselves.",
			"confidence": "medium"
		},
		{
			"field": "verification_and_benchmarking_methodology.benchmarking_best_practices",
			"citations": [
				{
					"title": "Unit 5: Performance & Benchmarking",
					"url": "https://www.cis.upenn.edu/~cis5710/spring2021/slides/05_performance.pdf",
					"excerpts": [
						"Perform many runs: how many is enough?",
						"Measure something that runs for at least several seconds",
						"Understand the system being measured",
						"Take odd number of samples"
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"Note that previous measurements included the initialization of their input,\nwhich dramatically increased timings especially for small inputs."
					]
				},
				{
					"title": "On the dangers of Intel's frequency scaling",
					"url": "https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/",
					"excerpts": [
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used."
					]
				}
			],
			"reasoning": "The requested benchmarking best practices call for running measurements on an otherwise idle system to minimize interference, performing warm-ups to populate caches and allow the processor to stabilize at its boost clock, and processing the full workload as a single batch to amortize overhead. One excerpt explicitly recommends performing many runs and discusses how to choose an adequate number of measurements, which aligns with collecting statistical data (mean, median, standard deviation) to account for system noise. Additionally, guidance about understanding the measurement target and ensuring system conditions are stable under load supports a careful, repeatable benchmarking methodology. Another excerpt notes that prior measurements can be biased by initializations or setup overhead, underscoring the importance of isolating and accounting for those costs (e.g., initialization timing). The discussion of processor frequency behavior under heavy workloads (including how frequency can downclock or adapt during long runs) highlights the need to report results under controlled frequency conditions or to acknowledge when frequency dynamics may affect outcomes. The inclusion of batch processing of a workload (processing all 4096 hashes in one batch) directly supports amortizing kernel/launch overhead and provides a consistent workload for measurement. Taken together, these excerpts collectively justify a benchmarking methodology that is idle-system focused, warm-up driven, batch-oriented, with pinning/affinity considerations and rigorous statistical collection, which matches the requested field value about best practices for reproducible and reliable performance results.",
			"confidence": "medium"
		},
		{
			"field": "verification_and_benchmarking_methodology.profiling_tools",
			"citations": [
				{
					"title": "Unit 5: Performance & Benchmarking",
					"url": "https://www.cis.upenn.edu/~cis5710/spring2021/slides/05_performance.pdf",
					"excerpts": [
						"Measure something that runs for at least several seconds",
						"Understand the system being measured",
						"Perform many runs: how many is enough?",
						"Take odd number of samples"
					]
				}
			],
			"reasoning": "The target field describes a suite of platform-specific profiling tools (Intel VTune, AMD uProf, ARM Streamline, perf for Linux, NVIDIA Nsight Systems/Compute, and ROCm rocprof) as essential for in-depth CPU/GPU profiling. The most relevant excerpts are those that explicitly reference performance benchmarking and profiling-oriented practices, as they provide context for how profiling is typically conducted across platforms. A fragment mentioning performance and benchmarking guidelines is directly aligned with the idea of structured profiling workflows used to measure port pressure, micro-op throughput, stalls, and microarchitectural events. Other excerpts discussing AVX frequency behavior or GPU/CPU performance considerations offer contextual background on performance measurement environments, which informs why platform-specific profiling tools are needed for accurate profiling. Finally, excerpts that explicitly discuss benchmarking methodology (such as performing repeated runs and sample considerations) also map to the practical use of profiling tools in a benchmarking workflow. Taken together, these excerpts collectively support the concept of using specialized profiling tools for in-depth, platform-specific performance analysis, even though they do not name every tool individually. The strongest support comes from references to performance benchmarking practices and the general emphasis on measurement and methodology, with weaker alignment to the exact named tools listed in the field value.",
			"confidence": "low"
		},
		{
			"field": "verification_and_benchmarking_methodology.verification_strategy",
			"citations": [
				{
					"title": "Unit 5: Performance & Benchmarking",
					"url": "https://www.cis.upenn.edu/~cis5710/spring2021/slides/05_performance.pdf",
					"excerpts": [
						"Perform many runs: how many is enough?",
						"Take odd number of samples",
						"Measure something that runs for at least several seconds",
						"Understand the system being measured"
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"It uses CPU fences and robust statistics to minimize variability, and also\navoids unrealistic branch prediction effects."
					]
				}
			],
			"reasoning": "The finegrained field value specifies a rigorous verification strategy requiring bit-for-bit equivalence between SIMD/GPU implementations and a scalar reference across millions of test vectors, with exhaustive edge-case coverage for shifts. The most relevant excerpts provide concrete guidance on how to perform rigorous performance and benchmarking experiments, which is a prerequisite often used to validate and stress-test verification methodologies in practice. Specifically, guidance on performing many runs and determining how many samples are enough directly informs how to structure large-scale test vectors and repeated executions to detect discrepancies reliably. Recommendations to take an odd number of samples help avoid parity biases in statistical measurements, which is valuable when assessing variability across many test vectors and implementations. Observations about reducing measurement variability through techniques like fences and robust statistics highlight the importance of controlling timing noise and non-determinism when comparing bit-identical results across architectures. Additional content reminding researchers to understand the system under measurement and to ensure measurements run long enough provides context for designing a deterministic, repeatable benchmark harness that accompanies a strict verification suite. Taken together, these excerpts support a methodology-focused aspect of the verification strategy by outlining best practices for rigorous measurement and benchmarking in the pursuit of bit-identical SIMD versus scalar results, even though they do not directly state the differential testing or exhaustive 32-bit edge-case verification itself.",
			"confidence": "low"
		},
		{
			"field": "portable_simd_frameworks_evaluation.overall_recommendation",
			"citations": [
				{
					"title": "GitHub - google/highway: Performance-portable, length-agnostic SIMD with runtime dispatch",
					"url": "https://github.com/google/highway",
					"excerpts": [
						"Content:\n[]()\nYou can use the `benchmark` inside examples/ as a starting point.\nA [quick-reference page](/google/highway/blob/master/g3doc/quick_reference.md) briefly lists all operations\nand their parameters, and the [instruction_matrix](/google/highway/blob/master/g3doc/instruction_matrix.pdf) indicates the number of instructions per operation."
					]
				},
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions.",
						"he argument or return type `V` denotes a vector\nwith `N` lanes, and `M` a mask. Operations limited to certain vector\ntypes begin with a constraint of the form `V` : `{prefixes}[{bits}]` ."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						" and performance-portable SIMD > []() []() Design philosophy\nContent:\nFor example, we write `ShiftLeft<3>(v)` instead of `v << 3` because MSVC 2017 (ARM64) does not propagate the literal ( https://godbolt.org/g/rKx5Ga ). Highway requires function-specific target attributes, supported by GCC 4.9 / Clang 3.9 / MSVC 2015. Efficient and safe runtime dispatch is important. Modules such as image or video codecs are typically embedded into larger applications such as browsers, so they cannot require separate binaries for each CPU. Libraries also cannot predict whether the application already uses AVX2 (and pays the frequency throttling cost), so this decision must be left to the application. Using only the lowest-common denominator instructions sacrifices too much performance. Therefore, we provide code paths for multiple instruction sets and choose the most suitable at runtime. To reduce overhead, dispatch should be hoisted to higher layers instead of checking inside every low-level function. Highway supports inlining functions in the same file or in *-inl.h headers. We generate all code paths from the same source to reduce implementation- and debugging cost. Not every CPU need be supported. For example, pre-SSE4.1 CPUs are increasingly rare and the AVX instruction set is limited to floating-point operations. To reduce code size and compile time, we provide specializations for SSE4, AVX2 and AVX-512 instruction sets on x86, plus a scalar fallback"
					]
				},
				{
					"title": "GitHub - jfalcou/eve: Expressive Vector Engine - SIMD in C++ Goes Brrrr",
					"url": "https://github.com/jfalcou/eve",
					"excerpts": [
						"In terms of SIMD extension sets, we actively support (ie code is optimised and regularly tested) the following:\nFull support with tests:\n**Intel**\nSSE2, SSSE3, SSE3, SSE4.1, SSE4.2\nAVX, AVX2, FMA3\nAVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)\n**ARM**\nNEON A32 (64 & 128 bits)\nNEON A64 (64 & 128 bits)\nASIMD\nSVE with fixed sizes of 128, 256 and 512 bits registers.\nPartial/In-progress support with minimal checks:\n**PowerPC**\nVMX\nVSX\nWe **do not support** ARM SVE with **dynamic size** or GPGPU; this is the job of another tool."
					]
				}
			],
			"reasoning": "The strongest support comes from the Highway project itself, which explicitly markets itself as performance-portable and length-agnostic SIMD with runtime dispatch. This directly aligns with the need to expose parallelism in a seemingly sequential hash computation by adapting to different ISAs and vector lengths at runtime. Highway models operations as platform-agnostic, yet mapped to intrinsics per target, enabling a consistent API across architectures. This capability is crucial for writing high-performance, portable SIMD code without maintaining separate intrinsics for each platform. To contrast, other excerpts emphasize Highway’s broad target support (scalar, SSE, AVX, NEON, WASM SIMD) and its conceptual design philosophy of providing multiple code paths for different instruction sets while minimizing maintenance cost, which reinforces Highway’s suitability for the described optimization task. Highway’s quick-reference material also reiterates its core idea: operations act on vector lanes with a portable, target-agnostic interface, which is exactly the kind of abstraction needed to structure a complex, chain-like hash computation while preserving bit-identical results across platforms. In addition, the content describing Highway’s design philosophy stresses runtime dispatch and the need to balance code paths across multiple instruction sets, which is essential when aiming to interleave work across diverse architectures without sacrificing correctness. As a comparison point, the excerpts related to other SIMD libraries (e.g., EVE) show that while they are capable, they have explicit limitations such as SVE support being fixed-size in their current listings, which can hinder dynamic-length or flexible SVE support required for optimal ARM server performance. Taken together, the evidentiary points converge on Highway as the most suitable framework for this optimization objective, with dynamic-length SVE and runtime dispatch capabilities being central advantages highlighted by multiple excerpts.)",
			"confidence": "high"
		},
		{
			"field": "algebraic_simplification_techniques",
			"citations": [
				{
					"title": "Using the LEA instruction for arbitrary arithmetic | Handmade Network",
					"url": "https://handmade.network/wiki/7111-using_the_lea_instruction_for_arbitrary_arithmetic",
					"excerpts": [
						"But `lea` allows us to do all of that math in a single instruction.",
						"` is designed for use with arrays. But the math it's doing is just math, and we can use it for other things.\nL"
					]
				},
				{
					"title": "assembly - How does LEA work to perform arithmetic operation - Stack Overflow",
					"url": "https://stackoverflow.com/questions/71384422/how-does-lea-work-to-perform-arithmetic-operation",
					"excerpts": [
						"3AT&T syntax is, in my opinion, just a mess (because it's one step too low-level). In Intel syntax, you'd write `lea eax, [esi+esi*2+5]` - there is no intermediate step here, it's all done in one instruction, and the value in `eax` is actually `x` 's value and not its address.",
						"The reason `lea` is called \"load effective *address* \" is just because it's using the processor's addressing modes to do a calculation without then actually reading memory from somewhere."
					]
				},
				{
					"title": "CS107 Guide to x86-64",
					"url": "https://web.stanford.edu/class/archive/cs/cs107/cs107.1166/guide_x86-64.html",
					"excerpts": [
						"Note that `lea` does not dereference the source address, it simply calculates its location. This means `lea` is nothing more than an arithmetic operation and commonly used to calculate the value of simple linear combinations that have nothing to do with memory locations!"
					]
				},
				{
					"title": "Understanding the LEA x86 instruction - ratfactor",
					"url": "https://ratfactor.com/cards/lea",
					"excerpts": [
						"It may also be a clever way of **doing more calculations than normal in one instruction** ; for example, using such an instruction with the addressing mode \"base+index+offset\" (detailed below) allows one to add two registers and a constant together in one instruction."
					]
				},
				{
					"title": "assembly - What's the purpose of the LEA instruction? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/1658294/whats-the-purpose-of-the-lea-instruction",
					"excerpts": [
						"ss) is often used as a \"trick\" to do certain computations, but that's not its primary purpose. The x86 instruction set was designed to support high-level languages like Pascal and C, where arrays—especially arrays of ints or small structs—are common.",
						"The biggest reason that you use `LEA` over a `MOV` is if you need to perform arithmetic on the registers that you are using to calculate the address. Effectively, you can perform what amounts to pointer arithmetic on several of the registers in combination effectively for \"free.\"",
						"LEA is just a shift-and-add instruction. It was probably added to 8086 because the hardware is already there to decode and calculate addressing modes, not because it's \"intended\" only for use with addresses. Remember that pointers are just integers in assembly.",
						"Another important feature of the `LEA` instruction is that it does not alter the condition codes such as `CF` and `ZF` , while computing the address by arithmetic instructions like `ADD` or `MUL` does."
					]
				},
				{
					"title": "x86 64 - Assembly why lea is fast? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/67034266/assembly-why-lea-is-fast",
					"excerpts": [
						"The `lea` (load effective address) is a way to perform the common operation of pointer arithmetic. How an instruction refers to its operands is called its [addressing mode](https://en.wikipedia.org/wiki/Addressing_mode) and `lea` supports [scaled](https://en.wikipedia.org/wiki/Addressing_mode) or [base plus index plus offset](https://en.wikipedia.org/wiki/Addressing_mode) addressing modes (among others).",
						"the scaling value can be one of few powers of two (1, 2, 4, 8).",
						"A multiplication instruction on the other hand, goes through a multiplication circuitry that can multiply two arbitrary full-width (64 bits) operands. This is an operation with significantly higher complexity.",
						"On AMD, `lea` is 1 or 2-cycle latency, with similar throughput reduction (fewer ports) for slow LEA. And the conditions for being fast are more restrictive: a scale factor other than `1` makes it slow.",
						"LEA latency is 1 cycle only for simple-enough addressing modes (before Ice Lake). On Intel SnB-family CPUs, there aren't any uops with 2-cycle latency, and LEA with 3 components (two + operations) has 3 cycle latency.",
						"The `lea` (load effective address) is a way to perform the common operation of pointer arithmetic.",
						"Ice Lake is always 1c latency, 1 uop. 2/clock throughput for addressing modes including a scaled-index (shift-count != 0), or 4/clock otherwise.",
						"Intel SnB-family CPUs, there aren't any uops with 2-cycle latency, and LEA with 3 components (two + operations) has 3 cycle latency",
						"On AMD, `lea` is 1 or 2-cycle latency, with similar throughput reduction (fewer ports) for slow LEA."
					]
				},
				{
					"title": "GitHub - jandrewrogers/MetroHash: MetroHash v1: Exceptionally fast and statistically robust hash functions",
					"url": "https://github.com/jandrewrogers/MetroHash",
					"excerpts": [
						"64-bit hash functions, \"metrohash64_1\" and \"metrohash64_2\"\n128"
					]
				},
				{
					"title": "IA-32 Intel® Architecture Optimization",
					"url": "https://cr.yp.to/bib/2004/-intel-optimization.pdf",
					"excerpts": [
						"Use the lea instruction and the full range of addressing modes to do\naddress calculation.",
						"Using lea in this way may avoid register usage by not tying up registers\nfor operands of arithmetic instructions. This use may also save code\nspace.",
						"If the lea instruction uses a shift by a constant amount then the latency\nof the sequence of µops is shorter if add s are used instead of a shift, and\nthe lea instruction may be replaced with an appropriate sequence of\nµops. This, however, this increases the total number of µops, leading to\na trade-off.",
						"e the addressing modes for computing addresses rather than using the\ngeneral-purpose computation. Internally, memory reference instructions\ncan have four operands:\n•\nrelocatable load-time constant\n•\nimmediate constant\n•\nbase register\n•\nscaled index re"
					]
				},
				{
					"title": "Instruction Set Reference, A-Z",
					"url": "https://cdrdv2-public.intel.com/782156/325383-sdm-vol-2abcd.pdf",
					"excerpts": [
						"REX.W + 8D /r\nLEA r64,m\nRM\nValid\nN.E.\nStore effective address for m in register r64.",
						"Table 3-54. Non-64-bit Mode LEA Operation with Address and Operand Size Attributes",
						"Table 3-55. 64-bit Mode LEA Operation with Address and Operand Size Attributes",
						"In 64-\nbit mode, address size of 16 bits is not encodable. See Table 3-55"
					]
				},
				{
					"title": "LEA_B_D32 (R64)",
					"url": "https://uops.info/html-instr/LEA_B_D32_R64.html",
					"excerpts": [
						"Section Title: LEA_B_D32 (R64) > Ice Lake",
						"\nMeasurements\nLatencies\n[Latency operand 2 → 1 (address, base register):](",
						"[Measured (loop):](../html-tp/ICL/LEA_B_D32_R64-Measurements.html) 0.25",
						"[Measured (unrolled):](../html-tp/ICL/LEA_B_D32_R64-Measurements.html) 0.44",
						"Throughput\nComputed from the port usage: 0.25"
					]
				},
				{
					"title": "Theoretical Scalar Integer Performance KabyLake",
					"url": "https://stackoverflow.com/questions/62074838/theoretical-scalar-integer-performance-kabylake",
					"excerpts": [
						"The LEA is \"slow\" (3 cycle latency, port 1 only) because it has 3 components (2 + in the addressing mode). If you count lea eax, [rdi + rsi*2 + ...Read more"
					]
				},
				{
					"title": "How x86_64 addresses memory - ENOSUCHBLOG",
					"url": "https://blog.yossarian.net/2020/06/13/How-x86_64-addresses-memory",
					"excerpts": [
						"“Scale-Index-Base-Displacement” addressing · Scale: A 2-bit constant factor that is either 1, 2, 4, or 8. · Index: Any general purpose register ( ...Read more"
					]
				},
				{
					"title": "c++ - Performance differ significantly when inner loop iterates less times? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/72075412/performance-differ-significantly-when-inner-loop-iterates-less-times",
					"excerpts": [
						"An LEA with 3 components (two `+` operations) in the addressing mode has 3 cycle latency on your Skylake-derived CPU.",
						"So the loop-carried dependency chain is a slow-LEA (3 cycles) between load/store.",
						"So the loop-carried dep chain through the store/reload only includes an `add` , 1 cycle latency instead of 3.",
						"I assume your performance ratios were something like 3:4 or so, from the 5+1 cycles (6) vs. 5+3 (8) cycles."
					]
				},
				{
					"title": "Intel® 64 and IA-32 Architectures Optimization Reference ...",
					"url": "https://kib.kiev.ua/x86docs/Intel/Intel-OptimGuide/248966-046.pdf",
					"excerpts": [
						"LEA ECX, [EAX + EBX*4 + A]",
						"In Sandy Bridge microarchitecture, there are two significant changes to the performance characteristics\nof LEA instruction:",
						"For LEA instructions with three source operands and some specific situations, instruction latency has\nincreased to 3 cycles, and must dispatch via port 1:",
						"—\nLEA that has all three source operands: base, index, and offset.",
						"—\nLEA that uses base and index registers where the base is EBP, RBP, or R13.",
						"—\nLEA that uses RIP relative addressing mode.",
						"—\nLEA that uses 16-bit addressing mode.",
						"GENERAL OPTIMIZATION GUIDELINES\n3-22\n.\nThe LEA instruction or a sequence of LEA, ADD, SUB and SHIFT instructions can replace constant multiply\ninstructions. The LEA instruction can also be used as a multiple operand addition instruction, for\nexample:\nLEA ECX, [EAX + EBX*4 + A]",
						"Using LEA in this way may avoid register usage by not tying up registers for operands of arithmetic\ninstructions. This use may also save code space.",
						"Using LEA in this way may avoid register usage by not tying up registers for operands of arithmetic\ninstructions. This use may also save code space.",
						"If the LEA instruction uses a shift by a constant amount then the latency of the sequence of µops is\nshorter if adds are used instead of a shift, and the LEA instruction may be replaced with an appropriate\nsequence of µops. This, however, increases the total number of µops, leading to a trade-off.",
						"0\n**Assembler/Compiler Coding Rule 30. (ML impact, L generality)** If an LEA instruction using the\nscaled index is on the critical path, a sequence with ADDs may be better..........................3-22",
						".5.1.6\nAddress Calculations\nFor computing addresses, use the addressing modes rather than general-purpose computations. Inter -\nnally, memory reference instructions can have four operands",
						"•\nRelocatable load-time constant.",
						"•\nImmediate constant.",
						"•\nBase register.",
						"•\nScaled index register.",
						"Note that the latency and throughput of LEA with more than two operands are slower (see Section\n3.5.1.2) in Sandy Bridge microarchitecture. Addressing modes that uses both base and index registers",
						"The LEA instruction can also be used as a multiple operand addition instruction, for\nexample:\nLEA ECX, [EAX + EBX*4 + A]",
						"If the LEA instruction uses a shift by a constant amount then the latency of the sequence of µops is\nshorter if adds are used instead of a shift, and the LEA instruction may be replaced with an appropriate\nsequence of µops.",
						".\n3.5.1.2\nUsing LEA\nIn Sandy Bridge microarchitecture, there are two significant changes to the performance characteristics\nof LEA instruction:\n•\nL",
						"A can be dispatched via port 1 and 5 in most cases, doubling the throughput over prior genera -\ntions. However this apply only to LEA instructions with one or two source operands",
						"A can be dispatched via port 1 and 5 in most cases, doubling the throughput over prior genera -\ntions. However this apply only to LEA instructions with one or two source operands",
						"For LEA instructions with three source operands and some specific situations, instruction latency has\nincreased to 3 cycles, and must dispatch via port 1:\n—\nLEA that has all three source operands: base, index, and offset.\n—\nLEA that uses base and index registers where the base is EBP, RBP, or R13.\n—\nLEA that uses RIP relative addressing mode.\n—\nLEA that uses 16-bit addressing mode."
					]
				},
				{
					"title": "Optimization Reference Manual",
					"url": "https://cdrdv2-public.intel.com/821613/355308-Optimization-Reference-Manual-050-Changes-Doc.pdf",
					"excerpts": [
						"Fast LEA,",
						"Fast LEA,",
						"Slow LEA",
						"•\nLEAs without a scaled index and with only two sources among base, index, and displacement execute as a normal\nALU operation on any port (00 through 03).",
						"\nLEAs with three sources fracture into two operations and take an additional cycle of latency.\n",
						"LEAs with a scaled index without a displacement execute as a single operation but are statically bound to port 02.",
						"]\nlea r12,[rdi+r10*8] ; using LEA to avoid",
						"EAs with three sources fracture into two operations and take an additional cycle of latency. Index consuming por -\ntion, regardless of scale value, will bind to port 02 while second operation binds to either port 00 or 0",
						"The ALU ports are optimized to execute standard two-source arithmetic/logical operations, while the AGUs are\noptimized to handle the complexities of x86 memory addressing.",
						"SLOW LEA represents a lea with two registers and displacement, all other lea versions considered as FAST LEA.",
						". LEAs without a scaled index and only two sources (among base, index, and displacement inputs) execute as one\noperation on any ALU port (00, 01, or 02).\nAL",
						"LEAs with a scaled index but without a displacement execute as one operation on port 02.",
						"execution"
					]
				},
				{
					"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
					"url": "https://www.agner.org/optimize/microarchitecture.pdf",
					"excerpts": [
						"The LEA instruction uses the address generation unit (AGU) rather than the ALU. This\ncauses a latency of 4 clock cycles when dependent on a pointer register or index register\nbecause of the distance between the AGU and the ALU. It is therefore faster to use addition\nand shift instructions than to use the LEA instruction in most cases.",
						"Simple integer instructions have a latency of one clock cycle.",
						"integer arithmetic, logic, shift",
						"**Operations**",
						"**Port**",
						"**9.5 Micro-op fusion**",
						"The processor uses µop fusion in the same way as previous processors. Some instructions\nthat need two µops in the execution units can use the µop fusion technique to keep these\ntwo µops together as one through most of the pipeline in order to save pipeline bandwidth.",
						"See page 93 and 109.",
						"**9.6 Macro-op fusion**",
						"The Sandy Bridge and Ivy Bridge can fuse two instructions into one µop in more cases than\nprevious processors can (see page 110).",
						"The decoders will fuse an arithmetic or logic instruction and a subsequent conditional jump\ninstruction into a single compute-and-branch µop in certain cases. The compute-and-branch\nµop is not split in two at the execution units but executed as a single µop by the branch unit\nat execution port 5.",
						"INC and DEC can fuse with signed branch instructions, and TEST and AND instructions can\nfuse with all branch instructions (including useless combinations), as indicated in the\nfollowing table:",
						"**10.5 Micro-op fusion**",
						"ses µop fusion in the same way as previous processors. Some instructions\nthat need two µops in the execution units can use the µop fusion technique to keep these\ntwo µops together as one through most of the pipeline in order to save pipeline bandwidth.\nSee",
						"The decoders can handle four µop-fused instructions per clock cycle.",
						"You can see which\ninstructions use µop fusion by looking at the tables in manual 4: \"Instruction tables\".\nInstructions with µop fusion have a higher number of µops listed under \"unfused domain\"\nthan under \"fused domain\".",
						"**10.6 Macro-op fusion**",
						"The Haswell and Broadwell can fuse two instructions into one µop in the same way that\nprevious processors can (see page 110 and 127).",
						"The decoders will fuse an arithmetic or logic instruction and a subsequent conditional jump\ninstruction into a single compute-and-branch µop in certain cases. The compute-and-branch\nµop is not split in two at the execution units but executed as a single µop by the branch unit\nat execution port 0 or 6.",
						"The CMP , ADD and SUB instructions can fuse with signed and unsigned branch instructions.",
						"The CMP , ADD and SUB instructions can fuse with signed and unsigned branch instructions.",
						"**Latency**",
						"0",
						"0",
						"1"
					]
				},
				{
					"title": "Arm® Neoverse™ N1 Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/6687d73a69e89f01e39c4507?token=",
					"excerpts": [
						"Arithmetic, basic\nADD{S},\nADC{S},\nSUB{S}, SBC{S}\n1",
						"Arithmetic, extend and\nshift",
						"Arithmetic, LSL shift,\nshift <= 4",
						"3",
						"3",
						"I",
						"I",
						"Arithmetic,\nLSR/ASR/ROR shift or\nLSL shift > 4",
						"ADD{S},",
						"ADD{S},",
						"ADD{S},",
						"SUB{S}",
						"SUB{S}",
						"SUB{S}",
						"M",
						"M",
						"Instructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cycle.",
						"Table 3: Neoverse N1 pipeline",
						"The execution pipelines support different types of operations, as shown in the following table.",
						"e following diagram describes the high-level Neoverse N1 instruction processing pipeline.\nInstructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cycle",
						" This document provides high-level information about the Neoverse N1 pipeline, instruction\nperformance characteristics, and special performance considerations. This information is\nintended to aid people who are optimizing software and compilers for Neoverse N1. For a\nmore complete description of the Neoverse N1 processor, please refer to the Arm®\nNeoverse™ N1 Technical Reference Manual.",
						"Up to 2 uops utilizing B pipeline",
						"Up to 4 uops utilizing S pipelines",
						"Up to 2 uops utilizing M pipeline",
						"Up to 2 uops utilizing each of the V pipelines.",
						"Up to 2 uops utilizing each of the L pipelines",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in\n ..",
						"Arithmetic, LSL shift by\nimmed, shift <= 4,\nunconditional",
						"Arithmetic, basic\nADD{S},\nADC{S},\nSUB{S}, SBC{S}\n1\n3\nI",
						"ADC{S},",
						"RSB{S},",
						"RSC{S},",
						"SUB{S},",
						"SBC{S}",
						"ALU, shift by register,\nunconditional\n(same as\nALU, basic)",
						"unconditional",
						"ALU, shift by register,\nconditional\n(same as\nALU, basic)",
						"ALU, shift by register,\nconditional\n(same as\nALU, basic)",
						"m",
						"ALU, shift by register,",
						"conditional",
						"(same as",
						"ALU, basic)",
						"I, M",
						"PJDOC-466751330-9707",
						"The following diagram describes the high-level Neoverse N1 instruction processing pipeline.",
						"Instructions are first fetched, then decoded into internal macro-operations (Mops). From\nthere, the Mops proceed through register renaming and dispatch stages. A Mop can be split\nfurther into two Uops at dispatch stage. Once dispatched, uops wait for their operands and\nissue out-of-order to one of eight execution pipelines. Each execution pipeline can accept and\ncomplete one uop per cyc",
						"Table 2: Neoverse N1 operations",
						"Instruction groups",
						"Instructions",
						"Branch forms",
						"Branch µOPs",
						"Integer Single-Cycle\n0/1",
						"Integer ALU µOPs",
						"nteger Single/Multi-\ncycle 0/1",
						"nteger shift-ALU, multiply, divide, CRC and sum-of-absolute-\ndifferences µOPs",
						"Load/Store Address\nGeneration 0/1",
						"Load, Store address generation and special memory µOPs",
						"FP/ASIMD-0",
						"ASIMD ALU, ASIMD misc, ASIMD integer multiply, FP convert, FP\nmisc, FP add, FP multiply, FP divide, FP sqrt, crypto µOPs, store\ndata µOPs",
						"FP/ASIMD-1",
						"ASIMD ALU, ASIMD misc, FP misc, FP add, FP multiply, ASIMD\nshift µOPs, store data µOPs, crypto µOPs.",
						"Issue 4.0",
						"Issue 4.0",
						"Instruction characteristics",
						"Instruction characteristics",
						"Instruction characteristics",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32\nand A64 instructions. A series of tables summarize the effective execution latency and\nthroughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours\nassociated with each group of instructions. Utilized pipelines correspond to the execution\npipelines described in chapter 2.",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32\nand A64 instructions. A series of tables summarize the effective execution latency and\nthroughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours\nassociated with each group of instructions. Utilized pipelines correspond to the execution\npipelines described in chapter 2.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation\ndependent on an instruction in the described group.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation\ndependent on an instruction in the described group.",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in",
						"Execution Throughput",
						"throughput",
						"utilized",
						"pipelines",
						"Logical, basic\nAND{S},\nBIC{S}, EON,\nEOR, ORN,\nORR\n1\n3\nI",
						"AArch32",
						"exec",
						"latency",
						"s",
						"i",
						"i",
						"+1",
						"b",
						"**3.4** **Arithmetic and logical instructions**",
						"**Table 6: AArch64 Arithmetic and logical instructions**",
						"**Instruction Group**",
						"**AArch64**",
						"**Instructions**",
						"**Exec**",
						"**Latency**",
						"**Execution**",
						"**Throughput**",
						"**Utilized**",
						"**Pipeline**",
						"**s**",
						"**Notes**",
						"Arithmetic, basic",
						"SUB{S}, SBC{S}",
						"s.\nArm® Neoverse™ N1 Software Optimization Guide\nPJDOC-466751330-9707\nIssue 4.0\nInstruction characteristics\nCopyright © 2018-2023 Arm Limited (or its affiliates). All rights reserved.\nNon-Confidential\nPage 11 of 66\nA",
						"AArch64",
						"3.1",
						"Instruction tables",
						"This chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32",
						"and A64 instructions. A series of tables summarize the effective execution latency and",
						"throughput (instruction bandwidth per cycle), pipelines utilized, and special behaviours",
						"associated with each group of instructions. Utilized pipelines correspond to the execution",
						"pipelines described in chapter 2.",
						"In the tables below, Exec Latency is defined as the minimum latency seen by an operation",
						"dependent on an instruction in the described group.",
						"Note",
						"ALU, basic",
						"ADR,",
						"ORR{S},",
						"SBC{S}, TEQ,",
						"TST",
						"Arithmetic, LSL shift by",
						"Arithmetic, LSL shift by",
						"immed, shift <= 4,",
						"immed, shift <= 4,",
						"Arithmetic,",
						"LSR/ASR/ROR shift by",
						"immed or LSL shift by",
						"immed > 4",
						"noflagset",
						"AND, BIC,",
						"EOR, ORN,",
						"orr",
						"Logical, shift by immed,",
						"Logical, shift by immed,",
						"flagset",
						"AND{S},",
						"AND{S},",
						"BIC{S},",
						"BIC{S},",
						"EOR{S},",
						"EOR{S},",
						"ORN{S},",
						"ORN{S},",
						"ORR{S}",
						"Test/Compare, shift by",
						"immed",
						"CMN, CMP,",
						"CMN, CMP,",
						"TEQ, TST",
						"+B",
						"\nNote:\nBranch forms are possible when the instruction destination register is the PC. For those cases,\nan additional branch µOP is required. This adds 1 cycle to the latency.",
						" ... ",
						" ... ",
						" ... ",
						"s\nCopyright © 2018-2023 Arm Limited (or its affiliates). All rights reserved.",
						"Non-Confidential",
						"Page 58 of 66",
						"Instruction group",
						"Instruction group",
						"Instruction group",
						"execution",
						"execution",
						"execution",
						"Notes",
						"Notes",
						"Notes",
						"Crypto SHA1 hash",
						"acceleration op",
						"SHA1H",
						"Crypto SHA1 schedule",
						"acceleration ops",
						"SHA1SU0,",
						"SHA1SU1",
						"V0",
						"V0",
						"-",
						"-",
						"1",
						"1",
						"1",
						"1",
						"2",
						"2"
					]
				},
				{
					"title": "4. Instruction tables",
					"url": "https://www.agner.org/optimize/instruction_tables.pdf",
					"excerpts": ["3", "1", "1", "1", "1", "1", "1", "1", "2", "2"]
				},
				{
					"title": "A78 Core - Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/60a5413bd63d3c31550c391e/",
					"excerpts": [
						"3",
						"3",
						"M",
						"Instructions",
						"throughput",
						"utilized",
						"pipelines",
						"latency",
						"i",
						"i",
						"i",
						"i",
						"AArch64",
						"ALU, basic",
						"Instruction group",
						"execution",
						"execution",
						"-",
						"-",
						"-",
						"-",
						"-",
						"1",
						"1",
						"1",
						"1",
						"2",
						"2"
					]
				},
				{
					"title": "Software Pipelining",
					"url": "https://people.iith.ac.in/ramakrishna/fc5264/soft-pipe.pdf",
					"excerpts": ["3", "1", "2"]
				},
				{
					"title": "Cortex-A72 Microarchitecture",
					"url": "https://wordpress-courses2425.wolfware.ncsu.edu/ece-785-sprg-2025/wp-content/uploads/sites/80/2025/03/Cortex-A72-Microarchitecture.pdf",
					"excerpts": ["3", "1", "1", "2"]
				},
				{
					"title": "Arm Neoverse V2 Core Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/668bc0a369e89f01e39c4668?token=",
					"excerpts": [
						"Instructions",
						"Branch µOPs",
						"Integer ALU µOPs",
						"Load, Store address generation and special memory µOPs",
						"FP/ASIMD-0",
						"FP/ASIMD-1"
					]
				},
				{
					"title": "Cortex-A76 Software Optimization Guide",
					"url": "https://documentation-service.arm.com/static/61c1f57d2183326f217720de",
					"excerpts": [
						"Arithmetic, basic\nADD{S}, ADC{S}, SUB{S},\nSBC{S}\n1\n3\nI",
						"Arithmetic, LSL shift, shift <= 4\nADD{S}, SUB{S}\n1\n3\nI",
						"Arithmetic, LSR/ASR/ROR shift\nor LSL shift > 4\nADD{S}, SUB{S}\n2\n1\nM",
						"Logical, basic\nAND{S}, BIC{S}, EON, EOR, ORN,\nORR\n1\n3\nI",
						"The following diagram describes the high-level Cortex-A76 instruction processing pipeline. Instructions are first fetched, then\ndecoded into internal macro-operations (Mops). From there, the Mops proceed through register renaming and dispatch stages.",
						"A Mop can be split further into two Uops at dispatch stage. Once dispatched, uops wait for their operands and issue out-of-order\nto one of eight execution pipelines. Each execution pipeline can accept and complete one uop per cycle.",
						"Unroll the loop to include multiple load and store operations per iteration, minimizing the overheads of looping.",
						"Align stores on 16B boundary wherever possible.",
						"The Cortex-A76 processor includes two load/store pipelines, which allow it to execute two 128 bit load uops and one 128 bit\nstore uop every cycle",
						"*\nThis chapter describes high-level performance characteristics for most Arm v8.2-A A32, T32 and A64 instructions. A series of\ntables summarize the effective execution latency and throughput (instruction bandwidth per cycle), pipelines utilized, and\nspecial behaviours associated with each group of instructions. U",
						".\nIn the tables below, Exec Latency is defined as the minimum latency seen by an operation dependent on an instruction in the\ndescribed group.",
						"In the tables below, Execution Throughput is defined as the maximum throughput (in instructions per cycle) of the specified\n ... ",
						"\nNotes:\n1. Branch forms are possible when the instruction destination register is the PC. For those cases, an additional branch uop\nis required. This adds 1 cycle to the latency.\n",
						"Software Optimization Guide**"
					]
				},
				{
					"title": "Arm Neoverse N1 Core: Performance Analysis Methodology",
					"url": "https://armkeil.blob.core.windows.net/developer/Files/pdf/white-paper/neoverse-n1-core-performance-v2.pdf",
					"excerpts": [
						"Neoverse N1 can fetch up to 4 instructions per cycle and dispatch a maximum 8 micro- operations per cycle. The Back End of the CPU handles the ..."
					]
				},
				{
					"title": "ROL (R32, 1)",
					"url": "https://uops.info/html-instr/ROL_R32_1.html",
					"excerpts": [
						"Section Title: ROL (R32, 1) > Skylake",
						"[Latency operand 1 → 1:](.",
						"[Latency operand 1 → 3:](.",
						"[Number of μops]",
						"Executed: 2",
						"Retire slots: 2",
						"Decoded (MITE): 2",
						"Requires the complex decoder",
						"(3 other instructions can be decoded with simple decoders in the same cycle)",
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"[Number of μops:]",
						"[Number of μops:]",
						"Microcode Sequencer (MS): 0",
						"IACA 2.3",
						"IACA 3.0",
						"Measurements",
						"Latencies",
						"Throughput",
						"Throughput",
						"Throughput",
						"2",
						"2"
					]
				},
				{
					"title": "CMOVBE (R32, R32)",
					"url": "https://uops.info/html-instr/CMOVBE_R32_R32.html",
					"excerpts": [
						"Computed from the port usage: 1.00",
						"Computed from the port usage: 1.00",
						"Throughput",
						"Throughput"
					]
				},
				{
					"title": "ADD (R32, 0)",
					"url": "https://uops.info/html-instr/ADD_R32_0.html",
					"excerpts": [
						"Section Title: ADD (R32, 0) > Skylake",
						"[Latency operand 1 → 1:](../html-lat/SKL/ADD_R32_0-Measurements.html%3E1) 1",
						"[Latency operand 1 → 3:](../html-lat/SKL/ADD_R32_0-Measurements.html%3E3) 1",
						"[Measured (loop):](../html-tp/SKL/ADD_R32_0-Measurements.html) 0.25",
						"[Measured (unrolled):](../html-tp/SKL/ADD_R32_0-Measurements.html) 0.25",
						"[Number of μops](../html-tp/SKL/ADD_R32_0-Measurements.html)",
						"Retire slots: 1",
						"Decoded (MITE): 1",
						"Microcode Sequencer (MS): 0",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-Measurements.html) 1*p0156",
						"[Macro-fusible with:](../html-tp/SKL/ADD_R32_0-Measurements.html) JB, JBE, JL, JLE, JNB, JNBE, JNL, JNLE, JNZ, JZ",
						"IACA 2.3",
						"[IACA:](../html-tp/SKL/ADD_R32_0-IACA2.3.html) 0.25",
						"[Number of μops:](../html-tp/SKL/ADD_R32_0-IACA2.3.html) 1",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-IACA2.3.html) 1*p0156",
						"IACA 3.0",
						"Computed from the port usage: 0.25",
						"Computed from the port usage: 0.25",
						"Computed from the port usage: 0.25",
						"[IACA:](../html-tp/SKL/ADD_R32_0-IACA3.0.html) 0.25",
						"[Number of μops:](../html-tp/SKL/ADD_R32_0-IACA3.0.html) 1",
						"[Port usage:](../html-ports/SKL/ADD_R32_0-IACA3.0.html) 1*p0156",
						"Measurements",
						"Latencies",
						"Throughput",
						"Throughput",
						"Throughput",
						"Executed: 1"
					]
				},
				{
					"title": "4. Instruction tables",
					"url": "https://www.cs.utexas.edu/~hunt/class/2016-spring/cs350c/documents/Agner-Fog/instruction_tables.pdf",
					"excerpts": [
						"ADC, SBB\nr,r/i\n1\n1\n1/3\nALU",
						"AND, OR, XOR\nr,r\n1\n1\n1/3\nALU",
						"AND, OR, XOR\nr,m\n1\n1\n1/2\nALU, AGU",
						"SHL, SHR, SAR\nr,i/CL\n1\n1\n1/3\nALU",
						"tables of instruction latencies, throughputs and micro-operation\nbreakdown and other tables for x86 family microprocessors from Intel, AMD and VIA",
						" figures in the instruction tables represent the results of my measurements rather than the offi-\ncial values published by microprocessor vendors.",
						"Copyright © 1996 – 2016. Last updated 2016-01-09."
					]
				},
				{
					"title": "CS107 Guide to x86-64",
					"url": "https://web.stanford.edu/class/cs107/guide/x86-64.html",
					"excerpts": [
						"Registers. The table below lists the commonly used registers (sixteen general-purpose plus two special). Each register is 64 bits wide; the lower 32-, 16 ...Read more"
					]
				},
				{
					"title": "Pipeline of Intel Core CPUs",
					"url": "https://www.uops.info/background.html",
					"excerpts": [
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these ...",
						"The latency of an instruction is [commonly](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) defined as the *\"number of clock cycles that are required for the execution\ncore to complete the execution of all of the μops that form\nan instruction\"* (assuming that there are no other instructions\nthat compete for execution resources). Thus",
						"Thus, it denotes the\ntime from when the operands of the instruction are ready and\nthe instruction can begin execution to when the results of the\ninstruction are ready.",
						"*To take this into account, we therefore provide separate latency numbers for each pair of input and output operands.*",
						"nt:\nWe use the following notation for the port usage: *3*p015+1*p23* , for example, denotes an instruction with four μops;\nthree of these μops can each be executed on ports 0, 1, and 5, and one μop can be executed on ports 2 and 3.\n ",
						"Section Title: Supported Microarchitectures\nContent:\n**Conroe:** Measurements (Core 2 Duo E6750)\n**Wolfdale:** Measurements (Core 2 Duo E8400)\n**Nehalem:** Measurements (Core i5-750), IACA 2.1, IACA 2.2\n**Westmere:** Measurements (Core i5-650), IACA 2.1, IACA2.2\n**Sandy Bridge:** Measurements (Core i7-2600), IACA 2.1, IACA 2.2, IACA 2.3\n**Ivy Bridge:** Measurements (Core i5-3470), IACA 2.1, IACA 2.2, IACA 2.3\n**Haswell:** Measurements (Xeon E3-1225 v3), IACA 2.1, IACA 2.2, IACA 2.3, IACA 3.0\n**Broadwell:** Measurements (Core i3-5005U), IACA 2.2, IACA 2.3, IACA 3.0\n**Skylake:** Measurements (Core i5-6500), IACA 2.3, IACA 3.0\n**Skylake-X:** Measurements (Core i9-9900X), IACA 2.3, IACA 3.0\n**Kaby Lake:** Measurements (Core i7-7700)\n**Coffee Lake:** Measurements (Core i7-8700K)\n**Cannon Lake:** Measurements (Core i3-8121U)\n**Cascade Lake:** Measurements (Core i9-10980XE)\n**Ice Lake:** Measurements (Core i5-1035G1), [Documentation](https://software.intel.com/en-us/download/10th-generation-intel-core-processor-instruction-throughput-and-latency-docs)\n**Tiger Lake:** Measurements (Core i7-1165G7)\n**Rocket Lake:** Measurements (Core i9-11900)\n**Alder Lake:** Measurements (Core i5-12600K)\n**Bonnell:** Measurements (Atom D525)\n**Airmont:** Measurements (Celeron N3000)\n**Goldmont:** Measurements (Celeron J3455)\n**Goldmont Plus:** Measurements (Celeron J4125)\n**Tremont:** Measurements (Pentium Silver N6005)\n**AMD Zen+:** Measurements (Ryzen 5 2600),",
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these ...Read more",
						"Knowledge of the latency, throughput and port usage is necessary for building performance-analysis tools, cycle-accurate simulators, and optimzing compilers.\nIt is also helpful when manually fine-tuning a piece of code for a specific processor.",
						"On this website, we present latency, throughput and port usage data for all generations of Intel's core microarchitecture that were obtained using these microbenchmarks.",
						"he latency of an instruction is [commonly](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) defined as the *\"number of clock cycles that are required for the execution\ncore to complete the execution of all of the μops that form\nan instruction\"* (assuming that there are no other instructions\nthat compete for execution resources). Thus, it denotes the\ntime from when the operands of the instruction are ready and\nthe instruction can begin execution to when the results of the\ninstruction are ready",
						"When comparing throughput data from different publications,\nit is important to note that these publications do not all use the\nsame definition of throughput. Intel defines throughput in its [manual](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf) as follows:\n*\"The number of clock cycles\nrequired to wait before the issue ports are free to accept\nthe same instruction again.\"*\nOn the other hand, [Agner Fog](https://www.agner.org/optimize/instruction_tables.pdf) uses the following definition\nfor (reciprocal) throughput:\n*\"The average number of\ncore clock cycles per instruction for a series of independent\ninstructions of the same kind in the same thread.\"*\nThese two definitions are not equivalent, as there can ",
						"**Conroe:** Measurements (Core 2 Duo E6750)\n**Wolfdale:** Measurements (Core 2 Duo E8400)\n**Nehalem:** Measurements (Core i5-750), IACA 2.1, IACA 2.2\n**Westmere:** Measurements (Core i5-650), IACA 2.1, IACA2.2\n**Sandy Bridge:** Measurements (Core i7-2600), IACA 2.1, IACA 2.2, IACA 2.3\n**Ivy Bridge:** Measurements (Core i5-3470), IACA 2.1, IACA 2.2, IACA 2.3\n**Haswell:** Measurements (Xeon E3-1225 v3), IACA 2.1, IACA 2.2, IACA 2.3, IACA 3.0\n**Broadwell:** Measurements (Core i3-5005U), IACA 2.2, IACA 2.3, IACA 3.0\n**Skylake:** Measurements (Core i5-6500), IACA 2.3, IACA 3.0\n**Skylake-X:** Measurements (Core i9-9900X), IACA 2.3, IACA 3.0\n**Kaby Lake:** Measurements (Core i7-7700)\n**Coffee Lake:** Measurements (Core i7-8700K)\n**Cannon Lake:** Measurements (Core i3-8121U)\n**Cascade Lake:** Measurements (Core i9-10980XE)\n**Ice Lake:** Measurements (Core i5-1035G1), [Documentation](https://software.intel.com/en-us/download/10th-generation-intel-core-processor-instruction-throughput-and-latency-docs)\n**Tiger Lake:** Measurements (Core i7-1165G7)\n**Rocket Lake:** Measurements (Core i9-11900)\n**Alder Lake:** Measurements (Core i5-12600K)\n**Bonnell:** Measurements (Atom D525)\n**Airmont:** Measurements (Celeron N3000)\n**Goldmont:** Measurements (Celeron J3455)\n**Goldmont Plus:** Measurements (Celeron J4125)\n**Tremont:** Measurements (Pentium Silver N6005)",
						"ntent:\n[Documentation](https://developer.amd.com/wordpress/media/2013/12/55723_3_00.ZIP)\n**AMD Zen 2:** Measurements (Ryzen 7 3700X), [Documentation](https://developer.amd.com/wp-content/resources/56305.zip)\n**AMD Zen 3:** Measurements (Ryzen 5 5600X), [Documentation](https://www.amd.com/system/files/TechDocs/56665.zip)\n**AMD Zen 4:** Measurements (Ryzen 5 7600X)"
					]
				},
				{
					"title": "AMD Ryzen 5800 - Agner's CPU blog",
					"url": "https://www.agner.org/forum/viewtopic.php?t=56",
					"excerpts": [
						"The throughput of the Zen 3 is now as high as six instructions per clock cycle. This may be six integer instructions or six floating point/ ...Read more"
					]
				},
				{
					"title": "x86-64 - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/X86-64",
					"excerpts": [
						"In addition to increasing the size of the general-purpose registers, the number of named general-purpose registers is increased from eight (i.e. eax , ebx , ecx , edx , esi , edi , esp , ebp ) in x86 to 16 (i.e. rax , rbx , rcx , rdx , rsi , rdi , rsp , rbp , r8 , r9 , r10 , r11 , r12 , r13 , r14 , r15 ). It is therefore possible to keep more local variables in registers rather than on the stack, and to let registers hold frequently accessed constants; arguments for small and fast subroutines may also be passed in registers to a greater extent.",
						"64-bit integer capability",
						"All [general-purpose registers](/wiki/General-purpose_register \"General-purpose register\") (GPRs) are expanded from 32 [bits](/wiki/Bit \"Bit\") to 64 bits, and all arithmetic and logical operations, memory-to-register and register-to-memory operations, etc., can operate directly on 64-bit integers."
					]
				},
				{
					"title": "Exploring 32-bit vs 64-bit Architectures: A Deep Dive into CPU Registers, Performance, and Memory in AMD and ARM | by Gitesh Wadhwa | Medium",
					"url": "https://medium.com/@GiteshWadhwa/exploring-32-bit-vs-64-bit-architectures-a-deep-dive-into-cpu-registers-performance-and-memory-fd853deaebb4",
					"excerpts": [
						"ARM64 operates with 64-bit wide registers, which means that each register can hold 64 bits of data (8 bytes).",
						"**31 general-purpose registers** : `X0-X30` , each 64-bit wide. These registers can be used for storing variables, addresses, or intermediate values during computation.",
						"In 32-bit ARM architecture (AArch32), these registers were 32-bit wide, significantly limiting data processing capacity.",
						"The x86 CPU has **general-purpose registers** , like `EAX` , `EBX` , `ECX` , etc., which are all 32-bit registers.",
						"In 64-bit mode, these general-purpose registers expand to 64 bits. The names change: `EAX` becomes `RAX` , `EBX` becomes `RBX` , and so on. These registers can hold larger values and memory addresses.",
						"In 64-bit mode, 16 general-purpose registers are available compared to 8 in 32-bit mode."
					]
				},
				{
					"title": "ADD (R32, M32)",
					"url": "https://www.uops.info/html-instr/ADD_R32_M32.html",
					"excerpts": [
						"Section Title: ADD (R32, M32) > Skylake",
						"Throughput\nComputed from the port usage: 0.50"
					]
				},
				{
					"title": "Zen5's AVX512 Teardown + More...",
					"url": "https://www.numberworld.org/blogs/2024_8_7_zen5_avx512_teardown/",
					"excerpts": [
						"**General Integer:**",
						"\n4 x 512-bit / cycle is possible and very easy to achieve. (no real bottlenecks in the way)\n4 x 51",
						"6 x ALU / cycle is not possible. Easy to hit 5/cycle, hard to get more. Most I can get in a synthetic is 5.5/cycle. Reason unknown - front end bottleneck?",
						"This is a drastic increase from the 2-way ILP that was sufficient to satisfy all prior AVX512 implementations on both Intel and AMD.\nSom",
						":**\nThe full Zen5 core doubles up the 512-bit load/throughput from Zen4, but it doesn't do it symmetrically.",
						"e)\n2 x 256-bit vector load ports. (2 x 256-bit load/cycle, or 1 x 512-bit load/cycle)\n",
						"PEXT/PDEP improves from 1 -> 3/cycle.",
						"**Integer Load/Store:**",
						"\n2 x 256-bit vector store ports. But only one can be used at a time. (1 x 256-bit store/cycle, or 0.5 x 512-bit store/cycle)\nMa",
						"3 integer load ports. (3 integer loads/cycle)\n2 i",
						"Maximum 4 load/store per cycle of any type. Though it's difficult to get more than 3.5/cycle when a store is involved unless the loads are mirrored/forwarded.",
						"**General SIMD (512-bit Datapath):**",
						"\n4 x 512-bit / cycle is possible and very easy to achieve. (no real bottlenecks in the way)\n4 ",
						"y.\nThis caught me by surprise since it wasn't revealed in AMD's GCC patch. Initially I suspected that this regression was a trade-off to achieve the full 256 -> 512-bit widening. So I asked AMD about this and they gave a completely different explanation. While I won't disclose their response (which I assume remains under NDA), I'll describe it as a CPU hazard that \"almost always\" turns 1-cycle SIMD instructions into 2-cycle latency.\n",
						"e)\n2 integer store ports. (2 integer stores/cycle)\n",
						"2 x 512-bit / cycle FADD.",
						"D.\n2 x 512-bit / cycle FMUL/FMA/IMUL/IFMA.\n",
						"A.\n2 x 512-bit FADD + 2 x 512-bit FMA / cycle (4 IPC) is possible.\n",
						".\n4 x 512-bit / cycle simple shuffle (unpacks, VPSHUFD).\n2",
						".\n2 x 512-bit / cycle complex shuffle (anything crossing a 128-bit boundary including permutes such as VPERMT2B).\nF",
						"FADD latency drops from 3 -> 2 cycles, but only if it can be forwarded. Otherwise remains 3 cycles.\nAll",
						"s.\nAll formerly 1-cycle latency SIMD instructions now have 2-cycle latency. Applies to all widths - including scalar.\n",
						":\nIf the problem really is just a hazard, we can hope that it will be fixed in a future AMD processor. But for now, developers should just assume 2-cycle latency. Combined with the 4 x 512-bit capability, you now need a minimum of 8-way ILP to saturate Zen5 even for formerly 1-cycle instructions. T",
						"hs.\nAll mask instructions are 2/cycle regardless of width. (Zen4 was 1/cycle for 64-bit mask)",
						"\n10-port limit remains, though Zen5 is better at avoiding it either by forwarding or caching of operands.\n**",
						"t:\nAs awesome as Zen5's AVX512 is, not everything is perfect. So let's start with the biggest regression I found:\n",
						"\nThroughput remains unchanged. The regression is only for latency.\nIn",
						"cy.\nInstructions that can be rename-eliminated (i.e. XOR zeroing) are unaffected and remain zero latency.",
						"**Load/Store Throughput:**",
						"le)\nMaximum 3 of any type of load/store each cycle.",
						"Key Differences from Zen4: Throughput of nearly all 512-bit instructions has been doubled up. (full Zen5 cores only); Latency of all 1-cycle ...Read more"
					]
				},
				{
					"title": "Do computers add four 16-bit numbers in one cycle already?",
					"url": "https://electronics.stackexchange.com/questions/735766/do-computers-add-four-16-bit-numbers-in-one-cycle-already",
					"excerpts": [
						"It has 4/clock throughput on Zen 5 vs. 2/clock on Zen 4. (Both also 1c latency). 16-bit SIMD multiplies also have at least 1/clock throughput ...Read more"
					]
				},
				{
					"title": "AVX512 Performance suggestions · Issue #21 · minio/md5-simd · GitHub",
					"url": "https://github.com/minio/md5-simd/issues/21",
					"excerpts": [
						"VPXORD would be the correct instruction. Benchmarks shows no impact. On Skylake, I wouldn't actually expect any, as I believe there's few bypass ...Read more"
					]
				},
				{
					"title": "Intel® 64 and IA-32 Architectures Optimization Reference ...",
					"url": "https://kib.kiev.ua/x86docs/Intel/Intel-OptimGuide/355308-001.pdf",
					"excerpts": [
						"\nWider machine:\n—\nWider allocation (5  6 uops per cycle) and retirement (4  8 uops per cycle) width.\n—\nIncrease in number of execution ports (10  12).\n—\nGreater capabilities per execution port.\nT",
						"Table 2 - 1 summarizes the OOO engine's capability to dispatch different types of operations to ports.",
						"The IDQ can hold 144 uops per logical processor in single thread mode, or 72 uops per thread when\nSMT is active.",
						"Ice Lake Client Microarchitecture Power Management",
						"Processors based on Ice Lake Client microarchitecture are the first client processors whose cores may\nexecute at a different frequency from one another. The frequency is selected based on the specific\ninstruction mix; the type, width and number of vector instructions of the program that executes on each\ncore, the ratio between active time and idle time of each core, and other considerations such as how\nmany cores share similar characteristics.",
						"Compute heavy workloads, especially those that span multiple Ice Lake client cores, execute at a lower\nfrequency than P0n, both under Intel AVX-512 and under Intel AVX2 instruction sets, due to power\nlimitations. In this scenario, Intel AVX-512 architecture, which requires less dynamic instructions to\ncomplete the same task than Intel AVX2 architecture, consumes less power and thus may achieve higher\nfrequency. The net result may be higher performance due to the shorter path length and a bit higher\nfrequency.",
						"Workloads that execute Intel AVX-512 instructions as a large proportion of their whole instruction count\ncan gain performance compared to Intel AVX2 instructions, even though they may operate at a lower\nfrequency. For example, maximum frequency bound Deep Learning workloads that target Intel AVX-512\nheavy instructions at a very high percentage can gain 1.3x-1.5x performance improvement vs. the same\nworkload built to target Intel AVX2 (both operating on Skylake Server microarchitecture).",
						"Table 2 - 10. Maximum Intel® Turbo Boost Technology Core Frequency Levels\nLev",
						"Level",
						"category",
						"Frequency Level",
						"Max Frequency (P0n)",
						"Instruction Types",
						"0",
						"Highest",
						"t\nMax\nScalar, AVX128, SSE, Intel® AVX2 w/o FP\nor INT MUL/FMA\n",
						"1",
						"Medium",
						"Lowest",
						"A significant increase in size of reorder buffer, load buffer, store buffer, and reservation stations\nenable deeper OOO execution and higher cache bandwidth.",
						"Wider machine: 4  5 wide allocation, 8  10 execution ports.",
						"Greater capabilities per execution port (e.g., SIMD shuffle, LEA).",
						"Reduced latency Integer Divider.",
						"A new iDIV unit was added that significantly reduces the latency and improves the of throughput of\ninteger divide operations.",
						"Table 2 - 4 summarizes the OOO engine's capability to dispatch different types of operations to ports.",
						"Table 2 - 5 lists execution units and common representative instructions that rely on these units.",
						"Execution\nUnit\n# of\nUnit\nInstructions\nALU\n4\nadd, and, cmp, or, test, xor, movzx, movsx, mov, (v)movdqu, (v)movdqa, (v)movap*, (v)movup*",
						"SHFT\n2\nsal, shl, rol, adc, sarx, adcx, adox, etc.",
						"ec ALU\n3\n(v)pand, (v)por, (v)pxor, (v)movq, (v)movq, (v)movap*, (v)movup*, (v)andp*, (v)orp*,\n(v)paddb/w/d/q, (v)blendv*, (v)blendp*, (v",
						"Vec_Shft\n2\n(v)psllv*, (v)psrlv*, vector shift count in imm8",
						"Vec Add\n2\n(v)addp*, (v)cmpp*, (v)max*, (v)min*, (v)padds*, (v)paddus*, (v)psign, (v)pabs, (v)pavgb,\n(v)pcmpeq*, (v)pmax, (v)cvtps2dq, (v)cvtdq2ps, (v)cvtsd2si, (v)cvtss2si",
						"The attributes that are relevant to the producer/consumer micro-ops for bypass are a triplet of abbrevi -\nation/one or more port number/latency cycle of the uop. ",
						"All instructions of the form “load and jump,” for example:\nJMP\n[RDI+200]\nRET",
						"CMP and TEST with immediate operand and memory.",
						"An Intel 64 instruction with RIP relative addressing is not micro-fused in the following cases:",
						"Optimizing for Macrofusion",
						"The first instruction of the macro-fused pair must be a CMP or TEST instruction. This instruction can be\nREG-REG, REG-IMM, or a micro-fused REG-MEM comparison. The second instruction (adjacent in the\ninstruction stream) should be a conditional branch.",
						"Since these pairs are common ingredient in basic iterative programming sequences, macrofusion\nimproves performance even on un-recompiled binaries. All of the decoders can decode one macro-fused\npair per cycle, with up to three other instructions, resulting in a peak decode bandwidth of 5 instructions\nper cycle.",
						"The following examples are some of the types of micro-fusions that can be handled by all decoders:\n•\nAll stores to memory, including store immediate. Stores execute internally as two separate\nmicro-ops: store-address and store-data.\n",
						"The following examples are some of the types of micro-fusions that can be handled by all decoders:\n•\nAll stores to memory, including store immediate. Stores execute internally as two separate\nmicro-ops: store-address and store-data.\n",
						"3.4.2.2\nOptimizing for Macrofusion",
						"Macrofusion merges two instructions to a single micro-op. Intel Core microarchitecture performs this\nhardware optimization under limited circumstances.",
						"Macrofusion merges two instructions to a single micro-op. Intel Core microarchitecture performs this\nhardware optimization under limited circumstances.",
						"2"
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": ["0", "1", "1", "1", "1", "1", "1", "2", "2", "2", "2"]
				},
				{
					"title": "356477-Optimization-Reference-Manual-V2-002.pdf",
					"url": "https://cdrdv2-public.intel.com/814199/356477-Optimization-Reference-Manual-V2-002.pdf",
					"excerpts": [
						"The execution core contains the following three execution stacks:\n•\nSIMD integer.\n•\nRegular integer.\n•\nx87/SIMD floating-point.",
						"LEA should be used for address computation.",
						"Thus, LEA should not be used in the technique of\nadding two values and produce the result in a third register. LEA should be used for address computation.",
						"Partial Register Access**\nAlthough partial register access does not cause additional delay, the in-order hardware tracks dependency on the full\nregister. Thus 8-bit registers like AL and AH are not treated as independent registers. Additionally some instructions\nlike LEA, vanilla loads, and pop are slower when the input is smaller than 4 bytes.",
						"Integer shift instructions that encodes shift count in the immediate byte have one-cycle latency. In contrast, shift\ninstructions using shift count in the ECX register may need to wait for the register count are updated. Thus shift\ninstruction using register count has 3-cycle latency.",
						"•\n**Implicit ESP updates** : When the ESP register is not used as the destination of an instruction (explicit ESP updates),\nan implicit ESP update will occur with instructions like PUSH, POP, CALL, RETURN. Mixing explicit ESP updates and\nimplicit ESP updates will also lead to dependency between address generation and data execution.",
						"The address generation unit (AGU) may be used directly in three situations that affect execution throughput of the\ntwo-wide machine. The situations are:",
						"mov eax, 0x01",
						"*Assembly/Compiler Coding Rule 5. (MH impact, H generality)*** *For Intel Atom processors, LEA should be used for*\n*address manipulation; but software should avoid the following situations which creates dependencies from ALU to*\n*AGU: an ALU instruction (instead of LEA) for address manipulation or ESP updates; a LEA for ternary addition or non-*\n*destructive writes which do not feed address generation. Alternatively, hoist producer instruction more than 3 cycles*\n*above the consumer instruction that uses the AGU.*",
						"•\n**LEA** : The LEA instruction uses the AGU instead of the ALU. If one of the source register of LEA must come from an\nexecution unit. This dependency will also cause a 3 cycle delay. Thus, LEA should not be used in the technique of\nadding two values and produce the result in a third register. LEA should be used for address computation.",
						"**Assembly/Compiler Coding Rule 6. (M impact, M generality)*** *For Intel Atom processors, sequence an independent*\n*FP or integer multiply after an integer multiply instruction to take advantage of pipelined IMUL execu",
						"***Assembly/Compiler Coding Rule 7. (M impact, M generality)*** *For Intel Atom processors, hoist the producer*\n*instruction for the implicit register count of an integer shift instruction before the shift instruction by at least two*\n*cycles.",
						"**6.3.2.4**",
						"**Integer Shift Instructions**",
						"LEA should not be used in the technique of\nadding two values and produce the result in a third register. LEA should be used for address computation.",
						"The address generation unit (AGU) may be used directly in three situations that affect execution throughput of the\ntwo-wide machine.",
						"6.3.2.2",
						"Address Generation",
						"The hardware optimizes the general case of instruction ready to execute must have data ready, and address genera -\ntion precedes data being ready. If address generation encounters a dependency that needs data from another instruc -\ntion, this dependency in address generation will incur a delay of 3 cycles.",
						"6-34",
						"The rule of thumb for ADDs and LEAs is that it is justified to use LEA with a valid index and/or\ndisplacement for non-destructive destination purposes (especially useful for stack offset cases), or to use a SCALE.",
						"Otherwise, ADD(s) are preferable.",
						"Table 6 - 1 summarizes the characteristics of various execution units in Intel Atom microarchitecture that are likely\nused most frequently by software."
					]
				},
				{
					"title": "ADD (R64, I8)",
					"url": "https://uops.info/html-instr/ADD_R64_I8.html",
					"excerpts": [
						"Throughput\nComputed from the port usage: 0.25",
						"[Port usage:](../html-ports/SKX/ADD_R64_I8-Measurements.html) 1*p0156",
						"Section Title: ADD (R64, I8) > AMD Zen 3",
						"Content:",
						"Measurements",
						"Latencies",
						"[Latency operand 1 → 1:](../html-lat/ZEN3/ADD_R64_I8-Measurements.html%3E1)",
						"[Latency operand 1 → 3:](../html-lat/ZEN3/ADD_R64_I8-Measurements.html%3E3)",
						"Throughput",
						"[Measured (loop):](../html-tp/ZEN3/ADD_R64_I8-Measurements.html) 0.25",
						"[Measured (unrolled):](../html-tp/ZEN3/ADD_R64_I8-Measurements.html) 0.25",
						"[Number of μops](../html-tp/ZEN3/ADD_R64_I8-Measurements.html)",
						"Executed: 1"
					]
				},
				{
					"title": "ADD_03 (R64, R64)",
					"url": "https://uops.info/html-instr/ADD_03_R64_R64.html",
					"excerpts": [
						"Throughput\nComputed from the port usage: 0.25",
						"Throughput\nComputed from the port usage: 0.25"
					]
				},
				{
					"title": "Find out instruction latency and throughput for NEON and SVE instructions - Architectures and Processors forum - Support forums - Arm Community",
					"url": "https://community.arm.com/support-forums/f/architectures-and-processors-forum/53027/find-out-instruction-latency-and-throughput-for-neon-and-sve-instructions",
					"excerpts": ["Content:"]
				},
				{
					"title": "How to Load Constants in Assembly for Arm Architecture",
					"url": "https://developer.arm.com/community/arm-community-blogs/b/architectures-and-processors-blog/posts/how-to-load-constants-in-assembly-for-arm-architecture",
					"excerpts": [
						"We'll cover shifted registers in a future post. For now, we're only interested in the constants. Examples of such instructions are: add r0, r1, ...Read more"
					]
				},
				{
					"title": "Bitwise operation - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/Bitwise_operation",
					"excerpts": [
						"Boolean algebra is used to simplify complex bitwise expressions. AND. edit. x & y = y & x; x & (y & z) = (x & y) & z; x & 0xFFFF = x; x & 0 = 0. x & x = x. OR.Read more"
					]
				},
				{
					"title": "c - XORing a 32 bit integer with itself - Stack Overflow",
					"url": "https://stackoverflow.com/questions/40144208/xoring-a-32-bit-integer-with-itself",
					"excerpts": [
						"To accumulate the XOR of 8-bit values, you simply shift and XOR each part of the value.",
						"However, since XOR can be done in any order, you can do the same with fewer operations:",
						"This is quite similar to how parallel commutative operations are done in larger registers with technologies like SIMD (and indeed, compilers with SIMD support should be able to optimize the following code to make it much faster):"
					]
				},
				{
					"title": "\nBit Twiddling Hacks\n",
					"url": "https://graphics.stanford.edu/~seander/bithacks.html",
					"excerpts": [
						"\nThis is an old trick to exchange the values of the variables a and b *without using extra space for a temporary variable*",
						".\nOn January 20, 2005, Iain A. Fleming pointed out that the macro above doesn't work when you swap with the same memory location, such as\nSWAP(a[i], a[j]) with i == j. So if that may occur, consider\ndefining the macro as (((a) == (b)) || (((a) ^= (b)), ((b) ^= (a)), ((a) ^= (b)))).",
						"\nOn July 14, 2009, Hallvard Furuseth suggested that on some machines,\n(((a) ^ (b)) && ((b) ^= (a) ^= (b), (a) ^= (b))) might be faster, since the (a) ^ (b) expression is reused.",
						"Section Title: Bit Twiddling Hacks > By Sean Eron Anderson > Swapping values with XOR",
						"Section Title: Bit Twiddling Hacks > By Sean Eron Anderson > Swapping individual bits with XOR",
						"This method of swapping is similar to the general purpose XOR swap\ntrick, but intended for operating on individual bits.  The variable x stores the result of XORing the pairs of bit values we want to swap, and then the bits are set to the result of themselves XORed with x.  Of course, the result is undefined if the sequences overlap.\nO"
					]
				},
				{
					"title": "Bit Hacks",
					"url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cc6983c9ebd77c28e8ae85bc0e575360_MIT6_172F18_lec3.pdf",
					"excerpts": [
						"XOR is its own inverse:",
						"'\nXOR is its own inverse:\n",
						"XOR (exclusive OR)",
						"\nXOR (exclusive OR)\n",
						"Set the kth Bit\nProblem\nSet ! th bit in a word \"% to # .\nIdea\nShift and OR.",
						"Set the kth Bit",
						"Idea\nShift, complement, and AND."
					]
				},
				{
					"title": "\n Guide to x86 Assembly\n",
					"url": "https://www.cs.virginia.edu/~evans/cs216/guides/x86.html",
					"excerpts": [
						"These instructions shift the bits in their first operand's contents\nleft and right, padding the resulting empty bit\npositions with zeros. The shifted operand can be shifted up to 31 places. The\nnumber of bits to shift is specified by the second operand, which can be\neither an 8-bit constant or the register CL. In either case, shifts counts of\ngreater then 31 are performed modulo 32.",
						"shl eax, 1 — Multiply the value of EAX\nby 2 (if the most significant bit is 0)",
						"shr ebx, cl — Store in EBX the floor of result of dividing the value of EBX\nby 2 *n* where *n* is the value in CL."
					]
				},
				{
					"title": "SAL/SAR/SHL/SHR\n\t\t— Shift",
					"url": "https://www.felixcloutier.com/x86/sal:sar:shl:shr",
					"excerpts": [
						"However, all other IA-32 processors (starting with the Intel 286 processor) do mask the shift count to 5 bits, resulting in a maximum count of 31. This masking ...Read more"
					]
				},
				{
					"title": "SAL/SAR/SHL/SHR: Shift (x86 Instruction Set Reference)",
					"url": "https://c9x.me/x86/html/file_module_x86_id_285.html",
					"excerpts": [
						"IA-32 Architecture Compatibility. The 8086 does not mask the shift count. However, all other IA-32 processors (starting with the Intel 286 processor) do mask ...Read more"
					]
				},
				{
					"title": "Arithmetic bit-shift on a signed integer",
					"url": "https://stackoverflow.com/questions/4009885/arithmetic-bit-shift-on-a-signed-integer",
					"excerpts": [
						"I am trying to figure out how exactly arithmetic bit-shift operators work in C, and how it will affect signed 32-bit integers.Read more"
					]
				},
				{
					"title": "Instruction Set Reference, A-Z",
					"url": "https://cdrdv2-public.intel.com/835757/325383-sdm-vol-2abcd.pdf",
					"excerpts": [
						".. \nRORX — Rotate Right Logical Without Affecting Flags\nInstruction Operand Encoding\nDescription\nRotates the bits of second operand right by the count value specified in imm8 without affecting arithmetic flags.\nThe RORX instruction does not read or write the arithmetic flags.\nThis instruction is not supported in real mode and virtual-8086 mode. The operand size is always 32 bits if not in\n64-bit mode. In 64-bit mode operand size 64 requires VEX.W1. VEX.W1 is ignored in non-64-bit modes. An\nattempt to execute this instruction with VEX.L not equal to 0 will cause #UD.\nOperation\nIF (OperandSize = 32)\ny := imm8 AND 1FH;\nDEST := (SRC >> y) | (SRC << (32-y));\nELSEIF (OperandSize = 64 )\ny := imm8 AND 3FH;\nDEST := (SRC >> y) | (SRC << (64-y));\nFI;\nFlags Affected\nNone.",
						"IA-32 Architecture Compatibility\nThe 8086 does not mask the shift count. However, all other IA-32 processors (starting with the Intel 286 processor)\ndo mask the shift count to 5 bits, resulting in a maximum count of 31.",
						"SHLD r/m64, r64, CL\nMRC\nValid\nN.E.\nShift r/m64 to left CL places while shifting bits\nfrom r64 in from the right.",
						"SHRD r/m64, r64, CL\nMRC\nValid\nN.E.\nShift r/m64 to right CL places while shifting\nbits from r64 in from the left."
					]
				},
				{
					"title": "Volume 2 (2A, 2B, 2C & 2D): Instruction Set Reference, AZ",
					"url": "https://cdrdv2-public.intel.com/671110/325383-sdm-vol-2abcd.pdf",
					"excerpts": [
						"(In 64-Bit Mode and REX.W = 1)\nTHEN COUNT := COUNT MOD 64;\nELSE COUNT := COUNT MOD 32;\nFI",
						"tempCOUNT := (COUNT AND countMASK);",
						"tempDEST := DEST;",
						"The shift arithmetic left (SAL) and shift logical left (SHL) instructions perform the same operation; they shift the\nbits in the destination operand to the left (toward more significant bit locations). For each shift count, the most\nsignificant bit of the destination operand is shifted into the CF flag, and the least significant bit is cleared (see\n ..",
						"s.\nIA-32 Architecture Compatibility\nThe 8086 does not mask the shift count. However, all other IA-32 processors (starting with the Intel 286 processor)\ndo mask the shift count to 5 bits, resulting in a maximum count of 31. This masking is done in all operating modes\n(including the virtual-8086 mode) to reduce the maximum execution time of the instructions.",
						"\nIn 64-bit mode, the instruction’s default operation size is 32 bits and the mask width for CL is 5 bits. Using a REX\nprefix in the form of REX.R permits access to additional registers (R8-R15). Using a REX prefix in the form of REX.W\npromotes operation to 64-bits and sets the mask width for CL to 6 bits. See the summary chart at the beginning of\nthis section for encoding data and limits.",
						"SAL/SAR/SHL/SHR—Shift\nINSTRUCTION SET REFERENCE, M-U\nVol. 2B 4-601\nThe shift arithmetic right (SAR) and shift logical right (SHR) instructions shift the bits of the destination operand to\nthe right (toward less significant bit locations). For each shift count, the least significant bit of the destination\noperand is shifted into the CF flag, and the most significant bit is either set or cleared depending on the instruction\ntype. The SHR instruction clears the most significant bit (see Figure 7-8 in the *Intel® 64 and IA-32 Architectures*\n*Software Developer’s Manual, Volume 1* ); the SAR instruction sets or clears the most significant bit to correspond\nto the sign (most significant bit) of the original value in the destination operand. In effect, the SAR instruction fills\nthe empty bit position’s shifted value with the sign of the unshifted value (see Figure 7-9 in the *Intel® 64 and IA-32*\n*Architectures Software Developer’s Manual, Volume 1* ).",
						" instructions can be used to perform signed or unsigned division, respectively, of the destination\noperand by powers of 2. For example, using the SAR instruction to shift a signed integer 1 bit to the right divides\nthe value by 2.\nUsing the SAR instruction to perform a division operation does not produce the same result as the IDIV instruction.\nThe quotient from the IDIV instruction is rounded toward zero, whereas the “quotient” of the SAR instruction is\nrounded toward negative infinity. This difference is apparent only for negative numbers. For example, when the\nIDIV instruction is used to divide -9 by 4, the result is -2 with a remainder of -1. If the SAR instruction is used to\nshift -9 right by two bits, the result is -3 and the “remainder” is + 3; however, the SAR instruction stores only the\nmost significant bit of the remainder (in the CF flag).\nThe OF"
					]
				},
				{
					"title": "bit manipulation - When does bit shifting cause undefined behavior in C++? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/76694990/when-does-bit-shifting-cause-undefined-behavior-in-c",
					"excerpts": [
						"Section Title: Before C++20 > Positive signed integer\nContent:\n**Left shift** : Shifted-out bits get discarded and the least significant bits are filled with zeros. **If the shifted-out bits are not all zeros, the behavior is undefined.**\n**Right shift** : Shifted-out bits get discarded and the most significant bits are filled with zeros.",
						"Section Title: Before C++20 > Negative signed integer\nContent:\n**Left shift** : **Undefined behavior.**\n**Right shift** : **Implementation defined** . Usually for 2's complement systems, the shifted-out bits get discarded, and the most significant bits are filled with **ones** .",
						"Section Title: Before C++20 > Unsigned integer\nContent:\n**Left shift** : Shifted-out bits get discarded and the least significant bits are filled with zeros.\n**Right shift** : Shifted-out bits get discarded and the most significant bits are filled with zeros.",
						"Section Title: After C++20 > Positive signed integer\nContent:\n**Left shift** : Shifted-out bits get discarded and the least significant bits are filled with zeros.\n**Right shift** : Shifted-out bits get discarded and the most significant bits are filled with zeros.",
						"Section Title: After C++20 > Negative signed integer\nContent:\n**Left shift** : Shifted-out bits get discarded and the least significant bits are filled with zeros.\n**Right shift** : Shifted-out bits get discarded, and the most significant bits are filled with **ones** .\n",
						"In all cases, if the number of bits to shift(the right operand) is greater than or equal to the number of bits of the left operand after [integer promotion](https://en.cppreference.com/w/cpp/language/implicit_conversion) , the behavior is undefined."
					]
				},
				{
					"title": "c++ - Is right shift undefined behavior if the count is larger than the width of the type? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/18918256/is-right-shift-undefined-behavior-if-the-count-is-larger-than-the-width-of-the-t",
					"excerpts": [
						"e behavior is undefined if the right operand** is negative, or **greater than or equal to the length in bits of the promoted left operan",
						"warning: right shift count >= width of type",
						"The value of E1 >> E2 is E1 right-shifted E2 bit positions."
					]
				},
				{
					"title": "Left shift and right shift operators: << and >> | Microsoft Learn",
					"url": "https://learn.microsoft.com/en-us/cpp/cpp/left-shift-and-right-shift-operators-input-and-output?view=msvc-170",
					"excerpts": [
						"The left-shift operator causes the bits in *`shift-expression`* to be shifted to the left by the number of positions specified by *`additive-expression`* . The bit positions that have been vacated by the shift operation are zero-filled. A left shift is a logical shift (the bits that are shifted off the end are discarded, including the sign bit). For more information about the kinds of bitwise shifts, see [Bitwise shifts](https://en.wikipedia.org/wiki/Bitwise_shift) .",
						"If you left-shift a signed number so that the sign bit is affected, the result is undefined.",
						"The right-shift operator causes the bit pattern in *`shift-expression`* to be shifted to the right by the number of positions specified by *`additive-expression`* . For unsigned numbers, the bit positions that have been vacated by the shift operation are zero-filled. For signed numbers, the sign bit is used to fill the vacated bit positions. In other words, if the number is positive, 0 is used, and if the number is negative, 1 is used.",
						"The result of a right-shift of a signed negative number is implementation-dependent.",
						"The result of a shift operation is undefined if *`additive-expression`* is negative or if *`additive-expression`* is greater than or equal to the number of bits in the (promoted) *`shift-expression`* . No shift operation takes place if *`additive-expression`* is"
					]
				},
				{
					"title": "Arithmetic operators - cppreference.com",
					"url": "http://en.cppreference.com/w/cpp/language/operator_arithmetic.html",
					"excerpts": [
						"For the built-in bitwise shift operators, both operands must be prvalues of integral or unscoped enumeration type. Integral promotions are performed on both operands.",
						"If the value of rhs is negative or is not less than the number of bits in lhs , the behavior is undefined.",
						"For unsigned a , the value of a << b is the value of a * 2 b\n, reduced modulo 2 N\nwhere N is the number of bits in the return type (that is, bitwise left shift is performed and the bits that get shifted out of the destination type are discarded)",
						"For negative a , the behavior of a << b is undefined.",
						"For unsigned a and for signed and non-negative a , the value of a >> b is the integer part of a/2 b\n"
					]
				},
				{
					"title": "ADD (shifted register) -- A64",
					"url": "https://www.scs.stanford.edu/~zyedidia/arm64/add_addsub_shift.html",
					"excerpts": [
						"Add (shifted register) adds a register value and an optionally-shifted\nregister value, and writes the result to the destination register.",
						"gister.\nSection Title: ADD (shifted register)\nContent:\n| 31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| sf | 0 | 0 | 0 | 1 | 0 | 1 | 1 | shift | 0 | Rm | imm6 | Rn | Rd |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  | op | S |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\nSection Title: ADD (shifted register) > 32-bit (sf == 0)\nContent:\nADD []( \"Is the 32-bit name of the general-purpose destination register, encoded in the \"Rd\" field.\") , []( \"Is the 32-bit name of the first general-purpose source register, encoded in the \"Rn\" field.\") , []( \"Is the 32-bit name of the second general-purpose source register, encoded in the \"Rm\" field.\") {, []( \"Is the optional shift type to be applied to the second source operand, defaulting to LSL and \") # []( \"For the 32-bit variant: is the shift amount, in the range 0 to 31, defaulting to 0 and encoded in the \"imm6\" field.\") }\nSection Title: ADD (shifted register) > 32-bit (sf == 0) > 64-bit (sf == 1)\nContent:\nADD []( \"Is the 64-bit name of the general-purpose destination register, encoded in the \"Rd\" field.\") , []( \"Is the 64-bit name of the first general-purpose source register, encoded in the \"Rn\" field.\") , []( \"Is the 64-bit name of the second general-purpose source register, encoded in the \"Rm\" field.\") {, []( \"Is the optional shift type to be applied to the second source operand, defaulting to LSL and \") # []( \"For the 64-bit variant: is the shift amount, in the range 0 to 63, defaulting to 0 and encoded in the \"imm6\" field.\") }\ninteger ",
						"\nSection Title: ADD (shifted register) > 32-bit (sf == 0) > Assembler Symbols\nContent:\n|  | Is the 32-bit name of the general-purpose destination register, encoded in the \"Rd\" field. |\nS",
						"Section Title: ADD (shifted register) > 32-bit (sf == 0) > Operation\nContent:\nbits(datasize) result;\nbits(datasize) operand1 = [X](shared_pseudocode.html.X.read.2 \"accessor: bits(width) X[integer n, integer width]\") [n, datasize];\nbits(datasize) operand2 = [ShiftReg](shared_pseudocode.html.ShiftReg.4 \"function: bits(N) ShiftReg(integer reg, ShiftType shiftype, integer amount, integer N)\") (m, shift_type, shift_amount, datasize);\n(result, -) = [AddWithCarry](shared_pseudocode.html.AddWithCarry.3 \"function: (bits(N), bits(4)) AddWithCarry(bits(N) x, bits(N) y, bit carry_in)\") (operand1, operand2, '0'); [X](shared_pseudocode.html.X.write.2 \"accessor: X[integer n, integer width] = bits(width) value\") [d, datasize] = result;",
						"Section Title: ADD (shifted register)\nContent:\nAdd (shifted register) adds a register value and an optionally-shifted\nregister value, and writes the result to the destination register.",
						"Section Title: ADD (shifted register)\nContent:\n| 31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |\n| sf | 0 | 0 | 0 | 1 | 0 | 1 | 1 | shift | 0 | Rm | imm6 | Rn | Rd |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |",
						"|  | op | S |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"
					]
				},
				{
					"title": "Microarchitectural analysis of image quality assessment ...",
					"url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-23/issue-01/013030/Microarchitectural-analysis-of-image-quality-assessment-algorithms/10.1117/1.JEI.23.1.013030.full",
					"excerpts": [
						"LEA instruction is an X86 assembly instruction generated during compilation. On an Intel processor, one solution is to use an Intel compiler ...Read more"
					]
				},
				{
					"title": "Hacker's Delight: The Basics | 2-1 Manipulating Rightmost Bits | InformIT",
					"url": "https://www.informit.com/articles/article.aspx?p=1959565",
					"excerpts": [
						"Henry S. Warren, author of \"Hacker's Delight,\" offers timesaving techniques, algorithms, and tricks that help programmers build more elegant and efficient ..."
					]
				},
				{
					"title": "linear algebra - \"GF(2)-like\" field with addition operation defined by OR instead of XOR - Mathematics Stack Exchange",
					"url": "https://math.stackexchange.com/questions/2038143/gf2-like-field-with-addition-operation-defined-by-or-instead-of-xor",
					"excerpts": [
						"The appeal of using a field such as GF(2) is that all the standard results from linear algebra can be applied to vectors and matrices defined ...Read more"
					]
				},
				{
					"title": "C6.2.5 ADD (shifted register) Assembler symbols",
					"url": "https://www.cs.princeton.edu/courses/archive/spr20/cos217/precepts/18overrun/precept18.pdf",
					"excerpts": [
						"Add (shifted register) adds a register value and an optionally-shifted register value, and writes the result to the\ndestination register.",
						"32-bit variant",
						"Applies when sf == 0 .",
						"64-bit variant",
						"Applies when sf == 1 .",
						"integer shift_amount = UInt (imm6);",
						"ShiftType shift_type = DecodeShift (shift);",
						"ShiftType shift_type = DecodeShift (shift);",
						"Assembler symbols",
						"Is the 32-bit name of the general-purpose destination register, encoded in the \"Rd\" field.",
						"Is the 32-bit name of the first general-purpose source register, encoded in the \"Rn\" field.",
						"Is the 32-bit name of the second general-purpose source register, encoded in the \"Rm\" field.",
						"Is the 64-bit name of the general-purpose destination register, encoded in the \"Rd\" field.",
						"Is the 64-bit name of the first general-purpose source register, encoded in the \"Rn\" field.",
						"Is the 64-bit name of the second general-purpose source register, encoded in the \"Rm\" field.",
						"Is the optional shift type to be applied to the second source operand, defaulting to LSL and encoded",
						"in the \"shift\" field. It can have the following values:\nLSL\nwhen shift = 00\nLSR\nwhen shift = 01\nASR\nwhen shift = 10\nThe encoding shift = 11 is reserved.",
						"For the 32-bit variant: is the shift amount, in the range 0 to 31, defaulting to 0 and encoded in the\n\"imm6\" field.",
						"For the 32-bit variant: is the shift amount, in the range 0 to 31, defaulting to 0 and encoded in the\n\"imm6\" field.",
						"For the 64-bit variant: is the shift amount, in the range 0 to 63, defaulting to 0 and encoded in the\n\"imm6\" field.",
						"For the 64-bit variant: is the shift amount, in the range 0 to 63, defaulting to 0 and encoded in the\n\"imm6\" field.",
						"X [d] = result;",
						"2",
						"C6 A64 Base Instruction Descriptions",
						"C6.2 Alphabetical list of A64 base instructions"
					]
				},
				{
					"title": "\n  Encoding of immediate values on AArch64 · Dominik's Blog\n",
					"url": "https://dinfuehr.github.io/blog/encoding-of-immediate-values-on-aarch64/",
					"excerpts": [
						"All instructions of the add/sub immediate instruction class allow a 12-bit unsigned immediate that can optionally be shifted by 12 bits (1 ...Read more"
					]
				},
				{
					"title": "The AArch64 processor (aka arm64), part 4: Addition and subtraction - The Old New Thing",
					"url": "https://devblogs.microsoft.com/oldnewthing/20220729-00/?p=106915",
					"excerpts": [
						"To ask for flags to be set based on the result, apply an `S` suffix to the opcode, producing `ADDS` .",
						"e first two versions add an immediate. It is either a 12-bit unsigned immediate (0 ≤ n ≤ 4095) or a 12-bit unsigned immediate shifted left by 12. This means that you can express constants of the form `0x00000XXX` and `0x00XXX000` . The disassembler does the `LSL #12` for you, so you won’t actually see the `, LSL #12` version disassembled as such. Instead, you’ll see the shifted constan",
						"The next block of variants adds a shifted register. You are allowed to shift doublewords by up to 63 positions and words up to 31 positions.",
						"The last block lets you use the extended registers. You can use all of the extended forms, and the shift amount can be up to four positions. These extended registers with shifts are convenient for calculating array offsets:",
						"In this case, x1 is the base of an array where each element is of size 16, and x2 is a 32-bit signed array index, and we calculate the address of the element into x0 .",
						"The subtraction instruction has the same available variants as the addition instructions.",
						"Adding and subtracting with carry have only one encoding option."
					]
				},
				{
					"title": "xxHash: xxHash",
					"url": "https://xxhash.com/doc/v0.8.2/index.html",
					"excerpts": ["|\n| HighwayHash |  | 64 | 1.4 GB/s | 6.0 |\n"]
				},
				{
					"title": "Arm Architecture Reference Manual for A-profile architecture",
					"url": "https://developer.arm.com/documentation/ddi0487/latest/",
					"excerpts": [
						"This Manual describes the Arm® architecture v8, Armv8, and the Arm® architecture v9, Armv9. The architecture describes the operation of an Armv8-A and an ..."
					]
				},
				{
					"title": "A64 Instruction Set Architecture Guide",
					"url": "https://documentation-service.arm.com/static/674d8b61c7fc0d1f211dc776?token=",
					"excerpts": [
						"A64 instruction set is used when executing in the AArch64 Execution state. It is a fixed-length\n32-bit instruction set.",
						"The Arm architecture describes instructions following a Simple Sequential Execution (SSE) model.",
						"Armv8-A supports three instruction sets: A32, T32 and A64.",
						"The 64 in the name refers to the use of this instruction by the AArch64\nExecution state. It does not refer to the size of the instructions in memory.",
						"The architecture provides 31 general purpose registers.\nEach register can be used as a 64-bit X register (X0..X30), or as a 32-bit W register (W0..W30).",
						"Using\nX registers will result in 64-bit calculations, and using W registers will result in 32-bit calculations.",
						"\nThe ADR instruction returns the address of a label, calculated based on the current location. Dot ( . )\nmeans ‘here’, so the shown instruction is returning the address of itself. This is equivalent to reading\nthe PC",
						"There is a separate set of 32 registers used for floating point and vector operations. These registers\nare 128-bit, but like the general-purpose registers, can be accessed in several ways.",
						"operations"
					]
				},
				{
					"title": "ARM Instruction Set",
					"url": "https://iitd-plos.github.io/col718/ref/arm-instructionset.pdf",
					"excerpts": [
						"The data processing operations may be classified as logical or arithmetic. The logical\noperations (AND, EOR, TST, TEQ, ORR, MOV, BIC, MVN) perform the logical action\non all corresponding bits of the operand or operands to produce the result.",
						"The instruction encoding is shown in **Figure 4-4: Data processing instructions**\nbelow.",
						"The second operand may be a shifted register (Rm) or a rotated 8 bit immediate value\n(Imm) according to the value of the I bit in the instruction.",
						"The second operand may be a shifted register (Rm) or a rotated 8 bit immediate value\n(Imm) according to the value of the I bit in the instruction.",
						"The condition codes in the\nCPSR may be preserved or updated as a result of this instruction, according to the\nvalue of the S bit in the instruction.",
						"**4.5.7 Instruction cycle times**\nData Processing instructions vary in the number of incremental cycles taken as follows:\nS, N and I are as defined in **6.2 Cycle Types** on page 6-3.\n**4.5.8 Assembler syntax**\n1\nMOV,MVN (single operand instructions"
					]
				},
				{
					"title": "Armv8-A Instruction Set Architecture",
					"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Learn%20the%20Architecture/Armv8-A%20Instruction%20Set%20Architecture.pdf",
					"excerpts": [
						"When operand 2 is a constant, it is encoded within the instruction itself. This means that the\nrange of constants available is limited.",
						"•\nOperand 2: This will be a register or a constant, and is the second input to the instruction.\nWhen operand 2 is a register, it may include an optional shift.\nWhen"
					]
				},
				{
					"title": "ARMv8-A A64 ISA Overview",
					"url": "https://armkeil.blob.core.windows.net/developer/Files/pdf/graphics-and-multimedia/ARMv8_InstructionSetOverview.pdf",
					"excerpts": [
						"*Data processing and control flow*",
						"Values in registers can be processed using many different instructions**",
						"\nArithmetic, logic, data moves, bit field manipulations, shifts, conditional comparisons, and more\n",
						"\nThese instructions always operate between registers, or between a register and an immediate",
						"Shifts and rotates**",
						"*Standalone instructions for shifts and rotates*",
						"**Shift amount may be an immediate or a register",
						"\nImmediate shifts up to (register_size – 1)",
						"Register values taken modulo 32-bit or 64-bit\n*",
						"\nMultiply by power of 2\n",
						"\nDivide by power of 2\n",
						"\n(preserves sign bit)\n",
						"Shift right with wrap\naround from\nLSB to MSB\n*"
					]
				},
				{
					"title": "Arithmetic operators",
					"url": "http://en.cppreference.com/w/c/language/operator_arithmetic.html",
					"excerpts": [
						"When signed integer arithmetic operation overflows (the result does not fit in the result type), the behavior is undefined: it may wrap around ..."
					]
				},
				{
					"title": "Do I have correct understanding of Undefined Behavior for shift operators in C? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/72984533/do-i-have-correct-understanding-of-undefined-behavior-for-shift-operators-in-c",
					"excerpts": [
						"x << y` is UB if any of the following:\n`x < 0` (even if `y == 0` )\n`y < 0`\n`y >= N",
						" `x_t` is a signed type and `y >= M` ⇒ UB",
						"x >> y` is UB if any of the following:\n`y < 0`\n`y >= N`",
						"The integer promotions are performed on each of the operands. The type of the result is that of the promoted left operand. If the value of the right operand is negative or is greater than or equal to the width of the promoted left operand, the behavior is undefined.",
						" If `E1` has an unsigned type, the value of the result is `E1` × 2 `E2` , reduced modulo one more than the maximum\nvalue representable in the result type."
					]
				},
				{
					"title": "Left Shift and Right Shift Operators in C/C++ - GeeksforGeeks",
					"url": "https://www.geeksforgeeks.org/cpp/left-shift-right-shift-operators-c-cpp/",
					"excerpts": [
						"**Output**\n****2.**** If the number is shifted more than the size of the integer, the behavior is undefined. For example, 1 << 33 is undefined if integers are stored using 32 bits. For bit shift of larger values 1ULL<<62 ****ULL**** is used for Unsigned Long Long which is defined using 64 bits that can store large values.\nC++ ```````",
						"// left shift by 65 digits\ncout << \"3 << 65\" << ( 3 << 65 ) << endl ;",
						"// left shift of 65 digits\nprintf ( \"3 << 65 = %d\" , ( 3 << 65 ));"
					]
				},
				{
					"title": "6.5.7 Bitwise shift operators",
					"url": "https://c0x.shape-of-code.com/6.5.7.html",
					"excerpts": [
						"The integer promotions are performed on each of the operands.",
						"The type of the result is that of the promoted left operand.",
						"The result of E1 << E2 is E1 left-shifted E2 bit positions;",
						"vacated bits are filled with zeros.",
						"181 If E1 has an unsigned type, the value of the result\nis E1 × 2 E2 ,\n",
						"reduced modulo one more than the maximum value representable in the",
						"result type.",
						"182 If E1 has a signed type and nonnegative value, and E1 × 2 E2 is\n",
						"representable in the result type, then that is the resulting value;",
						"otherwise, the behavior is undefined.",
						"The result of E1 >> E2 is E1 right-shifted E2 bit positions.",
						"If E1 has an unsigned type or if E1 has a signed type and a nonnegative value, the value of the result is",
						"the integral part of the quotient of E1 / 2 E2 .",
						"If E1 has a signed type and a negative value, the",
						"resulting value is implementation-defined."
					]
				},
				{
					"title": "Bitwise operations in C - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/Bitwise_operations_in_C",
					"excerpts": [
						"The symbol of right shift operator is `>>` . For its operation, it requires two [operands](/wiki/Operand \"Operand\") . It shifts each bit in its left operand to the right.\nThe number following the operator decides the number of places the bits are shifted (i.e. the right operand).\nThus by doing `ch >> 3` all the bits will be shifted to the right by three places and so on.\nHowever, do note that a shift operand value which is either a negative number or is greater than or equal to the total number of bits in this value results in [undefined behavior](/wiki/Undefined_behavior \"Undefined behavior\") . For example, when shifting a 32 bit unsigned integer, a shift amount of 32 or higher would be undefined.",
						" shifts each bit in its left-hand operand to the left by the number of positions indicated by the right-hand operand. It works opposite to that of right shift operator. Thus by doing `ch << 1` in the above example ( `11100101` ) we have `11001010` .\nBlank spaces generated are filled up by zeroes as above.\nHowever, do note that a shift operand value which is either a negative number or is greater than or equal to the total number of bits in this value results in [undefined behavior](/wiki/Undefined_behavior \"Undefined behavior\") . This is defined in the standard at [ISO 9899:2011 6.5.7 Bit-wise shift operators](https://stackoverflow.com/questions/4945703/left-shifting-with-a-negative-shift-count) . For example, when shifting a 32 bit unsigned integer, a shift amount of 32 or higher would be undefined.\nLeft shift can be used to multiply an integer by powers of 2 as i"
					]
				},
				{
					"title": "The OpenCL™ C Specification - Khronos Registry",
					"url": "https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_C.html",
					"excerpts": [
						"The operators right-shift ( **>>** ), left-shift ( **<<** ) operate on all scalar\nand vector built-in types except the built-in scalar and vector `float` types.\nFor built-in vector types, the operators are applied component-wise.",
						"The result of `E1` **<<** `E2` is `E1` left-shifted by log 2 (N) least significant\nbits in `E2` viewed as an unsigned integer value, where N is the number of bits\nused to represent the data type of `E1` after integer promotion [ [30]( \"View footnote.\") ] , if `E1` is a scalar, or the number of bits\nused to represent the type of `E1` elements, if `E1` is a vector.\nThe vacated bits are filled with zeros.",
						"The result of `E1` **>>** `E2` is `E1` right-shifted by log 2 (N) least\nsignificant bits in `E2` viewed as an unsigned integer value, where N is the\nnumber of bits used to represent the data type of `E1` after integer\npromotion, if `E1` is a scalar, or the number of bits used to represent the\ntype of `E1` elements, if `E1` is a vector.\nIf `E1` has an unsigned type or if `E1` has a signed type and a nonnegative\nvalue, the vacated bits are filled with zeros.\nIf `E1` has a signed type and a negative value, the vacated bits are filled\nwith ones."
					]
				},
				{
					"title": "shiftOperators(3) - Khronos Registry",
					"url": "https://registry.khronos.org/OpenCL/sdk/3.0/docs/man/html/shiftOperators.html",
					"excerpts": [
						"The operators right-shift ( **>>** ), left-shift ( **<<** ) operate on all scalar\nand vector built-in types except the built-in scalar and vector `float` types.",
						"For built-in vector types, the operators are applied component-wise.",
						"the right-shift ( **>>** ), left-shift ( **<<** ) operators, the rightmost\noperand must be a scalar if the first operand is a scalar, and the rightmost\noperand can be a vector or scalar if the first operand is a vector.",
						"The result of `E1` **<<** `E2` is `E1` left-shifted by log 2 (N) least significant\nbits in `E2` viewed as an unsigned integer value, where N is the number of bits\nused to represent the data type of `E1` after integer promotion [ [1]( \"View footnote.\") ] , if `E1` is a scalar, or the number of bits\nused to represent the type of `E1` elements, if `E1` is a vector.",
						"The vacated bits are filled with zeros.",
						"The result of `E1` **>>** `E2` is `E1` right-shifted by log 2 (N) least\nsignificant bits in `E2` viewed as an unsigned integer value, where N is the\nnumber of bits used to represent the data type of `E1` after integer\npromotion, if `E1` is a scalar, or the number of bits used to represent the\ntype of `E1` elements, if `E1` is a vector.",
						"If `E1` has an unsigned type or if `E1` has a signed type and a nonnegative\nvalue, the vacated bits are filled with zeros.",
						"If `E1` has a signed type and a negative value, the vacated bits are filled\nwith ones."
					]
				},
				{
					"title": "ISO/IEC 9899:yyyy",
					"url": "https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2310.pdf",
					"excerpts": [
						"aints**\n2\nEach of the operands shall have integer type.\n**Semantics**\n3\nThe integer promotions are performed on each of the operands. The type of the result is that of the\npromoted left operand. If the value of the right operand is negative or is greater than or equal to the\nwidth of the promoted left operand, the behavior is undefine",
						"\nThe result of E1 << E2 is E1 left-shifted E2 bit positions; vacated bits are filled with zeros. If E1 has\nan unsigned type, the value of the result is E1 *×* 2 E2 , reduced modulo one more than the maximum\nvalue representable in the result type. If E1 has a signed type and nonnegative value, and E1 *×* 2 E2 is\nrepresentable in the result type, then that is the resulting value; otherwise, the behavior is undefined.\n",
						"The result of E1 >> E2 is E1 right-shifted E2 bit positions. If E1 has an unsigned type or if E1 has a\nsigned type and a nonnegative value, the value of the result is the integral part of the quotient of\nE1 */* 2 E2 . If E1 has a signed type and a negative value, the resulting value is implementation-defined."
					]
				},
				{
					"title": "Intel® 64 and IA-32 Architectures Software Developer's Manual",
					"url": "https://raw.githubusercontent.com/wiki/8dcc/naos/manuals/intel-sdm-vol-2abcd.pdf",
					"excerpts": [
						"Shift doublewords in xmm2 left by amount specified in\nthe corresponding element of xmm3/m128 while\nshifting in 0s."
					]
				},
				{
					"title": "⚙ D56474 [ARM] [NEON] Add ROTR/ROTL lowering",
					"url": "https://reviews.llvm.org/D56474?id=180788",
					"excerpts": [
						"SDValue Temporary = DAG.getNode(Left ? ISD::SHL : ISD::SRL, DL, VT, Value, Amount);",
						"Left ? ISD::SHL : ISD::SRL",
						"v = vsliq_n_u32(vshrq_n_u32(v, 19), v, 13);",
						"vshr.u32 tmp, v, #19",
						"vshl.i32 v, v, #13",
						"vorr v, v, tmp"
					]
				},
				{
					"title": "Advanced SIMD (Neon) intrinsics",
					"url": "https://developer.arm.com/documentation/101028/0010/Advanced-SIMD--Neon--intrinsics",
					"excerpts": [
						"The intrinsics in this section provide C and C++ programmers with a simple programming model allowing easy access to code-generation of the Advanced SIMD instructions for both AArch64 and AArch32 execution states.",
						"The Advanced SIMD extension provides for arithmetic, logical and saturated arithmetic operations on 8-bit, 16-bit and 32-bit integers (and sometimes on 64-bit integers) and on 32-bit and 64-bit floating-point data, arranged in 64-bit and 128-bit vectors.",
						"For example, `vmul_u16(a, b)` is a vector intrinsic which takes two `uint16x4_t` vector arguments `a` and `b` , and returns the result of multiplying corresponding elements from each vector together.\nT"
					]
				},
				{
					"title": "4th Generation Intel® Xeon® Scalable Processor Family (based on Sapphire Rapids Architecture) Instruction Throughput and Latency",
					"url": "https://www.intel.com/content/www/us/en/content-details/765484/4th-generation-intel-xeon-scalable-processor-family-based-on-sapphire-rapids-architecture-instruction-throughput-and-latency.html",
					"excerpts": [
						"Section Title: 4th Generation Intel® Xeon® Scalable Processor Family (based on Sapphire Rapids Architecture) Instruction Throughput and Latency > Description\nContent:\nDescribes throughput and latency for the 4th Generation Intel® Xeon® Scalable Processor Family based on Sapphire Rapids architecture."
					]
				},
				{
					"title": "insn_bench_aarch64/optimization_notes_apple_m1.md at master · ocxtal/insn_bench_aarch64 · GitHub",
					"url": "https://github.com/ocxtal/insn_bench_aarch64/blob/master/optimization_notes_apple_m1.md",
					"excerpts": [
						"Shift-form arithmetic and logical instructions increase the latency by one cycle from the base form.",
						"Table 2: Estimated allocation table for the SIMD/fp domain.",
						"All execution units, V0~V3, processes basic arithmetic and logical operations with a latency of two cycles and a throughput of four per clock cycle."
					]
				},
				{
					"title": "Vectorizing xxHash for Fun and Profit | The Pseudo Random Bit Bucket",
					"url": "https://moinakg.wordpress.com/2013/01/19/vectorizing-xxhash-for-fun-and-profit/",
					"excerpts": [
						"The rotate (XXH_rotl32) operation is easily doable with 2 bitwise shifts and a bitwise or (as is done in xxHash original code as well)."
					]
				},
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/Passes.html",
					"excerpts": [
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"This pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.",
						"\nFor example: 4 + (x + 5) ⇒ x + (4 + 5)",
						"\nFor example: 4 + (x + 5) ⇒ x + (4 + 5)",
						"In the implementation of this algorithm, constants are assigned rank = 0,",
						"In the implementation of this algorithm, constants are assigned rank = 0,",
						"function arguments are rank = 1, and other values are assigned ranks\ncorresponding to the reverse post-order traversal of the current function (starting\nat 2), which effectively gives values in deep loops higher rank than values not\nin loops.",
						"function arguments are rank = 1, and other values are assigned ranks\ncorresponding to the reverse post-order traversal of the current function (starting\nat 2), which effectively gives values in deep loops higher rank than values not\nin loops.",
						"Section Title: LLVM’s Analysis and Transform Passes [¶]( \"Link to this heading\") > ... > [`reassociate` : Reassociate expressions]() [¶]( \"Link to this heading\")\nContent:\nThis pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc.\nFor example: 4 + (x + 5) ⇒ x + (4 + 5)\nIn the implementation of this algorithm, constants are assigned rank = 0,\nfunction arguments are rank = 1, and other values are assigned ranks\ncorresponding to the reverse post-order traversal of the current function (starting\nat 2), which effectively gives values in deep loops higher rank than values not\nin loops.",
						"Content:\nCombine instructions to form fewer, simpler instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.\nThis pass combines things like:\ninto:\nThis is a simple worklist-driven algorithm.\nThis pass guarantees that the following canonicalizations are performed on the\nprogram:\nIf a binary operator has a constant operand, it is moved to the right-hand\nside.\nBitwise operators with constant operands are always grouped so that shifts\nare performed first, then `or` s, then `and` s, then `xor` s.\nCompare instructions are converted from `<` , `>` , `≤` , or `≥` to `=` or `≠` if possible.\nAll `cmp` instructions on boolean values are replaced with logical\noperations.\n`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\nMultiplies with a constant power-of-two argument are transformed into\nshifts.\n… etc.\nThis ",
						"`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\n",
						"Multiplies with a constant power-of-two argument are transformed into\nshifts."
					]
				},
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 12 documentation",
					"url": "https://releases.llvm.org/12.0.0/docs/Passes.html",
					"excerpts": [
						"Combine instructions to form fewer, simple instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.",
						"Multiplies with a constant power-of-two argument are transformed into\nshifts.",
						"his pass reassociates commutative expressions in an order that is designed to\npromote better constant propagation, GCSE, [LICM]() , PRE, etc",
						"For example: 4 + (x + 5) ⇒ x + (4 + 5)"
					]
				},
				{
					"title": "InstCombine contributor guide — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/InstCombineContributorGuide.html",
					"excerpts": [
						"InstCombine is a target-independent canonicalization pass. This means that it\ntries to bring IR into a “canonical form” that other optimizations (both inside\nand outside of InstCombine) can rely on. For this reason, the chosen canonical\nform needs to be the same for all targets, and not depend on target-specific\ncost modelling",
						" many cases, “canonicalization” and “optimization” coincide. For example, if\nwe convert `x * 2` into `x << 1` , this both makes the IR more canonical\n(because there is now only one way to express the same operation, rather than\ntwo) and faster (because shifts will usually have lower latency than\nmultiplies)",
						"| Original Pattern | Canonical Form | Condition |",
						"| `icmp spred X, Y` | `icmp samesign upred X, Y` | `sign(X) == sign(Y)` |",
						"| `mul X, C` | `shl X, Log2(C)` | `isPowerOf2(C)` |",
						"eading\")\nContent:\nAvoid transforms that are hardcoded to specific constants. Try to figure\nout what the general rule for arbitrary constants is. Add handling for conjugate patterns. For example, if you implement a fold\nfor `icmp eq` , you almost certainly also want to support `icmp ne` , with the\ninverse result."
					]
				},
				{
					"title": "⚙ D65614 [Reassociate] Stop linearizing all associative expression trees w/o profitability",
					"url": "https://reviews.llvm.org/D65614",
					"excerpts": [
						"The Reassociate pass currently does two things. First, it factors out common sub-expressions and otherwise *optimizes* the expression tree.Read more"
					]
				},
				{
					"title": "llvm::AArch64TargetLowering Class Reference",
					"url": "https://llvm.org/doxygen/classllvm_1_1AArch64TargetLowering.html",
					"excerpts": [
						"Given the pattern (X & (C l>>/<< Y)) ==/! = 0 return true if it should be transformed into: ((X <</l>> Y) & C) ==/! = 0 WARNING: if 'X' is a constant, the fold ...Read more"
					]
				},
				{
					"title": "MurmurHash: The Scrappy Algorithm That Secretly Powers Half the Internet | by Ashutosh Jha | Medium",
					"url": "https://medium.com/@thealonemusk/murmurhash-the-scrappy-algorithm-that-secretly-powers-half-the-internet-2d3f79b4509b",
					"excerpts": [
						"MurmurHash’s genius lies in its simplicity and clever bit manipulation. Let’s break down MurmurHash3 (the most common version):",
						"**The Magic Numbers** : Those seemingly random constants (0xcc9e2d51, 0x1b873593, etc.) aren’t random at all. They’re carefully chosen to maximize bit mixing and minimize patterns. Austin Appleby tested millions of combinations to find these optimal values.",
						"**The Mixing Process** : Each 4-byte chunk goes through a multiplication, rotation, and XOR dance that ensures even tiny input changes create massive output changes (avalanche effect).",
						"**The Finalization** : The `fmix32` function ensures that even if the main loop didn't fully mix the bits, the final hash will be well-distributed."
					]
				},
				{
					"title": "Mastering Programming",
					"url": "https://r-libre.teluq.ca/3831/1/progbook.pdf",
					"excerpts": [
						"The murmur function consists of two multiplications and\nand three shift/xor operations.",
						"The following Go program will display\nrandom-looking 64-bit integers, using the murmur function:"
					]
				},
				{
					"title": "2023 – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2023/",
					"excerpts": [
						"A reasonable example to hash non-zero integer values is the murmur function. The murmur function consists of two multiplications and and three shift/xor operations.",
						"There are better and faster hash functions, but the result from recursive hashing with a murmur finalizer is reasonable."
					]
				},
				{
					"title": "Fast keyed hash/pseudo-random function using SIMD ...",
					"url": "https://arxiv.org/pdf/1612.06257",
					"excerpts": [
						"Mixing via addition is slightly more\nthorough than XOR because of carry ripples."
					]
				},
				{
					"title": "Crazily fast hashing with carry-less multiplications – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2015/10/26/crazily-fast-hashing-with-carry-less-multiplications/",
					"excerpts": [
						"To test the idea out, we created a non-cryptographic 64-bit hash function (CLHash). For good measure, we made it XOR universal: a strong property that ensures your algorithms will behave probabilistically speaking. Most of our functions is a series of carry-less multiplications and bitwise XOR.",
						"With carry-less multiplications, it is possible to define a modulo operation such that\nmodulo(a \"*\" c) == modulo(b \"*\" c)\nimplies a == b . And it works with all 32-bit or 64-bit integers.",
						"|**CLHash** |**0.45** |**0.16** |",
						"CPU cycles per input byte:",
						"That's right: CLHash is much faster that competing alternatives as soon as your strings are a bit large. It can hash 8 input bytes per CPU cycles. You are more likely to run out of memory bandwidth than to wait for this hash function to complete.",
						"rther reading** :\n[Faster 64-bit universal hashing using carry-less multiplications](http://arxiv.org/abs/1503.03465) , Journal of Cryptographic Engineering 6 (3), 2016.\nDaniel Lemire, \"Crazily fast hashing with carry-less multiplications,\" in *Daniel Lemire's blog* , October 26, 2015, https://lemire.me/blog/2015/10/26/crazily-fast-hashing-with-carry-less-multiplications/ ."
					]
				},
				{
					"title": "Faster 64-bit universal hashing using carry-less multiplications | Daniel Lemire, Computer Science Professor",
					"url": "https://lemire.me/en/publication/arxiv150303465/",
					"excerpts": [
						"Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set in their x64 processors. We use CLMUL to implement an almost universal 64-bit hash family (CLHASH). We compare this new family with what might be the fastest almost universal family on x64 processors (VHASH). We find that CLHASH is at least 60% faster. We also compare CLHASH with a popular hash function designed for speed (Google's CityHash). We find that CLHASH is 40% faster than CityHash on inputs larger than 64 bytes and just as fast otherwise."
					]
				},
				{
					"title": "Your software should follow your hardware: the CLHash example – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2015/12/24/your-software-should-follow-your-hardware-the-clhash-example/",
					"excerpts": [
						"|Haswell |Skylake |Progress (Skylake vs. Haswell) |",
						"| --- | --- | --- | --- |",
						"\n|**CLHash** |**0.16** |**0.096** |**1.7Ã— faster** |",
						"A few weeks ago, I [reported on a new family of hash functions (CLHash)](https://lemire.me/blog/2015/10/26/crazily-fast-hashing-with-carry-less-multiplications/) designed to benefit from the new instructions available in recent processors to compute the “carryless multiplication” (or “polynomial multiplications”). This multiplication used to be many times slower than the regular integer multiplications, and required sophisticated software libraries. ",
						"\nWhat does it mean? It means that hash functions based on carryless multiplications can “smoke” conventional hash functions. For example, the following tables gives us the CPU cycles per input byte when hashing 4kB inputs:",
						"Your software should follow your hardware.",
						"Reference** : The [clhash C library](https://github.com/lemire/clhash)"
					]
				},
				{
					"title": "Which AVX-512 Instructions are Supported by Intel® Xeon® Scalable...",
					"url": "https://www.intel.com/content/www/us/en/support/articles/000058341/processors/intel-xeon-processors.html",
					"excerpts": ["| --- | --- | --- | --- |"]
				}
			],
			"reasoning": "The `x86_lea_optimization` field is supported by multiple excerpts detailing the use of the `LEA` instruction for arithmetic. Several sources explain that `LEA` can combine additions and shifts (specifically multiplications by 1, 2, 4, or 8) into a single instruction, such as `LEA reg, [base + index*scale + offset]`. This allows for expressions like `a + (a << k) + const` to be computed efficiently. It is noted that `LEA` does not affect the flags register, which can help break false dependencies and improve scheduling. Performance characteristics vary by architecture: on modern Intel Ice Lake and AMD Zen 4, simple `LEA` operations have a 1-cycle latency, but on older architectures like Skylake and Haswell, a `LEA` with three components (base, index, offset) can have a 3-cycle latency. This suggests that for complex address calculations, a sequence of `ADD` instructions might sometimes be faster on older CPUs if `LEA` is on the critical path. The `LEA` instruction utilizes Address Generation Units (AGUs), which can offload work from general-purpose ALUs.\n\nFor `aarch64_shifted_operand_optimization`, excerpts highlight that AArch64 instructions like `ADD` and `EOR` can take a shifted register as an operand. This means an operation like `a + (a << k)` or `a ^ (a << k)` can be performed in a single instruction, such as `ADD wD, wN, wM, LSL #k`. This mechanism is more versatile than x86's `LEA` for arithmetic, supporting various shift types (LSL, LSR, ASR, ROR) and a full range of immediate shift amounts (0-31 for 32-bit registers, 0-63 for 64-bit). These fused operations typically have a 1-cycle latency and execute as a single micro-op, offering significant performance benefits by reducing instruction count and latency.\n\nRegarding `reassociation_opportunities`, the core idea is that if `op1` and `op2` are both associative and commutative (like `+` or `^`), operations can be reordered to expose parallelism or enable fusion. For example, `a + (b + c)` can become `(a + b) + c`. This is particularly useful for the hash pattern `a = ((a op1 const1) op2 (a shift_op shift_amount))`. If `op1` and `op2` are the same and are associative/commutative, the `const1` can be moved to the end, allowing the `a`-dependent terms to be grouped, which can then be optimized by `LEA` or shifted-operand instructions. However, for mixed operations like `(a + const1) ^ (a << amount)`, reassociation is generally not possible while maintaining bit-exactness due to the non-linear nature of such combinations, which is a common design choice in cryptographic hashes to resist algebraic attacks.\n\nFor `safety_and_bit_exactness`, the critical aspect is handling integer overflow and shift behavior. For 32-bit unsigned integers, C/C++ standards define arithmetic operations to wrap around modulo 2^32, which aligns with how modern hardware handles these operations. However, C/C++ standards specify undefined behavior for shift amounts greater than or equal to the operand's bit width. In contrast, x86 ISAs explicitly mask shift counts (e.g., to 5 bits for 32-bit operands, meaning `shift_amount & 31`), ensuring defined behavior even for large shift amounts. ARM NEON and AVX2/AVX-512 variable shift instructions also have defined behaviors, often zeroing the result for out-of-range shifts. To ensure bit-exactness across platforms and optimization levels, any mathematical transformation must carefully consider these hardware-specific behaviors and avoid C/C++ undefined behavior. Rigorous testing with diverse inputs, including edge cases for shift amounts, is crucial to validate the correctness of such transformations.",
			"confidence": "high"
		},
		{
			"field": "portable_simd_frameworks_evaluation.architecture_support",
			"citations": [
				{
					"title": "GitHub - jfalcou/eve: Expressive Vector Engine - SIMD in C++ Goes Brrrr",
					"url": "https://github.com/jfalcou/eve",
					"excerpts": [
						"In terms of SIMD extension sets, we actively support (ie code is optimised and regularly tested) the following:\nFull support with tests:\n**Intel**\nSSE2, SSSE3, SSE3, SSE4.1, SSE4.2\nAVX, AVX2, FMA3\nAVX512 in Skylake-AVX512 mode (F, CD, VL, DQ, BW)\n**ARM**\nNEON A32 (64 & 128 bits)\nNEON A64 (64 & 128 bits)\nASIMD\nSVE with fixed sizes of 128, 256 and 512 bits registers.\nPartial/In-progress support with minimal checks:\n**PowerPC**\nVMX\nVSX\nWe **do not support** ARM SVE with **dynamic size** or GPGPU; this is the job of another tool."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0."
					]
				},
				{
					"title": "API synopsis / quick reference",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway/+/refs/tags/upstream/0.12.2/g3doc/quick_reference.md",
					"excerpts": [
						"Content:\nHighway vectors consist of one or more ‘lanes’ of the same built-in type `uint#, int#` for `## = 8, 16, 32, 64` , plus `float#` for `## = 16, 32, 64` .\nIn Highway, `float16_t` (an IEEE binary16 half-float) only supports load, store, and conversion to/from `float32_t` ; the behavior of `float16_t` infinity and NaN are implementation-defined due to ARMv7.\nOn RVV, vectors are sizeless and cannot be wrapped inside a class. The Highway API allows using built-in types as vectors because operations are expressed as overloaded functions. Instead of constructors, overloaded initialization functions such as `Set` take a zero-sized tag argument called `d` of type `D = Simd<T, N>` and return an actual vector of unspecified type.\n`T` is one of the lane types above, and may be retrieved via `TFromD<D>` .\n`N` is target-dependent and not directly user-specified. The actual lane count may not be known at compile time, but can be obtained via `Lanes(d)` . Use this value, which is potentially different from `N` , to increment loop counters etc. It is typically a power of two, but that is not guaranteed e.g. on SVE.\n`d` lvalues (a tag, NOT actual vector) are typically obtained using two aliases:\nSection Title: []() []() API synopsis / quick reference > []() []() Vector and descriptor types\nContent:\nMost common: pass `HWY_FULL(T[, LMUL=1]) d;` as an argument to return a native vector. This is preferred because it fully utilizes vector lanes.For targets (e.g. RVV) that support register groups, the optional `LMUL` (1, 2, 4, 8) specifies the number of registers in the group. This effectively multiplies the lane count in each operation by `LMUL` . For mixed-precision code, `LMUL` must be at least the ratio of the sizes of the largest and smallest type. `LMUL > 1` is more efficient on single-issue machines, but larger values reduce the effective number of registers, which may cause the compiler to spill them to memory.\nLess common: pass `HWY_CAPPED(T, N) d;` as an argument to return a vector which may be native width, but no more than `N` lanes have observable effects such as loading/storing to memory. This is less performance-portable because it may not use all available lanes. Note that the resulting lane count may also be less than `N` .For targets (e.g. RVV) that have compile-time-unknown lane counts, such vectors incur additional runtime cost in `Load` etc.\nUser-specified lane counts or tuples of vectors could cause spills on targets with fewer or smaller vectors. By contrast, Highway encourages vector-length agnostic code, which is more performance-portable.\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Operations\nContent:\nIn the following, the argument or return type `V` denotes a vector with `N` lanes, and `M` a mask. Operations limited to certain vector types begin with a constraint of the form `V` : `{prefixes}[{bits}]` . The prefixes `u,i,f` denote unsigned, signed, and floating-point types, and bits indicates the number of bits per lane: 8, 16, 32, or 64. Any combination of the specified prefixes and bits are allowed. Abbreviations of the form `u32 = {u}{32}` may also be used.\nNote that Highway functions reside in `hwy::HWY_NAMESPACE` , whereas user-defined functions reside in `project::[nested]::HWY_NAMESPACE` . Highway functions generally take either a `Simd` or vector/mask argument. For targets where vectors and masks are defined in namespace `hwy` , the functions will be found via Argument-Dependent Lookup. However, this does not work for function templates, and RVV and SVE both use builtin vectors. Thus we recommend a `using hwy::HWY_NAMESPACE;` directive inside `project::[nested]::HWY_NAMESPACE` .\n ... \nSection Title: []() []() API synopsis / quick reference > []() []() Compiler support\nContent:\nClang and GCC require e.g. -mavx2 flags in order to use SIMD intrinsics. However, this enables AVX2 instructions in the entire translation unit, which may violate the one-definition rule and cause crashes. Instead, we use target-specific attributes introduced via #pragma. Function using SIMD must reside between `HWY_BEFORE_NAMESPACE` and `HWY_AFTER_NAMESPACE` . Alternatively, individual functions or lambdas may be prefixed with `HWY_ATTR` .\nImmediates (compile-time constants) are specified as template arguments to avoid constant-propagation issues with Clang on ARM."
					]
				},
				{
					"title": "API synopsis / quick reference — HIGHWAY nightly documentation",
					"url": "https://google.github.io/highway/en/master/quick_reference.html",
					"excerpts": [
						"Highway is a collection of ‘ops’: platform-agnostic pure functions that\noperate on tuples (multiple values of the same type). These functions\nare implemented using platform-specific intrinsics, which map to\nSIMD/vector instructions."
					]
				}
			],
			"reasoning": "The most relevant excerpt explicitly contrasts SVE capabilities by noting that SVE is present in the ecosystem but with fixed sizes (128, 256, 512 bits). This directly informs the debate around dynamic versus fixed vector lengths and provides a basis for comparing Highway to competitors, highlighting potential limitations in SVE support. The next excerpt lists the concrete target architectures Highway claims to support, including NEON and AVX variants, which establishes Highway’s breadth across mainstream SIMD backends. A further excerpt describes Highway’s SIMD abstraction and portable API, reinforcing that Highway is designed around vector operations that map to platform-specific intrinsics, which is central to supporting multiple architectures. A final excerpt discusses the design philosophy and practical considerations for SIMD dispatch and portability, offering additional context on how Highway handles cross-target support and runtime selection, thereby supporting the claim of comprehensive architecture coverage. Taken together, these excerpts collectively support the notion that Highway provides broad architectural support (including NEON and AVX families) and offers insights into SVE handling, though the explicit claim of dynamic SVE lengths is contradicted by the fixed-size SVE note in the contrasted excerpt.",
			"confidence": "medium"
		},
		{
			"field": "practitioner_codebases_and_libraries",
			"citations": [
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0.",
						"Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD."
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` ).",
						"The library files `xxhash.c` and `xxhash.h` are BSD licensed."
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"`x86` / `x64` systems can enjoy a new unit, `xxh_x86dispatch` , which can detect at runtime the best vector instruction set present on host system (none, `sse2` , `avx2` or `avx512` ), thanks to a cpu feature detector designed by [@easyaspi314](https://github.com/easyaspi314) . It then automatically runs the appropriate vector code.",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` .",
						"It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"e `M1 Pro` , this translates into +20% speed for `xxh3` and `xxh128` (from 30.0 GB/s to 36 GB/s).\n",
						"On the `M1 Pro` , this translates into +20% speed for `xxh3` and `xxh128` (from 30.0 GB/s to 36 GB/s)."
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": [
						"   - AVX512",
						"   - AVX2",
						"   - SSE2",
						"   - ARM NEON",
						"   - WebAssembly SIMD128",
						"   - POWER8 VSX",
						"enum XXH_VECTOR_TYPE /* fake enum */ {\n    XXH_SCALAR = 0,  /*!< Portable scalar version */\n    XXH_SSE2   = 1,  /*!<\n                      * SSE2 for Pentium 4, Opteron, all x86_64. *\n                      * @note SSE2 is also guaranteed on Windows 10, macOS, and\n                      * Android x86. */\n    XXH_AVX2   = 2,  /*!< AVX2 for Haswell and Bulldozer */\n    XXH_AVX512 = 3,  /*!< AVX512 for Skylake and Icelake */\n    XXH_NEON   = 4,  /*!<\n                       * NEON for most ARMv7-A, all AArch64, and WASM SIMD128\n                       * via the SIMDeverywhere polyfill provided with the\n                       * Emscripten SDK. */\n    XXH_VSX    = 5,  /*!< VSX and ZVector for POWER8/z13 (64-bit) */\n    XXH_SVE    = 6,  /*!< SVE for some ARMv8-A and ARMv9-A */\n};",
						"t: Auto detected.\n ... \n*/\n#ifndef XXH_ACC_ALIGN\n#  if defined(XXH_X86DISPATCH)\n#     define XXH_ACC_ALIGN 64  /* for compatibility with avx512 */\n#  elif XXH_VECTOR == XXH_SCALAR  /* scalar */\n#     define XXH_ACC_ALIGN 8\n#  elif XXH_VECTOR == XXH_SSE2  /* sse2 */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX2  /* avx2 */\n#     define XXH_ACC_ALIGN 32\n#  elif XXH_VECTOR == XXH_NEON  /* neon */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_VSX   /* vsx */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX512  /* avx512 */\n#     define XXH_ACC_ALIGN 64\n#  elif XXH_VECTOR == XXH_SVE   /* sve */\n#     define XXH_ACC_ALIGN 64\n#  endif\n#endif\n\n#if",
						"\n#if defined(XXH_X86DISPATCH) || XXH_VECTOR == XXH_SSE2 \\\n    || XXH_VECTOR == XXH_AVX2 || XXH_VECTOR == XXH_AVX512\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#elif XXH_VECTOR == XXH_SVE\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#else\n#  define XXH_SEC_ALIGN 8\n#endif",
						" automatic runtime dispatcher is included separately in @ref xxh_x86dispatch.c.",
						" automatic runtime dispatcher is included separately in @ref xxh_x86dispatch.c.",
						" automatic runtime dispatcher is included separately in @ref xxh_x86dispatch.c.",
						" automatic runtime dispatcher is included separately in @ref xxh_x86dispatch.c."
					]
				},
				{
					"title": "xxHash: XXH3 family",
					"url": "https://xxhash.com/doc/v0.8.2/group___x_x_h3__family.html",
					"excerpts": [
						"XXH3 has a fast scalar implementation, but it also includes accelerated SIMD implementations for many common platforms:\nAVX512\nAVX2\nSSE2\nARM NEON\nWebAssembly SIMD128\nPOWER8 VSX\ns390x ZVector",
						"XXH3 is a more recent hash algorithm featuring:\nImproved speed for both small and large inputs\nTrue 64-bit and 128-bit outputs\nSIMD acceleration\nImproved 32-bit viability",
						"Compared to XXH64, expect XXH3 to run approximately ~2x faster on large inputs and >3x faster on small ones, exact differences vary depending on platform.",
						"XXH3's speed benefits greatly from SIMD and 64-bit arithmetic, but does not require it. Most 32-bit and 64-bit targets that can run XXH32 smoothly can run XXH3 at competitive speeds, even without vector support. Further details are explained in the implementation.",
						"For the x86 family, an automatic runtime dispatcher is included separately in [xxh_x86dispatch.c](xxh__x86dispatch_8c.html) .",
						"XXH3 implementation is portable: it has a generic C90 formulation that can be compiled on any platform, all implementations generate exactly the same hash value on all platforms. Starting from v0.8.0, it's also labelled \"stable\", meaning that any future version will also generate the same hash value."
					]
				},
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.\nF",
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.",
						"As one can see, the `AVX2` build *is* much faster than main memory, and the impact of cache size is now clearly observable, at 32 KB (L1), 256 KB (L2) and 8 MB (L3).",
						"The base speed is very competitive, and the `NEON` vectorial code path designed by Devin makes wonder, pushing speed to new boundaries.",
						"\nFor this reason, the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts.\n`SSE2` is",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set. `SSE2` is also free of dynamic throttling issues, and is automatically enabled on all x64 compilers. Hence I expect it to be the most common target.",
						"For this reason, the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts."
					]
				},
				{
					"title": "RealTime Data Compression",
					"url": "http://fastcompression.blogspot.com/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.",
						"the code offers 4 paths : scalar (universal), `SSE2`, `AVX2`, and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314).",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set."
					]
				},
				{
					"title": "XXH3 - a new speed-optimized hash algorithm",
					"url": "http://fastcompression.blogspot.com/2019/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.",
						"As one can see, the `AVX2` build *is* much faster than main memory, and the impact of cache size is now clearly observable, at 32 KB (L1), 256 KB (L2) and 8 MB (L3).",
						", the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts.\n`SSE2`",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set. `SSE2` is also free of dynamic throttling issues, and is automatically enabled on all x64 compilers."
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"| HighwayHashAVX2 | 7.34 | 1.81 | 1.71 | 1.04 | 0.95 | 0.24 |",
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.",
						"SipTreeHash(13) requires an AVX2-capable CPU (e.g. Haswell). HighwayHash\nincludes a dispatcher that chooses the implementation (AVX2, SSE4.1, VSX or\nportable) at runtime, as well as a directly callable function template that can\nonly run on the CPU for which it was built.",
						"The internal state is quite large (1024 bits) but fits within SIMD registers.",
						"Due to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase.",
						"Our x86 implementations use custom vector classes with overloaded operators\n(e.g. `const V4x64U a = b + c` ) for type-safety and improved readability vs.\ncompiler intrinsics (e.g. `const __m256i a = _mm256_add_epi64(b, c)` ).",
						"We also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast).",
						"hh_{avx2,sse41,vsx,portable}.h are its various implementations.",
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.\nThe internal state is quite large (1024 bits) but fits within SIMD registers.\nDue to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase. The\nalgorithm outputs 64 bit digests or up to 256 bits at no extra cost.\nIn addition to high throughput, the algorithm is designed for low finalization\ncost. The result is more than twice as fast as SipTreeHash.\nWe also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast). A third-party ARM implementation is referenced below.\nStatistical analyses and preliminary cryptanalysis are given in https://arxiv.org/abs/1612.06257 .",
						"AVX2 and SSE4.1 HighwayHash are faster than SipHash for all input\nsizes due to their highly optimized handling of partial vectors.",
						"In addition to high throughput, the algorithm is designed for low finalization\ncost."
					]
				},
				{
					"title": "GitHub - minio/highwayhash: Native Go version of HighwayHash with optimized assembly implementations on Intel and ARM. Able to process over 10 GB/sec on a single core on Intel CPUs - https://en.wikipedia.org/wiki/HighwayHash",
					"url": "https://github.com/minio/highwayhash",
					"excerpts": [
						"Below are the single core results on an Intel Core i7 (3.1 GHz) for 256 bit outputs:\nSo for moderately sized messages it tops out at about 15 GB/sec. Also for small messages (1K) the performance is already at approximately 60% of the maximum throughput.",
						"HighwayHash uses a new way of mixing inputs with AVX2 multiply and permute instructions. The multiplications are 32x32 bit giving 64 bits-wide results and are therefore infeasible to reverse. Additionally permuting equalizes the distribution of the resulting bytes.",
						"Additionally permuting equalizes the distribution of the resulting bytes.",
						"The algorithm outputs digests ranging from 64 bits up to 256 bits at no extra cost."
					]
				},
				{
					"title": "xxHash … Fast, Fast, Fast",
					"url": "https://medium.com/asecuritysite-when-bob-met-alice/xxhash-fast-fast-fast-c2d57eb5aba",
					"excerpts": [
						"Overall xxHash works at close RAM limits. In a recent test, xxH3 achieved a hashing rate of 31GB/s, and was especially efficient for a small ...Read more"
					]
				},
				{
					"title": "xxHash: xxHash",
					"url": "https://xxhash.com/doc/v0.8.2/index.html",
					"excerpts": [
						"| [XXH3_64bits()](group___x_x_h3__family.html \"64-bit unseeded variant of XXH3.\") | **AVX2** | 64 | 59.4 GB/s | 133.1 |",
						"| [XXH3_64bits()](group___x_x_h3__family.html \"64-bit unseeded variant of XXH3.\") | **AVX2** | 64 | 59.4 GB/s | 133.1 |",
						"| [XXH3_64bits()](group___x_x_h3__family.html \"64-bit unseeded variant of XXH3.\") | **SSE2** | 64 | 31.5 GB/s | 133.1 |",
						"| [XXH3_128bits()](group___x_x_h3__family.html \"Unseeded 128-bit variant of XXH3.\") | **SSE2** | 128 | 29.6 GB/s | 118.1 |",
						"| [XXH3_128bits()](group___x_x_h3__family.html \"Unseeded 128-bit variant of XXH3.\") | **AVX2** | 128 | 57.9 GB/s | 118.1 |",
						"| [XXH3_128bits()](group___x_x_h3__family.html \"Unseeded 128-bit variant of XXH3.\") | **AVX2** | 128 | 57.9 GB/s | 118.1 |"
					]
				},
				{
					"title": "GitHub - awxkee/mm3h: Murmur3 hash in Rust",
					"url": "https://github.com/awxkee/mm3h",
					"excerpts": [
						"This project is licensed under either of\nBSD-3-Clause License (see [LICENSE](/awxkee/mm3h/blob/master/LICENSE.md) )\nApache License, Version 2.0 (see [LICENSE](/awxkee/mm3h/blob/master/LICENSE-APACHE.md) )",
						"A Rust implementation of MurmurHash3 with platform-specific SIMD optimizations (NEON, AVX2, SSE2). This crate provides a Hasher compatible struct that can be used with Rust's hashing APIs."
					]
				},
				{
					"title": "highwayhash/LICENSE at master · google/highwayhash · GitHub",
					"url": "https://github.com/google/highwayhash/blob/master/LICENSE",
					"excerpts": [
						"Licensed under the Apache License, Version 2.0 (the \"License\");. you may not use this file except in compliance with the License. You may ...Read more"
					]
				},
				{
					"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
					"url": "https://www.agner.org/optimize/microarchitecture.pdf",
					"excerpts": [
						"*\nSimple integer vector instructions such as addition, shift, and boolean operations, have a\nlatency of only one clock cycle, versus two clock cycles on previous AMD processors."
					]
				},
				{
					"title": "Improving xxHash",
					"url": "https://encode.su/threads/2556-Improving-xxHash",
					"excerpts": [
						"And for fast AVX2/AVX512 implementations we should process 2-4 times more data, i.e. split data stream into as much as 16 lanes, each 128-bit ...Read more"
					]
				},
				{
					"title": "Trying harder to make AVX-512 look bad: my quantified and reproducible results – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/",
					"excerpts": [
						"So at the extreme, doing everything I can do make Intel AVX-512 look bad using a few AVX-512 instructions in a larger program, I get a 15-20% increase in the running time."
					]
				},
				{
					"title": "Escaping strings faster with AVX-512 – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2022/09/14/escaping-strings-faster-with-avx-512/",
					"excerpts": [
						"Recent Intel processors have fast instructions (AVX-512) that are well suited for such problems."
					]
				},
				{
					"title": "XXH3 performance issue on ARM & WASM with >128byte inputs · Issue #211 · Cyan4973/xxHash",
					"url": "https://github.com/Cyan4973/xxHash/issues/211",
					"excerpts": [
						"This works better for `SSE2` and `AVX2` , as it was designed for these ones.",
						"An alternative could be to force the implementation to use a specific code path, using the `XXH_VECTOR` build macro. For `NEON` , the code is `3` , which gives for `gcc` : `-DXXH_VECTOR=3` .",
						"| 1 \"sse2\" | 30 GB/s | 27 GB/s |",
						"| 2 \"avx2\" | 55 GB/s | 45 GB/s |",
						"Comparison with AES-based algorithm : `SSE2` is not enough to face special-purpose hardware. `XXH3` is designed to leverage `AVX2` , when available, in order to reach similar performance territory. For example, on my laptop, it reaches ~45 GB/s on \"large\" data. Yet, while `SSE2` is a guaranteed capability of any `x64` cpu, later SIMD versions are not. I suspect that's the reason why `AVX2` wasn't benchmarked."
					]
				},
				{
					"title": "On the Power of Bitslice Implementation on Intel Core2 Processor",
					"url": "https://www.researchgate.net/publication/221291657_On_the_Power_of_Bitslice_Implementation_on_Intel_Core2_Processor",
					"excerpts": [
						"Modern ciphers, meant to be executed in software, are now designed with bitslicing in mind [12,18,56,72]. Their design purposefully allows the programmer to exploit SIMD instruction sets (from Arm's Neon extension to Intel's SSE, AVX and AVX512 extensions)."
					]
				}
			],
			"reasoning": "The `xxHash` library, particularly `XXH3`, and Google's `HighwayHash` are excellent examples of high-performance hash function implementations that leverage advanced optimization techniques. Both libraries extensively use **manual vectorization with intrinsics** because compilers often struggle to auto-vectorize complex hash logic effectively, as explicitly stated for `xxHash` implementations. They incorporate a **runtime dispatch mechanism** to detect the host CPU's SIMD capabilities (e.g., SSE2, AVX2, AVX-512, NEON, VSX) and dynamically select the most optimized code path, ensuring performance portability across diverse architectures. This is evident from `xxHash`'s `xxh_x86dispatch` and `HighwayHash`'s dispatcher. To combat instruction latency and maximize instruction-level parallelism, these libraries employ **significant loop unrolling and interleaving**. For instance, `xxHash` uses multiple independent compute streams and accumulators, and `HighwayHash` partitions registers into independent halves. They also feature **highly optimized memory access patterns** and careful use of shuffle and permute instructions to achieve good bit distribution and high throughput, as seen in `HighwayHash`'s mixing strategy. `xxHash` even utilizes a hybrid scalar/vector approach for ARM NEON to balance pipeline usage.\n\nThese libraries offer broad support for SIMD instruction sets, including `SSE2`, `SSE4.1`, `AVX2`, and `AVX-512` for x86/x64, `NEON` (for ARMv7 and ARMv8), `SVE` for ARM architectures, and `VSX` for POWER architecture. This wide coverage is a hallmark of production-grade, high-performance libraries.\n\nTheir reported performance is state-of-the-art. `XXH3` can achieve approximately **59 GB/s using AVX2**, with `AVX-512` providing a further **+50% performance boost** over `AVX2`. On an Apple M1 Pro, its `NEON` implementation reaches **30-36 GB/s**. `HighwayHash` reports throughput of over **10 GB/s on a single Intel core**, achieving **0.24 cycles/byte** with `AVX2` on 1KB inputs. These metrics demonstrate the substantial performance gains from hand-tuned SIMD.\n\nFinally, the licensing for these libraries is permissive, making them suitable for reuse in both open-source and commercial projects. The core `xxHash` library is available under the **BSD 2-Clause License**, and Google's `HighwayHash` is available under the **Apache License 2.0**.",
			"confidence": "high"
		},
		{
			"field": "avx512_performance_analysis",
			"citations": [
				{
					"title": "On the dangers of Intel's frequency scaling",
					"url": "https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/",
					"excerpts": [
						"AVX-512 is a new Intel instruction set that adds many new 512-bit wide SIMD instructions and promotes most of the existing ones to 512-bit. The problem with such wide instructions is that they consume power.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used.",
						"To keep power in check Intel introduced something called dynamic frequency scaling. It reduces the base frequency of the processor whenever AVX2 or AVX-512 instructions are used."
					]
				},
				{
					"title": "Intel® AVX-512 - Instruction Set for Packet Processing",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-instruction-set-for-packet-processing-technology-guide-1645717553.pdf",
					"excerpts": [
						"INSTRUCTION SET MIX",
						"Power Level 0\nIntel® AVX2 Light\nMaximum frequency\nScalar, SSE integer and floating-point, and AVX2 integer instructions, except\nfor AVX2 integer multiplication and fused-multiply-add (FMA) instructions\n",
						"Power Level 1\nIntel® AVX2 Heavy/\nAVX-512 Light\nGeneration & SKU dependent\nAVX2 integer multiplication, FMA, and floating-point instructions. AVX-512\ninteger instructions, except for AVX-512 integer multiplication and FMA\ninstructions",
						"ple of Power Level transitions is shown in Figure 13 . In the figure, before time interval **t,** a given core is executing Intel®\nAVX2 integer instructions. At interval **t** an Intel® AVX-512 integer instruction runs and consequently a new *AVX-512 Light* power\nlevel is required. The core then blocks for 10-20 µsec until a new frequency is applied. Intel® AVX-512 integer instructions are then\nrun intermittently until **t+2000** µsec, after which the workload returns to executing *AVX2 Light* instructions exclusively. After an\nadditional 2000 µsec, without any additional Intel® AVX-512 instructions executing, the core returns to the *AVX2 Light* power le",
						"en the CPU detects that a\nnew power level is required, it decides which new core frequency to apply. For example, if the Intel® AVX-512 Light instructions run\non a given core with a current power level of *AVX2 Light (Level 0)* *,* and if that core now requires the *AVX-512 Light* *(Level 1)* power\nlevel, a frequency change may occur. When a frequency change occurs on the 1st Generation Intel® Xeon® Scalable processors for\nservers, the core frequency transition has a block time of 10-20 µsec to move between frequencies",
						"ower Level Transitions**\nPower level transitions are another important\nconsideration when developing software with Intel®\nAVX-512 instructions; however, as you see in section\n4.4 , these may be less relevant for packet processing\napplications.",
						"\nThere are three power levels in the Intel® Xeon® Scalable processor that the CPU may apply in non-power limited scenarios. When\npower is limited, the microprocessor may also choose to adjust the frequency of the cores to stay within its stated TDP (Thermal\nDesign Power) limit.\n",
						"s**\nFrequency transitions are significantly improved in the 3rd Generation Intel® Xeon® Scalable processors where the block time is\nreduced to ~0 µsec, allowing for transitions without a latency cost.",
						"*Note:***\nThe 3rd Generation Intel® Xeon® Scalable processors is\noptimized to reduce the frequency impact for Intel®\nAVX-512 instructions when executing at Intel® AVX-512\nLight (Level 1) and Intel® AVX-512 Heavy (Level 2)\npower levels, when compared to 1st Generation Intel®\nXeon® Scalable processors for servers. See subsection\n4.2.1 for more informati",
						"cessors**\nIn the 3rd Generation Intel® Xeon® Scalable\nprocessors, the Power Level 1/Intel® AVX-512 Light\npower level frequency ranges are significantly\nimproved, as shown in Figure 12 . This means that on\nthe 3rd Generation Intel® Xeon® Scalable processors,\nshow as ICX, the IPC gains of using Intel® AVX-512\ninstructions are typically maintained or enhanced\nwith an improvement in Turbo frequency ranges,\nwhen compared to the 1st and 2 nd Generation Intel®\nXeon® Scalable processors, shown as SKX and CLX\nrespectively.\n**",
						"4.3.1\nPower Level Update for 3rd Generation Intel® Xeon® Scalable Processors ."
					]
				},
				{
					"title": "Gathering Intel on Intel AVX-512 Transitions | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/01/17/avxfreq1.html",
					"excerpts": [
						"Now, something more than nothing has been written about this already, including [cautionary tales](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/) of performance loss and some [broad guidelines](https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/) [2](:dmore) , so do we really need to add to the pile?"
					]
				},
				{
					"title": "optimization - SIMD instructions lowering CPU frequency - Stack Overflow",
					"url": "https://stackoverflow.com/questions/56852812/simd-instructions-lowering-cpu-frequency",
					"excerpts": [
						"\n2Trevis Down (aka Beeonrope on OS) wrote about this in the comments in this [post](https://lemire.me/blog/2018/08/13/the-dangers-of-avx-512-throttling-myth-or-reality/) and continued the discussion [here](https://www.realworldtech.com/forum/?threadid=179654&curpostid=179727) . He found that each ties (scalar, AVX/AVX2, AVX-512) has \"cheap\" (no FP, simple operations) instructions and \"heavy\" instruction. Cheap instructions drop the frequency to the one of the next higher tier (e.g. cheap AVX-512 inst use the AVX/AVX2 tier) even if used sparsely. Heavy inst must be used more than 1 every ...Margaret Bloom– [Margaret Bloom](/users/5801661/margaret-bloom \"44,771 rep",
						"You can find the behaviour of each instruction with a bit of patience or by his rule of thumb. Finally note that this frequency scaling is a problem iif the ratio of vector to scalar instruction is low enough so that the drop in frequency is not balanced by the bigger width at which data is processed.",
						"2... two cycles and drop the frequency according to their tier (e.g. AVX-512 heavy instrs drop the frequency to the AV-512 base)."
					]
				},
				{
					"title": "Trying harder to make AVX-512 look bad: my quantified and reproducible results – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/",
					"excerpts": [
						"Intel documents that the use of AVX-512 instructions can lower the frequency of the processor. How big the effect is depends on the processor."
					]
				},
				{
					"title": "\n\tState of AVX 512 on Skylake-X - Intel Community\n",
					"url": "https://community.intel.com/t5/Intel-ISA-Extensions/State-of-AVX-512-on-Skylake-X/td-p/1143063",
					"excerpts": [
						"Section Title: Using Intel.com Search > State of AVX 512 on Skylake-X\nContent:\nAs has been stated on a number of [review sites](http://www.anandtech.com/show/11550/the-intel-skylakex-review-core-i9-7900x-i7-7820x-and-i7-7800x-tested/3) , AVX 512 performance on the 6/8 core Skylake-X is compromised.\nOnly on the 10 core, the present hardware is fully enabled.\nWould Intel be so kind as to provide in depth detail of what the performance difference means ?\nFrom the vague information available it seems one of 2(3?) AVX 512 ports is disabled (port 5).\nCan we get more detailed information, which ports are used for AVX 512 ?\nWhat AVX 512 instructions can the ports execute, do they have 512-bit data paths to registers/cache ?\nHow is AVX 512 gather affected regarding the 6/8 core versus 10 core ?\nA similar drawing as below for AVX2 would be appreciated."
					]
				},
				{
					"title": "Memory Performance of AMD EPYC Rome and Intel ...",
					"url": "https://research.spec.org/icpe_proceedings/2022/proceedings/p165.pdf",
					"excerpts": [
						"Schöne et al. describe side effects for transitions\nbetween normal and AVX-512 frequencies for SKX in [28]."
					]
				},
				{
					"title": "Cascade Lake Processors - HECC Knowledge Base",
					"url": "https://www.nas.nasa.gov/hecc/support/kb/cascade-lake-processors_579.html",
					"excerpts": [
						"Maximum Turbo Frequency is 3.90 GHz for non-AVX, and 3.8 GHz for AVX2 and AVX-512. Cache Hierarchy. The cache hierarchy of Cascade Lake is as ..."
					]
				},
				{
					"title": "Packet Processing with Intel® AVX-512 Instruction Set",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-packet-processing-with-intel-avx-512-instruction-set-solution-brief-1678190247.pdf",
					"excerpts": [
						"of data.\nIntel® Xeon® Scalable processors have a number of *power levels* that correspond to workload instruction set mixes and Turbo\nfrequency ranges (refer to Table 1 ). The instruction set mix of a given workload can comprise integer and floating-point\ninstructions, and 8-bit scalar to 512-bit SIMD register widths.\nChanges in instruction set mixes during workload execution can result in a change to the power level of a given core, resulting in\na different set of corresponding frequency ranges becoming active.\n",
						".\nChanges in instruction set mixes during workload execution can result in a change to the power level of a given core, resulting in\na different set of corresponding frequency ranges becoming active.",
						"ons.\nOn the 1st Generation Intel® Xeon® Scalable processors, this can mean a Turbo frequency reduction when operating with Intel®\nAVX2 Heavy power level, which is then offset by the Instruction-Per-Clock (IPC) gain associated with using Intel® AVX-512\ninstructions. W",
						" ).\nOn the 1st Generation Intel® Xeon® Scalable processors, power level\ntransitions may incur a block time of 10-20 µs while the core identifies\na new frequency. During this time instructions are not being retired.\nThe e"
					]
				},
				{
					"title": "Detailed Specifications of the \"Cascade Lake SP\" Intel Xeon Processor Scalable Family CPUs - Microway",
					"url": "https://www.microway.com/knowledge-center-articles/detailed-specifications-of-the-cascade-lake-sp-intel-xeon-processor-scalable-family-cpus/",
					"excerpts": [
						"Just as in the previous generation, the “Cascade Lake-SP” architecture acknowledges that highly-parallel/vectorized applications place the highest load on the processor cores (requiring more power and generating more heat). While a CPU core is executing intensive vector tasks (AVX2 or AVX-512 instructions), the clock speed will be adjusted downwards to keep the processor within its power limits (TDP).",
						"In effect, this will result in the processor running at a lower frequency than the standard clock speed advertised for each model. For that reason, each processor is assigned three frequency ranges:\n**AVX-512 mode** : due to the high requirements of AVX-512/FMA instructions, clock speeds will be lowered while executing AVX-512 instructions *\n**AVX2 mode** : due to the higher requirements of AVX2/FMA instructions, clock speeds will be somewhat lower while executing AVX instructions *\n**Non-AVX mode** : while not executing “heavy” AVX instructions, the processor will operate at the “stock” frequency",
						"Intel has applied a 1-millisecond hysteresis window on each CPU core to prevent rapid frequency transitions*",
						"Clock Speeds & Turbo Boost > AVX-512, AVX, and Non-AVX Turbo Boost in Xeon “Cascade Lake-SP” Scalable Family processors\nContent:\nDespite the perceived reduction in performance when running these vector instructions, keep in mind that AVX-512 roughly doubles the number of operations which can be completed per cycle. Although the clock speeds might be reduced by nearly 1GHz, the overall throughput is increased. **HPC users should expect their processors to be running in AVX or AVX-512 mode most of the time.**"
					]
				},
				{
					"title": "ARM Takes Wing:  Qualcomm vs. Intel CPU comparison",
					"url": "https://blog.cloudflare.com/arm-takes-wing/",
					"excerpts": [
						"When executing AVX-512 on all cores, the base frequency goes down to just 1.4GHz---keep that in mind if you are mixing AVX-512 and other code."
					]
				},
				{
					"title": "Technical reading from the Cloudflare blog for the holidays",
					"url": "https://blog.cloudflare.com/2017-holiday-reading-from-the-cloudflare-blog/",
					"excerpts": [
						"Intel processors contain special AVX-512 that provide 512-bit wide SIMD instructions to speed up certain calculations. However, these instructions have a downside: when used the CPU base frequency is scaled down slowing down other instructions."
					]
				},
				{
					"title": "AVX-512: when and how to use these new instructions",
					"url": "https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/",
					"excerpts": [
						"The processor does not immediately move to a higher license when encountering heavy instructions: it will first execute these instructions with reduced performance (say 4x slower) and only when there are many of them will the processor change its frequency.",
						"However, some of these instructions use a lot of power and generate a lot of heat. To keep power usage within bounds, Intel reduces the frequency of the cores dynamically.",
						"el cores can run in one of three modes: license 0 (L0) is the fastest (and is associated with the turbo frequencies *written on the box* ), license 1 (L1) is slower and license 2 (L2) is the slowest.",
						"o get into license 2, you need sustained use of heavy 512-bit instructions, where *sustained* means approximately one such instruction every cycle",
						"On server-class processor, the downclocking is determined on a per-core basis based on the license and the total number of active cores, on the same CPU socket, irrespective of the license of the other cores.",
						"the worst case, you might be running at half the advertised frequency and thus your whole application could run slower.",
						" On this basis, [some engineers have recommended that we disable AVX-512 instructions default on our servers](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/)",
						"Engineers should probably use tools to monitor the frequency of their cores to ensure they are running in the expected license.",
						"If your code at least partly involves sustained use of heavy numerical instructions you might consider isolating this work to specific threads (and hence cores), to limit the downclocking to cores that are taking full advantage of AVX-512. If this is not practical or possible, then you should mix this code with other (non-AVX-512 code) with care.",
						"You need to ensure that the benefits of AVX-512 are substantial (e.g., more than 2x faster on a per cycle basis)."
					]
				},
				{
					"title": "\n\tSolved: Re: Poor HPL performance of Icelake Server - Intel Community\n",
					"url": "https://community.intel.com/t5/Software-Tuning-Performance/Poor-HPL-performance-of-Icelake-Server/m-p/1379569",
					"excerpts": [
						"Xeon(R) Gold 6346\n- Base AVX512: 2.5 GHz\n- All-core AVX512: 3.5 GHz\n- Nominal: 3.1 GHz (realistic)\nU",
						"Content:\nComputing the \"Peak GFLOPS\" using the \"Base AVX-512 frequency\" will give a *lower bound* on the peak GFLOPS. If the processor does not sustain that frequency while running HPL, there is probably a problem with the processor or the cooling or the power supply. All of our systems are purchased with an HPL performance requirement that corresponds to a sustained frequency that is slightly higher than the \"Base AVX-512\" frequency (though the details differ for each generation of processors). Computing the \"Peak GFLOPS\" using the \"Max All-core AVX-512 Turbo frequency\" will give an *upper bound* on the peak GFLOPS, but due to power-limited frequency throttling, it is no longer as tight an upper bound as it used to be on earlier processors. Wh",
						"sors. When running HPL, every processor will adjust its frequency to run as close to the TDP as possible. The resulting average frequency will depend on the intrinsic leakage properties of the processor die combined with the specifics of the cooling system. (E.g., in many systems one socket is \"downwind\" of the other, so tends to run hotter. Leakage current increases exponentially with temperature, so these processors will have to reduce the frequency more to keep power consumption under TDP.) The average frequency should be between the \"Base AVX-512 frequency\" and the \"Max All-core AVX-512 Turbo frequency\", but sometimes this is a pretty wide range.\n ",
						"Xeon(R) Gold 6346\n- Base AVX512: 2.5 GHz\n- All-core AVX512: 3.5 GHz\n- Nominal: 3.1 GHz (realistic)\nUsin",
						"The average frequency should be between the \"Base AVX-512 frequency\" and the \"Max All-core AVX-512 Turbo frequency\", but sometimes this is a pretty wide range.\n ... "
					]
				},
				{
					"title": "Ice Lake AVX-512 Downclocking | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html",
					"excerpts": [
						"The Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.",
						"Content:\nSo what about Ice Lake, the newest chips which support both the SKX flavor of AVX-512 and also have a [whole host of new AVX-512 instructions](https://branchfree.org/2019/05/29/why-ice-lake-is-important-a-bit-bashers-perspective/) ? Will we be stuck gazing longly at these new instructions from afar while never being allowed to actually use them due to downclocking?",
						"license-based downclocking [1](:tiring) refers to the [semi-famous](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/) effect where lower than nominal frequency limits are imposed when certain SIMD instructions are executed, especially heavy floating point instructions or 512-bit wide instructions.",
						"le: Ice Lake AVX-512 Downclocking > Rocket Lake > Summary\nContent:\nHere’s what we’ve learned.\nThe Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.\nThe Rocket Lake i9-11900K CPU doesn’t exhibit any license-based downclock in the tested scenarios.\n",
						"Content:\nI ran avx-turbo as described above on an Ice Lake i5-1035G4, which is the middle-of-the-range Ice Lake client CPU running at up to 3.7 GHz. The full output is [hidden away in a gist](https://gist.github.com/travisdowns/c53f40fc4dbbd944f5613eaab78f3189) , but here are the all-important frequency results (all values in GHz):",
						"|Instruction Mix |Active Cores |",
						"| --- | --- | --- |",
						"|Scalar/128-bit |3.7 |3.6 |3.3 |3.3 |",
						"|Light 256-bit |3.7 |3.6 |3.3 |3.3 |",
						"|Heavy 256-bit |3.7 |3.6 |3.3 |3.3 |",
						"|Light 512-bit |3.6 |3.6 |3.3 |3.3 |",
						"|Heavy 512-bit |3.6 |3.6 |3.3 |3.3 |",
						"As expected, maximum frequency decreases with active core count, but scan down each column to see the effect of instruction category. Along this axis, there is almost no downclocking at all! Only for a single active core count is there any decrease with wider instructions, and it is a paltry only 100 MHz: from 3,700 MHz to 3,600 MHz when any 512-bit instructions are used.",
						"In any other scenario, including any time more than one core is active, or for heavy 256-bit instructions, there is *zero* license-based downclocking: everything runs as fast as scalar.",
						"Section Title: Ice Lake AVX-512 Downclocking > Rocket Lake",
						"ed.\nThe Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.\nT",
						"I ran avx-turbo as described above on an Ice Lake i5-1035G4, which is the middle-of-the-range Ice Lake client CPU running at up to 3.7 GHz. The full output is [hidden away in a gist](https://gist.github.com/travisdowns/c53f40fc4dbbd944f5613eaab78f3189) , but here are the all-important frequency results (all values in GHz):",
						"| --- | --- | --- | --- | --- |",
						"|1 |2 |3 |4 |",
						"The Ice Lake CPU has an all-core 512-bit turbo frequency of 3.3 GHz is 89% of the maximum single-core scalar frequency of 3.7 GHz, so within power and thermal limits this chip has a very “flat” frequency profile. The Rocket Lake 11900K is even flatter with an all-eight-cores frequency of 4.8 GHz clocking in at 94% of the 5.1 GHz single-core speed.",
						"\nThe Rocket Lake i9-11900K CPU doesn’t exhibit any license-based downclock in the tested scenarios.",
						"\nRocket Lake (shortened as RKL , see [wikipedia](https://en.wikipedia.org/wiki/Rocket_Lake) or [wikichip](https://en.wikichip.org/wiki/intel/microarchitectures/rocket_lake) for more) is more-or-less a backport of the 10nm Sunny Cove microarchitecture to Intel’s highly-tuned workhorse [5](:some) 14nm process.",
						"\n**Rocket Lake i9-11900K Frequency Matrix*",
						"|Active Cores |Instruction Mix |",
						"| --- | --- | -",
						"|1 Core |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|2 Cores |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|3 Cores |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|4 Cores |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|5 Cores |4.9 |4.9 |4.9 |4.9 |4.9 |",
						"|6 Cores |4.9 |4.9 |4.9 |4.9 |4.9 |",
						"|7 Cores |4.8 |4.8 |4.8 |4.8 |4.8 |",
						"|8 Cores |4.8 |4.8 |4.8 |4.8 |4.8 |",
						"The results paint a very promising picture of Rocket Lake’s AVX-512 frequency behavior: there is *no* license-based downclocking evident at any combination of core count and frequency [6](:rklcaveats) . Even heavy AVX-512 instructions can execute at the same frequency as lightweight scalar code.",
						" fact, the frequency behavior of this chip appears very simple: the full Turbo Boost 2.0 frequency [7](:tb2) of 5.1 GHz is available for any instruction mix up and up to 4 active cores, then the speed drops to 4.9 for 5 and 6 active cores, and finally to 4.8 GHz for 7 or 8 active c",
						"Here’s what we’ve learned."
					]
				},
				{
					"title": "Performance Matters | A blog about low-level software and hardware performance.",
					"url": "https://travisdowns.github.io/tags/avx512.html",
					"excerpts": [
						"Ice Lake AVX-512 Downclocking. Examining the extent of AVX related ... Travis Downs; travis.downs@gmail.com. A blog about low-level software and hardware ...Read more"
					]
				},
				{
					"title": "Performance Matters | A blog about low-level software and hardware performance.",
					"url": "https://travisdowns.github.io/tags/icelake.html",
					"excerpts": [
						"Aug 19, 2020[Ice Lake AVX-512 Downclocking](/blog/2020/08/19/icl-avx512-freq.html)Examining the extent of AVX related downclocking on Intel's Ice Lake CPU"
					]
				},
				{
					"title": "Performance Matters | A blog about low-level software and hardware performance.",
					"url": "https://travisdowns.github.io/",
					"excerpts": [
						"Ice Lake AVX-512 Downclocking. Examining the extent of AVX related ... travis.downs@gmail.com. A blog about low-level software and hardware performance.Read more"
					]
				},
				{
					"title": "Zen4's AVX512 Teardown | Hacker News",
					"url": "https://news.ycombinator.com/item?id=32983406",
					"excerpts": [
						"Zen4 AVX512 is mostly double-pumped: a 256-bit native hardware that processes two halves of the 512-bit register. No throttling observed.Read more"
					]
				},
				{
					"title": "AMD's Zen 4 Part 1: Frontend and Execution Engine",
					"url": "https://chipsandcheese.com/p/amds-zen-4-part-1-frontend-and-execution-engine",
					"excerpts": [
						"Zen 4 partially breaks this tradition, by keeping instructions that work on 512-bit vectors as one micro-op throughout most of the pipeline.",
						"Zen 4 partially breaks this tradition, by keeping instructions that work on 512-bit vectors as one micro-op throughout most of the pipeline. Each AVX-512 instruction thus only consumes one entry in the relevant out-of-order execution buffers. I assume they’re broken up into two operations after they enter a 256-bit execution pipe, meaning that the instruction is split into two 256-bit halves as late as possible in the pipeline. I also assume that’s what AMD’s “double pumping” is referring to."
					]
				},
				{
					"title": "Software Optimization Guide for the AMD Zen4 ...",
					"url": "https://www.numberworld.org/blogs/2024_8_7_zen5_avx512_teardown/57647_zen4_sog.pdf",
					"excerpts": [
						"The floating point unit supports AVX-512 with 512-bit macro ops and 512-bit storage in the register\nfile. Because the data paths are 256 bits wide, the scheduler uses two consecutive cycles to issue a\n512-bit operation."
					]
				},
				{
					"title": "Real World Technologies - Forums - Thread: 21 GB/s CSV Parsing on AMD with AVX-512",
					"url": "https://www.realworldtech.com/forum/?threadid=223950&curpostid=223958",
					"excerpts": [
						"Zen 4 can do 4 vector operations per clock cycle only with 256-bit operands, which is 4/3 > > better than Intel, but only 2 vector operations per clock cycle with 512-bit operands, > > which for many kinds of operations is the same as Intel, except that the most expensive > > of the Intel Xeon SKUs can do 2 FMA/cycle, while Zen 4 can do only 1 FMA + 1 FADD.",
						"Is that right?Zen 4 can do 4 vector operations per clock cycle only with 256-bit operands, which is 4/3\nbetter than Intel, but only 2 vector operations per clock cycle with 512-bit operands",
						"Zen 4 can do 4 vector operations per clock cycle only with 256-bit operands, which is 4/3 > > better than Intel, but only 2 vector operations per clock cycle with 512-bit operands, > > which for many kinds of operations is the same as Intel, except that the most expensive",
						"For 512-bit vector operations, Zen 5 (in the Turin/Strix Halo/Granite Ridge/Fire\nRange variants) doubles the throughput for almost all operations, with a few restrictions,\ne.g. no more than 10 operands read per cycle from the register file.With the exception of FMA and FMUL (exception that is typically valid only for Xeon Gold\nand Platinum) Zen 5 has double throughput vs. Intel Xeon for most 512-bit operations.",
						"For 512-bit vector operations, Zen 5 (in the Turin/Strix Halo/Granite Ridge/Fire\nRange variants) doubles the throughput for almost all operations, with a few restrictions,\ne.g. no more than 10 operands read per cycle from the register file.With the exception of FMA and FMUL (exception that is typically valid only for Xeon Gold\nand Platinum) Zen 5 has double throughput vs. Intel Xeon for most 512-bit operations."
					]
				},
				{
					"title": "Zen 5's AVX-512 Frequency Behavior | Hacker News",
					"url": "https://news.ycombinator.com/item?id=43215781",
					"excerpts": [
						"AMD did not start with a narrower datapath, even if this is a widespread myth. It only had a narrower path between the inner CPU core and the L1 data cache memory.",
						"On Zen 4, the advantage of using AVX-512 is not from a different throughput, but it is caused by a better instruction set and by the avoiding of bottlenecks in the CPU core front-end, at instruction fetching, decoding, renaming and dispatching.",
						"On Zen 4, the advantage of using AVX-512 is not from a different throughput, but it is caused by a better instruction set and by the avoiding of bottlenecks in the CPU core front-end, at instruction fetching, decoding, renaming and dispatching.",
						"For 512-bit AVX-512 instructions, Zen 4 and the Intel Xeon CPUs with P-cores have the same total datapath width for instructions other than FMUL/FMA/LD/ST, which has resulted in the same throughput per clock cycle for the programs that do not depend heavily on floating-point multiplications."
					]
				},
				{
					"title": "AVX-512",
					"url": "https://grokipedia.com/page/AVX-512",
					"excerpts": [
						"AMD enhanced its implementation with native 512-bit wide vector units in the Zen 5 microarchitecture (2024), improving throughput in Ryzen 9000 and EPYC Turin ...Read more"
					]
				},
				{
					"title": "Zen5's AVX512 Teardown + More...",
					"url": "https://www.numberworld.org/blogs/2024_8_7_zen5_avx512_teardown/",
					"excerpts": [
						".\nSo despite the 512-bit shuffle taking a different path to doubling up the 512-bit performance, the result is the same. Only 512-bit gets the improvement. T",
						"Latency of all 1-cycle SIMD instructions (regardless of width) has regressed to 2 cycles.",
						"\nAs a result, Zen5 brings little performance gain for scalar, 128-bit, and 256-bit SIMD code. It's 512-bit or bust.\nSo",
						"\n4 x 512-bit / cycle is possible and very easy to achieve. (no real bottlenecks in the way)\n4 x 51",
						"6 x ALU / cycle is not possible. Easy to hit 5/cycle, hard to get more. Most I can get in a synthetic is 5.5/cycle. Reason unknown - front end bottleneck?",
						":**\nThe full Zen5 core doubles up the 512-bit load/throughput from Zen4, but it doesn't do it symmetrically.",
						"**General SIMD (512-bit Datapath):**",
						"\n4 x 512-bit / cycle is possible and very easy to achieve. (no real bottlenecks in the way)\n4 ",
						"y.\nThis caught me by surprise since it wasn't revealed in AMD's GCC patch. Initially I suspected that this regression was a trade-off to achieve the full 256 -> 512-bit widening. So I asked AMD about this and they gave a completely different explanation. While I won't disclose their response (which I assume remains under NDA), I'll describe it as a CPU hazard that \"almost always\" turns 1-cycle SIMD instructions into 2-cycle latency.\n",
						"2 x 512-bit / cycle FADD.",
						"D.\n2 x 512-bit / cycle FMUL/FMA/IMUL/IFMA.\n",
						"A.\n2 x 512-bit FADD + 2 x 512-bit FMA / cycle (4 IPC) is possible.\n",
						".\n4 x 512-bit / cycle simple shuffle (unpacks, VPSHUFD).\n2",
						".\n2 x 512-bit / cycle complex shuffle (anything crossing a 128-bit boundary including permutes such as VPERMT2B).\nF",
						"FADD latency drops from 3 -> 2 cycles, but only if it can be forwarded. Otherwise remains 3 cycles.\nAll",
						"s.\nAll formerly 1-cycle latency SIMD instructions now have 2-cycle latency. Applies to all widths - including scalar.\n",
						":\nIf the problem really is just a hazard, we can hope that it will be fixed in a future AMD processor. But for now, developers should just assume 2-cycle latency. Combined with the 4 x 512-bit capability, you now need a minimum of 8-way ILP to saturate Zen5 even for formerly 1-cycle instructions. T",
						"hs.\nAll mask instructions are 2/cycle regardless of width. (Zen4 was 1/cycle for 64-bit mask)",
						"\n10-port limit remains, though Zen5 is better at avoiding it either by forwarding or caching of operands.\n**",
						"t:\nAs awesome as Zen5's AVX512 is, not everything is perfect. So let's start with the biggest regression I found:\n",
						"\nThroughput remains unchanged. The regression is only for latency.\nIn",
						"cy.\nInstructions that can be rename-eliminated (i.e. XOR zeroing) are unaffected and remain zero latency.",
						"**Load/Store Throughput:**",
						"le)\nMaximum 3 of any type of load/store each cycle.",
						"Key Differences from Zen4: Throughput of nearly all 512-bit instructions has been doubled up. (full Zen5 cores only); Latency of all 1-cycle ...Read more"
					]
				},
				{
					"title": "Zen 5's AVX-512 Frequency Behavior - by Chester Lam",
					"url": "https://chipsandcheese.com/p/zen-5s-avx-512-frequency-behavior",
					"excerpts": [
						"Zen 5 is AMD's first core to use full-width AVX-512 datapaths.",
						"Zen 5 benefits from a much better process node and also has no fixed clock speed offsets when running AVX-512 code.",
						"Evidently, Zen 5’s FPU is not troubled by getting hit with 512-bit vector operations, even when running at 5.7 GHz.",
						"If there is a transition period for the increased power draw, it’s so tiny that it can be ignored.",
						"\"Zen 5\": The AMD High-Performance 4nm x86-64 Microprocessor Core",
						"Zen 5\" can achieve full AVX512 performance at peak core frequency",
						"The way IPC drops more while clock speed (from perf counters) remains constant makes it feel like voltage droop. No way to tell though",
						"Quickly switching between scalar integer and heavy AVX-512 code basically spreads out the transition, so that the core eventually converges at a sustainable clock speed for the AVX-512 sequence in question."
					]
				},
				{
					"title": "Intel® AVX-512 Instructions",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/intel-avx-512-instructions.html",
					"excerpts": [
						"Intel AVX-512 features include 32 vector registers each 512 bits wide, eight dedicated mask registers, 512-bit operations on packed floating point data or packed integer data, embedded rounding controls (override global settings), embedded broadcast, embedded floating-point fault suppression, embedded memory fault suppression, new operations, additional gather/scatter support, high speed math instructions, compact representation of large displacement value, and the ability to have optional capabilities beyond the foundational capabilities."
					]
				},
				{
					"title": "Programming Reference",
					"url": "https://cdrdv2-public.intel.com/836496/architecture-instruction-set-extensions-programming-reference.pdf",
					"excerpts": [
						"The base of the 512-bit SIMD instruction extensions are referred to as Intel AVX-512 Foundation instructions. They\ninclude extensions of the Intel AVX and Intel AVX2 family of SIMD instructions but are encoded using a new\nencoding scheme with support for 512-bit vector registers, up to 32 vector registers in 64-bit mode, and condi-\ntional processing using opmask registers."
					]
				},
				{
					"title": "AVX-512",
					"url": "https://www.chessprogramming.org/AVX-512",
					"excerpts": [
						"AVX-512, an expansion of Intel's AVX and AVX2 instructions using the EVEX prefix, featuring 32 512-bit wide vector SIMD registers zmm0 through zmm31."
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"Multiple data can be packed into 64-bit MMX registers, 128-bit XMM registers, 256-bit YMM\nregisters, or 512-bit ZMM registers in the following ways:",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"256 bit (YMM)",
						"256 bit (YMM)",
						"256 bit (YMM)",
						"256 bit (YMM)",
						"256 bit (YMM)",
						"256 bit (YMM)",
						"8",
						"8",
						"8",
						"8",
						"8",
						"8",
						"8",
						"16",
						"16",
						"16",
						"16",
						"16"
					]
				},
				{
					"title": "BearSSL - Constant-Time Crypto",
					"url": "https://www.bearssl.org/constanttime.html",
					"excerpts": [
						"\nBitslicing uses a lot of variables, usually many more than the number of registers offered by a typical CPU (x86 CPU have 7 or 15 general-purpose registers, depending on operation mode; PowerPC raise that number to 30 or so). On big, modern platform, this is not that much of an issue: extra variables will be stored on the stack, and the exchange of data between stack and registers will be interleaved with the bitwise binary operations. Since such CPU are superscalar, theses data exchanges will be basically “free”: the ALU will be fully occupied, and there will be enough free slots for exchanges to be done without slowing down computations. However, the same is not true on reduced platforms, which are not necessarily able to run a bitwise operation *and* a memory access in the same clock cycle. Bitslicing tends to require large code. A rule of thumb is that the code that replaces a lookup table will be about as large as the table that it replaces, but this depends a lot on the architecture. On platforms where elementary opcodes are 32 bits each (e.g. PowerPC, SPARC…), bitslice code will typically be bigger. Thus, bitslicing tends to be incompatible with a goal of extreme code footprint reduction. Bitslicing works over orthogonalised data [3]() . This operation can be implemented relatively efficiently, but still has some non-negligible overhead. Bitslicing really shines in highly parallel contexts.\n"
					]
				},
				{
					"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
					"url": "https://upcommons.upc.edu/bitstreams/a90677c0-9f85-4a0b-8573-a170b09f0555/download",
					"excerpts": [
						"\nThe drawback of aggressive scheduling techniques, such\nas software pipelining, is their high register pressure.",
						"The addition of spill code can degrade performance [16]\ndue to additional cycles in the schedule, or due to memory\ninterferences.\nS"
					]
				},
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.\nF",
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "RealTime Data Compression",
					"url": "http://fastcompression.blogspot.com/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "XXH3 - a new speed-optimized hash algorithm",
					"url": "http://fastcompression.blogspot.com/2019/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "Intel® AVX-512 - Ultra Parallelized Multi-hash Computation ...",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-ultra-parallelized-multi-hash-computation-for-data-streaming-workloads-technology-guide-1693301077.pdf",
					"excerpts": [
						"3.3.1 Parallel Hash Computation with Splitting Input and Vectorized Seeds by Leveraging Intel® AVX-512 ",
						"To calculate eight independent hash values, we use eight random seeds, then continuously split the input data stream (for\nexample, an input key) into 64-bit width data chunks and then get consumed one by one. To fully take advantage of Intel AVX-\n512 ISA, the data chunk size multiplies the seed count should be equal to 512. For example, if the seed count is four, then the data\nstream should be divided into 128-bit chunks",
						"The eight different seeds are also put into a 512-bit wide vector register.",
						"The process is roughly illustrated in Figure 4 .",
						"We have pre-defined five different prime numbers (P1 to P5) and eight different seeds to be used during the computation. The\nfive prime numbers are getting involved in the hash computation later to target a good mixture of hash results. Each prime\nnumber is broadcasted into the vector of eight items. The eight different seeds are also put into a 512-bit wide vector register.\nThe algorithm also broadcasts the truncated 64-bit-wide input data into the 512-bit-wide vector with eight copies. Then it mixes\nthe 8-seed vector and the pre-defined five prime numbers by using addition, multiplication, rotation, shift, and xor arithmetic\noperations in a vectorized style.",
						"3.3.2 Accelerate Multiplication and Addition Operations by Leveraging Intel AVX-512 IFMA Instruction in Hash\nComputation",
						"Intel AVX-512 IFMA instruction is introduced in 8th Gen Intel® Core™ i3 Processors and 3rd Gen Intel® Xeon® Scalable\nProcessors, which can support fused multiply and add operation of integers belonging to Intel AVX-512 instruction sets. Since\nMAD (Multiply-Add-Divide) operation is essential in many hash functions to spread out the keys into the hash buckets, the\nvectorized Intel AVX-512 IFMA instruction will speed up the process of multiplication and addition operations. Replacing the\nvectorized multiplication and addition described in the above section with Intel AVX-512 IFMA instruction can accelerate the\ncomputation.\n3.",
						"3.3.3 Accelerate Sketch Counter Updates by Leveraging Intel AVX-512 Gather and Scatter Instructions",
						"Sketch-based algorithms require a counter-update after the proposed hashing computation. Instead of sequentially updating\nthe sketch counter arrays one by one, the Intel AVX-512 gather and scatter instructions can be used as the multiple hash digests\nare already generated in the previous vectorized hash computation. By using Intel AVX-512 gather and scatter instructions, the\nsketch counter array can be updated in parallel.",
						"This technology guide demonstrates a brand new ultra-parallelized multi-hash computation model for data streaming\nworkloads. The model offers several advantages, which have been detailed through the recently unveiled technique:\n1.\nHigh-performance: This technology guide proposes a novel model by leveraging Intel AVX-512 instruction sets to\nget data-level parallelism when processing multi-hash computations. Based on xxHash, we achieve a performance\ngain of up to 2x on key-add and key-lookup operations compared to the standard CRC-32 instruction."
					]
				},
				{
					"title": "Cortex-A72 Microarchitecture",
					"url": "https://wordpress-courses2425.wolfware.ncsu.edu/ece-785-sprg-2025/wp-content/uploads/sites/80/2025/03/Cortex-A72-Microarchitecture.pdf",
					"excerpts": ["16"]
				},
				{
					"title": "Vectorizing xxHash for Fun and Profit | The Pseudo Random Bit Bucket",
					"url": "https://moinakg.wordpress.com/2013/01/19/vectorizing-xxhash-for-fun-and-profit/",
					"excerpts": [
						"An SSE register is 128-bit and can perfectly hold four 32-bit packed integers."
					]
				}
			],
			"reasoning": "The frequency throttling behavior of AVX-512 has significantly evolved across different CPU generations. On older Intel architectures like Skylake-SP and Cascade Lake-SP (1st and 2nd Gen Xeon), using AVX-512 instructions often led to severe frequency downclocking and costly transition stalls, which could unpredictably degrade performance. These CPUs employed a multi-tiered 'power license' system where even 'light' AVX-512 operations could trigger a frequency drop and introduce microsecond-long delays during transitions between power states. This made AVX-512 adoption risky and often less performant than AVX2 for mixed workloads.\n\nHowever, newer Intel generations, starting with Cooper Lake and Ice Lake Server (3rd Gen Xeon), showed significant improvements. The frequency impact of AVX-512 was substantially reduced, and the problematic transition stalls were virtually eliminated, leading to much more stable and predictable performance. This trend continued with Ice Lake Client and Rocket Lake (10th and 11th Gen Core processors), where AVX-512 exhibited negligible to zero license-based downclocking. On these platforms, AVX-512 could run at nearly the same frequency as scalar code, making it a pure performance gain.\n\nAMD's approach to AVX-512 also differs. Zen 4 implemented AVX-512 by 'double-pumping' its 256-bit execution units, meaning 512-bit operations were effectively split into two 256-bit operations. While this didn't offer a direct 2x throughput increase over AVX2, it still provided benefits by avoiding front-end bottlenecks and offering a richer instruction set. Zen 5 further advanced this by introducing native 512-bit execution units and is reported to have no fixed clock speed offsets for AVX-512, allowing it to achieve full AVX-512 performance at peak core frequencies. This indicates that on Zen 5, AVX-512 is designed to be a straightforward performance enhancement without the frequency penalties seen in earlier Intel designs.",
			"confidence": "high"
		},
		{
			"field": "practitioner_codebases_and_libraries.licensing",
			"citations": [
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"The library files `xxhash.c` and `xxhash.h` are BSD licensed."
					]
				},
				{
					"title": "highwayhash/LICENSE at master · google/highwayhash · GitHub",
					"url": "https://github.com/google/highwayhash/blob/master/LICENSE",
					"excerpts": [
						"Licensed under the Apache License, Version 2.0 (the \"License\");. you may not use this file except in compliance with the License. You may ...Read more"
					]
				}
			],
			"reasoning": "The specific licensing information is directly stated in the excerpts: one excerpt declares that the xxHash library files are BSD licensed, which establishes the BSD 2-Clause licensing reference. Another excerpt states that HighwayHash is licensed under the Apache License, Version 2.0, confirming the Apache 2.0 licensing for HighwayHash. These statements precisely support the requested fine-grained field value about licensing, providing exact license identifiers for the two libraries mentioned.",
			"confidence": "high"
		},
		{
			"field": "algebraic_simplification_techniques.aarch64_shifted_operand_optimization",
			"citations": [
				{
					"title": "Armv8-A Instruction Set Architecture",
					"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Learn%20the%20Architecture/Armv8-A%20Instruction%20Set%20Architecture.pdf",
					"excerpts": [
						"•\nOperand 2: This will be a register or a constant, and is the second input to the instruction.\nWhen operand 2 is a register, it may include an optional shift.\nWhen"
					]
				},
				{
					"title": "ARMv8-A A64 ISA Overview",
					"url": "https://armkeil.blob.core.windows.net/developer/Files/pdf/graphics-and-multimedia/ARMv8_InstructionSetOverview.pdf",
					"excerpts": [
						"\nArithmetic, logic, data moves, bit field manipulations, shifts, conditional comparisons, and more\n",
						"Shifts and rotates**",
						"**Shift amount may be an immediate or a register",
						"Register values taken modulo 32-bit or 64-bit\n*",
						"\nMultiply by power of 2\n"
					]
				},
				{
					"title": "The AArch64 processor (aka arm64), part 4: Addition and subtraction - The Old New Thing",
					"url": "https://devblogs.microsoft.com/oldnewthing/20220729-00/?p=106915",
					"excerpts": [
						"To ask for flags to be set based on the result, apply an `S` suffix to the opcode, producing `ADDS` ."
					]
				}
			],
			"reasoning": "The finegrained field describes exploiting a shifted-register operand in AArch64 to fuse an operation like a + (a << k) into a single instruction. The most directly supportive content states that the second operand may be a register and may include an optional shift, establishing the mechanism to apply a shift to the same value in the same instruction. It also explains that adding a register value and an optionally-sh shifted register value corresponds to the ADD (shifted register) encoding, which directly implements the fused form. The ARM-specific detail that the shift amount can be provided immmediately or from a register confirms the flexibility of how k can be supplied. The notes about 32-bit vs 64-bit variants and the general shift/rotate discussion further clarify scope and constraints for such a fused form on AArch64. These points collectively support the view that the a op2 (a shift_op ...) pattern can be realized efficiently through a shifted-register operand in A64, reducing instruction count and latency for the hash-style algebraic pattern described in the query. The other ARM optimization resources reinforce this by illustrating how LEA-like fusion and shifted operands are treated in ARM documentation and related optimization guides, reinforcing that this fusion concept has practical ARM-specific backing beyond x86 characteristics.",
			"confidence": "high"
		},
		{
			"field": "algebraic_simplification_techniques.reassociation_opportunities",
			"citations": [
				{
					"title": "LLVM’s Analysis and Transform Passes — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/Passes.html",
					"excerpts": [
						"Content:\nCombine instructions to form fewer, simpler instructions. This pass does not\nmodify the CFG. This pass is where algebraic simplification happens.\nThis pass combines things like:\ninto:\nThis is a simple worklist-driven algorithm.\nThis pass guarantees that the following canonicalizations are performed on the\nprogram:\nIf a binary operator has a constant operand, it is moved to the right-hand\nside.\nBitwise operators with constant operands are always grouped so that shifts\nare performed first, then `or` s, then `and` s, then `xor` s.\nCompare instructions are converted from `<` , `>` , `≤` , or `≥` to `=` or `≠` if possible.\nAll `cmp` instructions on boolean values are replaced with logical\noperations.\n`add X, X` is represented as `mul X, 2` ⇒ `shl X, 1`\nMultiplies with a constant power-of-two argument are transformed into\nshifts.\n… etc.\nThis "
					]
				},
				{
					"title": "⚙ D65614 [Reassociate] Stop linearizing all associative expression trees w/o profitability",
					"url": "https://reviews.llvm.org/D65614",
					"excerpts": [
						"The Reassociate pass currently does two things. First, it factors out common sub-expressions and otherwise *optimizes* the expression tree.Read more"
					]
				},
				{
					"title": "InstCombine contributor guide — LLVM 23.0.0git documentation",
					"url": "https://llvm.org/docs/InstCombineContributorGuide.html",
					"excerpts": [
						"| Original Pattern | Canonical Form | Condition |",
						"| `icmp spred X, Y` | `icmp samesign upred X, Y` | `sign(X) == sign(Y)` |",
						"| `mul X, C` | `shl X, Log2(C)` | `isPowerOf2(C)` |",
						"eading\")\nContent:\nAvoid transforms that are hardcoded to specific constants. Try to figure\nout what the general rule for arbitrary constants is. Add handling for conjugate patterns. For example, if you implement a fold\nfor `icmp eq` , you almost certainly also want to support `icmp ne` , with the\ninverse result."
					]
				},
				{
					"title": "356477-Optimization-Reference-Manual-V2-002.pdf",
					"url": "https://cdrdv2-public.intel.com/814199/356477-Optimization-Reference-Manual-V2-002.pdf",
					"excerpts": [
						"Thus, LEA should not be used in the technique of\nadding two values and produce the result in a third register. LEA should be used for address computation.",
						"Partial Register Access**\nAlthough partial register access does not cause additional delay, the in-order hardware tracks dependency on the full\nregister. Thus 8-bit registers like AL and AH are not treated as independent registers. Additionally some instructions\nlike LEA, vanilla loads, and pop are slower when the input is smaller than 4 bytes.",
						"Integer shift instructions that encodes shift count in the immediate byte have one-cycle latency. In contrast, shift\ninstructions using shift count in the ECX register may need to wait for the register count are updated. Thus shift\ninstruction using register count has 3-cycle latency."
					]
				}
			],
			"reasoning": "The field value discusses when reassociation is possible based on the algebraic properties of the operators involved. Reassociation is a transformation that groups and reorders operands to create more efficient expressions, and its viability depends on associativity and commutativity of the operators. The strongest support comes from guidance that explicitly describes reassociation and related canonicalization: such sources explain that factoring and rearranging expressions to combine like terms or move constants can be safe and beneficial, and that implementation passes can perform these algebraic simplifications. For example, guidance showing that complex expressions can be reassociated or factored into simpler forms (like a + (a shift) + const) aligns with the claim that ordering can be rearranged when the structure is amenable (same operator, associative/commutative). Additional examples illustrate canonical forms and conditions under which a rearrangement holds, supporting the notion that certain ARX patterns may or may not be safely reordered depending on distributive properties. The excerpts that caution when to use certain instructions (like LEA) for address computation rather than arithmetic rearrangement provide practical boundaries for mapping these algebraic ideas to real hardware implementations, thereby supporting the idea that not all rearrangements are portable across architectures. Taken together, these excerpts substantiate the claim that reassociation opportunities exist under specific algebraic conditions and that mixed-pattern cases require careful handling to preserve bit-exact results.",
			"confidence": "high"
		},
		{
			"field": "algebraic_simplification_techniques.safety_and_bit_exactness",
			"citations": [
				{
					"title": "bit manipulation - When does bit shifting cause undefined behavior in C++? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/76694990/when-does-bit-shifting-cause-undefined-behavior-in-c",
					"excerpts": [
						"In all cases, if the number of bits to shift(the right operand) is greater than or equal to the number of bits of the left operand after [integer promotion](https://en.cppreference.com/w/cpp/language/implicit_conversion) , the behavior is undefined."
					]
				},
				{
					"title": "Bitwise operations in C - Wikipedia",
					"url": "https://en.wikipedia.org/wiki/Bitwise_operations_in_C",
					"excerpts": [
						"The symbol of right shift operator is `>>` . For its operation, it requires two [operands](/wiki/Operand \"Operand\") . It shifts each bit in its left operand to the right.\nThe number following the operator decides the number of places the bits are shifted (i.e. the right operand).\nThus by doing `ch >> 3` all the bits will be shifted to the right by three places and so on.\nHowever, do note that a shift operand value which is either a negative number or is greater than or equal to the total number of bits in this value results in [undefined behavior](/wiki/Undefined_behavior \"Undefined behavior\") . For example, when shifting a 32 bit unsigned integer, a shift amount of 32 or higher would be undefined."
					]
				},
				{
					"title": "The OpenCL™ C Specification - Khronos Registry",
					"url": "https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_C.html",
					"excerpts": [
						"The operators right-shift ( **>>** ), left-shift ( **<<** ) operate on all scalar\nand vector built-in types except the built-in scalar and vector `float` types.\nFor built-in vector types, the operators are applied component-wise.",
						"The result of `E1` **<<** `E2` is `E1` left-shifted by log 2 (N) least significant\nbits in `E2` viewed as an unsigned integer value, where N is the number of bits\nused to represent the data type of `E1` after integer promotion [ [30]( \"View footnote.\") ] , if `E1` is a scalar, or the number of bits\nused to represent the type of `E1` elements, if `E1` is a vector.\nThe vacated bits are filled with zeros.",
						"The result of `E1` **>>** `E2` is `E1` right-shifted by log 2 (N) least\nsignificant bits in `E2` viewed as an unsigned integer value, where N is the\nnumber of bits used to represent the data type of `E1` after integer\npromotion, if `E1` is a scalar, or the number of bits used to represent the\ntype of `E1` elements, if `E1` is a vector.\nIf `E1` has an unsigned type or if `E1` has a signed type and a nonnegative\nvalue, the vacated bits are filled with zeros.\nIf `E1` has a signed type and a negative value, the vacated bits are filled\nwith ones."
					]
				},
				{
					"title": "shiftOperators(3) - Khronos Registry",
					"url": "https://registry.khronos.org/OpenCL/sdk/3.0/docs/man/html/shiftOperators.html",
					"excerpts": [
						"The operators right-shift ( **>>** ), left-shift ( **<<** ) operate on all scalar\nand vector built-in types except the built-in scalar and vector `float` types.",
						"For built-in vector types, the operators are applied component-wise.",
						"the right-shift ( **>>** ), left-shift ( **<<** ) operators, the rightmost\noperand must be a scalar if the first operand is a scalar, and the rightmost\noperand can be a vector or scalar if the first operand is a vector.",
						"The result of `E1` **<<** `E2` is `E1` left-shifted by log 2 (N) least significant\nbits in `E2` viewed as an unsigned integer value, where N is the number of bits\nused to represent the data type of `E1` after integer promotion [ [1]( \"View footnote.\") ] , if `E1` is a scalar, or the number of bits\nused to represent the type of `E1` elements, if `E1` is a vector."
					]
				},
				{
					"title": "ISO/IEC 9899:yyyy",
					"url": "https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2310.pdf",
					"excerpts": [
						"The result of E1 >> E2 is E1 right-shifted E2 bit positions. If E1 has an unsigned type or if E1 has a\nsigned type and a nonnegative value, the value of the result is the integral part of the quotient of\nE1 */* 2 E2 . If E1 has a signed type and a negative value, the resulting value is implementation-defined."
					]
				},
				{
					"title": "Intel® 64 and IA-32 Architectures Software Developer's Manual",
					"url": "https://raw.githubusercontent.com/wiki/8dcc/naos/manuals/intel-sdm-vol-2abcd.pdf",
					"excerpts": [
						"Shift doublewords in xmm2 left by amount specified in\nthe corresponding element of xmm3/m128 while\nshifting in 0s."
					]
				},
				{
					"title": "⚙ D56474 [ARM] [NEON] Add ROTR/ROTL lowering",
					"url": "https://reviews.llvm.org/D56474?id=180788",
					"excerpts": [
						"SDValue Temporary = DAG.getNode(Left ? ISD::SHL : ISD::SRL, DL, VT, Value, Amount);",
						"Left ? ISD::SHL : ISD::SRL",
						"v = vsliq_n_u32(vshrq_n_u32(v, 19), v, 13);",
						"vshr.u32 tmp, v, #19",
						"vshl.i32 v, v, #13",
						"vorr v, v, tmp"
					]
				},
				{
					"title": "Advanced SIMD (Neon) intrinsics",
					"url": "https://developer.arm.com/documentation/101028/0010/Advanced-SIMD--Neon--intrinsics",
					"excerpts": [
						"The intrinsics in this section provide C and C++ programmers with a simple programming model allowing easy access to code-generation of the Advanced SIMD instructions for both AArch64 and AArch32 execution states.",
						"The Advanced SIMD extension provides for arithmetic, logical and saturated arithmetic operations on 8-bit, 16-bit and 32-bit integers (and sometimes on 64-bit integers) and on 32-bit and 64-bit floating-point data, arranged in 64-bit and 128-bit vectors."
					]
				}
			],
			"reasoning": "The field value asserts two core points about safety and exactness: (1) 32-bit unsigned arithmetic wraps around modulo 2^32, and (2) shift operations have platform-specific, well-defined semantics, with undefined behavior to be avoided. The excerpts directly addressing these points are the ones that discuss the undefined behavior for shifts when the shift amount is out of range, and the definitions/semantics of how shifts behave on various platforms. Specifically: a citation stating that shifting by a count greater than or equal to the bit width is undefined in C/C++ provides the foundational constraint that algebraic transformations must respect modulo behavior and cannot rely on naive shift handling in all cases. Additionally, excerpts that describe how certain environments (OpenCL, x86, ARM) define or constrain shift results (including masking or zero-filling of vacated bits) establish how to correctly model target semantics when performing algebraic simplifications. By paraphrasing these points, one can justify transformations that preserve bit-identical results only when they respect the exact semantics described (undefined behavior must be avoided, and platform-specific masking/zeroing rules must be followed). The most directly relevant content shows the boundaries: when a shift count is too large, behavior is undefined; other excerpts specify how vacated bits are filled or how certain architectures mask shift counts, which together map to the safe/unsafe regions for algebraic rewriting. The excerpts that discuss masking of shift counts on x86 and the OpenCL/IEEE/open-standards perspective provide concrete, actionable guidance for safe transformations. These excerpts collectively underpin the claim that one must simulate and validate with a golden reference across edge cases (0..31 for 32-bit shifts, and larger counts depending on platform) to ensure bit-identical results across platforms and optimization levels.",
			"confidence": "high"
		},
		{
			"field": "practitioner_codebases_and_libraries.supported_isas",
			"citations": [
				{
					"title": "xxHash: XXH3 family",
					"url": "https://xxhash.com/doc/v0.8.2/group___x_x_h3__family.html",
					"excerpts": [
						"XXH3 has a fast scalar implementation, but it also includes accelerated SIMD implementations for many common platforms:\nAVX512\nAVX2\nSSE2\nARM NEON\nWebAssembly SIMD128\nPOWER8 VSX\ns390x ZVector",
						"XXH3 is a more recent hash algorithm featuring:\nImproved speed for both small and large inputs\nTrue 64-bit and 128-bit outputs\nSIMD acceleration\nImproved 32-bit viability",
						"For the x86 family, an automatic runtime dispatcher is included separately in [xxh_x86dispatch.c](xxh__x86dispatch_8c.html) .",
						"XXH3 implementation is portable: it has a generic C90 formulation that can be compiled on any platform, all implementations generate exactly the same hash value on all platforms. Starting from v0.8.0, it's also labelled \"stable\", meaning that any future version will also generate the same hash value."
					]
				},
				{
					"title": "GitHub - Cyan4973/xxHash: Extremely fast non-cryptographic hash algorithm",
					"url": "https://github.com/Cyan4973/xxHash",
					"excerpts": [
						"`XXH_VECTOR` : manually select a vector instruction set (default: auto-selected at compilation time). Available instruction sets are `XXH_SCALAR` , `XXH_SSE2` , `XXH_AVX2` , `XXH_AVX512` , `XXH_NEON` and `XXH_VSX` . Compiler may require additional flags to ensure proper support (for example, `gcc` on x86_64 requires `-mavx2` for `AVX2` , or `-mavx512f` for `AVX512` )."
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"| HighwayHashAVX2 | 7.34 | 1.81 | 1.71 | 1.04 | 0.95 | 0.24 |",
						"We also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast).",
						"In addition to high throughput, the algorithm is designed for low finalization\ncost."
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": [
						"enum XXH_VECTOR_TYPE /* fake enum */ {\n    XXH_SCALAR = 0,  /*!< Portable scalar version */\n    XXH_SSE2   = 1,  /*!<\n                      * SSE2 for Pentium 4, Opteron, all x86_64. *\n                      * @note SSE2 is also guaranteed on Windows 10, macOS, and\n                      * Android x86. */\n    XXH_AVX2   = 2,  /*!< AVX2 for Haswell and Bulldozer */\n    XXH_AVX512 = 3,  /*!< AVX512 for Skylake and Icelake */\n    XXH_NEON   = 4,  /*!<\n                       * NEON for most ARMv7-A, all AArch64, and WASM SIMD128\n                       * via the SIMDeverywhere polyfill provided with the\n                       * Emscripten SDK. */\n    XXH_VSX    = 5,  /*!< VSX and ZVector for POWER8/z13 (64-bit) */\n    XXH_SVE    = 6,  /*!< SVE for some ARMv8-A and ARMv9-A */\n};"
					]
				},
				{
					"title": "XXH3 - a new speed-optimized hash algorithm",
					"url": "http://fastcompression.blogspot.com/2019/",
					"excerpts": [
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set. `SSE2` is also free of dynamic throttling issues, and is automatically enabled on all x64 compilers."
					]
				},
				{
					"title": "GitHub - minio/highwayhash: Native Go version of HighwayHash with optimized assembly implementations on Intel and ARM. Able to process over 10 GB/sec on a single core on Intel CPUs - https://en.wikipedia.org/wiki/HighwayHash",
					"url": "https://github.com/minio/highwayhash",
					"excerpts": [
						"The algorithm outputs digests ranging from 64 bits up to 256 bits at no extra cost."
					]
				}
			],
			"reasoning": "The most directly relevant evidence shows explicit enumeration or broad support for multiple SIMD targets. One excerpt lists AVX512, AVX2, SSE2, NEON, and VSX as available SIMD paths, which directly aligns with broad cross-ISA support. Another excerpt defines an enum of vector targets including SCALAR, SSE2, AVX2, AVX512, NEON, VSX, and SVE, signaling multi-ISA coverage. An additional excerpt discusses HighwayHash/xxHash implementations that provide SIMD acceleration across several architectures (AVX2, SSE4.1, VSX, portable), reinforcing the notion of production-grade libraries with wide ISA support. Further excerpts mention explicit SIMD paths or runtime dispatch for multiple architectures (e.g., NEON, AVX2, AVX512, SSE2, SVE, VSX) and the availability of dispatch mechanisms to select the best code path per host. Some excerpts also note64-bit SIMD acceleration and portability, which corroborate broad ISA support as a design goal. Taken together, these excerpts corroborate that production-grade hash libraries aim for broad SIMD coverage across x86/x64, ARM (NEON/SVE), and POWER (VSX), matching the described field value. The strongest, most direct connections are the explicit ISA lists and the enum of vector targets, followed by concrete examples of multi-architecture SIMD implementations in hash libraries, and finally notes on portable/dispatch-based approaches that enable such broad support.",
			"confidence": "high"
		},
		{
			"field": "avx512_performance_analysis.frequency_throttling_by_generation",
			"citations": [
				{
					"title": "Technical reading from the Cloudflare blog for the holidays",
					"url": "https://blog.cloudflare.com/2017-holiday-reading-from-the-cloudflare-blog/",
					"excerpts": [
						"Intel processors contain special AVX-512 that provide 512-bit wide SIMD instructions to speed up certain calculations. However, these instructions have a downside: when used the CPU base frequency is scaled down slowing down other instructions."
					]
				},
				{
					"title": "Ice Lake AVX-512 Downclocking | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html",
					"excerpts": [
						"The Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.",
						"| --- | --- | --- | --- | --- |",
						"|1 |2 |3 |4 |",
						"The Ice Lake CPU has an all-core 512-bit turbo frequency of 3.3 GHz is 89% of the maximum single-core scalar frequency of 3.7 GHz, so within power and thermal limits this chip has a very “flat” frequency profile. The Rocket Lake 11900K is even flatter with an all-eight-cores frequency of 4.8 GHz clocking in at 94% of the 5.1 GHz single-core speed.",
						"\nThe Rocket Lake i9-11900K CPU doesn’t exhibit any license-based downclock in the tested scenarios.",
						"The results paint a very promising picture of Rocket Lake’s AVX-512 frequency behavior: there is *no* license-based downclocking evident at any combination of core count and frequency [6](:rklcaveats) . Even heavy AVX-512 instructions can execute at the same frequency as lightweight scalar code.",
						" fact, the frequency behavior of this chip appears very simple: the full Turbo Boost 2.0 frequency [7](:tb2) of 5.1 GHz is available for any instruction mix up and up to 4 active cores, then the speed drops to 4.9 for 5 and 6 active cores, and finally to 4.8 GHz for 7 or 8 active c",
						"As expected, maximum frequency decreases with active core count, but scan down each column to see the effect of instruction category. Along this axis, there is almost no downclocking at all! Only for a single active core count is there any decrease with wider instructions, and it is a paltry only 100 MHz: from 3,700 MHz to 3,600 MHz when any 512-bit instructions are used.",
						"In any other scenario, including any time more than one core is active, or for heavy 256-bit instructions, there is *zero* license-based downclocking: everything runs as fast as scalar.",
						"Section Title: Ice Lake AVX-512 Downclocking > Rocket Lake",
						"I ran avx-turbo as described above on an Ice Lake i5-1035G4, which is the middle-of-the-range Ice Lake client CPU running at up to 3.7 GHz. The full output is [hidden away in a gist](https://gist.github.com/travisdowns/c53f40fc4dbbd944f5613eaab78f3189) , but here are the all-important frequency results (all values in GHz):"
					]
				},
				{
					"title": "AVX-512: when and how to use these new instructions",
					"url": "https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/",
					"excerpts": [
						"However, some of these instructions use a lot of power and generate a lot of heat. To keep power usage within bounds, Intel reduces the frequency of the cores dynamically.",
						"el cores can run in one of three modes: license 0 (L0) is the fastest (and is associated with the turbo frequencies *written on the box* ), license 1 (L1) is slower and license 2 (L2) is the slowest.",
						"o get into license 2, you need sustained use of heavy 512-bit instructions, where *sustained* means approximately one such instruction every cycle",
						"On server-class processor, the downclocking is determined on a per-core basis based on the license and the total number of active cores, on the same CPU socket, irrespective of the license of the other cores.",
						"the worst case, you might be running at half the advertised frequency and thus your whole application could run slower.",
						" On this basis, [some engineers have recommended that we disable AVX-512 instructions default on our servers](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/)",
						"Engineers should probably use tools to monitor the frequency of their cores to ensure they are running in the expected license.",
						"If your code at least partly involves sustained use of heavy numerical instructions you might consider isolating this work to specific threads (and hence cores), to limit the downclocking to cores that are taking full advantage of AVX-512. If this is not practical or possible, then you should mix this code with other (non-AVX-512 code) with care."
					]
				},
				{
					"title": "optimization - SIMD instructions lowering CPU frequency - Stack Overflow",
					"url": "https://stackoverflow.com/questions/56852812/simd-instructions-lowering-cpu-frequency",
					"excerpts": [
						"\n2Trevis Down (aka Beeonrope on OS) wrote about this in the comments in this [post](https://lemire.me/blog/2018/08/13/the-dangers-of-avx-512-throttling-myth-or-reality/) and continued the discussion [here](https://www.realworldtech.com/forum/?threadid=179654&curpostid=179727) . He found that each ties (scalar, AVX/AVX2, AVX-512) has \"cheap\" (no FP, simple operations) instructions and \"heavy\" instruction. Cheap instructions drop the frequency to the one of the next higher tier (e.g. cheap AVX-512 inst use the AVX/AVX2 tier) even if used sparsely. Heavy inst must be used more than 1 every ...Margaret Bloom– [Margaret Bloom](/users/5801661/margaret-bloom \"44,771 rep",
						"You can find the behaviour of each instruction with a bit of patience or by his rule of thumb. Finally note that this frequency scaling is a problem iif the ratio of vector to scalar instruction is low enough so that the drop in frequency is not balanced by the bigger width at which data is processed.",
						"2... two cycles and drop the frequency according to their tier (e.g. AVX-512 heavy instrs drop the frequency to the AV-512 base)."
					]
				},
				{
					"title": "Trying harder to make AVX-512 look bad: my quantified and reproducible results – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/",
					"excerpts": [
						"Intel documents that the use of AVX-512 instructions can lower the frequency of the processor. How big the effect is depends on the processor."
					]
				},
				{
					"title": "Intel® AVX-512 - Instruction Set for Packet Processing",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-instruction-set-for-packet-processing-technology-guide-1645717553.pdf",
					"excerpts": [
						"cessors**\nIn the 3rd Generation Intel® Xeon® Scalable\nprocessors, the Power Level 1/Intel® AVX-512 Light\npower level frequency ranges are significantly\nimproved, as shown in Figure 12 . This means that on\nthe 3rd Generation Intel® Xeon® Scalable processors,\nshow as ICX, the IPC gains of using Intel® AVX-512\ninstructions are typically maintained or enhanced\nwith an improvement in Turbo frequency ranges,\nwhen compared to the 1st and 2 nd Generation Intel®\nXeon® Scalable processors, shown as SKX and CLX\nrespectively.\n**",
						"4.3.1\nPower Level Update for 3rd Generation Intel® Xeon® Scalable Processors .",
						"INSTRUCTION SET MIX"
					]
				},
				{
					"title": "Packet Processing with Intel® AVX-512 Instruction Set",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-packet-processing-with-intel-avx-512-instruction-set-solution-brief-1678190247.pdf",
					"excerpts": [
						" ).\nOn the 1st Generation Intel® Xeon® Scalable processors, power level\ntransitions may incur a block time of 10-20 µs while the core identifies\na new frequency. During this time instructions are not being retired.\nThe e"
					]
				},
				{
					"title": "Intel® AVX-512 - Ultra Parallelized Multi-hash Computation ...",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-ultra-parallelized-multi-hash-computation-for-data-streaming-workloads-technology-guide-1693301077.pdf",
					"excerpts": [
						"3.3.1 Parallel Hash Computation with Splitting Input and Vectorized Seeds by Leveraging Intel® AVX-512 ",
						"To calculate eight independent hash values, we use eight random seeds, then continuously split the input data stream (for\nexample, an input key) into 64-bit width data chunks and then get consumed one by one. To fully take advantage of Intel AVX-\n512 ISA, the data chunk size multiplies the seed count should be equal to 512. For example, if the seed count is four, then the data\nstream should be divided into 128-bit chunks",
						"The eight different seeds are also put into a 512-bit wide vector register.",
						"The process is roughly illustrated in Figure 4 .",
						"We have pre-defined five different prime numbers (P1 to P5) and eight different seeds to be used during the computation. The\nfive prime numbers are getting involved in the hash computation later to target a good mixture of hash results. Each prime\nnumber is broadcasted into the vector of eight items. The eight different seeds are also put into a 512-bit wide vector register.\nThe algorithm also broadcasts the truncated 64-bit-wide input data into the 512-bit-wide vector with eight copies. Then it mixes\nthe 8-seed vector and the pre-defined five prime numbers by using addition, multiplication, rotation, shift, and xor arithmetic\noperations in a vectorized style.",
						"3.3.2 Accelerate Multiplication and Addition Operations by Leveraging Intel AVX-512 IFMA Instruction in Hash\nComputation",
						"Intel AVX-512 IFMA instruction is introduced in 8th Gen Intel® Core™ i3 Processors and 3rd Gen Intel® Xeon® Scalable\nProcessors, which can support fused multiply and add operation of integers belonging to Intel AVX-512 instruction sets. Since\nMAD (Multiply-Add-Divide) operation is essential in many hash functions to spread out the keys into the hash buckets, the\nvectorized Intel AVX-512 IFMA instruction will speed up the process of multiplication and addition operations. Replacing the\nvectorized multiplication and addition described in the above section with Intel AVX-512 IFMA instruction can accelerate the\ncomputation.\n3.",
						"3.3.3 Accelerate Sketch Counter Updates by Leveraging Intel AVX-512 Gather and Scatter Instructions",
						"Sketch-based algorithms require a counter-update after the proposed hashing computation. Instead of sequentially updating\nthe sketch counter arrays one by one, the Intel AVX-512 gather and scatter instructions can be used as the multiple hash digests\nare already generated in the previous vectorized hash computation. By using Intel AVX-512 gather and scatter instructions, the\nsketch counter array can be updated in parallel.",
						"This technology guide demonstrates a brand new ultra-parallelized multi-hash computation model for data streaming\nworkloads. The model offers several advantages, which have been detailed through the recently unveiled technique:\n1.\nHigh-performance: This technology guide proposes a novel model by leveraging Intel AVX-512 instruction sets to\nget data-level parallelism when processing multi-hash computations. Based on xxHash, we achieve a performance\ngain of up to 2x on key-add and key-lookup operations compared to the standard CRC-32 instruction."
					]
				}
			],
			"reasoning": "The finegrained field describes how AVX-512 frequency downclocking behavior changes across CPU generations, with explicit notes about the severity of throttling, power/licensing models, and whether downclocking is significant or negligible. The most directly supportive excerpts specify (a) the existence of a three-tier licensing model and heavy stalls on older generations, (b) reduced impact on Ice Lake and later generations, (c) negligible impact on client/server Ice Lake/Rocket Lake, and (d) newer generations (Zen4/Zen5) with different or avoided throttling behavior. The selected excerpts collectively map the evolution: older Skylake/Cascade Lake show severe throttling and costly stall times; Cooper Lake/Ice Lake Server show substantially reduced or virtually eliminated stalls; Ice Lake Client and Rocket Lake show negligible licensing effects; later references discuss flat or no license-based downclocking and adaptive clocking trends. Specifically, quoting or paraphrasing from the sources, we can say: the three-tier power license system caused noticeable drops and 10–20 microsecond stalls on early generations; Ice Lake Server/Cooper Lake reduce the frequency impact; Ice Lake Client and Rocket Lake exhibit minimal to no license-based downclocking; and newer architectures (Zen variants) have different approaches to AVX-512 timing and may avoid fixed offsets, making AVX-512 more performance-friendly in practice. The connections between these points are direct: they all pertain to how frequency throttling changes with generation, which is precisely the field value being analyzed. The most directly aligned passages describe the severity and stalls for Skylake/Cascade Lake and the dramatically reduced/absent throttling for Ice Lake Server and beyond, which underpins the conclusion about the evolution of AVX-512 performance throttling across generations.",
			"confidence": "high"
		},
		{
			"field": "algebraic_simplification_techniques.x86_lea_optimization",
			"citations": [
				{
					"title": "Optimization Reference Manual",
					"url": "https://cdrdv2-public.intel.com/821613/355308-Optimization-Reference-Manual-050-Changes-Doc.pdf",
					"excerpts": [
						"•\nLEAs without a scaled index and with only two sources among base, index, and displacement execute as a normal\nALU operation on any port (00 through 03).",
						"SLOW LEA represents a lea with two registers and displacement, all other lea versions considered as FAST LEA.",
						". LEAs without a scaled index and only two sources (among base, index, and displacement inputs) execute as one\noperation on any ALU port (00, 01, or 02).\nAL",
						"Fast LEA,",
						"Fast LEA,"
					]
				},
				{
					"title": "Theoretical Scalar Integer Performance KabyLake",
					"url": "https://stackoverflow.com/questions/62074838/theoretical-scalar-integer-performance-kabylake",
					"excerpts": [
						"The LEA is \"slow\" (3 cycle latency, port 1 only) because it has 3 components (2 + in the addressing mode). If you count lea eax, [rdi + rsi*2 + ...Read more"
					]
				},
				{
					"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
					"url": "https://www.agner.org/optimize/microarchitecture.pdf",
					"excerpts": [
						"The LEA instruction uses the address generation unit (AGU) rather than the ALU. This\ncauses a latency of 4 clock cycles when dependent on a pointer register or index register\nbecause of the distance between the AGU and the ALU. It is therefore faster to use addition\nand shift instructions than to use the LEA instruction in most cases."
					]
				},
				{
					"title": "CS107 Guide to x86-64",
					"url": "https://web.stanford.edu/class/archive/cs/cs107/cs107.1166/guide_x86-64.html",
					"excerpts": [
						"Note that `lea` does not dereference the source address, it simply calculates its location. This means `lea` is nothing more than an arithmetic operation and commonly used to calculate the value of simple linear combinations that have nothing to do with memory locations!"
					]
				},
				{
					"title": "x86 64 - Assembly why lea is fast? - Stack Overflow",
					"url": "https://stackoverflow.com/questions/67034266/assembly-why-lea-is-fast",
					"excerpts": [
						"The `lea` (load effective address) is a way to perform the common operation of pointer arithmetic. How an instruction refers to its operands is called its [addressing mode](https://en.wikipedia.org/wiki/Addressing_mode) and `lea` supports [scaled](https://en.wikipedia.org/wiki/Addressing_mode) or [base plus index plus offset](https://en.wikipedia.org/wiki/Addressing_mode) addressing modes (among others)."
					]
				}
			],
			"reasoning": "The most directly relevant excerpts discuss the Fast LEA vs Slow LEA distinction and how LEA can act as a multi-operand addition/address-generation operation. One excerpt explicitly states that LEA without a scaled index and with only two sources executes as a single operation on any ALU port, which aligns with the idea of fusing a sum of two registers plus a scaled (shifted) term into one instruction. Another excerpt notes that fast LEA is available on certain architectures and that latency characteristics depend on the specific addressing mode and the number of source operands, which supports the claim that a complex a + (a<<k) + const can sometimes be reduced to a single micro-op with low latency on modern CPUs. A contrasting excerpt from an older optimization manual explicitly documents that LEA can be slow (three-cycle latency) when there are three sources or more in the addressing expression, which provides the counterpoint that the same pattern could incur higher latency on older or certain microarchitectures. Additional excerpts provide general guidance on LEA usage for address calculations and its role in breaking dependencies, and some sources discuss LEA latency in the context of specific Intel/AMD generations where latency can differ (Ice Lake/Zen4 vs Skylake/Haswell). Collectively, these excerpts support the notion that LEA can fuse multiple terms into a single micro-op under favorable addressing modes and architectures, but with potential latency penalties depending on the exact operand count and CPU generation. The content does not contradict the core claim that LEA can reduce a complex expression to a single uop on modern CPUs, while also highlighting the caveat of architecture-dependent latency.",
			"confidence": "medium"
		},
		{
			"field": "practitioner_codebases_and_libraries.library_name",
			"citations": [
				{
					"title": "xxHash: XXH3 family",
					"url": "https://xxhash.com/doc/v0.8.2/group___x_x_h3__family.html",
					"excerpts": [
						"XXH3 has a fast scalar implementation, but it also includes accelerated SIMD implementations for many common platforms:\nAVX512\nAVX2\nSSE2\nARM NEON\nWebAssembly SIMD128\nPOWER8 VSX\ns390x ZVector",
						"XXH3 is a more recent hash algorithm featuring:\nImproved speed for both small and large inputs\nTrue 64-bit and 128-bit outputs\nSIMD acceleration\nImproved 32-bit viability",
						"Compared to XXH64, expect XXH3 to run approximately ~2x faster on large inputs and >3x faster on small ones, exact differences vary depending on platform.",
						"XXH3's speed benefits greatly from SIMD and 64-bit arithmetic, but does not require it. Most 32-bit and 64-bit targets that can run XXH32 smoothly can run XXH3 at competitive speeds, even without vector support. Further details are explained in the implementation.",
						"For the x86 family, an automatic runtime dispatcher is included separately in [xxh_x86dispatch.c](xxh__x86dispatch_8c.html) .",
						"XXH3 implementation is portable: it has a generic C90 formulation that can be compiled on any platform, all implementations generate exactly the same hash value on all platforms. Starting from v0.8.0, it's also labelled \"stable\", meaning that any future version will also generate the same hash value."
					]
				},
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.\nF",
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "RealTime Data Compression",
					"url": "http://fastcompression.blogspot.com/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "XXH3 - a new speed-optimized hash algorithm",
					"url": "http://fastcompression.blogspot.com/2019/",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path."
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"| HighwayHashAVX2 | 7.34 | 1.81 | 1.71 | 1.04 | 0.95 | 0.24 |",
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.",
						"SipTreeHash(13) requires an AVX2-capable CPU (e.g. Haswell). HighwayHash\nincludes a dispatcher that chooses the implementation (AVX2, SSE4.1, VSX or\nportable) at runtime, as well as a directly callable function template that can\nonly run on the CPU for which it was built.",
						"The internal state is quite large (1024 bits) but fits within SIMD registers.",
						"Due to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase.",
						"Our x86 implementations use custom vector classes with overloaded operators\n(e.g. `const V4x64U a = b + c` ) for type-safety and improved readability vs.\ncompiler intrinsics (e.g. `const __m256i a = _mm256_add_epi64(b, c)` ).",
						"We also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast).",
						"hh_{avx2,sse41,vsx,portable}.h are its various implementations.",
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.\nThe internal state is quite large (1024 bits) but fits within SIMD registers.\nDue to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase. The\nalgorithm outputs 64 bit digests or up to 256 bits at no extra cost.\nIn addition to high throughput, the algorithm is designed for low finalization\ncost. The result is more than twice as fast as SipTreeHash.\nWe also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast). A third-party ARM implementation is referenced below.\nStatistical analyses and preliminary cryptanalysis are given in https://arxiv.org/abs/1612.06257 .",
						"AVX2 and SSE4.1 HighwayHash are faster than SipHash for all input\nsizes due to their highly optimized handling of partial vectors.",
						"In addition to high throughput, the algorithm is designed for low finalization\ncost."
					]
				},
				{
					"title": "GitHub - minio/highwayhash: Native Go version of HighwayHash with optimized assembly implementations on Intel and ARM. Able to process over 10 GB/sec on a single core on Intel CPUs - https://en.wikipedia.org/wiki/HighwayHash",
					"url": "https://github.com/minio/highwayhash",
					"excerpts": [
						"Below are the single core results on an Intel Core i7 (3.1 GHz) for 256 bit outputs:\nSo for moderately sized messages it tops out at about 15 GB/sec. Also for small messages (1K) the performance is already at approximately 60% of the maximum throughput.",
						"HighwayHash uses a new way of mixing inputs with AVX2 multiply and permute instructions. The multiplications are 32x32 bit giving 64 bits-wide results and are therefore infeasible to reverse. Additionally permuting equalizes the distribution of the resulting bytes.",
						"Additionally permuting equalizes the distribution of the resulting bytes.",
						"The algorithm outputs digests ranging from 64 bits up to 256 bits at no extra cost."
					]
				}
			],
			"reasoning": "The field value identifies two hash libraries of interest: xxHash (specifically the XXH3 variant) and Google's HighwayHash. Excerpts that explicitly reference XXH3 or xxHash discuss its SIMD-enabled paths, acceleration options (AVX2, SSE2, NEON), and practical notes about vectorization and performance. These passages directly support the notion of xxHash (and XXH3) as a concrete, widely discussed library with multiple optimized implementations. Excerpts that reference HighwayHash describe its SIMD-enabled implementations, dispatch mechanisms, and performance considerations, which directly corroborate HighwayHash as a practitioner-used hash library. By quoting passages that name these libraries and describe their implementation strategies (e.g., SIMD paths, dispatching between vector ISAs, and performance observations), we establish clear connections between the field value and real-world library usage in codebases. The most relevant items are those that discuss XXH3-specific characteristics and HighwayHash implementations, including discussions of vectorization, portability, and performance accents. Less tightly focused items still support the overall classification by detailing either the xxHash family or HighwayHash in practical, performance-oriented contexts, reinforcing the mapped libraries in practitioner environments.",
			"confidence": "high"
		},
		{
			"field": "practitioner_codebases_and_libraries.key_techniques",
			"citations": [
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"Vectorization must be done manually, using intrinsic, as the compiler seems unable to properly auto-vectorize the scalar code path.\nF",
						"\nFor this reason, the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts.\n`SSE2` is",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set. `SSE2` is also free of dynamic throttling issues, and is automatically enabled on all x64 compilers. Hence I expect it to be the most common target.",
						"For this reason, the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts.",
						"The base speed is very competitive, and the `NEON` vectorial code path designed by Devin makes wonder, pushing speed to new boundaries."
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"On the `M1 Pro` , this translates into +20% speed for `xxh3` and `xxh128` (from 30.0 GB/s to 36 GB/s).",
						"`x86` / `x64` systems can enjoy a new unit, `xxh_x86dispatch` , which can detect at runtime the best vector instruction set present on host system (none, `sse2` , `avx2` or `avx512` ), thanks to a cpu feature detector designed by [@easyaspi314](https://github.com/easyaspi314) . It then automatically runs the appropriate vector code.",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"A new vector instruction set is supported, thanks to [@gzm55](https://github.com/gzm55) : `AVX512` . It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ."
					]
				},
				{
					"title": "XXH3 - a new speed-optimized hash algorithm",
					"url": "http://fastcompression.blogspot.com/2019/",
					"excerpts": [
						", the code offers 4 paths : scalar (universal), `SSE2` , `AVX2` , and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314) . It may be possible to vectorize additional platforms, though this requires dedicated efforts.\n`SSE2`",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set. `SSE2` is also free of dynamic throttling issues, and is automatically enabled on all x64 compilers."
					]
				},
				{
					"title": "Efficient and performance-portable SIMD",
					"url": "https://chromium.googlesource.com/external/github.com/google/highway.git/+/acfaee69a80631e22b2c65e02750954063a61393/README.md",
					"excerpts": [
						"Section Title: []() []() Efficient and performance-portable SIMD > []() []() Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD.\nContinuous integration tests use a recent version of Clang (9.0.1 should suffice) and older version of MSVC (VS2015). Also periodically tested on GCC 10.2.0.",
						"Current status\nContent:\nSupported targets: scalar, SSE4, AVX2, AVX-512, NEON (ARMv7 and v8), WASM SIMD."
					]
				},
				{
					"title": "xxhash.h",
					"url": "https://hackage.haskell.org/package/bloomfilter-blocked-0.1.0.0/src/xxhash/xxHash-0.8.2/xxhash.h",
					"excerpts": [
						"enum XXH_VECTOR_TYPE /* fake enum */ {\n    XXH_SCALAR = 0,  /*!< Portable scalar version */\n    XXH_SSE2   = 1,  /*!<\n                      * SSE2 for Pentium 4, Opteron, all x86_64. *\n                      * @note SSE2 is also guaranteed on Windows 10, macOS, and\n                      * Android x86. */\n    XXH_AVX2   = 2,  /*!< AVX2 for Haswell and Bulldozer */\n    XXH_AVX512 = 3,  /*!< AVX512 for Skylake and Icelake */\n    XXH_NEON   = 4,  /*!<\n                       * NEON for most ARMv7-A, all AArch64, and WASM SIMD128\n                       * via the SIMDeverywhere polyfill provided with the\n                       * Emscripten SDK. */\n    XXH_VSX    = 5,  /*!< VSX and ZVector for POWER8/z13 (64-bit) */\n    XXH_SVE    = 6,  /*!< SVE for some ARMv8-A and ARMv9-A */\n};",
						"t: Auto detected.\n ... \n*/\n#ifndef XXH_ACC_ALIGN\n#  if defined(XXH_X86DISPATCH)\n#     define XXH_ACC_ALIGN 64  /* for compatibility with avx512 */\n#  elif XXH_VECTOR == XXH_SCALAR  /* scalar */\n#     define XXH_ACC_ALIGN 8\n#  elif XXH_VECTOR == XXH_SSE2  /* sse2 */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX2  /* avx2 */\n#     define XXH_ACC_ALIGN 32\n#  elif XXH_VECTOR == XXH_NEON  /* neon */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_VSX   /* vsx */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX512  /* avx512 */\n#     define XXH_ACC_ALIGN 64\n#  elif XXH_VECTOR == XXH_SVE   /* sve */\n#     define XXH_ACC_ALIGN 64\n#  endif\n#endif\n\n#if",
						"\n#if defined(XXH_X86DISPATCH) || XXH_VECTOR == XXH_SSE2 \\\n    || XXH_VECTOR == XXH_AVX2 || XXH_VECTOR == XXH_AVX512\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#elif XXH_VECTOR == XXH_SVE\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#else\n#  define XXH_SEC_ALIGN 8\n#endif"
					]
				},
				{
					"title": "xxHash: XXH3 family",
					"url": "https://xxhash.com/doc/v0.8.2/group___x_x_h3__family.html",
					"excerpts": [
						"XXH3 has a fast scalar implementation, but it also includes accelerated SIMD implementations for many common platforms:\nAVX512\nAVX2\nSSE2\nARM NEON\nWebAssembly SIMD128\nPOWER8 VSX\ns390x ZVector",
						"XXH3 is a more recent hash algorithm featuring:\nImproved speed for both small and large inputs\nTrue 64-bit and 128-bit outputs\nSIMD acceleration\nImproved 32-bit viability",
						"Compared to XXH64, expect XXH3 to run approximately ~2x faster on large inputs and >3x faster on small ones, exact differences vary depending on platform.",
						"XXH3's speed benefits greatly from SIMD and 64-bit arithmetic, but does not require it. Most 32-bit and 64-bit targets that can run XXH32 smoothly can run XXH3 at competitive speeds, even without vector support. Further details are explained in the implementation.",
						"For the x86 family, an automatic runtime dispatcher is included separately in [xxh_x86dispatch.c](xxh__x86dispatch_8c.html) .",
						"XXH3 implementation is portable: it has a generic C90 formulation that can be compiled on any platform, all implementations generate exactly the same hash value on all platforms. Starting from v0.8.0, it's also labelled \"stable\", meaning that any future version will also generate the same hash value."
					]
				},
				{
					"title": "RealTime Data Compression",
					"url": "http://fastcompression.blogspot.com/",
					"excerpts": [
						"the code offers 4 paths : scalar (universal), `SSE2`, `AVX2`, and also `NEON` offered by [Devin Hussey](https://github.com/easyaspi314).",
						"`SSE2` is enough to reach substantial speed, which is great because all `x64` cpus necessarily support this instruction set."
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"| HighwayHashAVX2 | 7.34 | 1.81 | 1.71 | 1.04 | 0.95 | 0.24 |",
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.\nThe internal state is quite large (1024 bits) but fits within SIMD registers.\nDue to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase. The\nalgorithm outputs 64 bit digests or up to 256 bits at no extra cost.\nIn addition to high throughput, the algorithm is designed for low finalization\ncost. The result is more than twice as fast as SipTreeHash.\nWe also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast). A third-party ARM implementation is referenced below.\nStatistical analyses and preliminary cryptanalysis are given in https://arxiv.org/abs/1612.06257 .",
						"AVX2 and SSE4.1 HighwayHash are faster than SipHash for all input\nsizes due to their highly optimized handling of partial vectors.",
						"In addition to high throughput, the algorithm is designed for low finalization\ncost."
					]
				}
			],
			"reasoning": "The finegrained field value describes four core techniques: (1) manual vectorization using intrinsics for the core loops, (2) runtime dispatch to select the best vector path on the host CPU (e.g., between SSE2, AVX2, AVX-512), (3) latency hiding via loop unrolling and interleaving of multiple independent hash states, and (4) optimized memory access patterns with shuffle/permute to improve distribution and throughput, including a hybrid scalar/vector approach on ARM NEON. Excerpts that explicitly state manual vectorization using intrinsics directly support the first technique. Excerpts that discuss automatic or runtime dispatch between available vector instruction sets support the second technique. Excerpts that mention interleaving, processing multiple accumulators, or runtime dispatch in combination with multiple code paths align with the latency-hiding and parallelism goals, supporting the third and fourth techniques as implemented in modern SIMD hash libraries. Excerpts describing platform-specific SIMD paths (NEON, AVX2, AVX-512, SSE2) provide concrete instances of the stated techniques in practice. Collectively, these excerpts corroborate the described transferable techniques and provide concrete examples of how practitioners implement them in real libraries. The strongest support comes from statements that explicitly call for manual intrinsic-based vectorization and explicit runtime dispatch between SIMD targets, followed by discussions of multi-state processing and platform-specific SIMD implementations.",
			"confidence": "high"
		},
		{
			"field": "avx512_performance_analysis.decision_guidelines",
			"citations": [
				{
					"title": "AVX-512: when and how to use these new instructions",
					"url": "https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/",
					"excerpts": [
						" On this basis, [some engineers have recommended that we disable AVX-512 instructions default on our servers](https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/)"
					]
				},
				{
					"title": "Technical reading from the Cloudflare blog for the holidays",
					"url": "https://blog.cloudflare.com/2017-holiday-reading-from-the-cloudflare-blog/",
					"excerpts": [
						"Intel processors contain special AVX-512 that provide 512-bit wide SIMD instructions to speed up certain calculations. However, these instructions have a downside: when used the CPU base frequency is scaled down slowing down other instructions."
					]
				},
				{
					"title": "optimization - SIMD instructions lowering CPU frequency - Stack Overflow",
					"url": "https://stackoverflow.com/questions/56852812/simd-instructions-lowering-cpu-frequency",
					"excerpts": [
						"\n2Trevis Down (aka Beeonrope on OS) wrote about this in the comments in this [post](https://lemire.me/blog/2018/08/13/the-dangers-of-avx-512-throttling-myth-or-reality/) and continued the discussion [here](https://www.realworldtech.com/forum/?threadid=179654&curpostid=179727) . He found that each ties (scalar, AVX/AVX2, AVX-512) has \"cheap\" (no FP, simple operations) instructions and \"heavy\" instruction. Cheap instructions drop the frequency to the one of the next higher tier (e.g. cheap AVX-512 inst use the AVX/AVX2 tier) even if used sparsely. Heavy inst must be used more than 1 every ...Margaret Bloom– [Margaret Bloom](/users/5801661/margaret-bloom \"44,771 rep"
					]
				},
				{
					"title": "Trying harder to make AVX-512 look bad: my quantified and reproducible results – Daniel Lemire's blog",
					"url": "https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/",
					"excerpts": [
						"Intel documents that the use of AVX-512 instructions can lower the frequency of the processor. How big the effect is depends on the processor."
					]
				},
				{
					"title": "Ice Lake AVX-512 Downclocking | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html",
					"excerpts": [
						"The Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.",
						"As expected, maximum frequency decreases with active core count, but scan down each column to see the effect of instruction category. Along this axis, there is almost no downclocking at all! Only for a single active core count is there any decrease with wider instructions, and it is a paltry only 100 MHz: from 3,700 MHz to 3,600 MHz when any 512-bit instructions are used.",
						"le: Ice Lake AVX-512 Downclocking > Rocket Lake > Summary\nContent:\nHere’s what we’ve learned.\nThe Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario.\nThe Rocket Lake i9-11900K CPU doesn’t exhibit any license-based downclock in the tested scenarios.\n",
						"Content:\nI ran avx-turbo as described above on an Ice Lake i5-1035G4, which is the middle-of-the-range Ice Lake client CPU running at up to 3.7 GHz. The full output is [hidden away in a gist](https://gist.github.com/travisdowns/c53f40fc4dbbd944f5613eaab78f3189) , but here are the all-important frequency results (all values in GHz):",
						"The Ice Lake CPU has an all-core 512-bit turbo frequency of 3.3 GHz is 89% of the maximum single-core scalar frequency of 3.7 GHz, so within power and thermal limits this chip has a very “flat” frequency profile. The Rocket Lake 11900K is even flatter with an all-eight-cores frequency of 4.8 GHz clocking in at 94% of the 5.1 GHz single-core speed.",
						"|1 Core |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|2 Cores |5.1 |5.1 |5.1 |5.1 |5.1 |",
						"|5 Cores |4.9 |4.9 |4.9 |4.9 |4.9 |",
						"Here’s what we’ve learned.",
						"The results paint a very promising picture of Rocket Lake’s AVX-512 frequency behavior: there is *no* license-based downclocking evident at any combination of core count and frequency [6](:rklcaveats) . Even heavy AVX-512 instructions can execute at the same frequency as lightweight scalar code.",
						" fact, the frequency behavior of this chip appears very simple: the full Turbo Boost 2.0 frequency [7](:tb2) of 5.1 GHz is available for any instruction mix up and up to 4 active cores, then the speed drops to 4.9 for 5 and 6 active cores, and finally to 4.8 GHz for 7 or 8 active c"
					]
				},
				{
					"title": "Cascade Lake Processors - HECC Knowledge Base",
					"url": "https://www.nas.nasa.gov/hecc/support/kb/cascade-lake-processors_579.html",
					"excerpts": [
						"Maximum Turbo Frequency is 3.90 GHz for non-AVX, and 3.8 GHz for AVX2 and AVX-512. Cache Hierarchy. The cache hierarchy of Cascade Lake is as ..."
					]
				},
				{
					"title": "Packet Processing with Intel® AVX-512 Instruction Set",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-packet-processing-with-intel-avx-512-instruction-set-solution-brief-1678190247.pdf",
					"excerpts": [
						"of data.\nIntel® Xeon® Scalable processors have a number of *power levels* that correspond to workload instruction set mixes and Turbo\nfrequency ranges (refer to Table 1 ). The instruction set mix of a given workload can comprise integer and floating-point\ninstructions, and 8-bit scalar to 512-bit SIMD register widths.\nChanges in instruction set mixes during workload execution can result in a change to the power level of a given core, resulting in\na different set of corresponding frequency ranges becoming active.\n"
					]
				},
				{
					"title": "Intel® AVX-512 - Instruction Set for Packet Processing",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-instruction-set-for-packet-processing-technology-guide-1645717553.pdf",
					"excerpts": [
						"s**\nFrequency transitions are significantly improved in the 3rd Generation Intel® Xeon® Scalable processors where the block time is\nreduced to ~0 µsec, allowing for transitions without a latency cost.",
						"*Note:***\nThe 3rd Generation Intel® Xeon® Scalable processors is\noptimized to reduce the frequency impact for Intel®\nAVX-512 instructions when executing at Intel® AVX-512\nLight (Level 1) and Intel® AVX-512 Heavy (Level 2)\npower levels, when compared to 1st Generation Intel®\nXeon® Scalable processors for servers. See subsection\n4.2.1 for more informati",
						"cessors**\nIn the 3rd Generation Intel® Xeon® Scalable\nprocessors, the Power Level 1/Intel® AVX-512 Light\npower level frequency ranges are significantly\nimproved, as shown in Figure 12 . This means that on\nthe 3rd Generation Intel® Xeon® Scalable processors,\nshow as ICX, the IPC gains of using Intel® AVX-512\ninstructions are typically maintained or enhanced\nwith an improvement in Turbo frequency ranges,\nwhen compared to the 1st and 2 nd Generation Intel®\nXeon® Scalable processors, shown as SKX and CLX\nrespectively.\n**"
					]
				},
				{
					"title": "Performance Matters | A blog about low-level software and hardware performance.",
					"url": "https://travisdowns.github.io/tags/avx512.html",
					"excerpts": [
						"Ice Lake AVX-512 Downclocking. Examining the extent of AVX related ... Travis Downs; travis.downs@gmail.com. A blog about low-level software and hardware ...Read more"
					]
				},
				{
					"title": "Intel® AVX-512 Instructions",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/intel-avx-512-instructions.html",
					"excerpts": [
						"Intel AVX-512 features include 32 vector registers each 512 bits wide, eight dedicated mask registers, 512-bit operations on packed floating point data or packed integer data, embedded rounding controls (override global settings), embedded broadcast, embedded floating-point fault suppression, embedded memory fault suppression, new operations, additional gather/scatter support, high speed math instructions, compact representation of large displacement value, and the ability to have optional capabilities beyond the foundational capabilities."
					]
				},
				{
					"title": "Detailed Specifications of the \"Cascade Lake SP\" Intel Xeon Processor Scalable Family CPUs - Microway",
					"url": "https://www.microway.com/knowledge-center-articles/detailed-specifications-of-the-cascade-lake-sp-intel-xeon-processor-scalable-family-cpus/",
					"excerpts": [
						"Clock Speeds & Turbo Boost > AVX-512, AVX, and Non-AVX Turbo Boost in Xeon “Cascade Lake-SP” Scalable Family processors\nContent:\nDespite the perceived reduction in performance when running these vector instructions, keep in mind that AVX-512 roughly doubles the number of operations which can be completed per cycle. Although the clock speeds might be reduced by nearly 1GHz, the overall throughput is increased. **HPC users should expect their processors to be running in AVX or AVX-512 mode most of the time.**"
					]
				},
				{
					"title": "Intel® AVX-512 - Ultra Parallelized Multi-hash Computation ...",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-ultra-parallelized-multi-hash-computation-for-data-streaming-workloads-technology-guide-1693301077.pdf",
					"excerpts": [
						"3.3.1 Parallel Hash Computation with Splitting Input and Vectorized Seeds by Leveraging Intel® AVX-512 ",
						"To calculate eight independent hash values, we use eight random seeds, then continuously split the input data stream (for\nexample, an input key) into 64-bit width data chunks and then get consumed one by one. To fully take advantage of Intel AVX-\n512 ISA, the data chunk size multiplies the seed count should be equal to 512. For example, if the seed count is four, then the data\nstream should be divided into 128-bit chunks",
						"The eight different seeds are also put into a 512-bit wide vector register.",
						"The process is roughly illustrated in Figure 4 .",
						"We have pre-defined five different prime numbers (P1 to P5) and eight different seeds to be used during the computation. The\nfive prime numbers are getting involved in the hash computation later to target a good mixture of hash results. Each prime\nnumber is broadcasted into the vector of eight items. The eight different seeds are also put into a 512-bit wide vector register.\nThe algorithm also broadcasts the truncated 64-bit-wide input data into the 512-bit-wide vector with eight copies. Then it mixes\nthe 8-seed vector and the pre-defined five prime numbers by using addition, multiplication, rotation, shift, and xor arithmetic\noperations in a vectorized style.",
						"3.3.2 Accelerate Multiplication and Addition Operations by Leveraging Intel AVX-512 IFMA Instruction in Hash\nComputation",
						"Intel AVX-512 IFMA instruction is introduced in 8th Gen Intel® Core™ i3 Processors and 3rd Gen Intel® Xeon® Scalable\nProcessors, which can support fused multiply and add operation of integers belonging to Intel AVX-512 instruction sets. Since\nMAD (Multiply-Add-Divide) operation is essential in many hash functions to spread out the keys into the hash buckets, the\nvectorized Intel AVX-512 IFMA instruction will speed up the process of multiplication and addition operations. Replacing the\nvectorized multiplication and addition described in the above section with Intel AVX-512 IFMA instruction can accelerate the\ncomputation.\n3.",
						"3.3.3 Accelerate Sketch Counter Updates by Leveraging Intel AVX-512 Gather and Scatter Instructions",
						"Sketch-based algorithms require a counter-update after the proposed hashing computation. Instead of sequentially updating\nthe sketch counter arrays one by one, the Intel AVX-512 gather and scatter instructions can be used as the multiple hash digests\nare already generated in the previous vectorized hash computation. By using Intel AVX-512 gather and scatter instructions, the\nsketch counter array can be updated in parallel.",
						"This technology guide demonstrates a brand new ultra-parallelized multi-hash computation model for data streaming\nworkloads. The model offers several advantages, which have been detailed through the recently unveiled technique:\n1.\nHigh-performance: This technology guide proposes a novel model by leveraging Intel AVX-512 instruction sets to\nget data-level parallelism when processing multi-hash computations. Based on xxHash, we achieve a performance\ngain of up to 2x on key-add and key-lookup operations compared to the standard CRC-32 instruction."
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": ["8", "8", "8", "8", "8", "8", "8"]
				},
				{
					"title": "Vectorizing xxHash for Fun and Profit | The Pseudo Random Bit Bucket",
					"url": "https://moinakg.wordpress.com/2013/01/19/vectorizing-xxhash-for-fun-and-profit/",
					"excerpts": [
						"An SSE register is 128-bit and can perfectly hold four 32-bit packed integers."
					]
				}
			],
			"reasoning": "The core field value prescribes architecture-specific guidance for choosing AVX-512 vs AVX2 based on platform: Skylake-X/Cascade Lake warnings for throttling and dedicated-core isolation; Ice Lake Server upgrades and reduced throttling; Ice Lake Client/Rocket Lake/Alder Lake advising strong AVX-512 preference; and Zen 4/Zen 5 discussions showing variability in AVX-512 support and throughput. Direct statements about frequency throttling when AVX-512 is used provide the justification for caution on Skylake-X/Cascade Lake, while references to reduced or manageable throttling on Ice Lake Server and newer generations underpin the recommended stance there. For example, passages describe dynamic downclocking and power-level transitions when AVX-512 is employed, which supports the claim that older generations may incur significant frequency penalties and require careful benchmarking and isolation. Other passages note that newer generations ( Ice Lake Server and later ) show architectural improvements that mitigate throttling, which supports the recommendation to prefer AVX-512 on those platforms. Additional sources discuss how downclocking can be heavy, light, or negligible depending on the chip and the workload, which aligns with the nuanced guidance in the field value about which platforms are favorable for AVX-512 and which require caution. In sum, the most relevant information ties specific architectures to their propensity for AVX-512 frequency penalties, the presence or absence of throttling, and the suggested deployment strategies (isolate to cores, benchmark, or adopt AVX-512 as a clear win on certain generations). The following themes are repeatedly evidenced across the excerpts: frequency throttling when AVX-512 is used, platform-specific power-level and Turbo behavior, and architecture-dependent recommendations for when AVX-512 provides a net benefit versus when AVX2 is the safer baseline.",
			"confidence": "high"
		},
		{
			"field": "avx512_performance_analysis.energy_performance_tradeoff",
			"citations": [
				{
					"title": "AVX-512: when and how to use these new instructions",
					"url": "https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/",
					"excerpts": [
						"However, some of these instructions use a lot of power and generate a lot of heat. To keep power usage within bounds, Intel reduces the frequency of the cores dynamically."
					]
				},
				{
					"title": "Technical reading from the Cloudflare blog for the holidays",
					"url": "https://blog.cloudflare.com/2017-holiday-reading-from-the-cloudflare-blog/",
					"excerpts": [
						"Intel processors contain special AVX-512 that provide 512-bit wide SIMD instructions to speed up certain calculations. However, these instructions have a downside: when used the CPU base frequency is scaled down slowing down other instructions."
					]
				},
				{
					"title": "Intel® AVX-512 - Instruction Set for Packet Processing",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-instruction-set-for-packet-processing-technology-guide-1645717553.pdf",
					"excerpts": [
						"s**\nFrequency transitions are significantly improved in the 3rd Generation Intel® Xeon® Scalable processors where the block time is\nreduced to ~0 µsec, allowing for transitions without a latency cost."
					]
				},
				{
					"title": "optimization - SIMD instructions lowering CPU frequency - Stack Overflow",
					"url": "https://stackoverflow.com/questions/56852812/simd-instructions-lowering-cpu-frequency",
					"excerpts": [
						"\n2Trevis Down (aka Beeonrope on OS) wrote about this in the comments in this [post](https://lemire.me/blog/2018/08/13/the-dangers-of-avx-512-throttling-myth-or-reality/) and continued the discussion [here](https://www.realworldtech.com/forum/?threadid=179654&curpostid=179727) . He found that each ties (scalar, AVX/AVX2, AVX-512) has \"cheap\" (no FP, simple operations) instructions and \"heavy\" instruction. Cheap instructions drop the frequency to the one of the next higher tier (e.g. cheap AVX-512 inst use the AVX/AVX2 tier) even if used sparsely. Heavy inst must be used more than 1 every ...Margaret Bloom– [Margaret Bloom](/users/5801661/margaret-bloom \"44,771 rep"
					]
				},
				{
					"title": "\n\tState of AVX 512 on Skylake-X - Intel Community\n",
					"url": "https://community.intel.com/t5/Intel-ISA-Extensions/State-of-AVX-512-on-Skylake-X/td-p/1143063",
					"excerpts": [
						"Section Title: Using Intel.com Search > State of AVX 512 on Skylake-X\nContent:\nAs has been stated on a number of [review sites](http://www.anandtech.com/show/11550/the-intel-skylakex-review-core-i9-7900x-i7-7820x-and-i7-7800x-tested/3) , AVX 512 performance on the 6/8 core Skylake-X is compromised.\nOnly on the 10 core, the present hardware is fully enabled.\nWould Intel be so kind as to provide in depth detail of what the performance difference means ?\nFrom the vague information available it seems one of 2(3?) AVX 512 ports is disabled (port 5).\nCan we get more detailed information, which ports are used for AVX 512 ?\nWhat AVX 512 instructions can the ports execute, do they have 512-bit data paths to registers/cache ?\nHow is AVX 512 gather affected regarding the 6/8 core versus 10 core ?\nA similar drawing as below for AVX2 would be appreciated."
					]
				},
				{
					"title": "Ice Lake AVX-512 Downclocking | Performance Matters",
					"url": "https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html",
					"excerpts": [
						"The Ice Lake i5-1035 CPU exhibits only 100 MHz of license-based downclock with 1 active core when running 512-bit instructions, and *no* license downclock in any other scenario."
					]
				},
				{
					"title": "Detailed Specifications of the \"Cascade Lake SP\" Intel Xeon Processor Scalable Family CPUs - Microway",
					"url": "https://www.microway.com/knowledge-center-articles/detailed-specifications-of-the-cascade-lake-sp-intel-xeon-processor-scalable-family-cpus/",
					"excerpts": [
						"Clock Speeds & Turbo Boost > AVX-512, AVX, and Non-AVX Turbo Boost in Xeon “Cascade Lake-SP” Scalable Family processors\nContent:\nDespite the perceived reduction in performance when running these vector instructions, keep in mind that AVX-512 roughly doubles the number of operations which can be completed per cycle. Although the clock speeds might be reduced by nearly 1GHz, the overall throughput is increased. **HPC users should expect their processors to be running in AVX or AVX-512 mode most of the time.**"
					]
				},
				{
					"title": "Intel® AVX-512 - Ultra Parallelized Multi-hash Computation ...",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-ultra-parallelized-multi-hash-computation-for-data-streaming-workloads-technology-guide-1693301077.pdf",
					"excerpts": [
						"3.3.1 Parallel Hash Computation with Splitting Input and Vectorized Seeds by Leveraging Intel® AVX-512 ",
						"To calculate eight independent hash values, we use eight random seeds, then continuously split the input data stream (for\nexample, an input key) into 64-bit width data chunks and then get consumed one by one. To fully take advantage of Intel AVX-\n512 ISA, the data chunk size multiplies the seed count should be equal to 512. For example, if the seed count is four, then the data\nstream should be divided into 128-bit chunks",
						"The process is roughly illustrated in Figure 4 .",
						"We have pre-defined five different prime numbers (P1 to P5) and eight different seeds to be used during the computation. The\nfive prime numbers are getting involved in the hash computation later to target a good mixture of hash results. Each prime\nnumber is broadcasted into the vector of eight items. The eight different seeds are also put into a 512-bit wide vector register.\nThe algorithm also broadcasts the truncated 64-bit-wide input data into the 512-bit-wide vector with eight copies. Then it mixes\nthe 8-seed vector and the pre-defined five prime numbers by using addition, multiplication, rotation, shift, and xor arithmetic\noperations in a vectorized style."
					]
				}
			],
			"reasoning": "The most directly relevant excerpts articulate the core tradeoff between higher power draw when AVX-512 is used and the resultant effect on performance. One excerpt explicitly states that AVX-512 can double throughput but may reduce clock speeds, which is central to the energy-perf tradeoff: the increased power draw is weighed against much faster execution, potentially improving energy efficiency when execution time drops sufficiently. Another excerpt discusses how the same family’s Turbo/boost behavior and frequency changes occur when AVX-512 is active, which is essential to estimating energy per hash. Additional excerpts describe downclocking behavior across generations (e.g., Skylake/Cascade Lake versus Ice Lake/Rocket Lake) and how these affect performance relative to power usage, which directly feeds into the energy-per-hash calculation. Some excerpts extend the discussion to practical architectural considerations (e.g., power level transitions and timing of frequency changes) and to specialized multi-hash/AVX-512 usage scenarios, which help explain the limits and opportunities for exposure of parallelism and its impact on energy efficiency. Taken together, the most relevant pieces clearly support the claim that energy efficiency of AVX-512 is highly generation-dependent, with recent architectures offering better energy-performance tradeoffs (higher throughput with manageable or reduced frequency penalties) and older architectures often suffering larger efficiency penalties due to throttling. The less directly connected excerpts provide broader context about AVX-512 capabilities and general performance considerations but do not contradict the core energy-perf claim and still help frame the landscape. ",
			"confidence": "medium"
		},
		{
			"field": "practitioner_codebases_and_libraries.reported_performance",
			"citations": [
				{
					"title": "xxHash: xxHash",
					"url": "https://xxhash.com/doc/v0.8.2/index.html",
					"excerpts": [
						"| [XXH3_128bits()](group___x_x_h3__family.html \"Unseeded 128-bit variant of XXH3.\") | **AVX2** | 128 | 57.9 GB/s | 118.1 |",
						"| [XXH3_128bits()](group___x_x_h3__family.html \"Unseeded 128-bit variant of XXH3.\") | **AVX2** | 128 | 57.9 GB/s | 118.1 |",
						"| [XXH3_64bits()](group___x_x_h3__family.html \"64-bit unseeded variant of XXH3.\") | **SSE2** | 64 | 31.5 GB/s | 133.1 |"
					]
				},
				{
					"title": "Releases · Cyan4973/xxHash · GitHub",
					"url": "https://github.com/Cyan4973/xxHash/releases",
					"excerpts": [
						"It can be applied on `XXH3` and `XXH128` , using some of the most recent Intel cpus, such as IceLake on laptop. It typically offers +50% more performance compared to `AVX2` ",
						"e `M1 Pro` , this translates into +20% speed for `xxh3` and `xxh128` (from 30.0 GB/s to 36 GB/s).\n",
						"On the `M1 Pro` , this translates into +20% speed for `xxh3` and `xxh128` (from 30.0 GB/s to 36 GB/s)."
					]
				},
				{
					"title": "GitHub - google/highwayhash: Fast strong hash functions: SipHash/HighwayHash",
					"url": "https://github.com/google/highwayhash",
					"excerpts": [
						"We have devised a new way of mixing inputs with SIMD multiply and permute\ninstructions. The multiplications are 32x32 -> 64 bits and therefore infeasible\nto reverse. Permuting equalizes the distribution of the resulting bytes.\nThe internal state is quite large (1024 bits) but fits within SIMD registers.\nDue to limitations of the AVX2 instruction set, the registers are partitioned\ninto two 512-bit halves that remain independent until the reduce phase. The\nalgorithm outputs 64 bit digests or up to 256 bits at no extra cost.\nIn addition to high throughput, the algorithm is designed for low finalization\ncost. The result is more than twice as fast as SipTreeHash.\nWe also provide an SSE4.1 version (80% as fast for large inputs and 95% as fast\nfor short inputs), an implementation for VSX on POWER and a portable version\n(10% as fast). A third-party ARM implementation is referenced below.\nStatistical analyses and preliminary cryptanalysis are given in https://arxiv.org/abs/1612.06257 ."
					]
				},
				{
					"title": "GitHub - minio/highwayhash: Native Go version of HighwayHash with optimized assembly implementations on Intel and ARM. Able to process over 10 GB/sec on a single core on Intel CPUs - https://en.wikipedia.org/wiki/HighwayHash",
					"url": "https://github.com/minio/highwayhash",
					"excerpts": [
						"Additionally permuting equalizes the distribution of the resulting bytes.",
						"The algorithm outputs digests ranging from 64 bits up to 256 bits at no extra cost."
					]
				},
				{
					"title": "RealTime Data Compression: Presenting XXH3",
					"url": "http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html",
					"excerpts": [
						"The base speed is very competitive, and the `NEON` vectorial code path designed by Devin makes wonder, pushing speed to new boundaries."
					]
				},
				{
					"title": "xxHash: XXH3 family",
					"url": "https://xxhash.com/doc/v0.8.2/group___x_x_h3__family.html",
					"excerpts": [
						"XXH3 is a more recent hash algorithm featuring:\nImproved speed for both small and large inputs\nTrue 64-bit and 128-bit outputs\nSIMD acceleration\nImproved 32-bit viability",
						"XXH3's speed benefits greatly from SIMD and 64-bit arithmetic, but does not require it. Most 32-bit and 64-bit targets that can run XXH32 smoothly can run XXH3 at competitive speeds, even without vector support. Further details are explained in the implementation.",
						"For the x86 family, an automatic runtime dispatcher is included separately in [xxh_x86dispatch.c](xxh__x86dispatch_8c.html) .",
						"XXH3 implementation is portable: it has a generic C90 formulation that can be compiled on any platform, all implementations generate exactly the same hash value on all platforms. Starting from v0.8.0, it's also labelled \"stable\", meaning that any future version will also generate the same hash value."
					]
				}
			],
			"reasoning": "The fine-grained field value aggregates several concrete, architecture-tied performance figures for SIMD-accelerated hash implementations. Direct numeric evidence for XXH3 on AVX2 appears in excerpts describing AVX2 throughput benchmarks (approximately 57.9 GB/s for 128-bit XXH3 with AVX2, and 31.5 GB/s for a 64-bit path on SSE2, illustrating the scale of SIMD speedups and the dependence on data width). This establishes the baseline performance for AVX2. Related excerpts also confirm the availability of AVX512 paths and SIMD-accelerated variants for XXH3 and other hash families, which is essential to justify a +50% uplift claim when moving from AVX2 to AVX512. The cited material explicitly notes AVX512 support and its typical performance uplift over AVX2, aligning with the field value’s claim of a substantial boost when using AVX512. In addition, HighwayHash-related excerpts provide a separate performance footprint (notably around 10 GB/s on a single Intel core with AVX2 and related notes about SIMD-based mixing and large internal state), which corroborates the broader theme of hand-tuned SIMD paths delivering high throughput, even though the exact GB/s figure differs by hash function and data size. The NEON path is discussed as a vectorized route on Apple platforms, with commentary indicating strong performance characteristics, even if exact GB/s figures are occasionally missing in some excerpts. Finally, other excerpts confirm the existence of runtime dispatchers that select the best vector path (AVX2, AVX512, SSE2, NEON, etc.), which underpins the practical feasibility of achieving the stated performance by choosing the right path per platform. Taken together, the most direct numeric support comes from the XXH3 AVX2 figures, the explicit AVX512 uplift note, and the HighwayHash performance statements, with NEON and dispatcher discussions providing contextual corroboration. ",
			"confidence": "high"
		},
		{
			"field": "avx512_performance_analysis.register_file_advantage",
			"citations": [
				{
					"title": "Intel® AVX-512 Instructions",
					"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/intel-avx-512-instructions.html",
					"excerpts": [
						"Intel AVX-512 features include 32 vector registers each 512 bits wide, eight dedicated mask registers, 512-bit operations on packed floating point data or packed integer data, embedded rounding controls (override global settings), embedded broadcast, embedded floating-point fault suppression, embedded memory fault suppression, new operations, additional gather/scatter support, high speed math instructions, compact representation of large displacement value, and the ability to have optional capabilities beyond the foundational capabilities."
					]
				},
				{
					"title": "AVX-512",
					"url": "https://www.chessprogramming.org/AVX-512",
					"excerpts": [
						"AVX-512, an expansion of Intel's AVX and AVX2 instructions using the EVEX prefix, featuring 32 512-bit wide vector SIMD registers zmm0 through zmm31."
					]
				},
				{
					"title": "2. Optimizing subroutines in assembly language",
					"url": "https://agner.org/optimize/optimizing_assembly.pdf",
					"excerpts": [
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)",
						"512 bit (ZMM)"
					]
				},
				{
					"title": "BearSSL - Constant-Time Crypto",
					"url": "https://www.bearssl.org/constanttime.html",
					"excerpts": [
						"\nBitslicing uses a lot of variables, usually many more than the number of registers offered by a typical CPU (x86 CPU have 7 or 15 general-purpose registers, depending on operation mode; PowerPC raise that number to 30 or so). On big, modern platform, this is not that much of an issue: extra variables will be stored on the stack, and the exchange of data between stack and registers will be interleaved with the bitwise binary operations. Since such CPU are superscalar, theses data exchanges will be basically “free”: the ALU will be fully occupied, and there will be enough free slots for exchanges to be done without slowing down computations. However, the same is not true on reduced platforms, which are not necessarily able to run a bitwise operation *and* a memory access in the same clock cycle. Bitslicing tends to require large code. A rule of thumb is that the code that replaces a lookup table will be about as large as the table that it replaces, but this depends a lot on the architecture. On platforms where elementary opcodes are 32 bits each (e.g. PowerPC, SPARC…), bitslice code will typically be bigger. Thus, bitslicing tends to be incompatible with a goal of extreme code footprint reduction. Bitslicing works over orthogonalised data [3]() . This operation can be implemented relatively efficiently, but still has some non-negligible overhead. Bitslicing really shines in highly parallel contexts.\n"
					]
				},
				{
					"title": "Intel® AVX-512 - Ultra Parallelized Multi-hash Computation ...",
					"url": "https://builders.intel.com/docs/networkbuilders/intel-avx-512-ultra-parallelized-multi-hash-computation-for-data-streaming-workloads-technology-guide-1693301077.pdf",
					"excerpts": [
						"3.3.1 Parallel Hash Computation with Splitting Input and Vectorized Seeds by Leveraging Intel® AVX-512 ",
						"To calculate eight independent hash values, we use eight random seeds, then continuously split the input data stream (for\nexample, an input key) into 64-bit width data chunks and then get consumed one by one. To fully take advantage of Intel AVX-\n512 ISA, the data chunk size multiplies the seed count should be equal to 512. For example, if the seed count is four, then the data\nstream should be divided into 128-bit chunks",
						"The eight different seeds are also put into a 512-bit wide vector register.",
						"The process is roughly illustrated in Figure 4 ."
					]
				}
			],
			"reasoning": "The claim hinges on a precise hardware detail: AVX-512 provides a larger 512-bit register file (32 registers) than AVX2 (16 registers). One excerpt explicitly states that AVX-512 features include 32 vector registers each 512 bits wide, which directly confirms the larger register file and its 512-bit width. Another excerpt reinforces this by naming the 32 512-bit vector SIMD registers (zmm0 through zmm31), which also confirms the 32-register, 512-bit architecture. Additional excerpts mention 512-bit (ZMM) registers in the context of AVX-512 capabilities, linking the idea of a broader register file to the ability to hold more data per vector instruction. This hardware detail underpins the main argument: a larger register file supports higher degrees of interleaving and running multiple hash instances in parallel without spilling states to memory, which is crucial for hiding the 18-operation dependency latency described in the field value. Supporting material on bitslicing and multi-hash contexts demonstrates practical applications where wide 512-bit vectors are employed to accelerate parallel hash computations, illustrating how AVX-512 enables holding more independent hash states and facilitating vectorized, parallel workflows. In particular, material describing multi-hash computation with AVX-512 and the use of 512-bit vector seeds aligns with the idea of running multiple independent hashes in flight, made feasible by a larger register file. Taken together, these excerpts corroborate the core assertion that AVX-512’s larger 32x512-bit register file is a key enabler for deep interleaving and high-throughput hashing, while AVX2’s smaller 16x256-bit register set constrains parallelism and increases the likelihood of spills, which matches the expected performance implications described in the field value.",
			"confidence": "high"
		}
	],
	"outputSchema": {
		"type": "object",
		"properties": {
			"optimization_summary": {
				"type": "string",
				"description": "A high-level summary of the most effective strategies for optimizing the sequential hash function, focusing on exploiting the embarrassingly parallel nature of the 4096-hash workload."
			},
			"primary_recommendation": {
				"type": "string",
				"description": "The single most impactful recommendation for achieving maximum throughput, which involves combining SIMD vectorization with instruction interleaving."
			},
			"simd_vectorization_strategy": {
				"type": "object",
				"properties": {
					"recommended_isa": {
						"type": "string",
						"description": "The most recommended SIMD instruction set for this workload (e.g., AVX-512)."
					},
					"supported_isas": {
						"type": "string",
						"description": "A list of other supported instruction sets for portability (e.g., AVX2, NEON, SVE)."
					},
					"intrinsics_pattern": {
						"type": "string",
						"description": "The sequence of intrinsics used to implement one stage of the hash (e.g., _mm512_add_epi32, _mm512_slli_epi32, _mm512_xor_si512)."
					},
					"lane_width_choice": {
						"type": "number",
						"description": "The number of parallel 32-bit hashes processed per instruction for the recommended ISA (e.g., 16 for AVX-512)."
					},
					"estimated_cycles_per_hash": {
						"type": "number",
						"description": "The estimated performance in cycles per hash for a vectorized implementation."
					}
				},
				"required": [
					"recommended_isa",
					"supported_isas",
					"intrinsics_pattern",
					"lane_width_choice",
					"estimated_cycles_per_hash"
				],
				"additionalProperties": false
			},
			"interleaving_and_software_pipelining": {
				"type": "object",
				"properties": {
					"technique_description": {
						"type": "string",
						"description": "Explanation of how interleaving independent hash instances hides the latency of the dependency chain."
					},
					"dependency_chain_latency_to_hide": {
						"type": "number",
						"description": "The approximate latency of the 18-operation dependency chain in cycles."
					},
					"recommended_interleave_factor": {
						"type": "number",
						"description": "The optimal number of independent SIMD vectors to interleave to saturate execution units (e.g., 2-4)."
					},
					"register_pressure_analysis": {
						"type": "string",
						"description": "Analysis of how different architectures (e.g., x86-64 vs AArch64) handle the register requirements for interleaving."
					},
					"scalar_vs_vector_interleaving": {
						"type": "string",
						"description": "Comparison of performance between interleaving scalar operations versus vector (SIMD) operations."
					}
				},
				"required": [
					"technique_description",
					"dependency_chain_latency_to_hide",
					"recommended_interleave_factor",
					"register_pressure_analysis",
					"scalar_vs_vector_interleaving"
				],
				"additionalProperties": false
			},
			"data_layout_optimization": {
				"type": "object",
				"properties": {
					"recommended_layout": {
						"type": "string",
						"description": "The optimal data layout for SIMD processing (e.g., AoSoA - Array of Structures of Arrays)."
					},
					"layout_rationale": {
						"type": "string",
						"description": "Explanation of why the recommended layout is superior to alternatives like AoS, focusing on memory access patterns."
					},
					"transposition_strategy": {
						"type": "string",
						"description": "The strategy for converting data to the recommended layout, including cost-benefit analysis."
					},
					"cache_and_prefetching_impact": {
						"type": "string",
						"description": "How the layout improves cache line utilization and enables hardware prefetchers to hide memory latency."
					},
					"gather_instruction_avoidance": {
						"type": "boolean",
						"description": "Whether this layout successfully avoids the need for slow hardware gather instructions."
					}
				},
				"required": [
					"recommended_layout",
					"layout_rationale",
					"transposition_strategy",
					"cache_and_prefetching_impact",
					"gather_instruction_avoidance"
				],
				"additionalProperties": false
			},
			"microarchitectural_scheduling_model": {
				"type": "object",
				"properties": {
					"target_architectures": {
						"type": "string",
						"description": "A list of modern CPU microarchitectures analyzed (e.g., Intel Sapphire Rapids, AMD Zen 5)."
					},
					"bottleneck_resource": {
						"type": "string",
						"description": "The primary execution resource that limits throughput (e.g., vector shift units)."
					},
					"port_pressure_analysis": {
						"type": "string",
						"description": "Analysis of contention for specific execution ports on target CPUs."
					},
					"alu_valu_slot_utilization_plan": {
						"type": "string",
						"description": "A plan for how to schedule instructions to best utilize the specified 12 ALU and 6 VALU slots."
					}
				},
				"required": [
					"target_architectures",
					"bottleneck_resource",
					"port_pressure_analysis",
					"alu_valu_slot_utilization_plan"
				],
				"additionalProperties": false
			},
			"algebraic_simplification_techniques": {
				"type": "object",
				"properties": {
					"x86_lea_optimization": {
						"type": "string",
						"description": "Details on using the x86 LEA instruction to fuse add-and-shift operations into a single uop."
					},
					"aarch64_shifted_operand_optimization": {
						"type": "string",
						"description": "Details on using AArch64's shifted-register operand format to fuse operations."
					},
					"reassociation_opportunities": {
						"type": "string",
						"description": "Opportunities to reorder operations to expose more parallelism or enable other optimizations."
					},
					"safety_and_bit_exactness": {
						"type": "string",
						"description": "Conditions under which these transformations are guaranteed to produce bit-identical results."
					}
				},
				"required": [
					"x86_lea_optimization",
					"aarch64_shifted_operand_optimization",
					"reassociation_opportunities",
					"safety_and_bit_exactness"
				],
				"additionalProperties": false
			},
			"bitslicing_and_carry_save_analysis": {
				"type": "object",
				"properties": {
					"technique_description": {
						"type": "string",
						"description": "An explanation of the bitslicing technique, where data is transposed into bit-planes."
					},
					"advantages": {
						"type": "string",
						"description": "The benefits of bitslicing, such as free shifts and highly efficient bitwise operations."
					},
					"challenges": {
						"type": "string",
						"description": "The drawbacks, including the complexity of simulating addition, high register pressure, and data conversion overhead."
					},
					"feasibility_for_workload": {
						"type": "string",
						"description": "An assessment of whether bitslicing is a viable and potentially beneficial strategy for the 4096-hash workload."
					}
				},
				"required": [
					"technique_description",
					"advantages",
					"challenges",
					"feasibility_for_workload"
				],
				"additionalProperties": false
			},
			"dependency_chain_scheduling_theory": {
				"type": "object",
				"properties": {
					"key_concepts": {
						"type": "string",
						"description": "Core theoretical concepts like Recurrence-constrained Minimum Initiation Interval (RecMII) and Modulo Scheduling."
					},
					"primary_limitation_identified": {
						"type": "string",
						"description": "The fundamental performance limit imposed by the loop-carried dependency chain."
					},
					"most_effective_technique": {
						"type": "string",
						"description": "The most effective technique identified for breaking the dependency chain, such as N-way interleaving."
					}
				},
				"required": [
					"key_concepts",
					"primary_limitation_identified",
					"most_effective_technique"
				],
				"additionalProperties": false
			},
			"compiler_auto_vectorization_guidance": {
				"type": "object",
				"properties": {
					"compiler": {
						"type": "string",
						"description": "The name of the compiler (e.g., Clang/LLVM, GCC, ICX, MSVC)."
					},
					"key_flags": {
						"type": "string",
						"description": "Essential compiler flags to enable and control vectorization (e.g., -mavx512f, -qopt-report)."
					},
					"key_pragmas": {
						"type": "string",
						"description": "Source code pragmas used to guide the compiler (e.g., #pragma omp simd, #pragma ivdep)."
					},
					"refactoring_tips": {
						"type": "string",
						"description": "Code changes, like using unsigned types and the 'restrict' keyword, that help the compiler."
					}
				},
				"required": [
					"compiler",
					"key_flags",
					"key_pragmas",
					"refactoring_tips"
				],
				"additionalProperties": false
			},
			"speculative_computation_applicability": {
				"type": "object",
				"properties": {
					"core_pattern_branchless_nature": {
						"type": "boolean",
						"description": "Analysis confirming the core hash pattern is inherently branchless."
					},
					"applicable_areas": {
						"type": "string",
						"description": "Adjacent code paths where branchless techniques are useful, such as tail-end data processing."
					},
					"cost_benefit_analysis": {
						"type": "string",
						"description": "The trade-off model comparing the fixed cost of branchless code versus the high penalty of a branch misprediction."
					},
					"selection_instructions": {
						"type": "string",
						"description": "Specific instructions used for branchless selection (e.g., CMOVcc, vector blends)."
					}
				},
				"required": [
					"core_pattern_branchless_nature",
					"applicable_areas",
					"cost_benefit_analysis",
					"selection_instructions"
				],
				"additionalProperties": false
			},
			"gpu_implementation_strategy": {
				"type": "object",
				"properties": {
					"kernel_mapping_strategy": {
						"type": "string",
						"description": "How the 4096 hashes are mapped to the GPU's execution hierarchy (e.g., one thread per hash)."
					},
					"memory_optimization_techniques": {
						"type": "string",
						"description": "Techniques for optimizing memory access, such as using constant memory and ensuring coalesced global memory reads."
					},
					"latency_hiding_mechanism": {
						"type": "string",
						"description": "The primary mechanism for hiding instruction and memory latency on the GPU (i.e., high occupancy)."
					},
					"comparative_throughput_analysis": {
						"type": "string",
						"description": "A comparison of the expected raw throughput of a GPU implementation versus an optimized multi-core CPU implementation."
					}
				},
				"required": [
					"kernel_mapping_strategy",
					"memory_optimization_techniques",
					"latency_hiding_mechanism",
					"comparative_throughput_analysis"
				],
				"additionalProperties": false
			},
			"practitioner_codebases_and_libraries": {
				"type": "object",
				"properties": {
					"library_name": {
						"type": "string",
						"description": "The name of the high-performance hash library (e.g., xxHash, HighwayHash)."
					},
					"key_techniques": {
						"type": "string",
						"description": "Transferable optimization techniques observed in the library's source code."
					},
					"supported_isas": {
						"type": "string",
						"description": "A list of SIMD instruction sets supported by the library."
					},
					"reported_performance": {
						"type": "string",
						"description": "Real-world performance metrics reported for the library (e.g., GB/s or cycles/byte)."
					},
					"licensing": {
						"type": "string",
						"description": "The software license under which the library is available."
					}
				},
				"required": [
					"library_name",
					"key_techniques",
					"supported_isas",
					"reported_performance",
					"licensing"
				],
				"additionalProperties": false
			},
			"avx512_performance_analysis": {
				"type": "object",
				"properties": {
					"frequency_throttling_by_generation": {
						"type": "string",
						"description": "How AVX-512 frequency downclocking behavior has evolved from Skylake to modern CPUs like Ice Lake and Zen 5."
					},
					"register_file_advantage": {
						"type": "string",
						"description": "The benefit of AVX-512's 32 ZMM registers over AVX2's 16 YMM registers for reducing register pressure and spills."
					},
					"energy_performance_tradeoff": {
						"type": "string",
						"description": "The trade-off between higher power consumption and faster execution time, affecting overall energy efficiency (Joules/hash)."
					},
					"decision_guidelines": {
						"type": "string",
						"description": "A clear framework with recommendations on when to choose AVX-512 over AVX2 for each major CPU microarchitecture."
					}
				},
				"required": [
					"frequency_throttling_by_generation",
					"register_file_advantage",
					"energy_performance_tradeoff",
					"decision_guidelines"
				],
				"additionalProperties": false
			},
			"multi_core_scheduling_plan": {
				"type": "object",
				"properties": {
					"numa_and_core_partitioning": {
						"type": "string",
						"description": "Strategy for distributing the workload across NUMA nodes and physical cores, including thread affinity and SMT considerations."
					},
					"memory_placement_strategy": {
						"type": "string",
						"description": "Guidance on ensuring data is allocated in memory local to the processing core, using techniques like a 'first-touch' policy."
					},
					"tiling_strategy": {
						"type": "string",
						"description": "Recommendation for processing the data in small tiles to maximize L1 data cache locality."
					},
					"scaling_and_saturation_analysis": {
						"type": "string",
						"description": "Expected performance scaling curve as more cores are used and identification of the likely saturation points."
					}
				},
				"required": [
					"numa_and_core_partitioning",
					"memory_placement_strategy",
					"tiling_strategy",
					"scaling_and_saturation_analysis"
				],
				"additionalProperties": false
			},
			"portable_simd_frameworks_evaluation": {
				"type": "object",
				"properties": {
					"framework_name": {
						"type": "string",
						"description": "The name of the portable SIMD abstraction library (e.g., Google Highway, xsimd, EVE)."
					},
					"api_expressiveness": {
						"type": "string",
						"description": "How well the framework's API expresses the required add/xor/shift and broadcast operations."
					},
					"architecture_support": {
						"type": "string",
						"description": "Coverage of target ISAs, with a focus on support for dynamic-length SVE."
					},
					"generated_code_quality": {
						"type": "string",
						"description": "An assessment of how close the generated code's performance is to hand-written intrinsics."
					},
					"overall_recommendation": {
						"type": "string",
						"description": "A summary of the framework's suitability for this specific optimization task."
					}
				},
				"required": [
					"framework_name",
					"api_expressiveness",
					"architecture_support",
					"generated_code_quality",
					"overall_recommendation"
				],
				"additionalProperties": false
			},
			"verification_and_benchmarking_methodology": {
				"type": "object",
				"properties": {
					"verification_strategy": {
						"type": "string",
						"description": "The method for ensuring bit-identical results, primarily through differential testing against a golden scalar reference."
					},
					"portability_and_semantics": {
						"type": "string",
						"description": "How to handle differences in unsigned arithmetic and shift semantics across C++, x86, ARM, and GPU platforms."
					},
					"profiling_tools": {
						"type": "string",
						"description": "A list of recommended profiling tools for each platform (e.g., VTune, perf, Nsight, rocprof)."
					},
					"benchmarking_best_practices": {
						"type": "string",
						"description": "A checklist of practices for achieving reproducible results, including warm-up, batching, and thread affinity."
					},
					"key_metrics_to_report": {
						"type": "string",
						"description": "The standardized performance metrics that should be reported, such as 'Cycles/Hash' and 'Hashes/Sec'."
					}
				},
				"required": [
					"verification_strategy",
					"portability_and_semantics",
					"profiling_tools",
					"benchmarking_best_practices",
					"key_metrics_to_report"
				],
				"additionalProperties": false
			},
			"synthesized_kernel_blueprint": {
				"type": "object",
				"properties": {
					"simd_and_interleaving_plan": {
						"type": "string",
						"description": "The chosen SIMD width (e.g., AVX-512) and interleaving factor (e.g., 4) for the core kernel."
					},
					"data_layout_specification": {
						"type": "string",
						"description": "The precise data layout to be used (e.g., AoSoA with 64-byte aligned blocks)."
					},
					"register_allocation_plan": {
						"type": "string",
						"description": "A plan for allocating vector registers to states, constants, and temporaries to avoid spills."
					},
					"performance_model": {
						"type": "string",
						"description": "The estimated performance in cycles/hash based on a realistic microarchitectural model."
					},
					"implementation_pseudocode": {
						"type": "string",
						"description": "High-level pseudocode or an intrinsics-level example demonstrating the final kernel structure."
					}
				},
				"required": [
					"simd_and_interleaving_plan",
					"data_layout_specification",
					"register_allocation_plan",
					"performance_model",
					"implementation_pseudocode"
				],
				"additionalProperties": false
			}
		},
		"required": [
			"optimization_summary",
			"primary_recommendation",
			"simd_vectorization_strategy",
			"interleaving_and_software_pipelining",
			"data_layout_optimization",
			"microarchitectural_scheduling_model",
			"algebraic_simplification_techniques",
			"bitslicing_and_carry_save_analysis",
			"dependency_chain_scheduling_theory",
			"compiler_auto_vectorization_guidance",
			"speculative_computation_applicability",
			"gpu_implementation_strategy",
			"practitioner_codebases_and_libraries",
			"avx512_performance_analysis",
			"multi_core_scheduling_plan",
			"portable_simd_frameworks_evaluation",
			"verification_and_benchmarking_methodology",
			"synthesized_kernel_blueprint"
		],
		"additionalProperties": false
	}
}
