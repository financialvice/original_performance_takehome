{
	"input": "I'm optimizing parallel tree traversal where 256 independent walkers traverse a binary tree. Each walker:\n  - Reads a tree node value based on current index\n  - Computes hash(current_value ^ node_value)\n  - Uses hash LSB to decide left/right child\n  - Repeats for 16 rounds\n\n  The critical bottleneck is scattered memory access - each walker accesses a different tree node based on data-dependent indices. I only have\n  2 memory load slots per cycle.\n\n  Search for the most advanced and obscure techniques:\n\n  1. Research papers on \"SIMD-friendly tree traversal\", \"vectorized BVH traversal\", \"coherent ray tracing\", or \"warp-efficient tree algorithms\"\n   from GPU/graphics research - especially techniques for maintaining SIMD efficiency despite divergent traversal paths\n\n  2. Papers on \"speculative prefetching for irregular access patterns\", \"helper thread prefetching\", or \"software-directed prefetching for\n  pointer-chasing\"\n\n  3. Research on tree layout transformations: \"van Emde Boas layout\", \"cache-oblivious tree layouts\", \"bandwidth-optimal tree layouts\", or\n  Morton/Z-order curve layouts for trees\n\n  4. Forum posts or blog posts from game developers, ray tracing engineers, or database engineers about optimizing scattered/irregular memory\n  access\n\n  5. Academic papers on \"batched tree operations\", \"vectorized B-tree operations\", or \"SIMD database index traversal\"\n\n  6. Research on \"memory access scheduling\" or \"memory request coalescing\" in software\n\n  7. Any papers about exploiting tree structure to predict or batch memory accesses - techniques where knowing you're traversing a complete\n  binary tree could help\n\n  8. Research from hash table optimization, particularly \"batched hash lookups\" or \"SIMD hash table probing\" that might apply to our batched\n  tree access pattern\n\n  9. Obscure techniques from FPGA or ASIC tree traversal implementations that achieve high throughput with limited memory bandwidth\n\n  I'm particularly interested in techniques that exploit the fact that all 256 walkers start at the same root node and diverge - there may be\n  temporal locality or batching opportunities in early rounds.",
	"output": {
	  "executive_summary": "To optimize the parallel tree traversal for 256 independent walkers under the severe constraint of two memory load slots per cycle, the most effective strategy is a multi-faceted, hybrid approach that aggressively tackles memory latency and manages thread divergence. The core problem is that scattered, data-dependent memory accesses serialize execution and underutilize hardware. The optimal solution involves three main pillars: 1) **Data Structure Transformation:** Abandon a simple pointer-based binary tree in favor of a cache-conscious layout. This includes re-organizing the tree in memory using a cache-oblivious van Emde Boas (vEB) layout to improve locality at all levels of the memory hierarchy. Furthermore, nodes should be grouped to form a 'wide' N-ary tree (e.g., 4-ary or 8-ary) and compressed using techniques like quantization to reduce the overall memory footprint and bandwidth consumption, making each load operation more efficient. 2) **Hybrid Traversal Algorithm:** Exploit the initial coherence where all 256 walkers start at the same root. For the first few rounds (e.g., 1-5), employ a level-synchronous, breadth-first 'packet' traversal. This batches all 256 walkers together, maximizing cache reuse and enabling powerful, coordinated prefetching for the next level's nodes. Once walker paths diverge significantly (measured by a heuristic like active walker count), the algorithm should switch to a fully independent, asynchronous model. On GPUs, this is best handled by a 'persistent threads' architecture with work queues, which keeps compute units saturated despite irregular execution times. On CPUs, this translates to a thread pool with task-stealing. 3) **Latency Hiding and Parallel Execution:** Throughout the traversal, use vertical vectorization with SIMD instructions (e.g., AVX-512, SVE) to process batches of walkers in parallel, using `gather` instructions for the scattered node reads. The hash computation step should also be vectorized. To hide the high latency of these scattered loads, implement a software-directed prefetching strategy. In the batched phase, this involves prefetching all nodes for the next round. In the divergent phase, this can be achieved with helper-thread prefetching or a Decoupled Access/Execute (DAE) model where dedicated threads/warps issue memory requests ahead of the computation threads. This comprehensive strategy transforms the memory-latency-bound problem into a more throughput-oriented one by maximizing memory-level and instruction-level parallelism.",
	  "prioritized_recommendations": [
		{
		  "technique": "Hybrid Traversal: Level-Synchronous Batching to Asynchronous Divergence",
		  "priority": "High",
		  "rationale": "This technique directly exploits the problem's most significant opportunity: all 256 walkers start at the same root. Processing the first few rounds in a batched, level-synchronous (BFS-like) manner maximizes cache reuse and enables highly effective coordinated prefetching. Switching to an asynchronous, per-walker model (e.g., persistent threads/work queues) after coherence is lost provides the best of both worlds. The impact is very high, and the complexity is moderate, making it the top recommendation.",
		  "target_platform": "Both"
		},
		{
		  "technique": "Cache-Conscious Tree Layouts (vEB, Wide Nodes, Compression)",
		  "priority": "High",
		  "rationale": "Restructuring the tree in memory is a fundamental optimization that reduces memory traffic and improves cache/TLB performance, directly attacking the core bottleneck. Using a van Emde Boas (vEB) layout, transforming the binary tree into a wider N-ary structure, and applying node compression are proven techniques from graphics and database research. While the build-time complexity is higher, the traversal-time benefit is substantial and passive.",
		  "target_platform": "Both"
		},
		{
		  "technique": "Software-Level Request Coalescing and Scheduling",
		  "priority": "High",
		  "rationale": "Instead of letting walkers fetch memory independently, a software layer can group (or sort) pending node requests by address. This converts scattered accesses into more coherent ones, improving cache-line utilization and memory controller efficiency. For the batched phase, this is implicit. For the divergent phase, this requires an explicit scheduling step but can significantly improve effective memory bandwidth. The impact is high, justifying the medium implementation complexity.",
		  "target_platform": "Both"
		},
		{
		  "technique": "Vertical SIMD Vectorization with Masked Gathers",
		  "priority": "Medium",
		  "rationale": "Processing multiple walkers per SIMD instruction (e.g., 8 or 16 at a time) using masked gather instructions is the canonical way to apply SIMD to this problem. This is highly effective on architectures with mature support (AVX-512, SVE). The performance gain is significant, but the implementation complexity is high, especially on older architectures (AVX2) that lack key instructions, and it requires careful software pipelining to manage the 2-load-slot limit.",
		  "target_platform": "CPU"
		},
		{
		  "technique": "Software-Directed Prefetching (Inline or Helper-Thread)",
		  "priority": "Medium",
		  "rationale": "Given the pointer-chasing nature, hardware prefetchers are ineffective. A software-based approach is necessary to hide memory latency. In the batched phase, prefetching all nodes for the next round is straightforward. In the divergent phase, more complex techniques like inline prefetching (duplicating code to run ahead) or helper-thread prefetching are needed. These can yield large gains but are complex to tune correctly to avoid cache pollution.",
		  "target_platform": "CPU"
		},
		{
		  "technique": "Persistent Threads and Work Compaction",
		  "priority": "Medium",
		  "rationale": "This is a cornerstone GPU technique for managing highly divergent workloads. A persistent kernel with threads pulling from work queues ensures the GPU remains saturated. Work compaction actively regroups active threads into new, dense warps to restore SIMD efficiency. This is highly effective but is a specific GPU programming pattern and has overheads related to atomics and synchronization.",
		  "target_platform": "GPU"
		},
		{
		  "technique": "Decoupled Access/Execute (DAE) Software Model",
		  "priority": "Low",
		  "rationale": "This involves dedicating specific threads/warps to only issue memory requests (Access) while others perform computation (Execute), communicating via queues. It is a powerful pattern for latency hiding but has very high implementation complexity, requiring careful pipeline balancing, synchronization, and queue management. It should be considered an advanced optimization.",
		  "target_platform": "Both"
		},
		{
		  "technique": "System-Level Controls (Huge Pages, Prefetcher Toggles, NUMA Pinning)",
		  "priority": "Low",
		  "rationale": "Using huge pages to reduce TLB misses, disabling counter-productive hardware prefetchers, and ensuring correct NUMA node placement are all valuable tuning steps. However, they are system-level configurations rather than algorithmic changes. They offer moderate, sometimes unpredictable gains and may require administrative privileges. They should be applied as a final tuning step.",
		  "target_platform": "CPU"
		}
	  ],
	  "initial_coherence_exploitation_strategy": {
		"early_round_algorithm": "The optimal algorithm for the first few coherent rounds is a level-synchronous, breadth-first 'packet' traversal. In this model, all 256 walkers are processed in lockstep, level by level, for a small number of rounds (e.g., r=1-3). Instead of each walker traversing deep into the tree independently, all walkers complete round 1, then all complete round 2, and so on. At each level, a 'frontier' of unique child nodes to be visited is constructed. For level 1, this frontier contains the children of the root. All 256 walkers are tested against the nodes in this frontier. This approach, detailed in research findings from questions 19 and 22, turns the initially scattered access pattern into a highly structured, batched memory access problem. It maximizes temporal and spatial locality, keeping the working set of nodes small and cache-friendly. On SIMD architectures, this allows for maximum utilization as all walkers (or lanes within a vector) are performing the same operation on the same data.",
		"shared_cache_staging_method": "To maximize the performance of the early coherent rounds, the top levels of the tree (a 'treelet') should be staged in the fastest available memory. The method depends on the architecture. On CPUs, Intel's Cache Allocation Technology (CAT), part of Resource Director Technology (RDT) and managed via the `resctrl` filesystem, can be used to reserve a portion of the Last-Level Cache (LLC) for the application. This allows the top-level treelet to be explicitly 'pinned' in the LLC, protecting it from eviction by other processes or threads. This is a robust form of eviction control. On GPUs, the per-Streaming Multiprocessor (SM) shared memory serves as a programmer-managed scratchpad. If the top-level treelet is small enough, it can be explicitly loaded into shared memory, providing extremely low-latency access for all threads within the thread block. This is a common technique in GPU-based BVH traversal, as noted in research finding 19.",
		"switching_criterion": "A quantitative heuristic is required to efficiently switch from the batched, coherent mode to a divergent, independent mode. The goal is to abandon the synchronization overhead of the batch as soon as coherence is lost. The most practical and widely-used heuristic, employed by frameworks like Intel's Embree and detailed in research finding 19, is based on the 'active walker count'. After each level in the batched traversal, the number of walkers proceeding down each unique child path is counted. If this count drops below a predefined threshold (e.g., 16 or 32 walkers, or a relative threshold like 50% of the group), that smaller group is split off and handed to a separate, independent traversal kernel (e.g., a standard stack-based single-walker algorithm). Other potential criteria include monitoring the 'frontier size growth' in the BFS-like model (triggering a switch when the number of unique nodes to visit grows excessively) or, on GPUs, tracking warp occupancy (the number of active threads in a 32-thread warp) and switching when it drops significantly.",
		"coordinated_prefetch_strategy": "The level-synchronous nature of the early rounds enables a highly effective 'batch prefetching' strategy. After completing the traversal for level 'i' and determining the next node index for all 256 walkers, the system has a complete list of all memory locations that will be accessed in level 'i+1'. Before beginning the computation for level 'i+1', the program can issue software prefetch instructions (e.g., `_mm_prefetch` on CPU) for all 256 of these node addresses. This provides the maximum possible time for the memory system to fetch the data, effectively hiding latency. As described in research finding 14, even a simpler 'greedy prefetching' approach, where both children of a node are speculatively prefetched, can reduce memory stalls by 50-90%. The batch prefetching approach is superior as it is not speculative; it knows exactly which nodes will be needed. This coordinated prefetch is a key benefit of the batched early-round algorithm."
	  },
	  "gpu_optimization_techniques": [
		{
		  "technique": "Persistent Threads and Work Queues",
		  "mechanism_summary": "A single, large grid of threads is launched once to occupy the GPU. These 'persistent threads' continuously fetch new tasks (e.g., walker traversal steps) from a global 'work queue' using atomic operations. This model bypasses the standard hardware scheduler, which is inefficient for workloads with highly variable execution times and divergent paths.",
		  "primary_benefit": "Maintains high GPU compute occupancy and throughput by ensuring that as soon as a thread or warp becomes idle, it immediately pulls new work, effectively mitigating performance loss from control flow divergence.",
		  "reported_performance_gain": "1.5x-2.2x speedup for packet traversal and 1.5x for per-ray traversal. A 32% performance improvement can be gained from 'atomic coalescing' within the work queue management."
		},
		{
		  "technique": "Hybrid Packet/Single-Walker Traversal",
		  "mechanism_summary": "Traversal begins with walkers grouped into packets (e.g., a 32-thread warp) that traverse the tree in lockstep, maximizing SIMD efficiency. The number of active walkers in the packet is monitored using warp-level primitives (`__ballot`, `__popc`). When the active count drops below a certain threshold, the system switches the remaining walkers to an independent, single-walker traversal mode.",
		  "primary_benefit": "Dynamically adapts to the changing coherence of the workload, gaining the SIMD efficiency of packet traversal in early, coherent rounds and the robustness of single-walker traversal in later, divergent rounds.",
		  "reported_performance_gain": "Up to 2x speedup over pure packet or pure single-ray methods. In frameworks like Embree, this can improve performance by up to 50% over using packets alone."
		},
		{
		  "technique": "Work Compaction / Warp Repacking",
		  "mechanism_summary": "This technique actively manages SIMD lane utilization. After a traversal step, a compaction kernel identifies the active walkers from multiple sparse, divergent warps. These active walkers are then re-grouped into new, fully-packed warps for the next stage of execution. This can be implemented in software or via hardware mechanisms like Thread Block Compaction (TBC).",
		  "primary_benefit": "Directly restores SIMT efficiency to near 100% by eliminating idle lanes caused by divergence, at the cost of a periodic regrouping step.",
		  "reported_performance_gain": "Hardware-based Thread Block Compaction (TBC) provides an average speedup of 22% over baseline. When combined with other techniques like Treelet Scheduling, it can contribute to overall speedups of up to 2.55x."
		},
		{
		  "technique": "Treelet Scheduling",
		  "mechanism_summary": "The tree data structure is partitioned into smaller sub-trees called 'treelets', which are sized to fit within a specific cache level (e.g., L1 or L2). Walkers are then grouped into queues based on the treelet they need to access next. A scheduler loads a target treelet into cache and then processes all walkers in the corresponding queue.",
		  "primary_benefit": "Maximizes cache locality and significantly reduces main memory bandwidth by amortizing the cost of loading a treelet from memory over many walkers. This is critical for memory-bandwidth-bound scenarios.",
		  "reported_performance_gain": "Can achieve up to a 2.55x performance improvement in path tracing scenarios when combined with optimizations like virtualized queues and warp repacking."
		},
		{
		  "technique": "Cooperative BVH Traversal (CoopRT)",
		  "mechanism_summary": "A hardware-level optimization where idle threads within a divergent warp are used to assist the busy threads with their tree traversal. These 'helper' threads process nodes from the busy thread's traversal stack or queue, and a specialized memory scheduler can coalesce node fetch requests from multiple threads in the warp.",
		  "primary_benefit": "Directly combats low SIMT efficiency from ray divergence by reclaiming the compute power of idle threads to accelerate the progress of active ones.",
		  "reported_performance_gain": "Up to a 5.11x speedup (2.15x geometric mean) over a baseline traversal unit in simulation, with a moderate hardware area overhead."
		}
	  ],
	  "cpu_simd_vectorization_strategies": [
		{
		  "strategy": "Level-Synchronous Vectorization with Masked Gathers",
		  "description": "Processes all 256 walkers at a given tree depth before any proceed to the next level. The states of the walkers (e.g., current node indices) are stored across multiple SIMD vectors. In each round, a masked gather instruction is used to load the disparate 'node_value' data from memory into a vector register, but only for the walkers that are currently active, as tracked by a mask register.",
		  "required_simd_support": "AVX-512 is ideal due to native, high-performance masked gather instructions (VPGATHERDD). ARM SVE also has strong support. AVX2 has gather instructions, but they are less performant and lack native scatter or compaction, making the full strategy less efficient.",
		  "key_implementation_detail": "The core of the algorithm is the use of a masked gather intrinsic (e.g., `_mm512_mask_i32gather_epi32`) to efficiently load scattered data for active walkers into a single vector register for parallel processing."
		},
		{
		  "strategy": "Active Lane Compaction for Divergence Management",
		  "description": "As walkers complete their 16 rounds or become inactive, the SIMD vectors become sparse, leading to wasted computational power. This strategy periodically uses a compaction instruction to gather all active walkers from a sparse vector and pack them into a new, dense vector. This restores full SIMD efficiency for subsequent processing steps.",
		  "required_simd_support": "AVX-512 provides the native `_mm512_mask_compress_epi32` (VPCOMPRESSD) instruction, which is essential for efficient implementation. ARM SVE provides `svcompact`. This operation is very costly to emulate on AVX2, which lacks a native instruction.",
		  "key_implementation_detail": "After a traversal step, use the activity mask to drive a compress instruction, which filters out inactive lanes and consolidates the active ones, ensuring the SIMD units remain fully utilized."
		},
		{
		  "strategy": "Software Pipelining for Memory Bandwidth Management",
		  "description": "A single gather instruction can issue many memory requests (e.g., 16 for AVX-512), which can saturate the 2-load-slot-per-cycle hardware limit. This strategy structures the code as a software pipeline to manage memory pressure. It interleaves memory-intensive gather operations for one block of walkers with the computation-intensive hash calculations for a different block of walkers whose data has already been fetched.",
		  "required_simd_support": "Any SIMD instruction set with gather support. This is a software scheduling technique rather than a specific hardware feature.",
		  "key_implementation_detail": "The main loop must be unrolled and instructions reordered to explicitly space out gather instructions, hiding their latency and ensuring the average number of loads issued per cycle does not exceed the hardware's physical limit."
		},
		{
		  "strategy": "Branchless SIMD Search with Super-Nodes",
		  "description": "This strategy, adapted from vectorized B-trees and Adaptive Radix Trees (ART), redesigns the node layout. Multiple binary tree nodes are grouped into a single, cache-aligned 'super-node'. A walker can then use SIMD instructions to compare its key against multiple child paths within the super-node in parallel. The result is a bitmask, which is processed with bit manipulation instructions (`tzcnt`, `popcount`) to determine the next path without any conditional branches.",
		  "required_simd_support": "SSE, AVX2, or AVX-512. The key instructions are SIMD compares (e.g., `_mm_cmpeq_epi8`) and mask conversion (`_mm_movemask_epi8`), which are widely available.",
		  "key_implementation_detail": "The traversal logic is transformed from a branching 'if-else' structure to a branch-free sequence of SIMD compare, movemask, and bit-scan instructions to select the next node index."
		}
	  ],
	  "tree_layout_and_structure_optimizations": [
		{
		  "layout_technique": "van Emde Boas (vEB) Layout",
		  "description": "A cache-oblivious layout that recursively partitions a tree to enhance data locality across all levels of the memory hierarchy (L1/L2/L3 caches, TLB, main memory) without requiring prior knowledge of cache parameters. A tree of height 'h' is split at its middle level (h/2), creating a top subtree and multiple bottom subtrees. The final memory layout consists of the recursively laid-out top subtree stored contiguously, followed by the recursively laid-out bottom subtrees. This ensures that any subtree is stored in a contiguous memory block, improving spatial locality. For dynamic trees, this layout can be maintained via periodic global rebuilds.",
		  "layout_category": "Cache-Oblivious",
		  "primary_benefit": "Reduces cache misses and TLB misses for large, out-of-cache trees by providing an asymptotically optimal memory access pattern (O(log_B N) memory transfers). It is highly robust and scales well as the data size exceeds main memory."
		},
		{
		  "layout_technique": "Wide-Node Transformations (e.g., BVH4/BVH8)",
		  "description": "This structural transformation converts a binary tree into a k-ary tree (e.g., 4-ary or 8-ary) by collapsing or grouping nodes. A common method is to first build a high-quality binary tree and then convert it by 'pulling up' child nodes into their parent until the parent is saturated with the desired number of children (4, 8, or 16). The resulting tree is much shallower, and each wide node contains the data for all its children in a contiguous block, typically in a Structure-of-Arrays (SoA) format.",
		  "layout_category": "SIMD-Focused",
		  "primary_benefit": "Enables robust SIMD parallelism for divergent traversals. Instead of a packet of walkers traversing a binary tree (which fails with divergence), a single walker can use SIMD instructions to test against all k children of a wide node simultaneously. This also reduces tree depth, leading to fewer memory loads per traversal."
		},
		{
		  "layout_technique": "Cache-Oblivious BVH (COLBVH)",
		  "description": "A specialized cache-oblivious layout designed for Bounding Volume Hierarchies. It uses a probabilistic model, often based on node surface area, to predict runtime access patterns. The algorithm recursively partitions the tree into clusters of approximately sqrt(|N|) nodes by greedily merging nodes with the highest traversal probability into a 'root cluster'. This root cluster is placed first in the linear memory layout, followed by the recursive layouts of the remaining child clusters. This considers both parent-child relationships and spatial proximity.",
		  "layout_category": "Cache-Oblivious",
		  "primary_benefit": "Maximizes cache performance for specific applications like ray tracing and collision detection by creating a layout optimized for expected access patterns. It has demonstrated significant performance improvements (26% to 300%) over standard layouts."
		},
		{
		  "layout_technique": "Treelet-based Layouts (TDFS/TBFS/SWST)",
		  "description": "These are cache-aware layouts that use access statistics gathered from sample traversals to reorder nodes. The BVH is partitioned into 'treelets,' which are connected subtrees of frequently co-accessed nodes. The construction algorithm uses a frequency threshold to decide whether a child node should be merged into its parent's treelet or start a new one. The final memory layout is a concatenation of these treelets. SWST (Swapped Subtrees) is a simpler variant that just reorders left/right children based on access frequency.",
		  "layout_category": "Cache-Aware",
		  "primary_benefit": "Improves cache locality by grouping nodes that form 'hot paths' together in contiguous memory blocks. This is highly effective when access patterns are predictable or can be profiled, with TDFS showing runtime reductions of 30-40% in some scenarios."
		},
		{
		  "layout_technique": "Hybrid Multi-Level Layouts",
		  "description": "These layouts combine multiple strategies to optimize for different goals. One example is a 'Two-Level Clustering Scheme' that first decomposes the BVH into large 'Address Clusters' (ACs) to enable pointer compression, and then further partitions nodes within each AC into smaller 'Cache Clusters' (CCs) that are sized to fit perfectly into a single cache line. Another hybrid approach is to use a simple, fast layout like BFS for the top few levels of the tree (to exploit initial walker coherence) and then switch to a more sophisticated layout like vEB or COLBVH for the deeper, more divergent parts of the tree.",
		  "layout_category": "Hybrid (Cache-Aware/Cache-Oblivious)",
		  "primary_benefit": "Achieves a superior balance of optimization goals, such as reducing memory footprint via pointer compression while also maximizing cache line utilization. The Two-Level scheme demonstrated a 15-35% reduction in L2-to-L1 bandwidth."
		},
		{
		  "layout_technique": "Eytzinger Layout",
		  "description": "An implicit, cache-friendly array layout for binary search trees. Instead of using pointers, nodes are arranged in a specific permutation within a single array such that child nodes can be located via simple arithmetic (e.g., left child at index `2*i + 1`, right child at `2*i + 2`). This eliminates pointer overhead and places parent and child nodes close together in memory.",
		  "layout_category": "Cache-Aware / Implicit",
		  "primary_benefit": "Improves data density and cache locality by eliminating pointers. This layout is highly conducive to software prefetching, as the addresses of future nodes can be easily calculated."
		}
	  ],
	  "node_packing_and_compression_techniques": [
		{
		  "technique": "Sibling Co-location",
		  "description": "This strategy involves storing sibling nodes sequentially in memory. For a binary tree, the left and right children of a parent are placed adjacent to each other. This is an inherent feature of wide BVH layouts (like BVH4/BVH8), where all N children of a node are stored within a single contiguous memory block.",
		  "impact_on_memory_traffic": "Reduces memory traffic by improving spatial locality. When a walker accesses one sibling, the hardware prefetcher is highly likely to fetch the other sibling in the same cache line, preventing a future cache miss.",
		  "tradeoff_or_consideration": "This is a simple and highly effective technique with minimal overhead. Its main consideration is that it requires a controlled memory layout rather than arbitrary pointer-based allocation."
		},
		{
		  "technique": "Structure-of-Arrays (SoA) Layout",
		  "description": "Instead of the traditional Array-of-Structures (AoS) layout where all data for a single node is grouped together, SoA groups similar data fields across multiple nodes. For example, all X-coordinates of child bounding boxes are stored contiguously, followed by all Y-coordinates, and so on. This is the standard layout for wide BVHs.",
		  "impact_on_memory_traffic": "Dramatically improves memory bandwidth utilization for SIMD processing. A single SIMD load instruction can fetch a vector of homogeneous data (e.g., 8 x-coordinates) for parallel computation, avoiding the need for slow, scattered 'gather' instructions that would be required with an AoS layout. This can lead to an 8x or greater improvement in memory bandwidth efficiency.",
		  "tradeoff_or_consideration": "While it makes accessing all data for a single node slightly more complex, this is a minor trade-off for the massive performance gain in parallel workloads. It is essential for efficient SIMD/SIMT execution."
		},
		{
		  "technique": "Node Quantization and Compression",
		  "description": "This technique reduces node size by storing data using lower precision. A common method is to represent child bounding boxes with low-precision (e.g., 8-bit or 16-bit) coordinates relative to the parent's bounding box. Examples include Embree's QBVH8, which halves node size, and more aggressive GPU-focused methods that achieve a 3:1 compression ratio. A hybrid approach, CLBVH, compresses only leaf nodes, which make up over 80% of the tree's memory.",
		  "impact_on_memory_traffic": "Significantly reduces memory footprint and bandwidth consumption. Research shows memory traffic can be reduced to as little as 31% of an uncompressed baseline, which is critical for a bandwidth-limited system. This allows more of the tree to fit in cache.",
		  "tradeoff_or_consideration": "The primary cost is the computational overhead for decompressing the data during traversal. Quantization can also lead to slightly looser bounding boxes, which may increase the number of intersection tests. However, for memory-bound workloads, the net performance gain is substantial (e.g., 1.9-2.1x speedup reported)."
		},
		{
		  "technique": "Pointer Elimination via Implicit Indexing",
		  "description": "This strategy replaces explicit 64-bit pointers with calculated indices, which is possible when nodes are arranged in a predictable, contiguous memory layout. Examples include Depth-First Layouts (DFL), where a child's index is `parent_index + 1`, and the Eytzinger layout, where children are at `2*i+1` and `2*i+2`. This is also used in CSB+-trees and to remove pointers from compressed leaf nodes (CLBVH).",
		  "impact_on_memory_traffic": "Drastically reduces the memory footprint of each node by eliminating pointer overhead. This allows more nodes to be packed into a cache line and into the cache as a whole, directly reducing memory traffic from cache misses.",
		  "tradeoff_or_consideration": "This technique requires a more rigid memory layout and introduces more complex address calculation logic compared to simple pointer dereferencing. The computational overhead of the address arithmetic must be weighed against the memory savings."
		}
	  ],
	  "speculative_prefetching_and_latency_hiding": [
		{
		  "technique": "Inline Software Prefetching ('Helper Without Threads')",
		  "mechanism": "A software-only technique where prefetching logic (a simplified 'slice' of the main computation) is inserted directly into the application's code. This logic runs a set number of iterations ahead of the main computation, issuing prefetches for future data without creating separate OS threads.",
		  "suitability_for_problem": "Highly suitable. It avoids the high overhead of thread creation and synchronization associated with helper-thread prefetching, which would be unmanageable for 256 independent walkers. It directly targets the 'Delinquent Irregular Loads' (DILs) characteristic of this problem.",
		  "reported_gain_or_caveat": "Up to 2x single-thread performance improvement and up to 83% faster than helper-thread implementations for memory-bound irregular workloads. The prefetch distance is statically controlled, which simplifies tuning but is less adaptive. (Source: Sankaranarayanan et al. 2020)"
		},
		{
		  "technique": "Helper-Thread Prefetching (Speculative Precomputation)",
		  "mechanism": "Utilizes idle SMT contexts to run a dedicated 'helper thread'. This thread executes a pre-computed 'slice' of the main code to calculate the address of a future delinquent load and issues a prefetch for it, running ahead of the main application thread.",
		  "suitability_for_problem": "Conceptually suitable, but likely not scalable for 256 walkers due to the limited number of SMT contexts and the high overhead of managing hundreds of helper threads. It is effective for pointer-chasing but less practical at this scale.",
		  "reported_gain_or_caveat": "Can reduce memory stall time by up to 72%. However, it requires spare SMT contexts and suffers from significant thread management and synchronization overhead. (Source: Hashemi et al., Collins et al.)"
		},
		{
		  "technique": "Treelet Prefetching (GPU)",
		  "mechanism": "A specialized hardware prefetcher for GPUs. The tree (BVH) is partitioned into small subtrees called 'treelets'. When a walker's path enters a treelet's root node, the hardware automatically issues prefetch requests for all nodes within that entire treelet.",
		  "suitability_for_problem": "Extremely suitable for a GPU implementation. It directly attacks the pointer-chasing dependency chain by prefetching a coarse-grained, spatially local chunk of the tree, turning many dependent loads into a single, larger prefetch operation.",
		  "reported_gain_or_caveat": "Reduces BVH memory access latency by 54% on average, leading to a 32.1% IPC improvement in simulation. A key trade-off is that a significant percentage of prefetched nodes may go unused (43.5% in one study), wasting some bandwidth. (Source: Chou et al. 2023)"
		},
		{
		  "technique": "Jump-Pointer Prefetching (JPP)",
		  "mechanism": "Augments the tree data structure with additional 'jump pointers' that skip over several intermediate nodes to point to a distant descendant. A prefetch engine can follow these jump pointers to prefetch nodes far ahead in the traversal path.",
		  "suitability_for_problem": "Very suitable, as it is designed specifically for linked data structures. It can be implemented in software, hardware, or a cooperative model. It directly breaks the long dependency chain of pointer chasing.",
		  "reported_gain_or_caveat": "The cooperative software/hardware model can reduce memory stall time by 83%, yielding a 20% overall speedup. The software-only version reduced stall time by 72% for a 15% speedup. The main caveat is the overhead of creating and storing the extra pointers. (Source: Roth and Sohi 1999)"
		},
		{
		  "technique": "Correlation Prefetching (GHB, DCPT)",
		  "mechanism": "Hardware-based prefetchers that learn patterns in the memory access stream. The Global History Buffer (GHB) and Delta Correlating Prediction Tables (DCPT) track sequences of deltas (strides) between consecutive miss addresses to predict the next miss address.",
		  "suitability_for_problem": "Moderately suitable. While powerful for irregular patterns, their effectiveness on pure pointer-chasing can be limited if there are no repeating delta patterns. They are more effective for patterns like `A[B[i]]` than `A->next`.",
		  "reported_gain_or_caveat": "DCPT showed a 42% speedup over a baseline on SPEC2006 benchmarks with only 4KB of storage overhead. GHB can improve correlation prefetching performance by 20%. Their effectiveness for this specific tree traversal must be benchmarked. (Source: Nesbit & Smith 2004, Grannaes et al. 2011)"
		},
		{
		  "technique": "Content-Directed Prefetching (CDP)",
		  "mechanism": "A stateless hardware mechanism that scans the contents of fetched cache lines, identifies values that look like valid memory addresses (pointers), and speculatively prefetches them. It can recursively traverse pointer chains.",
		  "suitability_for_problem": "Conceptually a perfect fit for pointer-chasing. However, it can suffer from a high rate of mispredictions (interpreting non-pointer data as pointers), leading to significant cache pollution and wasted bandwidth.",
		  "reported_gain_or_caveat": "Achieved an 11.3% speedup with less than 0.5% space overhead in the L2 cache. Hybrid versions (ECDP) that use compiler hints to identify pointer-rich regions are more efficient. (Source: Cooksey et al. 2002)"
		},
		{
		  "technique": "Runahead Execution",
		  "mechanism": "A hardware technique where, upon a long-latency cache miss, the processor checkpoints its state and speculatively executes instructions down the miss path. The results are discarded, but the side effect is that it triggers the hardware prefetcher for subsequent data misses.",
		  "suitability_for_problem": "A general-purpose latency-hiding technique that can help with pointer chasing. However, it is known to be highly inefficient in terms of energy and wasted work, as many speculative instructions are executed and thrown away.",
		  "reported_gain_or_caveat": "Can increase memory latency tolerance. Its primary drawback is significant inefficiency and energy consumption. (Source: Kim et al. 2005)"
		}
	  ],
	  "execution_and_scheduling_models": [
		{
		  "model": "Decoupled Access/Execute (DAE)",
		  "core_concept": "The instruction stream is partitioned into two cooperating units: an 'Access' unit that only handles memory operations and an 'Execute' unit that only performs computation. They communicate via explicit FIFO queues, allowing the Access unit to run ahead of the Execute unit, effectively prefetching data and hiding memory latency.",
		  "platform_realization": "On CPUs, this is known as Decoupled Software Pipelining (DSWP), where different cores are assigned Access and Execute roles. On GPUs, it is realized as 'Warp Specialization', where some warps within a thread block are dedicated to memory access (e.g., using asynchronous copies) while others compute, communicating via shared memory and hardware barriers.",
		  "primary_benefit": "Maximizes memory latency hiding by overlapping computation with memory access. The model is designed to tolerate long, variable memory latencies and keep a large number of memory requests in flight, which is ideal for the 2-load-slot constraint and 256 walkers."
		},
		{
		  "model": "Wavefront Execution ('Megakernels Considered Harmful')",
		  "core_concept": "Instead of a single, large, monolithic kernel that performs all steps of the traversal (a 'megakernel'), the workload is broken down into multiple smaller, specialized kernels. Each kernel performs a single task (e.g., node intersection, hash computation). Walkers are sorted into work queues corresponding to the next task they need to perform.",
		  "platform_realization": "Primarily a GPU model. A large pool of active walkers is maintained in global memory. After completing a task in one kernel, a walker's state is updated, and it is enqueued for the next appropriate specialized kernel. This requires synchronization between kernel launches.",
		  "primary_benefit": "Dramatically reduces control flow divergence within a warp. Since each specialized kernel only does one thing, all threads within a warp are executing the same instructions, leading to maximum SIMT efficiency. It also reduces register pressure, increasing occupancy and latency-hiding capability."
		},
		{
		  "model": "Persistent Threads / Work Queues",
		  "core_concept": "A single, large grid of threads is launched once to occupy the GPU's compute resources. These 'persistent threads' enter a loop, continuously fetching tasks from a global work queue using atomic operations. This bypasses the hardware scheduler, which can be inefficient for workloads with highly variable task completion times.",
		  "platform_realization": "A common and powerful GPU programming pattern. The work queue is typically a simple counter in global memory, and atomics are used to claim work items. Warp-level primitives can be used to coalesce atomic operations to reduce contention.",
		  "primary_benefit": "Maximizes GPU throughput and utilization for irregular workloads. It ensures that as soon as a thread (or walker) finishes its task, it immediately pulls new work, preventing compute resources from going idle and effectively managing the 256 independent walkers."
		},
		{
		  "model": "Software Request Coalescing / Access Scheduling",
		  "core_concept": "Instead of walkers fetching memory independently, their requests are collected, grouped, and scheduled by a software layer. Walkers needing to access the same node or nodes in the same cache line are grouped together, and their memory requests are 'coalesced' into a single physical memory transaction.",
		  "platform_realization": "On GPUs, this can be implemented by sorting walkers (rays) by destination node ID or spatial locality before launching a traversal kernel. On CPUs, this is analogous to batched database lookups, where queries are grouped before execution. The 'singleflight' pattern is a general software implementation.",
		  "primary_benefit": "Improves memory access efficiency and effective bandwidth. It converts many scattered, random accesses into fewer, more regular accesses, which is ideal for modern memory systems. It directly reduces the number of transactions needed, making better use of the limited load slots."
		},
		{
		  "model": "Dynamic Warp Formation / Thread Block Compaction",
		  "core_concept": "A hardware-level execution model that dynamically manages thread grouping to combat divergence. After a divergent branch, the hardware identifies threads that have chosen the same path and re-groups them into new, fully coherent warps for execution. Thread Block Compaction is a more robust version that operates at the thread block level.",
		  "platform_realization": "This is a proposed hardware feature for GPUs, not typically available in software. It was detailed in research by Fung et al. (2007, 2011).",
		  "primary_benefit": "Automatically restores SIMD efficiency after divergent branches, directly increasing hardware utilization. TBC was shown to provide a 22% average speedup on divergent CUDA applications over standard reconvergence mechanisms."
		}
	  ],
	  "database_and_hashing_optimizations": [
		{
		  "technique": "Branchless SIMD Node Search",
		  "source_domain": "Vectorized B-Trees / Database Indexes",
		  "application_to_traversal_problem": "Instead of a conditional branch based on the LSB of the hash (`if (lsb == 0) ... else ...`), the decision can be vectorized. For a group of walkers, their LSBs can be used to form a bitmask. This mask can then be used with SIMD blend instructions (`_mm_blendv_...`) to select the next node indices from two vectors (one for left children, one for right) without any jumps, keeping the CPU pipeline full.",
		  "key_insight": "Conditional branches are extremely expensive on modern CPUs due to potential mispredictions. Replacing them with branchless SIMD instructions (compares, blends, bitmasking) that operate on batches of walkers eliminates this bottleneck, leading to higher and more predictable throughput."
		},
		{
		  "technique": "Pointer Elimination & Implicit Layouts (Eytzinger)",
		  "source_domain": "Vectorized B-Trees / Cache-Conscious Data Structures",
		  "application_to_traversal_problem": "Instead of storing nodes with explicit `left` and `right` pointers, the entire binary tree can be stored in a single contiguous array using an Eytzinger layout. In this layout, if a node is at index `i`, its left child is at `2*i + 1` and its right child is at `2*i + 2`. This completely eliminates pointers, increasing data density and improving cache locality.",
		  "key_insight": "Explicit pointers consume significant memory and lead to scattered accesses. By using an implicit, arithmetic-based layout, memory traffic is reduced, cache utilization is improved, and the pattern becomes more predictable for hardware prefetchers."
		},
		{
		  "technique": "SIMD Group Probing with H1/H2 Hash Splitting",
		  "source_domain": "High-Performance Hash Tables (e.g., Google SwissTable, Facebook F14)",
		  "application_to_traversal_problem": "The `hash(current_value ^ node_value)` step can be modeled as a hash table lookup. The computed hash is split into two parts: H1 (high bits) to select a 'group' of potential next nodes, and H2 (low bits) as a fingerprint. A single SIMD instruction can compare the H2 fingerprint of a walker against the fingerprints of all 16 nodes in the target group simultaneously, generating a bitmask of potential matches. This is vastly faster than scalar comparisons.",
		  "key_insight": "A full 64-bit key comparison is slow. By using a small, 7-bit fingerprint (H2), the vast majority of non-matching nodes can be filtered out in a single, cheap SIMD operation, drastically reducing the number of expensive full comparisons and making long probe chains computationally inexpensive."
		},
		{
		  "technique": "Vectorized Hashing (Vertical Vectorization)",
		  "source_domain": "High-Performance Hash Tables / SIMD Algorithms",
		  "application_to_traversal_problem": "The computation of `hash(current_value ^ node_value)` for all 256 walkers can be performed in a batched, SIMD-parallel manner. Using a SIMD-friendly hash function (like xxHash or wyhash), a block of walkers (e.g., 8 or 16) can have their hashes computed simultaneously, with each SIMD lane handling one walker. This creates a highly efficient pipeline where the output of vectorized hashing feeds directly into vectorized probing.",
		  "key_insight": "Modern hash functions are designed to be parallelizable. Applying them in a 'vertical' SIMD fashion (one walker per lane) allows the hashing step for a large batch of walkers to be completed in a fraction of the time it would take to process them serially, maximizing computational throughput."
		},
		{
		  "technique": "Contiguous Child Storage (Pointer Partial Elimination)",
		  "source_domain": "Cache-Sensitive B+-Trees (CSB+-Trees)",
		  "application_to_traversal_problem": "To improve data density, child nodes can be stored contiguously in memory. The parent node would only need to store a pointer to the first child; the second child's address is found via a simple arithmetic offset. This saves the space of one pointer per node, allowing more nodes to fit in cache.",
		  "key_insight": "By eliminating redundant pointers and enforcing a contiguous layout for siblings, cache line utilization is improved. A single memory fetch is more likely to bring in data for multiple related nodes, reducing overall memory traffic."
		}
	  ],
	  "hardware_accelerator_inspired_designs": [
		{
		  "hardware_concept": "Decoupled Access/Execute (DAE)",
		  "inspiration_source_accelerator": "General DAE Architecture / Graphicionado",
		  "software_analogue_description": "Implement Decoupled Software Pipelining (DSWP). Dedicate one or more CPU cores (or GPU warps) to act as an 'Access Unit' responsible only for issuing memory loads for the walkers. The other cores ('Execute Units') perform the hash computation and control flow. The units communicate via lock-free software queues in shared memory. The Access Unit effectively becomes a software-managed prefetch engine for the Execute Units.",
		  "expected_benefit_in_software": "Hides memory latency by overlapping memory access with computation. The system can tolerate latency up to `(Number of Walkers) / (Load Slots per Cycle)` cycles, which is `256 / 2 = 128` cycles in this case. It also centralizes memory requests, enabling batching and scheduling optimizations."
		},
		{
		  "hardware_concept": "Multi-Bank On-Chip Caches / Inter-Bank Dispersion",
		  "inspiration_source_accelerator": "Skewed-Associative Caches",
		  "software_analogue_description": "Emulate the conflict-avoidance property of multi-bank caches using memory allocation strategies. On Linux, this can be approximated with user-space page coloring libraries or careful use of `mmap` to distribute the tree's data across physical addresses that map to different sets in the CPU's Last-Level Cache (LLC). This reduces the probability of different walkers evicting each other's data from the cache.",
		  "expected_benefit_in_software": "Reduces LLC conflict misses for the scattered access pattern, leading to a higher cache hit rate and lower effective memory latency. This makes the limited load bandwidth more effective by reducing trips to main memory."
		},
		{
		  "hardware_concept": "Software-Managed Scratchpad Memory",
		  "inspiration_source_accelerator": "Graphicionado",
		  "software_analogue_description": "On a GPU, the on-chip Shared Memory can be used as an explicit, software-managed scratchpad. The top levels of the tree, which are accessed by all walkers, can be loaded into shared memory at the start of the kernel. On a CPU, a similar effect can be achieved by using Intel's Cache Allocation Technology (CAT) to reserve a portion of the LLC to 'pin' these hot nodes.",
		  "expected_benefit_in_software": "Accesses to the top levels of the tree become extremely fast (shared memory latency is ~20-30 cycles vs. 200-1000 for global memory on a GPU), fully exploiting the initial coherence of the 256 walkers."
		},
		{
		  "hardware_concept": "Computation Migration to Data",
		  "inspiration_source_accelerator": "Emu Chick",
		  "software_analogue_description": "On a multi-socket NUMA system, this concept translates to NUMA-aware task scheduling. Instead of pulling data across the high-latency NUMA interconnect to a fixed thread, the scheduler should migrate the walker's execution context (the task) to the CPU core on the NUMA node where the required tree data resides. This is achieved by combining thread pinning (`taskset`) with memory placement policies (`numactl`).",
		  "expected_benefit_in_software": "Eliminates the significant latency penalty of remote NUMA memory accesses, which can be over 60 ns per access. This ensures all memory accesses are local and fast, which is critical for a latency-bound workload."
		},
		{
		  "hardware_concept": "Fine-Grained Memory Access",
		  "inspiration_source_accelerator": "Emu Chick",
		  "software_analogue_description": "The Emu architecture's 8-byte access granularity highlights the inefficiency of fetching a full 64-byte cache line for a single pointer. The software analogue is to design data structures with maximum density. This involves using pointer compression, implicit layouts (like Eytzinger), and packing multiple logical nodes or metadata into a single cache line to ensure every byte fetched from memory is utilized.",
		  "expected_benefit_in_software": "Improves effective memory bandwidth by reducing the amount of useless data transferred. This makes each of the two available load slots per cycle more impactful."
		}
	  ],
	  "practitioner_and_community_techniques": [
		{
		  "technique": "Node Quantization and Compression",
		  "source_community": "Ray Tracing (Intel Embree, NVIDIA OptiX)",
		  "description": "Reduces the memory footprint of tree nodes by storing data with lower precision. For example, child bounding boxes in a BVH are stored as 8-bit or 16-bit integers relative to the parent's bounding box, instead of 32-bit floats. A popular hybrid approach is Compressed-Leaf BVH (CLBVH), which only compresses leaf nodes, avoiding decompression overhead for frequently traversed inner nodes.",
		  "reported_gain_or_caveat": "Can achieve memory reduction of nearly 3x. A fully compressed wide BVH can improve traversal performance by 1.9x-2.1x for incoherent rays by drastically cutting memory traffic. The main caveat is the added computational cost of decompression during traversal."
		},
		{
		  "technique": "Wide BVH Layouts (BVH4/BVH8)",
		  "source_community": "Game Development / Ray Tracing",
		  "description": "Instead of a binary tree, the structure is transformed into a 4-ary or 8-ary tree. This reduces the tree's depth and co-locates sibling nodes into a single memory block. This structure is ideal for SIMD, as a single instruction can test a walker against all 4 or 8 children simultaneously.",
		  "reported_gain_or_caveat": "Reduces the number of pointer-chasing steps. For coherent walkers, packet traversal on a BVH4 can be over 4x faster than on a BVH2. The main trade-off is that wider nodes can sometimes reduce early culling opportunities compared to a tightly-fit binary tree."
		},
		{
		  "technique": "Persistent Threads",
		  "source_community": "GPU / Ray Tracing",
		  "description": "A programming model where a fixed grid of long-running GPU threads is launched once. These threads continuously fetch work items (e.g., the next traversal step for a walker) from a global work queue using atomic operations. This bypasses the hardware's default work distribution mechanism, which is often inefficient for workloads with highly variable execution times like divergent tree traversal.",
		  "reported_gain_or_caveat": "Reported to make packet traversal 1.5-2.2x faster by keeping the GPU fully saturated. It is highly effective at managing divergence and preventing thread starvation. The main caveat is potential contention on the global work queue, which can be mitigated with techniques like atomic coalescing."
		},
		{
		  "technique": "Cooperative Traversal (CoopRT)",
		  "source_community": "Academic / Ray Tracing Hardware Research",
		  "description": "A hardware-inspired software concept to combat divergence within a SIMD group (warp). When some walkers in a group become idle, they are repurposed to help the remaining active walkers with their traversal tasks. This can involve fetching nodes from the active walker's traversal stack or coalescing memory requests for the same node from multiple walkers into a single fetch.",
		  "reported_gain_or_caveat": "Simulations show up to a 5.11x speedup (2.15x geometric mean) for highly divergent workloads. This directly reclaims the compute resources lost to divergence, making it ideal for the 256-walker scenario as paths diverge."
		},
		{
		  "technique": "Treelet Prefetching",
		  "source_community": "Academic / Ray Tracing Hardware Research",
		  "description": "The tree is partitioned into small, contiguous subtrees called 'treelets' (e.g., 512 bytes). When a walker's path is about to enter a treelet, a software prefetch is issued for the entire treelet block. This breaks the long chain of dependent pointer-chasing loads by fetching a chunk of the future path in advance.",
		  "reported_gain_or_caveat": "Simulations show a 32.1% average performance improvement and a 54% reduction in memory access latency. This is highly effective for the 2-load-slot limit, as it hides latency. The main challenge is the overhead of partitioning the tree and identifying the correct treelet to prefetch."
		},
		{
		  "technique": "Level-Synchronous Batching / Packet Traversal",
		  "source_community": "Ray Tracing / Game Development",
		  "description": "To exploit initial coherence, process the 256 walkers in lockstep for the first few levels (a BFS-like expansion). All walkers are tested against the same node(s) at each level. When coherence drops below a threshold (e.g., fewer than 50% of walkers take the same path), the system switches to a fully independent (divergent) traversal mode.",
		  "reported_gain_or_caveat": "Hybrid traversal that switches from packet to single-ray mode can improve performance by up to 50% over a pure packet approach. This perfectly matches the problem's structure of starting together and diverging."
		}
	  ],
	  "system_level_and_numa_controls": [
		{
		  "control": "Huge Pages (2MB / 1GB)",
		  "mechanism_and_usage": "Use the `madvise(address, length, MADV_HUGEPAGE)` syscall in the application to request that the memory region containing the tree data structure be backed by large pages instead of the default 4KB pages. This must be done on a system where huge pages are enabled.",
		  "primary_benefit_for_traversal": "Dramatically reduces Translation Lookaside Buffer (TLB) misses. A single TLB entry can now cover a 2MB or 1GB region instead of just 4KB, vastly increasing the memory reach of the TLB. This avoids slow page table walks, which are a major source of latency for scattered access patterns over a large memory footprint.",
		  "reported_performance_impact": "Up to 3.1x speedup in random-read microbenchmarks. PostgreSQL reports a consistent 1-5% performance boost. Google's TCMalloc saw a 7% improvement in requests-per-second."
		},
		{
		  "control": "NUMA Thread and Memory Placement",
		  "mechanism_and_usage": "Use the `taskset` command to pin each walker's thread to a specific CPU core. Use the `numactl --membind=<node>` or `numactl --localalloc` command to ensure that the memory allocated by that thread comes from the NUMA node physically closest to its assigned core.",
		  "primary_benefit_for_traversal": "Avoids high-latency remote memory accesses. On a multi-socket server, accessing memory on a different NUMA node can be significantly slower (~60 ns extra latency per access). Enforcing local access is a fundamental optimization for latency-sensitive workloads.",
		  "reported_performance_impact": "Directly avoids the NUMA remote access penalty on every memory access that would otherwise cross the interconnect, which is a major performance gain for memory-bound applications."
		},
		{
		  "control": "Hardware Prefetcher Toggling",
		  "mechanism_and_usage": "On Intel CPUs, hardware prefetchers can be disabled by writing to the Model-Specific Register (MSR) at address `0x1A4`. This requires root privileges and the `msr` kernel module (e.g., `wrmsr -p <core> 0x1a4 0xf` to disable all four prefetchers). This can also often be done in the system BIOS.",
		  "primary_benefit_for_traversal": "Reduces cache pollution and conserves memory bandwidth. Standard hardware prefetchers (stride, stream) are ineffective for the irregular, pointer-chasing access pattern of the walkers and will fetch useless data, evicting useful cache lines and wasting the limited memory bandwidth.",
		  "reported_performance_impact": "Workload-dependent, but can significantly improve performance for pointer-chasing benchmarks by preventing the prefetcher from harming cache performance. An adaptive prefetch scheme on POWER7 showed up to 2.7x improvement."
		},
		{
		  "control": "Last-Level Cache (LLC) Partitioning",
		  "mechanism_and_usage": "Use Intel's Cache Allocation Technology (CAT), part of Resource Director Technology (RDT), via the `resctrl` filesystem in Linux. Create a resource group for the walker processes and assign it an exclusive portion of the LLC.",
		  "primary_benefit_for_traversal": "Guarantees a non-polluted slice of the LLC for the traversal workload. This isolates the walkers' cache from interference by other system processes (the 'noisy neighbor' problem), leading to more predictable performance and a higher cache hit rate for tree nodes.",
		  "reported_performance_impact": "Can significantly reduce conflict and capacity misses, directly improving hit rate and reducing effective latency. Especially useful for ensuring the top, most-shared levels of the tree remain cached."
		},
		{
		  "control": "Page Coloring",
		  "mechanism_and_usage": "A complex, user-space-only technique. It involves writing a custom memory allocator that requests physical pages whose addresses map to different sets in the CPU's LLC, aiming to manually distribute memory accesses and avoid cache conflicts.",
		  "primary_benefit_for_traversal": "Theoretically reduces LLC conflict misses by ensuring scattered accesses from different walkers are less likely to evict each other from the cache.",
		  "reported_performance_impact": "Academic studies show best-case improvements of up to 66%. However, it is not supported by the Linux kernel, is highly complex to implement correctly, and its real-world benefits are controversial. It should be considered a highly experimental optimization."
		}
	  ],
	  "quantitative_performance_model": {
		"model_type": "Hybrid Roofline / Latency-Hiding Model",
		"cpu_model_parameters": "The CPU model is constrained by several key parameters. The primary execution bottleneck is the 2 load/store units per cycle. Memory latency is hidden via Memory-Level Parallelism (MLP), which is physically limited by the number of Miss Status Holding Registers (MSHRs), typically 8-16 entries on modern cores (e.g., Intel Skylake, AMD Zen 2). Even with 256 software walkers, the hardware can only sustain a small number of concurrent misses per core. Cache latencies are critical: L1D at ~4-5 cycles, L2 at ~12-17 cycles, L3 at ~40-90 cycles, and DRAM at over 100 cycles. The model must account for the working set size of the walkers versus these cache sizes to predict hit/miss rates, which will degrade as walkers diverge.",
		"gpu_model_parameters": "The GPU model is dominated by its massive multithreading capability for latency hiding. With 256 walkers, this provides 8 warps (on a 32-wide architecture). The key parameter is whether this number of active warps is sufficient to hide the long latency of global memory access (200-1000 cycles). The model must consider the memory hierarchy: a small, fast L1 cache/shared memory per Streaming Multiprocessor (SM) (~20-30 cycles), a larger shared L2 cache (~200 cycles), and high-bandwidth but high-latency global HBM memory. The performance ceiling is determined by the memory bandwidth of the HBM (e.g., ~2 TB/s on an A100) and the number of in-flight memory requests the SMs can sustain.",
		"key_insight_from_model": "The primary conclusion from the performance model is that the traversal problem is fundamentally memory-latency bound due to the serialized, data-dependent nature of pointer-chasing. The 2-load-slot constraint makes the core's execution throughput a potential bottleneck, but only if memory latency is perfectly hidden. The core challenge is that the available software parallelism (256 walkers) far exceeds the hardware's ability to exploit Memory-Level Parallelism on a single CPU core (limited by MSHRs). Therefore, the most effective optimizations are those that either 1) reduce the number and cost of memory accesses by improving spatial and temporal locality (e.g., cache-friendly layouts, compression), or 2) restructure the algorithm to create independent work that can be used to hide latency (e.g., batching, DAE, asynchronous operations)."
	  },
	  "cpu_vs_gpu_adaptation_summary": "The recommended optimization techniques are adapted differently to leverage the unique architectural strengths of CPUs and GPUs. \n\n**Parallelism and Divergence Management:**\n*   **CPU:** CPUs exploit parallelism using SIMD instructions (AVX, SVE) on small batches of walkers. Divergence is handled with masked execution and explicit lane compaction instructions (e.g., `VPCOMPRESSD` on AVX-512), which can be expensive if not natively supported. Parallelism across the 256 walkers is managed by a thread pool, often using work-stealing schedulers for load balancing.\n*   **GPU:** GPUs manage parallelism and divergence at the warp level. A group of 32 threads (a warp) executes in lockstep. Divergence leads to idle lanes, which is mitigated by hardware mechanisms like Dynamic Warp Formation (DWF) or Thread Block Compaction (TBC), or by software techniques like persistent threads pulling from a global work queue. The goal is to keep the massive number of execution units saturated.\n\n**Latency Hiding:**\n*   **CPU:** CPUs hide latency with deep caches and out-of-order execution, which enables Memory-Level Parallelism (MLP). However, MLP is limited by the small number of Miss Status Holding Registers (MSHRs, typically 8-16 per core). Software must be structured to expose enough independent memory requests to fill this hardware queue, often via software prefetching or coroutine-based scheduling.\n*   **GPU:** GPUs hide latency through massive thread-level parallelism. When one warp stalls on a memory access, the hardware scheduler instantly switches to another ready warp. The 256 walkers provide 8 warps, which contributes to this latency-hiding capability. Explicit asynchronous memory copies (`cp.async`) into shared memory are a key software technique for overlapping memory transfers with computation.\n\n**Memory Access and Layout:**\n*   **CPU:** Optimizations focus on cache line utilization (64 bytes). Techniques like node packing, pointer elimination (CSB+-Trees), and cache-oblivious layouts (vEB) are designed to maximize the useful data fetched per cache miss. Scattered access is handled by `gather` instructions, which are most efficient on the latest SIMD architectures.\n*   **GPU:** Optimizations focus on memory coalescing. The hardware automatically merges memory requests from threads in a warp into 128-byte transactions. To enable this, software must use a Structure-of-Arrays (SoA) data layout. Failure to do so results in serialized, uncoalesced accesses, which is a major performance killer. Techniques like treelet scheduling and ray reordering are software strategies to group memory accesses by location before execution.",
	  "summary_of_risks_and_tradeoffs": "Implementing advanced optimization techniques introduces significant risks and tradeoffs that must be carefully managed, especially given the short 16-round traversal workload where high startup costs may not be amortized.\n\n*   **Prefetching Overheads:** The most common risk is **cache pollution**. Aggressive or inaccurate prefetching (e.g., speculatively fetching both children in a divergent path) brings useless data into the cache, evicting useful data and potentially increasing the overall miss rate. This also consumes valuable memory bandwidth and can saturate the two available load slots with non-critical requests, delaying demand loads. Furthermore, certain hardware prefetchers that use data values for prediction (DMPs) have been shown to introduce severe security vulnerabilities akin to Spectre.\n\n*   **Sorting and Batching Costs:** Techniques that reorder or batch walkers to improve coherence (e.g., sorting by destination node address) incur a significant computational overhead. A full parallel sort is a non-trivial operation. For a short 16-round traversal, the cost of the sorting pass itself might exceed the performance gained from improved memory access patterns in the subsequent traversal step. This makes lightweight or hybrid batching essential.\n\n*   **Synchronization and Queueing Bottlenecks:** Systems that rely on work queues for scheduling (like persistent threads or DAE) can face contention on the atomic operations used to manage these queues. While techniques like warp-level atomic coalescing can mitigate this on GPUs, a global queue can still become a serialization point. In a Decoupled Access/Execute (DAE) model, there is a high risk of pipeline imbalance, where either the Access or Execute stage becomes a bottleneck, starving the other and leading to underutilization. There is also a risk of deadlock if queues are not sized and managed correctly.\n\n*   **Implementation Complexity and Layout Costs:** Advanced tree layouts like van Emde Boas (vEB) or compressed wide BVHs have a higher one-time construction cost than simple layouts. More importantly, the logic to navigate these structures can be more complex, leading to a higher instruction count per traversal step. For workloads that are small enough to fit in cache, this increased computational cost can make these layouts slower than simpler BFS or DFS layouts. The performance benefit is only realized when the problem is memory-bound.\n\n*   **Hardware-Specific Pathologies:** Some hardware-level optimizations have known failure modes. For example, Dynamic Warp Formation (DWF) on GPUs can suffer from 'starvation eddies,' where a few threads fall behind and degrade SIMD efficiency. This makes more robust (but hypothetical) mechanisms like Thread Block Compaction (TBC) theoretically superior."
	},
	"outputBasis": [
	  {
		"field": "hardware_accelerator_inspired_designs",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "This is\nachieved by sharing the traversal stack among the rays in a warp-\nsized packet, which implies rays will visit (potentially many) nodes\nthey do not intersect but also that memory accesses are very co-\nherent because each node is fetched only once per pack",
			  "We discuss the mapping of elementary ray tracing operations\nacceleration structure traversal and primitive intersectiononto\nwide SIMD/SIMT machines",
			  "he execution of trace()\nconsists of an unpredictable sequence of node traversal and primi-\ntive intersection operations. This unpredictability can cause some\npenalties on CPUs too, but on wide SIMD/SIMT machines it is a\nmajor cause of inefficiency. For example, if a warp 1 executes node\ntraversal, all threads (in that warp) that want to perform primitive\nintersection are idle, and vice versa",
			  "We discuss the mapping of elementary ray tracing operations acceleration structure traversal and primitive intersectiononto wide SIMD/SIMT machines.",
			  "Then we select the operation that has greater support,\nuse prefix sums to compute the SIMD lanes and shared memory\nlocations for rays, and finally shuffle the rays and corresponding\nstates to correct locations.",
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "The idea behind speculative traversal is simple: *if a warp is going*\n*to execute node traversal anyway, why not let all the rays partici-*\n*pate* ? It may be that some rays have already found triangles they\nwould like to test intersections with before (possibly) switching\nback to traversal, but if they dont execute node traversal, theyr"
			]
		  },
		  {
			"title": "Understanding the efficiency of ray traversal on GPUs | Proceedings of the Conference on High Performance Graphics 2009",
			"url": "https://dl.acm.org/doi/10.1145/1572769.1572792",
			"excerpts": [
			  "We discuss the mapping of elementary ray tracing operations---acceleration structure traversal and primitive intersection---onto wide SIMD/SIMT machines.Read more",
			  "Content:"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/items/35c0b361-759c-4075-b59c-0868d50b4780",
			"excerpts": [
			  "We discuss the mapping of elementary ray tracing operations- acceleration structure traversal and primitive intersection-onto wide SIMD/SIMT machines."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "*Abstract**\nWhen programming for GPUs, simply porting a large CPU program\ninto an equally large GPU kernel is generally not a good approach.\nDue to SIMT execution model on GPUs, divergence in control flow\ncarries substantial performance penalties, as does high register us-\nage that lessens the latency-hiding capability that is essential for the\nhigh-latency, high-bandwidth memory system of a GPU. In this pa-\nper, we implement a path tracer on a GPU using a wavefront formu-\nlation, avoiding these pitfalls that can be especially prominent when\nusing materials that are expensive to evaluate. We compare our per-\nformance against the traditional megakernel approach, and demon-\nstrate that the wavefront formulation is much better suited for real-\nworld use cases where multiple complex materials are present in\nthe scene.\n",
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "Path regeneration was first introduced by Novak et al. [2010],\nand further examined with the addition of stream compaction by\nWald [2011], who concluded that terminated threads in a warp incur\nno major performance penalties due to the remaining threads exe-\n ... \nnot affect",
			  "o alleviate this, we coalesce the atomic operations programmati-\ncally within each warp prior to performing the atomic operations.",
			  "This can be done efficiently using warp-wide ballot operations\nwhere each thread sets a bit in a mask based on a predicate, and\nthis mask is communicated to every thread in the warp in one cy-\ncl",
			  "The speedup provided by atomic coalescing is 40% in the total\nrendering speed of Figure 1.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "By producing compact queues of requests for the material and ray\ncast stages, we ensure that each launched kernel always has useful\nwork to perform on all threads of a warp.",
			  "To alleviate this, we coalesce the atomic operations programmati-\ncally within each warp prior to performing the atomic operations.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "By producing compact queues of requests for the material and ray\ncast stages, we ensure that each launched kernel always has useful\nwork to perform on all threads of a warp. Our queues are simple\npreallocated global memory buffers sized so that they can contain\nindices of every path in the pool. Each queue has an item counter\nin global memory that is increased atomically when writing to the\nqueue. Clearing a queue is achieved by setting the item counter to\nzero.",
			  "At queue writes, it would be possible for each thread in a warp to\nindividually perform the atomic increment and the memory write,\nbut this has two drawbacks. First, the individual atomics are not\ncoalesced, so increments to the same counter are serialized which\nhurts performance. Second, the individual atomics from different\nwarps become intermixed in their execution order. While this does\nnot affect the correctness of the results, it results in decreased co-\nherence. For example, if the threads in a logic kernel warp all have\npaths hitting the same material, placing each of them individually in\nthe corresponding material queue does not ensure that they end up\nin consecutive queue entries, as other warps can push to the queue\nbetween them.",
			  "To alleviate this, we coalesce the atomic operations programmati-\ncally within each warp prior to performing the atomic operations.\nThis can be done efficiently using warp-wide ballot operations\nwhere each thread sets a bit in a mask based on a predicate, and\nthis mask is communicated to every thread in the warp in one cy-\ncle. The speedup provided by atomic coalescing is 40% in the total\nrendering speed of Figure 1. The logic kernel speedup is 75%, new\npath kernel speedup is 240%, and material kernel speedup is 35%."
			]
		  },
		  {
			"title": "Megakernels considered harmful | Proceedings of the 5th High-Performance Graphics Conference",
			"url": "https://dl.acm.org/doi/10.1145/2492045.2492060",
			"excerpts": [
			  "When programming for GPUs, simply porting a large CPU program into an equally large GPU kernel is generally not a good approach. Due to SIMT execution model on GPUs, divergence in control flow carries substantial performance penalties, as does high register us-age that lessens the latency-hiding capability that is essential for the high-latency, high-bandwidth memory system of a GPU. In this paper, we implement a path tracer on a GPU using a wavefront formulation, avoiding these pitfalls that can be especially prominent when using materials that are expensive to evaluate. We compare our performance against the traditional megakernel approach, and demonstrate that the wavefront formulation is much better suited for real-world use cases where multiple complex materials are present in the scene"
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing on GPUs | Research",
			"url": "https://research.nvidia.com/publication/2013-07_megakernels-considered-harmful-wavefront-path-tracing-gpus",
			"excerpts": [
			  "When programming for GPUs, simply porting a large CPU program into an equally large GPU kernel is generally not a good approach.",
			  "Due to SIMT execution model on GPUs, divergence in control flow carries substantial performance penalties, as does high register usage that lessens the latency-hiding capability that is essential for the high-latency, high-bandwidth memory system of a GPU.",
			  "In this paper, we implement a path tracer on a GPU using a wavefront formulation, avoiding these pitfalls that can be especially prominent when using materials that are expensive to evaluate.",
			  "We compare our performance against the traditional megakernel approach, and demonstrate that the wavefront formulation is much better suited for real-world use cases where multiple complex materials are present in the scene."
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization.",
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence."
			]
		  },
		  {
			"title": "Embree: A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://cseweb.ucsd.edu/~ravir/274/15/papers/a143-wald.pdf",
			"excerpts": [
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "Single-Ray Vectorization**\nOther work has focused on using vectorization to accelerate spa-\ntial data structure traversal for individual rays. For example, multi-\nbranching BVH structures enable multiple nodes or primitives to\nbe tested for intersection against the same ray in parallel. Quad-\nbranching BVH (BVH4) data structures perform well with 4-wide\nand 16-wide vector units [Ernst and Greiner 2008; Dammertz et al.\n2008; Benthin et al. 2012], but higher branching factors offer di-\nminishing returns [Wald et al. 2008].\nGenerally speaking, single ray vectorization is faster than packet\ntracing for incoherent ray distributions (and can be used within a\nscalar renderer), but is slower for coherent rays. For this reason, hy-\nbrid techniques have been developed which can dynamically switch\nbetween packet tracing where rays are coherent, and single-ray vec-\ntorization where not [Benthin et al. 2012",
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "For this reason, we allow a given BVH type in Embree to be used\nwith both single-ray and packet traversal and intersection kernels,\nenabling the implementation of a hybrid ray traversal mode. This\nmode begins traversal using packets, and dynamically switches to\nsingle-ray traversal when the number of active rays in a packet falls",
			  "cket tracing is conceptually simple compared to single-ray SIMD\ntraversal and intersection. In classical packet tracing, rays in a given\npacket are intersected with the same BVH node or triangle at each\nstep of traversal. However, it is also possible to implement packet\ntracing following a single program multiple data (SPMD) program-\nming model. Here, the rays of a packet are independently traversed\nthrough the BVH, and each ray is potentially tested against different\nBVH nodes or triangles [Aila and Laine 2009].",
			  "**Hybrid Traversal and Intersection**",
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal",
			  "and intersection performance can benefit from dynamically switch-",
			  "ing between packet and single-ray kernels [Benthin et al. 2012].",
			  "For example, the Embree packet intersection kernels test multiple",
			  "rays against a single triangle while the single-ray kernels typically",
			  "intersect a single ray with multiple triangles. The storage order",
			  "For this reason, we allow a given BVH type in Embree to be used",
			  "with both single-ray and packet traversal and intersection kernels,",
			  "enabling the implementation of a hybrid ray traversal mode. This",
			  "mode begins traversal using packets, and dynamically switches to",
			  "single-ray traversal when the number of active rays in a packet falls",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone."
			]
		  },
		  {
			"title": "A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree.pdf",
			"excerpts": [
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "**Hybrid Traversal and Intersection**",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone."
			]
		  },
		  {
			"title": "(PDF) Combining Single and Packet-Ray Tracing for Arbitrary Ray Distributions on the Intel MIC Architecture",
			"url": "https://www.researchgate.net/publication/51798993_Combining_Single_and_Packet-Ray_Tracing_for_Arbitrary_Ray_Distributions_on_the_Intel_MIC_Architecture",
			"excerpts": [
			  "The idea is to start traversal with packets, and switch to single-ray traversal when too many rays in the packet are inactive.",
			  "This hybrid algorithm outperforms conventional packet tracers by up to 2X.",
			  "The solution to this problem is to combine packet traversal with single-ray traversal [Ben+12] to form a hybrid algorithm.",
			  "Wide-SIMD hardware is power and area efficient, but it is challenging to efficiently map ray tracing algorithms to such hardware especially when the rays are incoherent.",
			  "The two most commonly used schemes are either packet tracing, or relying on a separate traversal stack for each SIMD lane.",
			  "This method maps well\nto current CPU designs, but for incoherent rays suffers from\nlow SIMD utilization when too many rays get inactive.",
			  "A hybrid approach that uses compaction\nof large ray sets and uses a similar idea of a fall back to single\nray tracing in case no coherence is detected was presented\nby Tsakok et al. [20].",
			  "Unfortunately, single-ray traversal is not as e cient as packet traversal for coherent rays: It fails to take advantage of the fact that similar rays traverse similar parts of the tree."
			]
		  },
		  {
			"title": "Parallel Tree Traversal for Nearest Neighbor Query on the ...",
			"url": "http://dicl.skku.edu/publications/icpp2016.pdf",
			"excerpts": [
			  "Traversing hierarchical tree structures in an irregular manner makes it difficult to exploit parallelism since GPUs are tailored for deterministic memory ...Read more"
			]
		  },
		  {
			"title": "General Transformations for GPU Execution of Tree ...",
			"url": "https://engineering.purdue.edu/~milind/docs/sc13.pdf",
			"excerpts": [
			  "Tree traversal algorithms represent an interesting target for GPU\nparallelization. As naturally parallel algorithms (the points traver-\nsals of the tree are independent), they exhibit the massive paral-\nlelism that GPUs excel at.",
			  "However, because the tree structures\nare irregular, and the points traversals are input-dependent, sim-\nply running multiple traversals simultaneously on the GPU can-\nnot take advantage of efficient memory accesses, seriously hinder-\ning performance (Section 2.2 discusses GPU architectures and the\nGPU performance model in more detail).",
			  "ue to these character-\nistics, there have been several attempts to run tree-traversal algo-\nrithms on GPUs, but they have largely relied on algorithmic tweaks\nand application-specific optimizations to achieve reasonable per-\nformance [2, 57, 21]. This paper addresses the open question\nof whether there are *general, systematic, semantics-agnostic tech-*\n*niques* to map traversal codes to GPUs.",
			  "In this paper, we show that tree traversal algorithms have several\ncommon properties that can be exploited to produce efficient GPU\nimplementations.",
			  "chieve this\nthroughput, the GPU memory controller requires threads within the\nsame warp to access contiguous regions of memory so that a sin-\ngle wide block of memory can be transferred in a single transac-\ntion.",
			  "When threads access non-contiguous elements, accesses must\nbe serialized into multiple transactions, decreasing throughput.",
			  "For example, if all threads in a warp issue a load to the\nsame memory address, only a single transaction is sent to the mem-\nory controlle",
			  "In this paper, we show that tree traversal algorithms have several common properties that can be exploited to produce efficient GPU implementations. Crucially, ...Read more"
			]
		  },
		  {
			"title": "Generalizing Ray Tracing Accelerators for Tree Traversals ...",
			"url": "https://intra.engr.ucr.edu/~htseng/files/2024MICRO-TTA.pdf",
			"excerpts": [
			  "RTAs in GPUs are\ndesigned to mitigate these inefficiencies, and RTIndeX [ 34 ] and\nRTNN [ 105 ] have shown that RTAs can be adapted to support\ngeneral-purpose tasks.",
			  "Figure 1 shows the average SIMT efficiency (percent of\nactive threads per warp due to control flow divergence) and\nDRAM bandwidth utilization of several tree traversal applica-\ntions on GPUs, profiled on an NVIDIA RTX 2080 Ti GPU and\nalso measured using Vulkan-Sim [ 83 ] with configurations list",
			  "ire ray traversal as a single instruction (** **traceRay** **)**\n**on the GPU.** When threads diverge in the tree traversal,\nRTAs handle the control flow divergence directly, avoiding\ninactivating computing lanes in the SIMD pipeline [ 83 ]. Thus,\nthe warps only need to synchronize the rays at the end of\nthe traversal when RTAs complete the instruction",
			  ") RTAs use a dedicated hardware memory scheduler**\n**to improve memory bandwidth utilization.** This scheduler\ncoalesces node requests between threads when possible and\narbitrates for one memory request into the GPU memory system\nper cycle. The dedicated memory scheduler better handles\nirregular memory access patterns by only focusing on node\nrequests, allowing the scheduler to track more concurrent\ntraversals and increase DRAM utilization by nearly ",
			  " general-purpose cores for performing**\n**other tasks in parallel, effectively hiding the latency of**\n**ray traversal [** **13** **].** With the RTA, the compute cores can\nexecute shading tasks for completed traversals while the RTA\ntraverses the BVH for other rays. The GPU architecture can\nfurther exploit this parallelism if the RTA can support more\ntree traversal algorithm",
			  "RTAs apply the same traversal algorithm in ray tracing\nas the abstraction used by other tree structures illustrated\n ... \nSIMT architecture",
			  "Barringer et al. [ 11 ]\nintroduced a traversal algorithm that leverages ray coherence\nfor high SIMD efficiency, but packet tracing is ineffective for\nincoherent rays. Shkurko et al. [ 85 ] proposed a dual streaming\napproach that organizes ray-tracing memory accesses into\ntwo predictable data streams, and Vasiou et al. [ 90 ] analyzed\nthe energy and time cost of data movement for ray tracing.\nThese optimizations can be applied orthogonally to our work.",
			  "In contrast, RTAs are introduced to address\nthese limitations by providing a dedicated hardware accelerator\nspecifically for ray tracing. Our work focuses on generalizing\nRTAs for diverse tree traversal algorithms, enabling more\napplications to be accelerated.",
			  "Prior works have explored accelerating tree-based appli-\ncations on FPGAs and ASICs, such as k-nearest neighbor\nsearch [ 56 ], [ 86 ], dynamic search tree [ 98 ], and ray tracing [ 48 ],\n[ 66 ",
			  "RTIndeX [ 34 ] and\nRTNN [ 105 ] have shown that RTAs can be adapted to support\ngeneral-purpose tasks."
			]
		  },
		  {
			"title": "REGTT: Accelerating Tree Traversals on GPUs by ...",
			"url": "https://cgi.cse.unsw.edu.au/~jingling/papers/icpp16.pdf",
			"excerpts": [
			  "Repeated tree traversals for multiple queries can be solved\nin parallel on GPUs in the SIMT fashion as all the queries are\nhandled independently.",
			  "There are three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The thre",
			  "OROPES [11] represents the only solution for paralleliz-\ning general-purpose tree traversal algorithms on GPUs",
			  "re three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access overhead.\n** **Load Imbalance** Different threads in the same warp may\nhave different workloads due to query-dependent",
			  "presents the only solution for paralleliz-\ning general-purpose tree traversal algorithms on GPUs. A U -\nTOROPES parallelizes an algorithm by applying DFT (Depth-\nFirst Traversal) in either *lockstep* or *non-lockstep* mode. In\n*non-lockstep* , A UTOROPES aims to maximize GPU resource\nutilization by allowing different threads that handle different\nqueries in the same warp to visit different tree nodes at the\nsame time at the expense of branch divergence and memory-\naccess irregularity. In *lockstep* , A UTOROPES makes the oppo-\nsite tradeoff by ensuring that all the threads in the same warp\nvisit the same tree node at the same time, even though many\nthreads are idle traversing a truncated node or no node",
			  "e three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access over",
			  "To reduce cache misses caused by irregular tree traversals,\nsorting the queries into some order was tried before, such\nas the domain-specific efforts made for ray tracing [14] and\nn-body simulations [19].",
			  "improve memory ac-\ncess efficiency by enhancing memory coalescing as follows:\n** When constructing a search tree, we store its nodes\nconsecutively in the global memory according to the order\nin which they are likely to be accessed. The nodes at\ndepths less than *d* are stored in the BFT order. The\nremaining ones are stored in the DFT ord",
			  "In tree traversals, the same field of different struct objects\nis often requested simultaneously by multiple threads,\nwhich may cause great memory access overhead if the\nrequests cannot be coalesced. In order to avoid this, we\napply an AoS (Array of Structures) to SoA (Structure\nof Arrays) transformation when the tree data are copied\nfrom the host main memory to the device global memory."
			]
		  },
		  {
			"title": "SIMD Parallelization of Applications that Traverse Irregular ...",
			"url": "https://www.cs.wm.edu/~bren/files/papers/CGO13.pdf",
			"excerpts": [
			  "We address this problem by developing an intermediate\nlanguage for specifying such traversals, followed by a run-\ntime scheduler that maps traversals to SIMD units",
			  " key\nidea in our run-time scheme is converting branches to arith-\nmetic operations, which then allows us to use SIMD hard-\nwa",
			  "In order to make our approach fast, we demonstrate\nseveral optimizations including a stream compaction method\nthat aids with control flow in SIMD, a set of layouts that\nreduce memory latency, and a tiling approach that enables\nmore effective prefetching.",
			  "ine-grained data parallelism is increasingly common in\nmainstream processors in the form of longer vectors and on-\nchip GPUs.",
			  "This paper develops support for exploiting such\ndata parallelism for a class of non-numeric, non-graphic\napplications, which perform computations while travers-\ning many independent, irregular data structure",
			  "Our work has considered specific challenges arising\nfor pointer-based traversals, which have not been considered\nin the past."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://users.aalto.fi/~laines9/publications/aila2012tr1_paper.pdf",
			"excerpts": [
			  "Aila and Laine [AL09] hypothesize that it might be ben- eficial to replace terminated rays once the SIMD utilization drops below a threshold. In practice this ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.researchgate.net/publication/221249082_Understanding_the_Efficiency_of_Ray_Traversal_on_GPUs",
			"excerpts": [
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal. Aila and Laine [2009] proposed to increase SIMD efficiency by replacing already finished rays with new ones from a global queue.",
			  " and Laine [2009] proposed an efficient stack-based traversal algorithm of bounding volume hierarchies on the GPU with persistent warps and dynamic",
			  "Techniques such as speculative traversal slightly increase the redundancy of ray intersection tests because they work on possibly terminated rays.",
			  "Aila and Laine [2009] proposed to increase SIMD efficiency by replacing already finished rays with new ones from a global queue. Techniques such as speculative ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "rsistent threads**\nLaunch only enough threads to fill the machine\nonce\nWarps fetch work from global pool using atomic\ncounter until the pool is empty\nBypasses hardware work distribution\nSimple and generic solution\nPseudocode in the paper",
			  "Primary\n149.2\n122.1\n82\nAO\n100.7\n86.1\n86\nDiffuse\n36.7\n32.3\n88\n~2X performance from persistent threads\n~85% of simulated, also in other scenes\nHard to get much closer\nOptimal dual issue, no resource conflicts, infinitely\nfast memory, 20K threads",
			  "Persistent while-while",
			  "Speculative traversal",
			  "If a warp is going to execute node traversal\nanyway, why not let all rays participate?\nAlternative: be idle\nCan perform redundant node fetches\nShould help when not bound by memory speed\n5-10% higher performance in primary and AO\nNo improvement in diffuse\nDisagrees with simulation (10-20% expected)\nFirst evidence of memory bandwidth issues?\nNot latency, not computation, not load balancing",
			  "Two further improvements",
			  "Currently these are slow because crucial\ninstructions missing\nSimulator says 2 warp-wide instructions will help\nENUM (prefix-sum)",
			  "\nWhat limits the performance of fastest traversal\nmethods on GPUs?\n",
			  "?\nMemory speed (bandwidth, latency)?",
			  "Resource conflicts (serialization, scoreboard, )?\nL",
			  "?\nHow far from theoretical optimum?",
			  "How much performance on table?\nS",
			  "\nSolutions that help today\n",
			  "\nSolutions that may help tomorrow\n",
			  "\nSIMD with execution divergence handling built into\nhardware\n",
			  "\nGroup of threads that execute simultaneously in a\nSIMD/SIMT unit, 32 in NVIDIA hardware\n",
			  "npredictable sequence of *acceleration structure*\n*traversal* and *primitive intersecti",
			  "Understanding the Efficiency of Ray Traversal on GPUs. Timo Aila Samuli Laine. NVIDIA Research. Page 2. Agenda. What limits the performance of fastest traversal.Read more",
			  "Persistent threads**\nLaunch only enough threads to fill the machine\nonce\nWarps fetch work from global pool using atomic\ncounter until the pool is empty\nBypasses hardware work distribution\nSimple and generic solution\nPseudocode in the pa",
			  "**Speculative traversal**\nIf a warp is going to execute node traversal\nanyway, why not let all rays participate?\nAlternative: be idle\nCan perform redundant node fetches\nShould help when not bound by memory speed\n5-10% higher performance in primary and AO\nNo improvement in diffuse\nDisagrees with simulation (10-20% expected)\nFirst evidence of memory bandwidth issues?\nNot latency, not computation, not load balancing\n",
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed",
			  "**2. Local work queues**\nAssign 64 rays to a 32-wide warp\nKeep the other 32 rays in shared mem/registers\n32+ rays will always require either node traversal or\nprimitive intersection\nAlmost perfect SIMD efficiency (% threads active)\nShuffling takes time\nToo slow on GTX285\nWith ENUM + POPC, in Fairy scene\nAmbient occlusion +40%\nDiffuse +80%\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "\nThis paper implements a path tracer on a GPU using a wavefront formulation, avoiding pitfalls that can be especially prominent when using materials that are expensive to evaluate and demonstrates that the wavefront formulations is much better suited for real-world use cases where multiple complex materials are present in the scene. ",
			  " Megakernels considered harmful: wavefront path tracing on GPUs\n",
			  "Active thread compaction for GPU path tracing](",
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "This paper implements a path tracer on a GPU using a wavefront formulation, avoiding pitfalls that can be especially prominent when using materials that are ..."
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays",
			"url": "https://users.aalto.fi/~ailat1/publications/aila2010hpg_paper.pdf",
			"excerpts": [
			  "his paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for*\n*global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration*\n*structure and repeated queueing/postponing of rays to reduce cache pressure",
			  " show that our architecture can reduce the total memory bandwidth*\n*requirements by up to 90% in difficult scenes. Furthermore the architecture allows submitting rays in an arbitrary*\n*order with practically no performance penalty.",
			  "e describe a heuristic algorithm*\n*for determining the treelet subdivision, and show that our architecture can reduce the total memory bandwidth*\n*requirements by up to 90% in difficult scenes. Furthermore the architecture allows submitting rays in an arbitrary*\n*order with practically no performance pen",
			  "heduling algorithms can have an important*\n*effect on results, and that using fixed-size queues is not an appealing design choice. Increased auxiliary traffic,*\n*including traversal stacks, is identified as the foremost remaining challenge of this architectur",
			  "Traversal and intersection require approx-\nimately 30 registers for each thread (32 bits per register);\nabout 12 for storing the ray and ray state (Table 2 ), and the\nrest for intermediate results during computatio",
			  "This paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for global illumination.Read more"
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays | Research",
			"url": "https://research.nvidia.com/publication/2010-06_architecture-considerations-tracing-incoherent-rays",
			"excerpts": [
			  "This paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration structure and repeated queueing/postponing of rays to reduce cache pressure.",
			  "We describe a heuristic algorithm for determining the treelet subdivision, and show that our architecture can reduce the total memory bandwidth requirements by up to 90% in difficult scenes.",
			  "Furthermore the architecture allows submitting rays in an arbitrary order with practically no performance penalty.",
			  "We also conclude that scheduling algorithms can have an important effect on results, and that using fixed-size queues is not an appealing design choice.",
			  "Increased auxiliary traffic, including traversal stacks, is identified as the foremost remaining challenge of this architecture."
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays",
			"url": "https://dl.acm.org/doi/pdf/10.5555/1921479.1921497",
			"excerpts": [
			  "is paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for*\n*global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration*\n*structure and repeated queueing/postponing of rays to reduce cache pressure.",
			  " describe a heuristic algorithm*\n*for determining the treelet subdivision, and show that our architecture can reduce the total memory bandwidth*\n*requirements by up to 90% in difficult scenes.",
			  "Furthermore the architecture allows submitting rays in an arbitrary*\n*order with practically no performance penalty.",
			  "We also conclude that scheduling algorithms can have an important*\n*effect on results, and that using fixed-size queues is not an appealing design choice.",
			  "Increased auxiliary traffic,*\n*including traversal stacks, is identified as the foremost remaining challenge of this architecture."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  ". Aila et al. [ 2 ] proposed group-\ning rays into ray packets and traversing rays together, improving\nmemory coherence. ",
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr",
			  "Ray Sorting / Reordering.* ",
			  "Pharr et al. [ 44 ]\nintroduced the idea of reordering ray computation to improve ray\ncoherency and increase cache utilization. Garanzha and Loop [ 16 ]\nsorted rays based on ray origin and direction before processing them\nin packets. Moon et al. [ 37 ] took a different approach by sorting\nrays based on their final hit points. Meister et al. [ 34 ] improved\non sorting heuristics to further minimize ray incoherence. T",
			  "Acceleration Structure Optimizations.* Some works optimize the",
			  "\nIntersection Prediction for Accelerated GPU Ray Tracing\nMICRO 21, October 1822, 2021, Virtual Event, Greece"
			]
		  },
		  {
			"title": "Batching of divergent rays on GPU architectures",
			"url": "https://studenttheses.uu.nl/bitstream/handle/20.500.12932/41250/thesis_final.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "A number of advanced traversal algorithms have been devised in recent years that increase ray coherence by bundling active rays together dynamically as they ...Read more",
			  "Benthin et al.[ Ben+12 ] devised a hybrid traversal model that combines ray packets and single ray\ntraversal.",
			  "**2.2. Ray Batching**\nPharr et al sought to address the problem as a scheduling challenge [ Pha+97 ]. They developed a system\ndesigned to handle scenes far too large for system memory, based on voxels. When tracing through a\ngiven voxel, required data would need to be explicitly fetched from disc before tracing could continue\ninto that segment. They also ran into issues with the sheer volume of rays that are spawned by the\ntree structure of by Whitted-style ray tracing [ Whi79 ], especially when they scheduled a large number\nof active concurrent traces rather than the traditional depth-first implementation. Their schedul"
			]
		  },
		  {
			"title": "Cache-friendly micro-jittered sampling",
			"url": "https://inria.hal.science/hal-01325702/file/cacheFriendlyHemisphereSamplingUsingBlueNoiseConservativeJittering.pdf",
			"excerpts": [
			  "GARANZHA, K., AND LOOP, C. 2010. Fast ray sorting and breadth-first packet traversal for gpu ray tracing. In Computer. Graphics Forum, vol. 29, ..."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for GPU Ray Tracing | Request PDF",
			"url": "https://www.researchgate.net/publication/220507044_Fast_Ray_Sorting_and_Breadth-First_Packet_Traversal_for_GPU_Ray_Tracing",
			"excerpts": [
			  "Garanzha and Loop (2010) devised breadth-first packet traversal algorithms using fast ray sorting via a compress-sort-decompress scheme to generate ray packet bundles.",
			  "Garanzha et al. [GL10] present a similar technique that they call Compress-Sort-Decompress (CSD). Their goal is to find 3D (or 5D) clusters in a frame buffer, which are used to form ray packets.",
			  "Their reorder shading operations using prefix sum and radix sort operations. Garanzha and Loop (2010) devised breadth-first packet traversal algorithms using fast ray sorting via a compress-sort-decompress scheme to generate ray packet bundles.",
			  "Garanzha and Loop [2010] reported that the GPU path tracing performance increased up to 1.9 when ray sorting was enabled.",
			  "ions. The breadthfirst packet traversal method employs a hash-based approach inspired by Arvo and Kirk's 5D ray tracing [AK87]. ",
			  ". Garanzha and Loop (2010) devised breadth-first packet traversal algorithms using fast ray sorting via a compress-sort-decompress scheme to generate ray packet",
			  "The breadthfirst packet traversal method employs a hash-based approach inspired by Arvo and Kirk's 5D ray tracing [AK87].",
			  "Garanzha and Loop (Garanzha and Loop, 2010) introduced an algorithm using parallel primitives to adapt the ray-tracing algorithm to the GPU.Read more"
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation.",
			  "For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level."
			]
		  },
		  {
			"title": "Treelet Accelerated Ray Tracing on GPUs",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/chou.asplos2025.pdf",
			"excerpts": [
			  "This work explores how to efficiently implement treelet\nqueues on modern ray tracing capable GPUs. We propose\nVirtualized Treelet Queues, an architecture that increases\nthe number of concurrent rays in flight and treelet queues\nthat dynamically switch between treelet and ray stationary\ntraversal modes to increase efficiency.",
			  "ay virtualization greatly increases the\nnumber of concurrent rays in flight which would normally\nbe limited by the amount of warp slots in the RT unit since\neach ray is executed by a thread. We achieve this by ter-\nminating raygen shaders after a thread issues its trace ray\ninstruction to the RT unit, allowing the GPU to reclaim the\nCUDA cores and launch new threads / raygen shaders from\nalready queued up Cooperative Thread Arrays (CTAs)",
			  "o\ntake advantage of the increase in concurrent rays, we imple-\nment dynamic treelet queues in the GPUs RT unit to group\nup rays that access the same treelet together, achieving better\ncache locality and reduced miss rates",
			  "During later phases of\nray traversal when rays diverge more, treelet queues end up\nunderpopulated and it becomes inefficient to process rays\nin treelets. We propose to group up these underpopulated\ntreelet queues and traverse them regularly instead.",
			  "We propose to group up these underpopulated\ntreelet queues and traverse them regularly instead.",
			  "How-\never, this leads to a sharp decrease in SIMT efficiency due\nto varying BVH node access counts from different ray",
			  "We\napply warp repacking to regroup active rays together into a\nnew warp, boosting SIMT efficiency and performance.",
			  "With\nall optimizations, virtualized treelet queues achieve up to\n2.55  better path tracing performance under usage scenarios\ncomparable to video games on ray tracing capable GPUs.",
			  "a et al. [ 5 ]\nproposed to subdivide the BVH tree into smaller subtrees, or\ntreelets, that fit in the processors cache to reduce memory\ntraff",
			  "Treelet queues essentially achieve a similar goal by grouping\nup rays based on their accessed treelet, but without the high\noverhead.",
			  "Ray virtualization increases the number of concurrent rays in flight to cre- ate more cache reuse opportunities by terminating raygen shaders ...Read more"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://meistdan.github.io/publications/bvh_star/paper.pdf",
			"excerpts": [
			  "nd Kar-\nras [ AK10 ] propose a SIMD architecture for ray tracing, which is\ndesigned to reduce arithmetic and memory divergence for incoher-\nent ray distributions. The design consists of a set of *processors* ,\nwhich execute *warps* consisting of multiple threads, similar to con-\nventional GPUs. Each thread manages one ray. The authors note\nthat the design could accommodate fixed-function traversal units,\nbut choose to focus on the contribution of reducing memory traf",
			  " BVH is divided into *treelets* (small subtrees within the to-\ntal BVH) which are set to the size of either the L1 or L2 cache. The\narchitecture maintains a set of ray *queues* , with queues assigned\nto treelets at runtime. Rays begin tracing at the root treelet, and\nas rays cross treelet boundaries, they are placed in the ray queues\nto be processed later when their required treelet is present in the\ncache. The architecture thus attempts to maximize the number of\nrays which are processed each time a treelet is loaded on-chip, re-\nducing memory bandwidth",
			  "To reduce stack traffic, the architecture\nmaintains a stack-top cache on chip, while keeping the remainder\nin DRAM. A second key innovation of this work is the inclusion\nof *work compaction* logic which detects when the SIMD utilization\nof a given warp has fallen below an efficient level, at which point\nthe unit terminates the warp and diverts the remaining active rays\nto the *launcher* which is responsible for warp creation.",
			  "Yoon and Manocha [YM06] proposed a node layout algorithm known as cache-oblivious BVH (COLBVH) that recursively de- composes clusters of nodes and works ...Read more",
			  "d Manocha [ YM06 ] proposed a node layout algorithm\nknown as *cache-oblivious BVH* (COLBVH) that recursively de-\ncomposes clusters of nodes and works without prior knowledge\nof the cache, such as the block size. In initialization, each node\nis assigned the probability that the node is accessed, given that the\nclusters root is already accessed. The cluster with *|N|* nodes is\nthen decomposed into **\n\n*|N|* + 1 + 1 ** smaller ones by merging\nthe nodes with the highest probability into a root cluster. Next, the\ndecomposed clusters are ordered considering their spatial locality.\nThe root cluster is placed at the beginning, and the remaining child\nclusters are ordered according to their original BVH positions, from\nleft to right, in a multi-level depth-first layout. The same process is\nrecursively applied to chil",
			  "odniok et al. [ WSWG13 ] proposed new layouts: *swapped*\n*subtrees* (SWST) and *treelet-based depth-first-search/breadth-first-*\n*search* (TDFS/TBFS). These layouts are determined based on the\nnode access statistics obtained by casting a small number of sample\nrays in a preprocessing step. SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged. The latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order. The authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.\nHowever, none of these layouts is always better",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH. The key difference is the use of two different types of\nclusters to further reduce bandwidth and cache misses. The BVH\nis first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit within",
			  " The BVH\nis first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit within",
			  "differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order. The authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.",
			  "The root cluster is placed at the beginning, and the remaining child\nclusters are ordered according to their original BVH positions, from\nleft to right, in a multi-level depth-first layout.",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH.",
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Tree traversal is an intensive pointer-chasing operation, requiring traversing to a node in the tree and finding the child pointers, before being able to find the child node addresses and issue loads.",
			  "With treelet prefetching, as rays traverse the BVH tree and visit the root node of treelets, corresponding treelets can be prefetched to load deeper levels of the tree before they are needed.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulations show treelet based traversal reduces performance slightly by 3.7% over a DFS baseline. However, when combined with treelet prefetching, the overall speedup reaches 32.1% while maintaining the same power consumption.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping rays that traverse similar parts of the AS. Pharr et al. [ [39]() ] reordered ray computation to improve ray coherency and cache utilization. Garanzha and Loop [ [16]() ] sorted rays based on ray origin and direction before processing in packets. Moon et al. [ [32]() ] sorted rays with their final hit points. Meister et al. [ [30]() ] improved sorting heuristics to minimize ray divergence.",
			  "Ray sorting can be applied orthogonally to our work. However modern ray tracing APIs such as Vulkan and DXR generate rays dynamically in the ray generation shader, thus rays are not readily available to sort before the ray tracing kernel.",
			  "**Acceleration Structure Optimizations.** Ylitie et al. [ [48]() ] explored wide BVH trees to increase SIMD utilization. Lin et al. [ [26]() ] restructured BVH nodes with node splitting, reducing memory footprint. Benthin et al. [ [10]() ] and Liktor et al. [ [25]() ] perform BVH compression for memory bandwidth reduction. BVH optimizations benefit our work as more nodes fit into the same memory footprint, making prefetching more effective.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache. Ray tracing is a pointer-chasing application and memory accesses are divergent and hard to predict. With the treelet based traversal algorithm introduced previously, memory accesses are now clustered as individual treelets, making it possible to prefetch easily.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "... BVH tree statistics for each scene are outlined in Table 2. The scene ... We use the concept of treelets which are connected subpartitions of a BVH tree.Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "We form treelets by grouping connected BVH nodes to maximize the size of each treelet. It is a greedy algorithm that starts from the BVH root node and greedily adds nodes to the current treelet until the maximum treelet size is reached.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "We propose to add a treelet prefetcher that prefetches treelets into the L1 cache of the GPU based on the rays in the warp buffer.",
			  "The threshold comparator generates the prefetch enable signal if the treelet popularity is greater than a manually set threshold which ranges from 0 to the maximum number of rays in the warp buffer ().",
			  "The prefetch enable is ANDed with the upper bits of the treelet root node address to generate the treelet prefetch address and sent to the prefetch queue to be processed ().",
			  "We only require the upper bits of the treelet root node address because the treelets have a fixed maximum size and nodes within a treelet are organized to be packed together in memory.",
			  "The treelet prefetcher also records the address of the last treelet it prefetches to avoid pushing duplicate treelet addresses to the prefetch queue and prefetching the same treelet multiple times in a row.",
			  "We combine treelet prefetching with a treelet based traversal algorithm in the ray tracing accelerator to further reduce ray traversal latenc",
			  "\nRay traversal is typically done by traversing the BVH tree in a depth-first or breadth-first manner",
			  "This section describes our proposed treelet prefetching technique for ray tracing.",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "As a ray visits a treelet root node, its subsequent memory accesses will also be to the nodes in the treelet since accesses to nodes from different treelets are deferred to the *otherTreeletStack",
			  "Thus, we can prefetch the entire treelet to the GPU's cache and reduce the latency of accessing nodes in the current treelet.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "[15] Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First\nPacket Traversal for GPU Ray Tracing. *Computer Graphics Forum* (2010).",
			  "[30] Daniel Meister, Jakub Boksansky, Michael Guthe, and Jiri Bittner. 2020. On Ray\nReordering Techniques for Faster GPU Ray Tracing. In *Proc. ACM SIGGRAPH*\n*Symp. on Interactive 3D Graphics and Games (I3D)* . 19.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "d\nTreelet Prefetching For Ray Tracing\nMICRO 23, October 28November 01, 2023, Toronto, ON, Canada\nmore frequently than the lower levels. In the next sections, we pro-\npose a treelet based ray traversal algorithm with an accompanying\nprefetcher.",
			  "s a ray visits a treelet root node, its subse-\nquent memory accesses will also be to the nodes in the treelet since\naccesses to nodes from different treelets are deferred to the *oth-*\n*erTreelet",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "refetching techniques can\nbe used to improve memory latency tolerance in GPGPU applica-\ntions [ 22 , 24 , 42 ]",
			  "a treelet prefetcher to the RT unit to speed\nup ray traversal along with a prefetch queue to hold the issued\nprefetch addresses, both of which are highlighted in red in Figur",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "In Vulkan-Sim [ 41 ], the simulator we use\nfor evaluation, the ray tracing accelerator is referred to as the RT\nunit. When a warp issues a trace ray instruction, it enters the RT\nunit during the pipelines execute stage and is queued in the warp\nbuffer, which holds ray metadata for all 32 threads of the warp.",
			  "This work presents a treelet prefetching scheme to improve ray\ntraversal performance.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS. Pharr et al. [ 39 ] reordered\nray computation to improve ray coherency and cache utilization.",
			  "**7.3**\n**Data Structure Partitioning**\nFeng et al. [ 13 ] take advantage of data structure partitioning to ex-\nploit parallelism. Within parallel regions during traversal identified\nby the programmer with compiler pragmas, they subdivide graphs\nand trees and distribute each partition to be processed by different\ncores to improve parallelism for CPUs. Locality improves by hav-\ning the same cores process the same partitions repeatedly.",
			  "**7.4**\n**Treelet Based Ray Tracing Techniques**\nNavratil et al. [ 33 ] tackled incoherent rays by collecting rays into\n ... \nstaging buffer which might require non-trivial shader modifications\nto realize on a GPU and are not discussed in their paper.",
			  "**7.5**\n**Ray Traversal Acceleration Techniques**\n**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "We propose to add\na treelet prefetcher that prefetches treelets into the L1 cache of\nthe GPU based on the rays in the warp buffer.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict.",
			  "With 512B treelets, the treelet\nbased memory layout performs best with a 31.9% speedup over\nthe baseline.",
			  "Loose and Strict Wait represent a rough upper and\nlower bound performance when using an unmodified BVH tree that\nrequires mapping table loads, falling behind the repacked BVH with\nonly a 29.7% speedup and 2.5% slowdown respectively.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The original BVH node\nlayout is allocated to be 64 bytes and stores the bounding box,\npointers, and other metadata for the child nodes of a 6-wide BVH\ntree.",
			  "It is a greedy algorithm that starts from the BVH\nroot node and greedily adds nodes to the current treelet until the\nmaximum treelet size is reached. T",
			  "Figure 3 shows\nan example of a two-wide BVH tree partitioned into treelets with a\nmaximum size of 4 nodes each.",
			  "reelet formation initializes the *remainingBytes* to the maximum\ntreelet size and adds the BVH root address to the *pendingTreelets*\nqueue and traversal sta"
			]
		  },
		  {
			"title": "Efficient SIMD Single-Ray Traversal using Multi-branching ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/wald08_widebvh.pdf",
			"excerpts": [
			  "In this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.\nNote that this forces every multi-node to contain 16 node slots even\nif the parent node has less than 16 children; any unused nodes are\nflagged as invalid. ",
			  "\n**M** **ETHOD** **O** **VERVIEW**\nIn this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.",
			  "The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  "the optimal branching factor is somewhere between 4 and 8.",
			  "Concurrently to this paper, the idea of BVHs with branching fac-\ntors higher than two has also been investigated by Dammertz et\nal. [4]",
			  "The only difference to the standard SAH is that the cost function\nfor leaves and inner nodes is slightly different. Since we always\nperform 16 triangle tests respectively 16 axis-aligned box tests in\nparallel for the same ray, intersecting 16 triangles in a leaf now\ncosts roughly as much as intersecting a single triangle; the same\nargument holds true for box tests in inner nodes.",
			  "For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16.",
			  "**B** **UILDING** **G** **OOD** **M** **ULTI** **-** **BRANCHING** **BVH** **S**\nThough an efficient traversal routine is key to performance, the ac-\ntual way the data structure is built often can have a similar per-\nformance impact.",
			  "onsequently, if there were a method that would use the 16-wide\nSIMD to intersect less than 16 triangles (say, four), and that would\ndo that *faster* than we currently perform 16 triangle intersections,\nthen a BVH with a less extreme branching factor might be even\nfaster",
			  "We also introduce an efficient SIMD traversal technique for these\nBVHs that produces a strict front-to-back traversal with a minimum\nof scalar code to determine the traversal order.",
			  "ber of inner nodes drops even further, by consistently\naround 50 *x* . Though every node now is 16 *x* as large as a traditional\nnode, the net savings is still significant (we had, in fact, expected a\n*higher* memory consumption",
			  " unused nodes are\nflagged as invalid. The actual true hierarchy is governed by a sur-\nface area heuristic that takes the modified traversal and intersection\ncost into account. The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  " traditional assumptions for the surface area heuris-\ntic [6, 7], the probability of any node *n* being traversed by a random\nray is proportional to the nodes surface area *SA* ( *n* ) . Thus, the ag-\ngregate cost in SIMD triangle intersections and SIMD box tests for\na given multi-BVH can be estimated a",
			  "We demonstrate that with a properly built bounding volume hierarchy (BVH) and a front-to- back traversal algorithm, this approach is somewhat slower than."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://users.aalto.fi/~laines9/publications/ylitie2017hpg_paper.pdf",
			"excerpts": [
			  "Specifically, we start\nby building a binary BVH using an existing high-quality builder,\nand then convert it into an 8-wide BVH in a SAH-optimal fashion.\nWe also employ octant-aware fixed-order traversal [Garanzha and\nLoop 2010], and present an improved method for ordering the child\nnodes at build time to obtain a better traversal order.",
			  "We present a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "Wide BVHs.* Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instead. Shallower hierar-\nchies reduce the number of visited nodes and memory accesses\nduring traversal, but they typically increase the number of ray-box\nand ray-triangle intersection tests [Afra 2013]. Wider BVHs also",
			  "The high branching factor amortizes the memory fetch latencies\nover 8 bounding box intersection tests and increases instruction\nlevel parallelism during internal node tests.",
			  "he memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code.",
			  "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, U",
			  "In addition, we present an algorithmically efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "e also encode the quantiza-\ntion grid and child node indexing information in a compact for",
			  "We compare the performance of our method ( **Ours** ) to four pre-\nviously published GPU-based methods: the traversal kernels by\nAila et al. [2012] ( **Baseline** ), latency-optimized four-wide traver-\nsal by Guthe [2014] ( **4-wide** ), stackless traversal by Binder and\nKeller [2016] ( **Stackless** ), and irregular grids by Prard-Gayot et\nal. [2017] ( **IrrGrid** ). ",
			  " ). We use the authors original implementations\nfor all comparison methods, with no changes to the traversal code.\nFor *",
			  "**5.1**",
			  "**Memory usage**",
			  "Table 6 shows the amount of memory consumed by each method.",
			  "We only report the memory usage of the acceleration structure\nitself, excluding the triangle data to make the comparison as fair as\npossible.",
			  "icient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs\nHPG 17, July 28-30, 2017, Los Angeles, CA, USA\nRay-node tests\nRay-triangle tests\nBounce\n0\n1\n4\n0\n1\n4\nFull precision\n14.30\n14.54\n14.62\n6.45\n7.64\n8.61\n8 bits\n14.69\n14.95\n15.02\n6.59\n7.98\n9.03\n7 bits\n15.04\n15.32\n15.38\n6.73\n8.23\n9.31\n6 bits\n15.69\n16.03\n16.06\n7.01\n8.70\n9.81\n5 bits\n17.18\n17.53\n17.50\n7.62\n9.56\n10.76\n4 bits\n20.56\n20.86\n20.67\n8.94\n11.36\n12.71\n**Table 1: Effect of AABB quantization on intersection test**\n**counts. The numbers represent arithmetic means over our**\n**test scenes, excluding Pow",
			  "resent a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed B",
			  "Similarly to previous methods [Keely 2014; Mahovsky and Wyvill\n2006; Segovia and Ernst 2010; Vaidyanathan et al . 2016], we quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "an 8-wide BVH, and we\n ... \ninternal nodes require only ten bytes per child, obtaining 1 : 3 . 2\ncompression ratio compared to the standard uncompressed BVH\nnode format.",
			  "y efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "Child Bounding Box Compression",
			  "quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "ficient Incoherent Ray Traversal on GPUs Through**\n**Compressed Wide BV",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, USA",
			  "Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instea",
			  "Our acceleration structure is a compressed 8-wide BVH that uses\naxis-aligned bounding boxes (AABBs) as the bounding volumes.",
			  "Wide BVHs.",
			  "Managing a full traversal stack is costly in GPU ray tracers,\nbecause the caches in GPUs are too small to capture the stack traffic.\nThis tends to lead to high DRAM traffic from the traversal stacks\nonly.",
			  "We compress both bounding boxes and child pointers so that our"
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs | Research",
			"url": "https://research.nvidia.com/publication/2017-07_efficient-incoherent-ray-traversal-gpus-through-compressed-wide-bvhs",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal.",
			  "the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion",
			  "Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art.",
			  "Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CPUStyleSIMDRayTraversal.pdf",
			"excerpts": [
			  "**Same Old, Same Old**",
			  "**Same Old, Same Old**",
			  "Binary BVH\n",
			  "Binary BVH\n",
			  "Stack based\n",
			  " Root node on stack",
			  "Test for intersections\n",
			  "* Check distanc",
			  "* Push onto stac",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**Usual Suspects**",
			  "SIMT / GPU"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://jcgt.org/published/0011/04/01/paper.pdf",
			"excerpts": [
			  "odern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes",
			  "ald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *,*\n(",
			  "**3.**\n**Implementation**\nIn this section, we provide implementations details of the evaluated methods. Most\nof the tested methods have publicly available implementations. However, to provide\na fair comparison, we have to use the same settings for all methods, which might be\ndifficult in different frameworks. Thus, we created a single unified framework with\npublicly available implementations of the algorithm integrated into it. Our framework\nis based on Ailas ray tracing framework [ Aila and Laine 2009 ].\nAilas framework contains high-performance traversal kernels for binary BVHs.\nTo support wide BVHs, we adopted the publicly available implementations of the\nmethod proposed by Lier et al. [ 2018 ]. In the original version of these kernels, the",
			  " exploit.\nModern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes. Triangles in leaves are processed\nin a similar fashion, and thus it is desirable to have the number of triangles aligned\nwith the SIMD width as the number of executed intersection tests is equal to the SIMD\nwidth. Motivated by this fact, Wald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *",
			  "Traversal on the GPU might be challenging due to warp divergence and incoherent\nmemory accesses.",
			  "Aila and Laine [ 2009 ] proposed a stack-based traversal algorithm\nwith persistent warps and dynamic fetch.",
			  "To prevent warp divergence, the traversal\nis divided into two independent loops processing interior and leaf nodes separately\n(i.e., the *while-while* traversal).",
			  "Dynamic fetch allows to fetch new rays if a certain number of threads are\ninactive to keep parallel resources occupied enough."
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through compressed wide BVHs",
			"url": "https://www.researchgate.net/publication/318730238_Efficient_incoherent_ray_traversal_on_GPUs_through_compressed_wide_BVHs",
			"excerpts": [
			  "Wald et al. [WBB08] presented the way to convert a binary BVH to a wide BVH.",
			  "de BVHs on the GPU On GPUs, Guthe [Gut14] found that tracing a binary hierarchy was latency-bound on their hardware and that using a 4-wide BVH increases tracing performance",
			  ". In practice, we build a binary BVH (all the aforementioned methods produce binary BVHs), and then we convert it to a wide BVH by pulling child nodes to the parent nodes in a top-down manner [Wald et al. 2014]. Pinto [2010] and Ylitie et al. [2017] proposed an algorithm based on dynamic programming that performs this conversion optimally with regard to the BVH cost.",
			  "Ylitie et al. showed how to do the conversion in a SAH-optimal fashion and presented a compressed wide layout [YKL17] ",
			  "The direct construction of wide BVHs is considered difficult, remaining as an open problem.",
			  "wide BVHs are the state-of-the-art acceleration structure for GPU raytracing, therefore we want our construction algo-rithm to output the nodes in a compressed format. We use a representation similar to the one described by Ylitie et al. [YKL17] : ",
			  "By quantizing the coordinates of child bounding boxes to a single byte they reduce the size of an 8-wide node to 80 bytes.",
			  "The presented 8-wide BVH ray stream implementation reduces memory traffic to only 18% of traditional approaches by using 8-bit quantization for box and triangle coordinates and directly ray tracing these quantized structures.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH."
			]
		  },
		  {
			"title": "Wide BVH Traversal with a Short Stack",
			"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/wide-bvh-traversal-with-a-short-stack-837099.pdf",
			"excerpts": [
			  "In this paper we introduce an algorithm for wide bounding volume hierarchy (BVH) traversal that uses a short stack of just a few entries. This stack can be ...Read more"
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231583/07-1038-lier.pdf",
			"excerpts": [
			  "In this paper we describe and evaluate an implementation of CPU-\nstyle SIMD ray traversal on the GPU. We show how spreading\nmoderately wide BVHs (up to a branching factor of eight) across\nmultiple threads in a warp can improve performance while not\nrequiring expensive pre-processing. Te presented ray-traversal\nmethod exhibits improved traversal performance especially for\nincreasingly incoherent rays.",
			  "Te foundation of our approach is teaming multiple lanes of a warp\nand leting them traverse the BVH together for one single ray. Tis\nconcept mimics a regular SIMD-based BVH traversal known from\nmethods utilizing SSE and AVX extension on the CPU [ Dammertz\net al . 2008 ; Ernst and Greiner 2008 ; Wald et al . 2014 ]. But in con-\ntrast to CPUs, switching between vector instruction (e.g. parallel\nintersection tests) and scalar instruction (e.g. stack management) is\nnot easily (or even efciently) possible on GPUs. In our case, some\noperations (e.g. loading, storing, and stack management) have to\nbe handled individually and at times redundantly on each lane.\nTerefore, we supply each lane with its own copy of the ray data,\nnearest hit information, and stack pointer. However, the stack itself\nresides in shared memory and thus is not redundant.\n",
			  "We start the construction of our acceleration\nstructure with a regular binary BVH. Tere are no limits to the\nconstruction of this BVH, except that we strive for the number of\nleaf primitives to match the width of our target BVH, i.e. 2, 4, or\n8 elements, similarly to CPU implementations [ Ernst and Greiner\n2008 ].",
			  "BVH Construction.* Starting with an efcient binary BVH [ Stich\net al . 2009 ], a similarly efcient wide BVH can be constructed ef-\nciently by pulling up individual nodes to create a BVH of a specifc\nwidth [ Wald et al . 2008 ]. Similarly, fast construction methods for\nbinary BVHs [ Karras 2012 ; Lauterbach et al . 2009 ; Selgrad et al .\n2015 ] can be turned into efcient methods for wide BVHs in the\nsame way",
			  "all our BVHs, we slightly adjust the node layout\n(from Aila et al. [ 2012 ]) in order to improve coalesced memory\naccesses. Te original implementation loads two bounding boxes\nand two child indices at once. Each thread individually fetches four\n*Vec4* elements and both child indices are stored in the last *Vec4*\nelement:\n**int** i = node_index * 4;\nnodes[i+0] = **vec4** (box1.min.x , box1.max.x , box1.min.y , box1.max.y );\nnodes[i+1] = **vec4** (box2.min.x , box2.max.x , box2.min.y , box2.max.y );\nnodes[i+2] = **vec4** (box1.min.z , box1.max.z , box2.min.z , box2.max.z );\nnodes[i+3] = **vec4** (child1.idx , child2.idx , 0\n, 0\n);\nIn our version, nodes are individually loaded in parts by multiple\nadjacent threads. Terefore, it is benefcial to separate individual\nbounding volumes and child indices. In case of a binary BVH, the\nfollowing layout allows coalesced memory access to particular *Vec4*\n ... \nand distributing them among threads was generally slower than\nduplica",
			  "versal.* Te traversal can be classifed as *if-if* -based [ Aila and\nLaine 2009 ]. In each iteration, a node index is fetched from the\nstack. Based on that index, either a box-intersection or a triangle-\nhit test is performed. We do not incorporate a *while-while* setup,\nsince it resulted in slower traversal speed than the straight-forward\napproac",
			  "Triangle intersection is handled similarly by each thread apply-\ning an ofset and loading one single triangle from global memory.\nConsequently, each thread tests only one bounding box or one\nsingle triangle for intersections in each iteration"
			]
		  },
		  {
			"title": "CPU-style SIMD ray traversal on GPUs",
			"url": "https://www.researchgate.net/publication/326762001_CPU-style_SIMD_ray_traversal_on_GPUs",
			"excerpts": [
			  "Lier et al. [2018] proposed a stack-based traversal algorithm for wide BVHs. The idea is to process a single node by multiple threads in a similar manner as ...Read more"
			]
		  },
		  {
			"title": "(PDF) Embree ray tracing kernels: overview and new features",
			"url": "https://www.researchgate.net/publication/305455456_Embree_ray_tracing_kernels_overview_and_new_features",
			"excerpts": [
			  "Embree is an open source ray tracing library consisting of high-performance kernels optimized for modern CPUs with increasingly wide SIMD units.Read more"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree-tutorial.pdf",
			"excerpts": [
			  " Adopt algorithms from Embree into your code.  However Embree internals change frequently!  As a library through the Embree API (recommended).Read more"
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://dl.acm.org/doi/10.1145/3105762.3105773",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH.",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://code.google.com/archive/p/understanding-the-efficiency-of-ray-traversal-on-gpus",
			"excerpts": [
			  "Timo Aila and Samuli Laine, Proc. High-Performance Graphics 2009 http://www.tml.tkk.fi/~timo/publications/aila2009hpg_paper.pdf. In addition to the original ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://github.com/matt77hias/GPURayTraversal",
			"excerpts": [
			  "The results for these GPUs have been published in the following technical report: \"Understanding the Efficiency of Ray Traversal on GPUs - ...Read more"
			]
		  },
		  {
			"title": "Fused Collapsing for Wide BVH Construction",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/cgf.70213?af=R",
			"excerpts": [
			  "We propose a novel approach for constructing wide bounding volume hierarchies on the GPU by integrating a simple bottom-up collapsing ..."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://scispace.com/papers/understanding-the-efficiency-of-ray-traversal-on-gpus-kepler-15z9c6llb5",
			"excerpts": [
			  "This technical report is an addendum to the HPG2009 paper \"Understanding the Efficiency of Ray Traversal on GPUs\", and provides citable performance results ..."
			]
		  },
		  {
			"title": "Compressed Bounding Volume Hierarchies for Efficient ...",
			"url": "https://diglib.eg.org/bitstream/handle/10.2312/vmv20181258/097-102.pdf",
			"excerpts": [
			  "We then build a BVH for these segments with axis-aligned or oriented bounding boxes, avoiding the memory-heavier oriented bounding boxes in the deep levels of ...Read more"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2017/Papers-Session2/HPG2017_AcceleratedSingleRayTracing.pdf",
			"excerpts": [
			  "Encode fixed traversal orders into nodes",
			  "One order per ray signs combination (8)",
			  "Order defined by permute vector",
			  "Map node ordering to a single permute vector operation",
			  "Map stack operation to a single vector compress operation",
			  "Initially nodes are in memory order",
			  "Permute nodes (A)",
			  "Intersection test (B)",
			  "Compress (C)",
			  "Store to stack (D)",
			  "WiVe Node Order",
			  "Binary treelet with split axis labels (a)",
			  "Leaves form BVH8 node cluster",
			  "Split hierarchy determines permute vectors",
			  "BVH8 node cluster in spatial domain (b)",
			  "Ray with +x and -y signs (octant is 10b = 2)",
			  "If signs[split axis] negative: Swap default order",
			  "A)",
			  "x",
			  "x",
			  "x",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "B)",
			  "Permute_vector[2] =",
			  "WiVe Configurations (BVH8-half)",
			  "BVH branching-factor equals half the vector width",
			  "E.g BVH8 with 16-wide vector instructions",
			  "t min and t max values interleaved in register",
			  "Child offset and t min interleaved on stack",
			  "WiVe Configurations (BVH8-full)",
			  "BVH branching-factor equals full vector width",
			  "E.g. BVH8 with 8-wide vector instructions",
			  "t min and t max values in separate registers",
			  "Child offset and t min on separate stacks",
			  "4b",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "a",
			  "b",
			  "c",
			  "d",
			  "\n4b\nn\nn\nn\nn\nt $%&\nt $%&\nt $%&\nt $%&\nt $()\n*\nt $()\n'\nt $()\n+\nt $%&\n*\nt $%&\n'\nt $%&\n+\n**EVALUATION**\nWiVe\n"
			]
		  },
		  {
			"title": "Lectures 26-27 Compiler Algorithms for Prefetching Data",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s15/public/lectures/L26-27-Data-Prefetching.pdf",
			"excerpts": [
			  "3",
			  "4",
			  "2",
			  "2",
			  "1"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2015-Siggraph-Embree-talk.pdf",
			"excerpts": [
			  "Initial AVX512 support\n 16 wide AVX512 traversal kernels\n Full AVX512 optimizations will come when\nhardware available!",
			  "\nEmbree API (C++ and ISPC)\nRay Tracing Kernel Selection\nAccel. structure\nbvh4.triangle4,\nbvh8.triangle8,\nbvh4aos.triangle1,\nbvh4.grid\n\n"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://web.cs.ucdavis.edu/~hamann/FuetterlingLojewskiPfreundtHamannEbertHPG2017PaperFinal06222017.pdf",
			"excerpts": [
			  "The sweet spot for\nthe branching factor appears to be between two and eight, depend-\ning on the hardware architecture and the implementation used. The\ntwo approaches have also been combined for hybrid traversal meth-\nods. For real world applications the most relevant approach by far is\nsingle ray traversal of multi-branching BVHs due to its straightfor-\nward integration into complex shading pipelines.",
			  ".\nOur contribution is a novel single ray traversal algorithm which\nmaps all relevant traversal operations to vector instructions, includ-\ning front-to-back ordering for multi-branching BVHs with branch-\ning factors of 8 (BVH8) or higher. The bene  t is signi  cantly reduced\nalgorithm complexity and constant-time execution, which is ideal\nfor current and future wide vector micro architectures. In ",
			  "We have evaluated our ray traversal algorithm by generating per-\nformance data based on our AVX2 and AVX-512 implementations\non the dual-socket Intel  Xeon  E5 2680v3 @ 2.5GHz (HW) and\nthe Intel  Xeon Phi  7250 @ 1.4GHz (KNL), respectively. We com-\npare our results with those obtained with Embree 2.15.0 [Wald\net al . 2014], the leading high-performance ray tracing library for\nCPUs. In order to ensure comparability of performance data, we\nintegrated our code into the open source Embree benchmark suite\nProtoray [Intel Corporation 2017a], which by default o  ers Embree\nand Nvidia  OptiX  [Parker et al . 2010] kernels. A compariso",
			  "Embree constructs a native\nBVH8 using SAH-based centroid binning[Wald 2007], which we\ndirectly convert to our own data layout retaining the exact same\ntopology. W",
			  "We have disabled spatial splits [Stich et al . 2009] to en-\n ...",
			  ". The result is a simple and highly e  cient technique for\nmulti-branching BVH coherent traversal.",
			  "WiVe promises to accelerate\nsingle ray traversal for multi-branching bounding volume hierar-"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels: Overview and New Features",
			"url": "https://pdfs.semanticscholar.org/4f68/0f47d5b8a052fb6fa698508a42320a7f1cc4.pdf",
			"excerpts": [
			  "Embree features",
			  "Embree API (C++ and ISPC)",
			  "Ray Tracing Kernel Selection",
			  "Acceleration",
			  "structures",
			  "bvh4.triangle4",
			  "bvh8.triangle4",
			  "bvh4.quad4v",
			  "",
			  "builders",
			  "SAH Builder",
			  "Spatial Split Builder",
			  "Morton Builder",
			  "BVH Refitter",
			  "intersection",
			  "Mller-Trumbore",
			  "Single rays, ray packets (4, 8, 16), ray streams (N)",
			  "traversal",
			  "traversal",
			  "Single ray",
			  "Single ray",
			  "Packet/Hybrid",
			  "Packet/Hybrid",
			  "ray stream",
			  "ray stream",
			  "Common Vector and SIMD Library",
			  "Common Vector and SIMD Library",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)"
			]
		  },
		  {
			"title": "8-Wide BVH Ray Stream Implementation",
			"url": "https://www.emergentmind.com/topics/8-wide-bvh-ray-stream-implementation",
			"excerpts": [
			  "An 8-wide BVH (Bounding Volume Hierarchy) ray stream implementation is a memory-efficient, SIMD-friendly approach to accelerating ray tracing ...Read more"
			]
		  },
		  {
			"title": "The Minimal Bounding Volume Hierarchy",
			"url": "https://graphics.tu-bs.de/upload/publications/minimal-bounding-volume-hierarchy.pdf",
			"excerpts": [
			  "In this paper we present the Minimal Bounding Volume Hierarchy (MVH) as a k-ary object partitioning scheme. Similar to BVHs a ray traverses this tree in a top- ...Read more",
			  "... two-level BVH which uses both un- compressed BVH nodes for the top levels and compressed nodes for the rest of the hierarchy. This idea of a two-level.Read more"
			]
		  },
		  {
			"title": "Ray Classification for Accelerated BVH Traversal",
			"url": "https://meistdan.github.io/publications/shafts/paper.pdf",
			"excerpts": [
			  "The green nodes are entry points of our traversal algorithm. The yellow nodes are visited by neither traversal method and only denote the path to candidate list.Read more"
			]
		  },
		  {
			"title": "Using Embree generated BVH trees for GPU raytracing  Interplay of Light",
			"url": "https://interplayoflight.wordpress.com/2020/07/21/using-embree-generated-bvh-trees-for-gpu-raytracing/",
			"excerpts": [
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for SIMD acceleration. Such trees are harder to use during a GPU-based traversal.",
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for ...Read more"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray Tracing of Incoherent Rays",
			"url": "https://www.researchgate.net/publication/220506625_Shallow_Bounding_Volume_Hierarchies_for_Fast_SIMD_Ray_Tracing_of_Incoherent_Rays",
			"excerpts": [
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006; Dammertz et al. 2008; Ernst and Greiner 2008]. Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "A 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008]. Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008 ] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ..",
			  "MBVH has a more compact memory layout than BVH, as noted by Dammertz et al. [Dammertz et al. 2008 ] and illustrated in Figure 1. Consequently, it improves the hit rate of caches and reduces external memory traffic. ...",
			  "Storing nearby nodes grouped into small subtrees closer in memory increases the L1 or L2 cache hit rates, thus reducing bandwidth required to render the scene [Aila and Karras 2010]. ...",
			  "In this paper we describe and evaluate an implementation of CPU-style SIMD ray traversal on the GPU. We show how spreading moderately wide BVHs (up to a branching factor of eight) across multiple threads in a warp can improve performance while not requiring expensive pre-processing. The presented ray-traversal method exhibits improved traversal performance especially for increasingly incoherent rays."
			]
		  },
		  {
			"title": "Multi Bounding Volume Hierarchies",
			"url": "https://www.researchgate.net/publication/4375554_Multi_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ...",
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006;Dammertz et al. 2008; Ernst and Greiner 2008] . Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "In case of a leaf, the active child mask is set to zero, the five-byte index points to a list of triangles, and the number of triangles is saved in the look-up byte. This somewhat flexible structure allows a saving of about 5% of memory compared to the standard 128-byte layout [Dammertz et al. 2008; Ernst and Greiner 2008] and also performs marginally faster during traversal. ...",
			  " 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008] . Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "Quad-BVH structures in which child nodes are stored together in memory, enable efficient single-ray traversal on architectures with a vector width of 4 [Ernst and Greiner 2008; Dammertz et al. 2008]. At each traversal step, the ray is tested for intersection against the 4 child nodes in parallel. ..."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de- compression. Our breadth-first BVH traversal ...Read more",
			  "om-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs.",
			  " stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections",
			  " utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests ",
			  "ur breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  " novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections. We utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de-*\n*compress",
			  "A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack",
			  "Ray sorting is used to store spatially coherent rays in\nconsecutive memory locations. Compared to unsorted rays,\nthe tracing routine for sorted rays has less divergence on a\nwide SIMD machine such as GPU.",
			  "We explicitly maintain ray coherence in our pipeline\nby using this procedure.",
			  "his algorithm amortizes the cost of node access\npattern among the rays. A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  "The binary BVH is\nconstructed on the CPU and 2/3 rds of tree levels are\neliminated and an Octo-BVH is created (all the nodes are\nstored in a breadth-first storage layout).",
			  "he traversal pro-\ncedure we perform intersection tests for each frustum cor-\nresponding to *Qin.FrustumIDs* *i* with all the children of the\nBVH-node specified by *Qin.NodeIDs* *i* (usi",
			  "he new ray tracing pipeline provides the possibility to\ntrace relatively big packets of rays and perform efficient\nview-independent queries using a breadth-first frustum\ntraversal. Memory access patterns for breadth-first traversal\nare coherent as we perform operations in parallel for each\nBVH level (and the BVH is stored in a breadth-first lay-\nout)",
			  "Since we store all the leaf references for all the frustums\nthe memory consumption may be considerable (and we\nalso store the rays). But this consumption may be reduced\nthrough using a screen-space tiling (send reasonably big\ntiles to render on the GPU) or even frustum depth tiling.",
			  "Abstract"
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/10.1145/2601097.2601222",
			"excerpts": [
			  "While each new generation of processors gets larger caches and more compute power, external memory bandwidth capabilities increase at a much lower pace. Additionally, processors are equipped with wide vector units that require low instruction level divergence to be efficiently utilized.",
			  "Our main contribution is an efficient algorithm for traversing large packets of rays against a bounding volume hierarchy in a way that groups coherent rays during traversal.",
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing.",
			  "Ray tracing algorithms is a mature research field in computer graphics, and despite this, our new technique increases traversal performance by 36--53%, and is applicable to most ray tracers.",
			  "In order to exploit these trends for ray tracing, we present an alternative to traditional depth-first ray traversal that takes advantage of the available cache hierarchy, and provides high SIMD efficiency, while keeping memory bus traffic low."
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/2601097.2601222",
			"excerpts": [
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing."
			]
		  },
		  {
			"title": "Dynamic Ray Stream Traversal",
			"url": "http://cseweb.ucsd.edu/~ravir/274/15/papers/drst.pdf",
			"excerpts": [
			  "Divergence leads to underutilization since SIMD lanes will\nneed to be masked out. This is not a trait generally attributed to\ndepth-first traversal. Even when packets of rays are traced in a\nSIMD fashion, rays usually diverge quickly when incoherent, such\nas for diffuse interreflections, for example.",
			  "r algorithm is designed to\nhave a predictable memory access pattern with high data coherence,\nwhich substantially reduces the amount of memory bandwidth us-\nage in our tests.",
			  "Despite this, our results show that total ray tracing performance can\nbe improved by 2237%, while traversal alone is increased by 36\n53%, which is rather remarkable.",
			  "ck-less ray traver-\nsal [Hughes and Lim 2009; Laine 2010; Hapala et al. 2011; Bar-\nringer and Akenine-Moller 2013] is a more recent endeavor that\nwas, at least initially, motivated by the high overhead of main-\ntaining a traversal stack on previous generations of G",
			  "ually, a stack is maintained that contains the next node to be pro-\ncessed during ray traversal. The technique has been combined with\nvarious forms of ray sorting and tracing of whole packets [Wald\net al. 2001] of rays to improve performance.",
			  " rays in the ray stream take the same path in the tree, which decreases\ndivergence and increases SIMD utilization.",
			  ".\nEmbree also includes a set of packet traversal kernels [Wald et al.\n2001], as well as hybrid kernels that starts with packets and\nswitches to single-ray traversal as utilization becomes low [Ben-\nthin et al. 2012]. ",
			  "To use these kernels, it is the responsibility of the\nrenderer to manage ray packets.",
			  "s. The example renderer that supports\nthese kernels constitutes a total rewrite of the single-ray renderer us-\ning *ISPC* [Pharr and Mark 2012], which makes the entire renderer\nvectorized.",
			  "which makes the entire renderer\nvectorized. This makes it a bit difficult to compare performance di-\nrectly with our algorithm and single-ray traversal.",
			  "However, we have chosen to keep the scalar shaders"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving | Research",
			"url": "https://research.nvidia.com/publication/2022-01_gpu-subwarp-interleaving",
			"excerpts": [
			  "In this paper, we present an architectural enhancement called Subwarp Interleaving that exploits thread divergence to hide pipeline stalls in divergent ...Read more"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving - Cloudfront.net",
			"url": "https://d1qx31qr3h6wln.cloudfront.net/publications/Damani_Subwarp_Interleaving_HPCA_IT_2022.pdf",
			"excerpts": [
			  "Subwarp Interleaving allows for fine-grained interleaved execution of diverged paths within a warp with the goal of increasing hardware utilization and reducing ...Read more"
			]
		  },
		  {
			"title": "Dynamic Stackless Binary Tree Traversal",
			"url": "https://jcgt.org/published/0002/01/03/paper.pdf",
			"excerpts": [
			  "We evaluate our algorithm using a ray tracer with a bounding volume hierarchy for which source code is supplied. 1. Introduction. Traversing a ..."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Section Title: Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware > Abstract\nContent:\nRecent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  " key insight is that typical control flow inten-\nsive CUDA applications exhibit sufficient control flow lo-\ncality within the group of scalar threads used for bulk-\nsynchronous parallel execution that full DWF and/or MIMD\nflexibility is not necessary to regain most of the perfor-\nmance loss due to branch diverge",
			  "he compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  " The\nprevious best (on average) policy known, majority, incurs\npoor performance when a small number of threads falls\nbehind the majority of threads",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o",
			  "Dynamic warp formation,\n",
			  " dynamic warp\nformation and scheduling mechanism.",
			  "rn *graphics processing unit* (GPU) can be\nviewed as an example of the latter approach [18, 28, 6].\nEarlier generations of GPUs consisted of fi xed function 3D\nrendering pipelines.\nThis required new hardware to en-\nable new real-time rendering techniques, which impeded\nthe adoption of new graphics algorithms and thus motivated\nthe introduction of programmability, long available in tradi-\ntional of fl ine computer animation [35], into GPU hardwar",
			  "One approach is to add a stack*\n*to allow different SIMD processing elements to execute dis-*\n*tinct program paths after a branch instruc",
			  "Dynamic warp formation improves performance by cre-\nating new thread warps out of diverged warps as the shader\nprogram execute",
			  "Every cycle, the thread scheduler tries to\nform new warps from a pool of ready threads by combining\nscalar threads whose next PC values are the same.",
			  "We\nexplore the design space of this scheduling policy in detail\nin Section 4.3.",
			  "e thread scheduler policy is\ncritical to the performance impact of dynamic warp forma-\ntion in Section 6."
			]
		  },
		  {
			"title": "Thread block compaction for efficient SIMT control flow",
			"url": "https://ieeexplore.ieee.org/document/5749714/",
			"excerpts": [
			  "Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data cores to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22% over a baseline per-warp, stack-based reconvergence mechanism, and 17% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "A survey of architectural approaches for improving GPGPU ...",
			"url": "https://mkhairy.github.io/Docs/jpdc-survey.pdf",
			"excerpts": [
			  "Fung and Aamodt [46] proposed thread block compaction (TBC) that allows a group of warps, that belong to the same thread block, to share the same PDOM stack.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://www.researchgate.net/publication/392717030_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing | Request PDF",
			"url": "https://www.researchgate.net/publication/341162869_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[8] S. Damani, M. Stephenson, R. Rangan, D. Johnson, R. Kulkami, and S. W. Keckler, Gpu subwarp interleaving, in *International Symposium on High-Performance Computer Architecture (HPCA)* , 2022.",
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011",
			  " [14] W. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, Dynamic warp formation and scheduling for efficient gpu control flow, in *International Symposium on Microarchitecture (MICRO)* , 2007. "
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Evaluated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path",
			  "SIMT execution.",
			  "**Acknowledgments**",
			  "The authors would like to thank Wilson Fung, Tayler",
			  "Hetherington, Ali Bakhoda, Timothy G. Rogers, Ayub",
			  "Gubran, Hadi Jooybar and the reviewers for their insightful",
			  "feedback. This research was funded in part by a Four Year",
			  "Doctoral Fellowship from University of British Columbia,",
			  "the Natural Sciences and Engineering Research Council of",
			  "Canada and a grant from Advanced Micro Devices Inc.",
			  "**References**",
			  "[9] W. Fung, I. Sham, G. Yuan, and T. Aamodt. Dynamic Warp",
			  "Formation and Scheduling for Efficient GPU Control Flow.",
			  "[10] W. W. L. Fung and T. M. Aamodt.",
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages",
			  "2536, 2011.",
			  "[6] S. Collange. Stack-less SIMT Reconvergence at Low Cost.",
			  "hnical Report hal-00622654, ARENAIRE - Inria Greno-\nble Rhne-Alpes / LIP Laboratoire de lInformatique du Par-\nalllisme, 2011",
			  ". Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "W. Fung, I. Sham, G. Yuan, and T. Aamodt.",
			  "Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "407420, 2007.",
			  "407420, 2007.",
			  " Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "valuated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path\nSIMT execution"
			]
		  },
		  {
			"title": "Programming the EMU Architecture: Algorithm Design ...",
			"url": "https://sc18.supercomputing.org/proceedings/tech_poster/poster_files/post213s2-file2.pdf",
			"excerpts": [
			  "**References**"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://arxiv.org/html/2506.11273v1",
			"excerpts": [
			  "Garanzha and Loop (2010) Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First Packet Traversal for GPU Ray Tracing. *Computer Graphics Forum* 29, 2 (2010), 28929",
			  "/2506.11273v1.bib12) ) used breadth-first packet traversal after a ray sorting step. They proposed the idea of sorting rays to reduce divergence in computation using a hash-based method for sorting the rays into coherent packets. T",
			  "In addition, the rays are grouped into frusta, which are further tested against the scene as proposed by Reshetov\net al . ( [200",
			  "This way, the total number of intersection tests is reduced.",
			  "While they report impressive speedups for primary rays and deterministic ray tracing, this does not translate to path tracing because the frusta become too large and intersect most of the scene.",
			  "For production rendering, not only the trace kernel but also shading might be limited by memory bandwidth. Therefore, Eisenacher et al . ( [2013](https://arxiv.org/html/2506.11273v1.bib11) ) proposed to sort termination points to improve shading performance. While this approach is designed for out-of-core path tracing, grouping shading calculations by a material also improves in-core performance for complex shaders. For highly detailed scenes, Hanika\net al . ( [2010](https://arxiv.org/html/2506.11273v1.bib15) ) proposed to use a two-level hierarchy combined with ray sorting to facilitate efficient on the fly micro-polygon tessellation. The rays are traversed through the top-level hierarchy, and they are repeatedly sorted to determine sets of rays traversing the same leaf nodes of the top-level hierarchy.",
			  "When coherence among rays exists, the packet traversal (Gunther\net al . , [2007](https://arxiv.org/html/2506.11273v1.bib14) ) exploits it by forcing a SIMD processing of a group of rays. This, on the other hand, increases inter-thread communication and synchronization. Furthermore, it assumes high ray coherence and is significantly slower than depth-first traversal for incoherent rays. Bikker ( [2012](https://arxiv.org/html/2506.11273v1.bib7) ) proposed a packet traversal algorithm that uses batching to improve data locality.",
			  " \nSection Title: On Ray Reordering Techniques for Faster GPU Ray Tracing > References\nContent:\nDynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In *Proceedings of IEEE Symposium on Interactive Ray Tracing* . 95104. Nimier-David et al . (2019) Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2: A Retargetable Forward and Inverse Renderer. *ACM Transactions on Graphics* 38, 6 (2019), 203. Pharr et al . (1997) Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. 1997. Rendering Complex Scenes with Memory-coherent Ray Tracing. In *Proceedings of International Conference on Computer Graphics and Interactive Techniques* . 101108. Reis et al . (2017) Nuno T. Reis, Vasco S. Costa, and Joo M. Pereira. 2017. Coherent Ray-Space Hierarchy via Ray Hashing and Sorting. In *Proceedings of International Joint Conference on Computer Vision, Imaging, and Computer Graphics Theory and Applications* . Resh",
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal."
			]
		  },
		  {
			"title": "[PDF] Two-level ray tracing with reordering for highly complex scenes | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Two-level-ray-tracing-with-reordering-for-highly-Hanika-Keller/ee5673f3141a61924798d7642f06971dd41d871c",
			"excerpts": [
			  "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision Surfaces",
			  "Nikolaus Binder](/author/Nikolaus-Binder/2231395) [A. Keller](/author/A.-Keller/145661463)",
			  "* 2018",
			  "Besides introducing an optimized method to determine axis aligned bounding boxes of Gregory patches restricted in the parametric domain, several techniques are introduced that accelerate the recursive subdivision process including stackless operation, efficient work distribution, and control flow optimizations.",
			  "TLDR"
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Proceedings of High Performance Graphics",
			"url": "https://dl.acm.org/doi/10.5555/2977336.2977343",
			"excerpts": [
			  "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time ... Stackless multi-BVH traversal for CPU, MIC and GPU ray tracing."
			]
		  },
		  {
			"title": "Extending GPU Ray-Tracing Units for Hierarchical Search ...",
			"url": "https://engineering.purdue.edu/tgrogers/publication/barnes-micro-2024/barnes-micro-2024.pdf",
			"excerpts": [
			  "Binder and A. Keller, Efficient Stackless Hierarchy Traversal on. GPUs with Backtracking in Constant Time, in Proceedings of High. Performance Graphics (HPG).Read more"
			]
		  },
		  {
			"title": "A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees",
			"url": "https://jcgt.org/published/0014/01/03/paper.pdf",
			"excerpts": [
			  "BINDER, N. AND KELLER, A. Efficient stackless hierarchy traversal on GPUs with back- tracking in constant time. In Proceedings of High ...Read more"
			]
		  },
		  {
			"title": "[PDF] Thread block compaction for efficient SIMT control flow",
			"url": "https://www.semanticscholar.org/paper/Thread-block-compaction-for-efficient-SIMT-control-Fung-Aamodt/8bd6f67ef03b3c138c52f3e9b1716aebe937d244",
			"excerpts": [
			  "This paper proposes and evaluates the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, ..."
			]
		  },
		  {
			"title": "HIPRT: A Ray Tracing Framework in HIP",
			"url": "https://gpuopen.com/download/HIPRT-paper.pdf",
			"excerpts": [
			  "Binder and Keller [2016] proposed backtracking in constant time based on a path to the node in a bitset and perfect hashing. 3 HIPRT API OVERVIEW. Similarly to ...Read more",
			  "Later, Ylitie et al. [2017] showed that a compressed. 8-wide BVH can reduce memory traffic even further, improving the overall ray tracing performance.Read more"
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "http://www.danielwong.org/classes/_media/ee260_w17/threadblockcompaction.pdf",
			"excerpts": [
			  ", we propose and*\n*evaluate the benefits of extending the sharing of resources*\n*in a block of warps, already used for scratchpad mem-*\n*ory, to exploit control flow locality among threads (where*\n*such sharing may at first seem detrimental). In our pro-*\n*posal, warps within a thread block share a common block-*\n*wide stack for divergence handling. At a divergent branch,*\n*threads are compacted into new warps in har",
			  "he modifications consist of three major parts: a modi-\nfied branch unit ( 1 ), a new hardware unit called the thread\ncompactor ( 2 ), and a modified instruction buffer called the\nwarp buffer ( 3 ). The branch unit ( 1 ) has a block-wide re-\nconvergence stack for each block. Each entry in the stack\nconsists of the starting PC (PC) of the basic block that cor-\nresponds to the entry, the reconvergence PC (RPC) that in-\ndicates when this entry will be popped from the stack, a\nwarp counter (WCnt) that stores the number of compacted\nwarps this entry contains, and a block-wide activemask that\nrecords which thread is executing the curre",
			  " comparison to DWF [9], thread block compaction ac-\ncomplishes the lookup-and-merge operation of the warp\nLUT and the warp pool [9] with simpler hardware. In\nDWF, an incoming warp is broken down every cycle and\n .."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow | Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2007.12",
			"excerpts": [
			  "We show that a realistic hardware im- plementation that dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes improves performance by an average of 20.7% for an estimated area increase of 4.7%."
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Research",
			"url": "https://research.nvidia.com/publication/2016-06_efficient-stackless-hierarchy-traversal-gpus-backtracking-constant-time",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling and use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or state memory. We show that the next node in such a traversal actually can be determined in constant time and state memory. In fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and outperforms the fastest stack-based algorithms on GPUs. In addition, it reduces memory access during traversal, making it a very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal on GPUs with ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/1c026ceb-0c54-4e20-9fd2-2ff77222894d/content",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling",
			  "nd use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or*\n*state memory",
			  "We show that the next node in such a traversal actually can be determined in constant time and state memory.",
			  "*fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and*\n*outperforms the fastest stack-based algorithms on GPUs.",
			  " addition, it reduces memory access during traversal, making it a*\n*very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal with Backtracking in ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2016/2016-HPG-Binder-StacklessTraversalPerfectHash.pdf",
			"excerpts": [
			  "Efficient Stackless Hierarchy Traversal with Backtracking in Constant Time",
			  "Results: Performance in M rays/s, NVIDIA Titan X, for Primary/Shadow/Diffuse Rays"
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Software Developer's Manual ...",
			"url": "https://kib.kiev.ua/x86docs/Intel/SDMs/326018-062.pdf",
			"excerpts": [
			  "Page 1. Intel 64 and IA-32 Architectures. Software Developer's Manual ... _mm512_i64gather_epi64( __m512i vdx, void * base ... Intel C/C++ Compiler Intrinsic ..."
			]
		  },
		  {
			"title": "\n\tGather of byte/word with avx2 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Gather-of-byte-word-with-avx2/td-p/921687",
			"excerpts": [
			  "1) Use _mm256_i32gather_epi32. We would fetch an extra 16-bits that we do not want for each voxel, and then either mask off the extra bits or ...Read more"
			]
		  },
		  {
			"title": "_mm_i32gather_epi32, _mm256_i32gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-1F275401-A760-49B1-944A-B02C075514D8.htm",
			"excerpts": [
			  "Gathers 2/4 doubleword values from memory referenced by the given base address, dword indices, and scale. The corresponding Intel AVX2 instruction is"
			]
		  },
		  {
			"title": "_mm_i64gather_epi32, _mm256_i64gather_epi32",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/compiler_c/main_cls/intref_cls/common/intref_avx2_mm256_i64gather_epi32.htm",
			"excerpts": [
			  "Gather 2/4 doubleword values from memory referenced by the given base address, qword indices and scale. The corresponding Intel AVX2 instruction is VPGATHERQD."
			]
		  },
		  {
			"title": "An introduction to Arm Scalable Vector Extensions",
			"url": "https://epicure-hpc.eu/wp-content/uploads/2025/03/SVE_Vectorization_Ricardo_Fonseca.pdf",
			"excerpts": [
			  "SVE gather / scatter operations.  Gather operations.  Gather scalar values ...  https://developer.arm.com/documentation/101458/2404/Optimize.  Arm ...Read more"
			]
		  },
		  {
			"title": "Mirror of Intel Intrinsics Guide",
			"url": "https://www.laruence.com/sse/",
			"excerpts": [
			  "Intel. Intrinsics Guide. Technologies. MMX. SSE. SSE2. SSE3. SSSE3. SSE4.1. SSE4.2 ... This intrinsic is provided for conversion between little and big endian ...Read more"
			]
		  },
		  {
			"title": "Intrinsics for Integer Gather and Scatter Operations",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-35D298CC-B89B-4B38-856B-FCD0EBB3AA23.htm",
			"excerpts": [
			  "**_mm512_i32gather_epi32**\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i32gather_epi32**\nSection Title: Intrinsics for Integer Gather and Scatter Operations\nContent:\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i32scatter_epi32**\nScatters int32 from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ).",
			  "**_mm512_mask_i32scatter_epi64**\nScatters int64 from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ) subject to mask k (elements are not stored when the corresponding mask bit is not set).",
			  "**_mm512_i64scatter_epi64**\nScatters int64 from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale )."
			]
		  },
		  {
			"title": "Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel2021.7/HPC/cpp_compiler/cpp_compiler_classic_dev_guide/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.html",
			"excerpts": [
			  "\nThe prototypes for Intel Advanced Vector Extensions 512 (Intel AVX-512) intrinsics are located in the zmmintrin.h header file.\nTo u",
			  "To use these intrinsics, include the immintrin.h file as follows:",
			  "ement.\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Data Types for Intel AVX-512 Intrinsics\nC",
			  "s:\nIntel AVX-512 intrinsics have vector variants that use __m128 , __m128i , __m128d , __m256 , __m256i , __m256d , __m512 , __m512i , and __m512d data types.",
			  ".\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Naming and Usage Syntax\nCon"
			]
		  },
		  {
			"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
			"excerpts": [
			  "**AVX2 has gathers (not scatters), but they're only fast on Skylake and newer** . They're ok on Broadwell, slowish on Haswell, and slow on AMD. (Like one per 12 clocks for Ryzen's `vpgatherqq` ). See http://agner.org/optimize/ and other performance links in [the x86 tag wiki](https://stackoverflow.com/tags/x86/info) .\nIntel's optimization manual has a small section on manual gather / scatter (using insert/extract or `movhps` ) vs. hardware instructions, possibly worth reading. In this case where the indices are runtime variables (not a constant stride or something), I think Skylake can benefit from AVX2 gather instructions here",
			  "**extract indices, manually gather into a vector with `vmovq` / `vmovhps` for a SIMD `vpor` , then scatter back with `vmovq` / `vmovhps`** .Just like a HW gather/scatter, **correctness requires that all indices are unique** , so you'll want to use one of the above options until you get to that point in your algo. (vector conflict detection + fallback would not be worth the cost vs. just always extracting to scalar: [Fallback implementation for conflict detection in AVX2](https://stackoverflow.com/questions/44843518/fallback-implementation-for-conflict-detection-in-avx2) ).See [selectively xor-ing elements of a list with AVX2 instructions](https://stackoverflow.com/questions/50583718/selectively-xor-ing-elements-of-a-list-with-avx2-instructions) for an intrinsics version.",
			  "**AVX2 `vpgatherqq` for the gather ( `_mm256_i64gather_epi64(sieveX, srli_result, 8)` ), then extract indices and manually scatter.** So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather. (Two 128-bit gathers cost more than one 256-bit gather, so you would want to take the instruction-level parallelism hit and gather into a single 256-bit register).",
			  "Possibly a win on Skylake (where `vpgatherqq ymm` is 4 uops / 4c throughput, plus 1 uop of setup), but not even Broadwell (9 uops, one per 6c throughput) and definitely not Haswell (22 uops / 9c throughput). You do need the indices in scalar registers anyway, so you're *only* saving the manual-gather part of the work. That's pretty cheap.",
			  "**manual gather/scatter: 20 uops, 5 cycles of front-end throughput** (Haswell / BDW / Skylake). Also good on Ryzen.",
			  "**Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.** (Worse on any earlier uarch or on AMD).",
			  "\nvextracti128 indices (1 uop for port 5)",
			  "2x vmovq extract (2p0)",
			  "2x vpextrq (4 = 2p0 2p5)",
			  "`vpcmpeqd ymm0,ymm0,ymm0` create an all-ones mask for `vpgatherqq` (p015)",
			  "`vpgatherqq ymm1, [rdi + ymm2*8], ymm0` 4 uops for some ports.",
			  "`vpor ymm` (p015)",
			  "vextracti128 on the OR result (p5)\n",
			  "2x vmovq store (2x 1 micro-fused uop, 2p23 + 2p4). Note no port7, we're using indexed stores.",
			  "2x vmovhps store (2x 1 micro-fused uop, 2p23 + 2p4)."
			]
		  },
		  {
			"title": "c++ - AVX2 Gather Instruction Usage Details - Stack Overflow",
			"url": "https://stackoverflow.com/questions/58832024/avx2-gather-instruction-usage-details",
			"excerpts": [
			  "The offsets in `vindex` are in bytes. Therefore, you gather 32-bit integer values from addresses `{arr, arr+2, arr+4, ...}` .",
			  "Either change these indexes from `{0,2,4...}` to `{0,8,16,...}` , or update the scale factor as:\nThis prints out the expected values."
			]
		  },
		  {
			"title": "Arm C Language Extensions",
			"url": "https://arm-software.github.io/acle/main/acle.html",
			"excerpts": [
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B)",
			  "\nThe SVE ACLE intrinsics have the form:\nwhere the individual parts are as follows:\n**base**\nFor most intrinsics this is the lower-case name of an SVE\ninstruction, but with some adjustments:The most common change is to drop `F` , `S` and `U` if they\nstand for floating-point, signed and unsigned respectively,\nin cases where this would duplicate information in the type\nsuffixes below.Simple non-extending loads and non-truncating stores drop the\nsize suffix ( `B` , `H` , `W` or `D` ), which would always duplicate\ninformation in the suffixes.Conversely, extending loads always specify an explicit extension\ntype, since this information is not available in the suffixes.\nA sign-extending load has the same base as the architectural\ninstruction (for instance, `ld1sb` ) while a zero-extending load replaces\nthe `s` with a `u` (for instance, `ld1ub` for a zero-extending `LD1B` ).\nThus [`svld1ub_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1ub_u32) zero-extends 8-bit data to a vector of `uint32_t` s while [`svld1sb_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1sb_u32) sign-extends 8-bit data to a vector of `uint32_t` s.",
			  "| LD1D (vector plus immediate) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather%5B) |",
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B) ",
			  "| ST1D (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) |",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |"
			]
		  },
		  {
			"title": "Arm Scalable Vector Extension and application to Machine ...",
			"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Arm-scalable-vector-extensions-and-application-to-machine-learning.pdf",
			"excerpts": [
			  "the function svld1_gather_u32base_offset_s32 , with signature\nsvint32_t svld1_gather_u32base_offset_s32(svbool_t pg, svuint32_t bases, int64_t\noffset)\nis a *gather load* ( ld1_gather ) of *signed 32-bit integer* ( _s32 ) from a vector of *unsigned 32-bit integer* base\naddresses ( _u32base ) plus an *offset in bytes* ( _offset ).",
			  "The SVE ACLE are compatible with C++ overloading and C _Generic association, so that the names\ncan be contracted removing those parts that can be derived from the arguments types.",
			  "The SVE ACLE (or ACLE hereafter) is a set of functions and types that exposes the vectorization capabilities of\nSVE at C/C++ level.",
			  "They introduce a set of *size-less* types and *intrinsic functions* that a C/C++ compiler can directly convert into\nSVE assembly.",
			  "An additional svbool_t type is defined to represent predicates for masking operations.",
			  "SVE intrinsic functions",
			  "The naming convention of the intrinsic functions in the SVE ACLE is described in detail in section 4 of the SVE\nACLE document (ARM limited 2017b).",
			  "Most of them are in the form: svbase[_disambiguator][_type0][_type1]...[_predication] .",
			  "For example, the name of the intrinsic svadd_n_u16_m , with signature svuint16_t svadd_n_u16_m(svbool_t\npg, svuint16_t op1, uint16_t op1) , describes a vector *addition* ( add ) of *unsigned 16-bit integer* ( u16 ),\nwhere one of the arguments is a scalar ( _n ) and the predication mode is *merging* ( _m ).",
			  "Some of the functions, like loads and stores, have a different form for the names, with additional tokens that\nspecify the addressing mode.",
			  "All the examples of this document use the short form. For simplicity, we also assume no aliasing, meaning that"
			]
		  },
		  {
			"title": "Arm C Language Extensions for SVE - Version 00bet6",
			"url": "https://rci.stonybrook.edu/sites/default/files/documents/acle_sve_100987_0000_06_en.pdf",
			"excerpts": [
			  "COMPACT: Compact vector and fill with zero",
			  "These functions concatenate the active elements of the input vector, filling any remaining elements with\nzero.",
			  "6.23. Predicate creation",
			  "6.23.1. PTRUE: Return an all-true predicate for a given pattern",
			  "These functions return an all-true predicate for a particular vector pattern and element size. When an\nelement has more than one predicate bit associated with it, only the lowest of those bits is ever true.",
			  "There are two forms: one with a _pat suffix that takes an explicit vector pattern and one without a _pat\nsuffix in which the pattern is implicitly SV_ALL .",
			  "svbool_t **svptrue_b8** ()",
			  "svbool_t **svptrue_b16** ()",
			  "svbool_t **svptrue_b32** ()",
			  "svbool_t **svptrue_b64** ()"
			]
		  },
		  {
			"title": "ARM's Scalable Vector Extensions: A Critical Look at SVE2 ...",
			"url": "https://gist.github.com/zingaburga/805669eb891c820bd220418ee3f0d6bd",
			"excerpts": [
			  "Under ACLE, NEON  SVE value transfer must go through memory. Interestingly ... SVE adds support for gather/scatter operations, which helps vectorize ..."
			]
		  },
		  {
			"title": "_mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-8B147603-6A6A-4F23-8529-1609A13AB784.htm",
			"excerpts": [
			  "extern __m512i __cdecl _mm512_i32gather_epi32(_m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "tent:\n| extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "The non-masked variant of the intrinsic is equivalent to the masked variant with full mask ( k1 =0xffff).",
			  ":\nGather int32 vector with int32 indices. Corresponding instruction is VPGATHERDD . This intrinsic only applies to Intel Many\nIntegrated Core Architecture (Intel MIC Architecture).\n",
			  "Up-converts a set of 16 memory locations pointed by base address mv and int32 index vector index with scale scale , and gathers them into a int32 vector.",
			  "The resulting vector for the masked variant is populated by elements for which the corresponding bit in the writemask vector k1 is set. The remaining elements of the resulting vector for the masked variant is populated by corresponding elements from v1_old .",
			  "Returns the result of the up-convert load operation."
			]
		  },
		  {
			"title": "x86 Intrinsics Cheat Sheet",
			"url": "https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf",
			"excerpts": [
			  "AVX2\n**mi** **i64gather_epi32**\n( **i*** ptr, **mi** a, **i** s)\n**FOR** j := 0 to 1;\ni := j*32\nm := j*64\n**dst** [i+31:i] :=\n*(ptr + a[m+63:m]*s])\n**dst** [MAX:64] := 0\nMask Gather\nmask_ *i32* / *i64* gather\nepi32-64,ps/d\n**NOTE:** Same as gather but takes\nan additional mask and src\nregister. Each element is only\ngathered if the highest\ncorresponding bit in the mask is\nset. Otherwise it is copied from\nsrc. Memory does not need to\nbe aligned.",
			  "AVX2\n**mi mask_i64gather_epi32** ( **mi** src,\n**i*** ptr, **mi** a, **mi** mask, **i32** s)\n**FOR** j := 0 to 1; i:=j*32;m:=j*64\n**IF** mask[i+31]\n**dst** [i+31:i]:=*(ptr+a[i+63:i]*s)\nmask[i+31] := 0\n**ELSE**\n**dst** [i+31:i] := src[i+31:i]\nmask[MAX:64] := 0\n**dst** [MAX:64] := 0\n256bit Insert\ninsertf128\nsi256,ps/d\n**m** **insertf128_ps** ( **m** a, **m** b, **ii** i)\n**dst** [255:0] := a[255:0]\nsel := i*128\n**dst** [sel+15:sel]:=b[127:0]"
			]
		  },
		  {
			"title": "Filtering a Vector with SIMD Instructions (AVX-2 and AVX-512) | Quickwit",
			"url": "https://quickwit.io/blog/simd-range",
			"excerpts": [
			  "Let's start with `compact` .\nAVX2 does not exactly have an instruction for this. In the 128-bits world, [`PSHUFB`](https://www.felixcloutier.com/x86/pshufb.html#:~:text=PSHUFB%20performs%20in%2Dplace%20shuffles,leaving%20the%20shuffle%20mask%20unaffected.) is a powerful instruction that\nlets you apply a permutation over the bytes of your register.",
			  "The equivalent instruction exists and is called `vpshufd` , but there is a catch: it only\napplies two disjoint permutations within the two 128-bits lanes, which is perfectly useless to us.",
			  "This is a very common pattern in AVX2 instructions. Instructions crossing that dreaded 128bit-lane are seldom.",
			  "\nFortunately, applying a permutation over `u32s` (which is what we need) is actually possible,\nvia the `VPERMPS` instruction [__mm256_permutevar8x32_epi32](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-vector-extensions-2/intrinsics-for-permute-operations/mm256-permutevar8x32-epi32.html) .",
			  "Then I did what every sane engineer should do. I asked [Twitter](https://twitter.com/fulmicoton/status/1539534316405161984) (Ok I am lying a bit, at the time of the tweet I was playing with SSE2).",
			  "a single byte. The instruction is called `VMOVMSKPS` . I could not find it because it is presented as a floating point instruction to extract the sign of a bunch of 32-bits floats.\n"
			]
		  },
		  {
			"title": "On the usage of the Arm C Language Extensions for a High ...",
			"url": "https://hal.science/hal-03029933v1/document",
			"excerpts": [
			  "In our case, we\nuse the following code:\nauto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "the gath-\nering step of our kernel, we first need to gather indices of\nthe first point of each element considered.",
			  "At order 4, each\nelement is composed of 125 points.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "In our case, we\nuse the following code:",
			  "auto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "At iteration i , the first element of the vector is at position\ni*svcntw()*125 and we need to duplicate it and add the\nvstrides values to obtain the indices of the first point of\neach element in the vector:"
			]
		  },
		  {
			"title": "Introduction to SVE",
			"url": "https://documentation-service.arm.com/static/67ab35a4091bfc3e0a9478b5?token=",
			"excerpts": [
			  "The ACLE (Arm C Language Extension) for SVE defines which SVE instruction functions\nare available, their parameters and what they do.",
			  " To use the ACLE intrinsics,\nyou must include the header file arm_sve.h, which contains a list of vector types and instruction\nfunctions (for SVE) that can be used in C/C++.",
			  "The following example C code has been manually optimized with SVE intrinsics:\n//intrinsic_example.c\n\\ <arm_sve.h>\nsvuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)\n{\n// widening add of even elements\nsvuint64_t result = svaddlb(Zs1, Zs2);\nreturn result;\n}"
			]
		  },
		  {
			"title": "\n\tAgner's tables show the - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Throughput-MUL-FMA-Broadwell/m-p/1151730/highlight/true",
			"excerpts": [
			  "Agner's tables show the throughput for sequences of independent instructions, and the latency for sequences of dependent instructions."
			]
		  },
		  {
			"title": "Release Notes for Intel Intrinsics Guide",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/release-notes/intrinsics-guide-release-notes.html",
			"excerpts": [
			  "Removed extended gather/scatter intrinsics."
			]
		  },
		  {
			"title": "Surprising new feature in AMD Ryzen 3000 | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24302057",
			"excerpts": [
			  " But the scatter/gather instructions do random access memory operations. You have one SIMD register with a 8 (or whatever the width is) indexes to be applied to a base address in a scalar register, and the hardware then goes and does 8 separate memory operations on your behalf, packing the results into a SIMD register at the end.",
			  "That has to hit the cache 8 times in the general case. It's extremely expensive as a single instruction, though faster than running scalar code to do the same thing.",
			  "I looked Agner's tables, and was curious how Intel fared with it. All numbers are reciprocal throughput. So how many cycles per instruction in throughput. zen 2 has it's gather variants mostly 9 and 6 cycles and one variant with 16. Broadwell has only 6,7 and 5 cycles.",
			  "Skylake has mostly 4 and 2 and one variant with 5.",
			  "Now I was surprised by Agners figures for zen2 LOOP and CALL which both have reciprocal throughput of 2. Being equal to doing with just normal jump instructions."
			]
		  },
		  {
			"title": "4. Instruction tables",
			"url": "https://www.agner.org/optimize/instruction_tables.pdf",
			"excerpts": [
			  "VPGATHERDD\nx,[r+s*x],x\n24\n5\nP0123\nAVX2",
			  "VPGATHERDD\ny,[r+s*y],y\n42\n8",
			  "VPGATHERQQ\nx,[r+s*x],x\n18\n4",
			  "VPGATHERQQ\ny,[r+s*y],y\n24\n5",
			  "VPSCATTERDD\n[r+s*x]{k},x\n27\n6",
			  "VPSCATTERQQ\n[r+s*z]{k},z\n48\n12",
			  "VPCOMPRESSD/Q\nv{k},v\n2\n2",
			  "VPEXPANDB/W/D/Q\nx{k},x\n2\n2"
			]
		  },
		  {
			"title": "(PDF) Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://www.academia.edu/145129609/Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Section Title: Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads\nContent:\n[Karthik Sankaranarayanan](https://independent.academia.edu/KarthikSankaranarayanan)\n2020, ArXiv\nvisibility\n\ndescription See full PDF download Download PDF\nbookmark Save to Library share Share\nclose"
			]
		  },
		  {
			"title": "Prefetching for complex memory access patterns",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-923.pdf",
			"excerpts": [
			  "This thesis makes three contributions. I first contribute an automated software prefetch- ing compiler technique to insert high-performance prefetches into ...Read more"
			]
		  },
		  {
			"title": "On Reusing the Results of Pre-Executed Instructions in a ...",
			"url": "https://dl.acm.org/doi/abs/10.1109/L-CA.2005.1",
			"excerpts": [
			  "Previous research on runahead execution took it for granted as a prefetch-only technique. Even though the results of instructions independent of an L2 miss ..."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/pdf/2505.21669",
			"excerpts": [
			  "Hardware prefetcher designs have been previously created with pointer-chasing access patterns in mind. Some techniques compute and store jump ...",
			  "oth et al. describe a prefetching technique in [ **50** ] that utilizes dependency chains to\ndetermine the shape of a linked data structure node in hardware, and issue requests ac-\ncordingly. While this approach requires no modification of the executable, it cannot learn\nthe full structure of an object without observing accesses to all children. Additionally, the\nauthors limit their prefetcher to only a single node ahead, meaning prefetches are only\ntriggered when the core issues new loads, *and* blocks must be returned from the memory\nsystem before new prefetches can be issuedleading to significantly worse performance\nas effective memory access latencies incr",
			  " [ **37** ] build on dependence based prefetching (DBP) as described in [ **50** ] to en-\nable *timely* prefetches of children; this is needed as increased memory access times require\nprefetches to be issued earlier to avoid stalls."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Observing the backward slice shown in Figure 3a, we see that\nthe one cycle in the graph is comprised of a single instruction\n0x6cf , *i.e.* , the stride address increment, and that it is the only\nloop-carried dependence in this backward slice.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "We duplicate the backward slice code\nand assign new registers to it. By analogy, this code is the carrot\nand the main computation is the horse.",
			  "Prior to the entry into\nthe loop, the carrot is first extended *k* iterations ahead of the horse.\nWe call this phase in the dynamic execution the *head start* phase.",
			  "After the entry into the loop, the carrot locks steps with the horse\nand stays a constant *k* iterations ahead. We call this phase in the\ndynamic execution the *stay ahead* phase.",
			  "During the last *k* iterations\nof the loop, the carrot ceases to stay ahead and merges with the\nhorse. We call this phase of dynamic execution the *join* phase.",
			  "**4**",
			  "**M** **ETHOD**",
			  "In the previous section, we explained the problem of memory-\nbound DILs through a hash table example and outlined the\nchallenges in implementing a prefetcher with helper threads",
			  "Here,\nwe will outline our approach to a solution, with a reminder that we\nwant to create a prefetcher implementation without threads.",
			  "We exclude\nsuch scenarios by design for two reasons: first, such situations\nare rare and second, prefetcher complexity increases tremendously\nin such cases.",
			  "To see why, let us consider the example of the\nbinary tree where both the paths are equally likely. If we want to\nprefetch *k* iterations ahead, then there are 2 *k* possible addresses\nto prefetch.",
			  "We have the option of either prefetching all of those\naddresses or implementing a software-based branch predictor to\nselect one of the addresses to prefetch."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "Content:\nAugust 2020\nDOI: [10.48550/arXiv.2009.00202](https://doi.org/10.48550/arXiv.2009.00202)",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/html/2505.21669v1",
			"excerpts": [
			  "[52] Sankaranarayanan, K., Lin, C.-K., and Chinya, G. N. Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads. ArXiv (Sept. 2020).",
			  "These two benchmarks ( bintree_dfs and bintree_bfs ) perform common traversals of binary tree data structures to compute a sum.",
			  "The DFS benchmark is implemented recursively, while the BFS benchmark uses a std::queue from the C++ STL.",
			  "This tree is staticonly lookups are performed.",
			  "In [ [61](https://arxiv.org/html/2505.21669v1.bib61) ] , Wang et al. describe a prefetch engine that utilizes hints from a compiler analysis to inform issued requests.",
			  "Notably, the analysis detects common cases of recursive pointers as used in linked-list traversals, which hardware then exploits to issue prefetches of the LDS up to six levels deep.",
			  "However, layout information is not provided to the hardware prefetch engineit instead speculates on values in a cache block being pointers, falling victim to the same cache pollution flaws as other CDPs.",
			  "Nodes are also assumed to be smaller than two cache blocks, which may not hold across real-world applications.",
			  "The researchers also state the technique does not perform well on trees.",
			  "Most importantly, prefetch requests for an LDS are issued sequentially, and thus performance degrades with increased effective memory access times."
			]
		  },
		  {
			"title": "(PDF) On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor",
			"url": "https://www.academia.edu/126501330/On_Reusing_the_Results_of_Pre_Executed_Instructions_in_a_Runahead_Execution_Processor",
			"excerpts": [
			  "Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs.",
			  "runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient.",
			  "In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance."
			]
		  },
		  {
			"title": "Pointer Cache Assisted Prefetching - Computer Science",
			"url": "https://cseweb.ucsd.edu/~calder/papers/MICRO-02-PCache.pdf",
			"excerpts": [
			  ".\nSpeculative precomputation [6] works by identifying the\nsmall number of static loads, known as delinquent loads, that\nare responsible for the vast majority of memory stall cycles.",
			  "\nPrecomputation slices (p-slices), sequences of dependent in-\nstructions which, when executed, produce the address of a\nfuture delinquent load, are extracted from the program be-\ning accelerated. ",
			  " When an instruction in the non-speculative\nthread that has been identified as a trigger instruction reaches\nsome point in the pipeline (typically commit or rename),\nthe corresponding p-slice is spawned into an available SMT\nthread context.\n",
			  "Speculative slices [28] focus largely on the use of precom-\nputation to predict future branch outcomes and to correlate\npredictions to future branch instances in the non-speculative\nthread, but they also support load prefetching.",
			  "\nSoftware controlled pre-execution [13] focuses on the use\nof specialized, compiler inserted code that is executed in\n",
			  "e trace to extract data reference sequences that fre-\nquently repeat in the same order. At this point, the system\ninserts prefetch instructions to detect and prefetch these fre-\nquent data references.",
			  "The sampling and optimization are\ndone dynamically at runtime with very low overhead.\n*",
			  "The Pointer Cache holds mappings between heap point-\ners and the address of the heap object they point to.",
			  "\nThe primary function of the pointer cache is to break the\nserial dependence chains in pointer chasing code. ",
			  ". When one\nload depends on the data loaded by another, a cache miss by\nthe first load forces the second load to stall until the first load\ncompletes.",
			  "When executing a long sequence of such depen-\ndent pointer-chasing loads, instructions can only be executed\nat the speed of the serial accesses to memory.",
			  "*\nOnly pointer loads are candidates to be inserted into the\npointer cache.",
			  "The\nprograms static instructions are analyzed in the reverse order\nof execution from a delinquent load, building up a slice of in-\nstructions the load is directly and indirectly dependent upon.",
			  "Slice construction terminates when an-\nalyzing an instruction far enough from the delinquent load\nthat a spawned thread can provide a timely prefetch, or when\nfurther analysis will add additional instructions to the slice\nwithout providing further performance benefits.",
			  ". In this form,\na slice consists of a sequence of instructions in the order they\nwere analyzed.",
			  "The single path slices constructed in this work are trig-",
			  "Jump pointers are a software technique for prefetching\nlinked data structures.",
			  "Artificial jump pointers are extra\npointers stored into an object that point to other objects some\ndistance ahead in the traversal order.",
			  "Natural jump pointers are existing pointers in the\ndata structure used for prefetching.",
			  "These techniques were introduced by\nLuk and Mowry [12] and refined in [11] and [17].",
			  "Chilimbi and Hirzel [4] proposed an automated\nsoftware approach based on correlation. Their scheme first\ngathers a data reference profile via sampling. Next, they pro-",
			  "Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer ...Read more",
			  "However, traditional prefetching techniques have diffi- culty with sequences of irregular accesses. A common ex- ample of this type of access is pointer chains, ..."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Speculative precomputation: long-range prefetching of delinquentloads",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Deep-Learning-Driven Prefetching for Far Memory",
			"url": "https://arxiv.org/html/2506.00384v1",
			"excerpts": [
			  "Section Title: Deep-Learning-Driven Prefetching for Far Memory > 1. Introduction",
			  "Many data-center workloads including graph processing ( [PageRank ,](https://arxiv.org/html/2506.00384v1.bib25) ; [han2024graph ,](https://arxiv.org/html/2506.00384v1.bib20) ) , tree and index structures ( [guttman1984r ,](https://arxiv.org/html/2506.00384v1.bib19) ; [gusfield1997algorithms ,](https://arxiv.org/html/2506.00384v1.bib18) ) , pointer chasing ( [hsieh2017implementing ,](https://arxiv.org/html/2506.00384v1.bib24) ) , and recursive data structures ( [harold2004xml ,](https://arxiv.org/html/2506.00384v1.bib21) ) exhibit memory access patterns that defy rule-based prefetching.",
			  "If these access patterns could be learned and predicted accurately, far-memory systems could proactively fetch data and mitigate the performance penalties associated with remote access, even in the absence of new hardware.",
			  "Figure 1. FarSight Achieving Three Key Goals Together. FarSight, FastSwap ( [fastswap ,](https://arxiv.org/html/2506.00384v1.bib2) ) , and Hermit ( [hermit ,](https://arxiv.org/html/2506.00384v1.bib35) ) are far-memory systems that run in the Linux kernel. Voyager ( [voyager ,](https://arxiv.org/html/2506.00384v1.bib39) ) , Hashemi etal. ( [pmlr-v80-hashemi18a ,](https://arxiv.org/html/2506.00384v1.bib22) ) , and Twilight ( [duong2024twilight ,](https://arxiv.org/html/2506.00384v1.bib15) ) are micro-architecture CPU cache prefetchers implemented in simulation or with offline traces.",
			  "Content:"
			]
		  },
		  {
			"title": "An Event-Triggered Programmable Prefetcher for Irregular ...",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/programmableprefetcher.pdf",
			"excerpts": [
			  "Ainsworth and T. M. Jones. Graph prefetching using data structure knowledge. In ICS, 2016. [2] S. Ainsworth and T. M. Jones. Software prefetching for indirect ...Read more"
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads ...",
			"url": "https://www.bohrium.com/paper-details/helper-without-threads-customized-prefetching-for-delinquent-irregular-loads/867745821129441724-108609",
			"excerpts": [
			  "Download the full PDF of Helper Without Threads: Customized Prefetching for Delinquent. Includes comprehensive summary, implementation ..."
			]
		  },
		  {
			"title": "[PDF] Speculative precomputation: long-range prefetching ...",
			"url": "https://www.semanticscholar.org/paper/Speculative-precomputation%3A-long-range-prefetching-Collins-Wang/cd42d31aa8f4d07a41556ee4640cb47d3401b9ef",
			"excerpts": [
			  "This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of ..."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~tmj32/papers/docs/ainsworth19-tocs.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automatically generate software prefetches for indirect\nmemory accesses, a special class of irregular memory accesses often seen in high-performance workloads.",
			  "Across a set of memory-bound benchmarks, our automated pass achieves average\nspeedups of 1.3  for an Intel Haswell processor, 1.1  for both an ARM Cortex-A57 and Qualcomm Kryo,\n1.2  for a Cortex-72 and an Intel Kaby Lake, and 1.35  for an Intel Xeon Phi Knights Landing, each of which\nis an out-of-order core, and performance improvements of 2.1  and 2.7  for the in-order ARM Cortex-A53\nand first generation Intel Xeon Phi.",
			  "Hardware prefetchers in real systems focus on stride patterns [ 10 , 12 , 18 ,\n37 , 40 ]. These pick up and predict regular access patterns, such as those in dense-matrix and array\niteration, based on observation of previous addresses being accessed."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses | Department of Computer Science and Technology",
			"url": "https://www.cst.cam.ac.uk/blog/tmj32/software-prefetching-indirect-memory-accesses",
			"excerpts": [
			  "In the paper we create a compiler pass that will automatically identify opportunities to insert prefetches where we find these memory-indirect accesses.",
			  "One of these is that there must be an induction variable within the transitive closure of the source operand (which, for a load, is the operand that calculates the address to load from).",
			  "This means we search backwards through the data dependence graph, starting at the load, until we find this induction variable.",
			  "When we have identified loads that need to be prefetched then we duplicate all of the necessary computation to calculate the address and insert the software prefetch instructions.",
			  "We also have to add code around any loads that are part of this address computation to prevent them from causing errors at runtime if they calculate an invalid address, for example running beyond the end of an array.",
			  "There are more details in the paper including information about how we schedule the prefetches so that data is available immediately before being used."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automat-\nically generate software prefetches for indirect memory\naccesses, a special class of irregular memory accesses of-\nten seen in high-performance workloads. We evaluate this\nacross a wide set of systems, all of which gain benefit from\nthe technique. We then evaluate the extent to which good\nprefetch instructions are architecture dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Categ",
			  " dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Cat"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective: ACM Transactions on Computer Systems: Vol 36, No 3",
			"url": "https://dl.acm.org/doi/10.1145/3319393",
			"excerpts": [
			  "CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being required.",
			  ")Indirect memory accesses have irregular access patterns that limit the performance\nof conventional software and hardware-based prefetchers. To address this problem,\nwe propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect\nmemory ...",
			  "Software prefetching for indirect memory accesses\")CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being require"
			]
		  },
		  {
			"title": "Filtered runahead execution with a runahead buffer | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830812",
			"excerpts": [
			  "Section Title: Filtered runahead execution with a runahead buffer",
			  "Section Title: Filtered runahead execution with a runahead buffer > References",
			  "Authors : [Milad Hashemi](# \"Milad Hashemi\") Milad Hashemi",
			  "Milad Hashemi",
			  "The University of Texas at Austin",
			  "Pages 358 - 369",
			  "https://doi.org/10.1145/2830772.2830812"
			]
		  },
		  {
			"title": "Copyright by Milad Olia Hashemi 2016",
			"url": "https://repositories.lib.utexas.edu/bitstreams/4d988cbc-f809-418f-972d-8202b4c72bf4/download",
			"excerpts": [
			  "Jeffery A. Brown, Hong Wang, George Chrysos, Perry H. Wang, and John P.\nShen.\nSpeculative precomputation on chip multiprocessors.\nIn *Workshop on*\n*Multithreaded Execution, Architecture, and Compilation* , 2001.",
			  "Luis Ceze, James Tuck, Josep Torrellas, and Calin Cascaval. Bulk disambiguation\nof speculative threads in multiprocessors. In *ISCA* , 2006.",
			  "Murali Annavaram, Jignesh M. Patel, and Edward S. Davidson. Data prefetching\nby dependence graph precomputation. In *ISCA* , 2001."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "putation*\n*enables*\n*effective*\n*cache*\n*prefetching for even irregular memory access behavior, by*\n*using an alternate thread on a multithreaded or multi-core*\n*architecture. This paper describes a system that constructs*\n*and runs precomputation based prefetching threads via*\n*event-driven dynamic optimization. Precomputation threads*\n*are dynamically constructed by a runtime compiler from the*\n*programs frequently executed hot traces, and are adapted*\n*to the memory behavior automatically. Both construction*\n*and execution of the prefetching threads happen in another*\n*thread, imposing little overhead on the main thread. This*\n*paper also presents several techniques to accelerate the pre-*\n*computation threads, including colocation of p-threads with*\n*hot traces, dynamic stride prediction, and automatic adap-*\n*tation of runahead and jumpstart distan",
			  "While in-\nlined prefetches are typically effective for simple addressing\npatterns (e.g., strided addresses), p-thread based prefetching\nhas the potential to handle more complex address patterns\n(e.g. pointer chasing), or accesses embedded in more com-\nplex control flow. This is because the prefetching address is\ncomputed via actual code extracted from the main thread.",
			  "ead.\nA successful precomputation-based prefetcher must ad-\ndress several challenges. It must be able to determine the\nproper distance by which the prefetching thread should lead\nthe main thread, and it should have the ability to control that\ndistance. It must create lightweight threads that can actually\nproceed faster than the main thread, so that they stay out in\nfront. It must prevent p-threads from diverging from the ad-\ndress stream of the main thread, or at least detect when it\nhas happened. This divergence may be the result of control\nflow or address value speculation in the p-thread. Runaway\nprefetching may unnecessarily displace useful data, resulting\nin more data cache m",
			  " sophistication of slice creation.\nMore recent work by Lu, et al. [12] dynamically con-\nstructs p-slices via a runtime optimizer running on an idle\ncore. A single user-level thread is multiplexed to detect the\nprograms phases, construct the p-thread code, and perform\nprecomputation prefetching.",
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "**Loop Re-rolling**  A hot trace may contain multiple\ncopies of the same code due to loop unrolling done during\nstatic compilation. We perform loop *re-rolling* for the p-\nslice (i.e. removing the redundant loop copies) to reduce\nduplicated computation inside a p-slice. This optimization\nincreases the granularity at which we can set the prefetch\nrun-ahead distance, since the prefetch distance is always an\nintegral number of iterations.",
			  "**Object-Based Prefetching**  We perform same-object\nbased prefetching, as in our prior work on inline prefetch-\ning [26]. Same-object prefetching clusters prefetches falling",
			  "**P-Thread Jump Starting**\nSometimes, the only way to get the prefetch thread ahead\nof the main thread is to give it a head start. Existing dy-\nnamic precomputation schemes (e.g.\n[12]) typically start\np-threads from the same starting point (same iteration) as the\nmain thread.",
			  "**6**\n**Results**\nThis section evaluates the cost and performance of our dy-\nnamically generated precomputation based prefetching tech-\nnique.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches).",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "Our approach is complementary to and does not interfere with existing hardware prefetchers since we target only delinquent irregular load instructions (those with no constant or striding address patterns).",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support.",
			  "Delinquent Irregular Loads",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://hlitz.github.io/papers/crisp.pdf",
			"excerpts": [
			  "We also compare CRISP to a hardware-only design referred to as IBDA which performs load slice extraction via iterative backwards dependency analysis. IBDA ...Read more"
			]
		  },
		  {
			"title": "Caching and Performance of CPUs - Jyotiprakash's Blog",
			"url": "https://blog.jyotiprakash.org/caching-and-performance-of-cpus",
			"excerpts": [
			  "Nonblocking Caches to Handle Multiple Misses:** Nonblocking or \"lockup-free\" caches take cache optimization a step further by allowing out-of-order execution. With a nonblocking cache, the CPU doesn't need to stall on a cache miss; instead, it can continue to fetch other instructions while waiting for the missing data. This technique, referred to as \"hit under miss\" or \"miss under miss,\" reduces effective miss penalties by overlapping misses and allowing more efficient cache utilizatio",
			  "ltibanked Caches to Increase Cache Bandwidth:** Instead of treating the cache as a single large block, **multibanked caches** split it into independent banks that can support simultaneous accesses. This approach effectively increases cache bandwidth by allowing multiple memory accesses at the same time.",
			  "For example, modern processors like the Intel Core i7 use multiple banks in their L1 cache, enabling up to two memory accesses per clock cycle. By using **sequential interleaving** , addresses are spread evenly across different banks, ensuring that memory accesses are well distributed, reducing contention, and improving cache performance."
			]
		  },
		  {
			"title": "Understanding the Backward Slices of Performance ...",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/2000/slice.isca.pdf",
			"excerpts": [
			  "backward slice (the subset of the program that relates to*\n*the instruction) of these performance degrading instructions, if*\n*small compared to the whole dynamic instruction stream, can be*\n*pre-executed to hide the instructions latenc",
			  " be effective with respect to a given instruction, a pre-execu-\ntion technique needs three things.",
			  "First, at an *initiation point* ahead\nof the instructions execution, the pre-execution technique needs to\nknow that the performance degrading instruction *will* be executed.",
			  "Second, it has to know which other instructions contribute to the\nperformance degrading instruction.",
			  "-fetches.\nPre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. U",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "The key to answering all of these questions lies in the backward\nslice of the performance degrading instruction.",
			  "Pre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. Unlike the previous two cases, this\npre-executed branch outcome (and perhaps target) needs to be\nbound to a particular dynamic branch instance to fully benefit from\nthe pre-execution.",
			  "n general, pre-execution amounts\nto guessing the existence of a future performance degrading\ninstruction and executing it (or what we think it will be) some time\nprior to its actual encounter in the machine, thereby at least par-\ntially hiding its latency"
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "In this section requirements for\neffective direct pre-execution are examined and hardware\nmechanisms supporting these requirements are described.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://llvm.org/devmtg/2017-03/assets/slides/software_prefetching_for_indirect_memory_accesses.pdf",
			"excerpts": [
			  "Software Prefetching for Indirect. Memory Accesses. Sam Ainsworth and Timothy M. Jones. Computer Laboratory. Page 2. What should we software prefetch? Stride ...Read more"
			]
		  },
		  {
			"title": "Long-range Prefetching of Delinquent Loads",
			"url": "http://cseweb.ucsd.edu/~tullsen/isca2001.pdf",
			"excerpts": [
			  "Speculative Precomputation, a tech-*\n*nique that uses idle thread contexts in a multithreaded ar-*\n*chitecture to improve performance of single-threaded appli",
			  "ecula-\ntive threads are spawned under one of two conditions: when\nencounteringa basic trigger, which occurs when a designated\ninstruction in the main thread reaches a particular pipeline\nstage (such as the commit stage), or a chaining trigger, when\none speculative thread explicitly spawns another.",
			  "A speculative thread is spawned by allocating a hardware\nthread context, copying necessary live-in values into its reg-\nister file, and providing the thread context with the address of\nthe first instruction of the threa",
			  "If a free hardware context\nis not available the spawn request is ignored.",
			  "Necessary live-in values are always copied into the thread\ncontext when a speculative thread is spawned.",
			  "peculative threads execute precomputation slices (p-\nslices), which are sequences of dependent instructions which\nhave been extracted from the non-speculative thread and\ncompute the address accessed by delinquent loads.",
			  "When\na speculative thread is spawned, it precomputes the address\nexpected to be accessed by a future delinquent load, and\nprefetches the data.",
			  "wo primary forms of Speculative Precom-*\n*putation are evaluat",
			  "Delinquent Loads",
			  "We find that in most programs the set of delinquent\nloads is quite small; commonly 10 or fewer static loads cause\nmore than 80% of L1 data cache misses.",
			  " precomputa-\ntion slices used by our work are constructed within an in-\n ...",
			  "ulative precom-\nputation could be thought of as a special prefetch mech-\nanism that effectively targets load instructions that tradi-\ntionally have been difficult to handle via prefetching, such\nas loads that do not exhibit predictable access patterns and\nchains of dependent load",
			  "Speculative threads can be spawned",
			  "hardware structure is called the Outstanding Slice\nCounter (OSC). This structure tracks, for a subset of delin-\nquent loads, the number of instances of delinquent loads\nfor which a speculative thread has been spawned but for\nwhich the main thread has not yet committed the corre-\nsponding load."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3319393",
			"excerpts": [
			  "S. Ainsworth and Timothy M. Jones. 2017. Software prefetching for indirect memory accesses. In *Proceedings of the International Symposium on Code Generation and Optimization (CGO17)* . Navigate to citation 1 citation 2",
			  "Sam Ainsworth and Timothy M. Jones. 2018. An event-triggered programmable prefetcher for irregular workloads. In *Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)* . Navigate to citation 1"
			]
		  },
		  {
			"title": "Orchestrated Scheduling and Prefetching for GPGPUs",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/orchestrated-gpgpu-scheduling-prefetching_isca13.pdf",
			"excerpts": [
			  "In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General. Purpose Graphics Processing Unit (GPGPU) ...Read more"
			]
		  },
		  {
			"title": "Evaluating and Mitigating Bandwidth Bottlenecks Across ...",
			"url": "https://users.cs.utah.edu/~vijay/papers/ispass17.pdf",
			"excerpts": [
			  "For instance, we observe that to prevent throttling of L1 cache, increasing the L1 bandwidth by increasing the MSHRs to handle more outstanding misses can lead ..."
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://hparch.gatech.edu/papers/lee_micro10.pdf",
			"excerpts": [
			  " **In some cases, blindly applying prefetching degrades perfor-**\n**mance. To reduce such negative effects, we propose an** ***adaptive***\n***prefetch throttling*** **scheme, which permits automatic GPGPU**\n**application- and hardware-specific adjustment. We show that**\n**adaptation reduces the negative effects of prefetching and can**\n**even improve performance. Overall, compared to the state-of-**\n**the-art software and hardware prefetching, our MT-prefetching**\n**improves performance on average by 16% (software pref.) / 15%**\n**(hardware pref.) on our benchmarks.**",
			  "IP may not be useful in two cases. The first case is when\ndemand requests corresponding to prefetch requests have al-\nready been generated. This can happen because warps are not\nexecuted in strict sequential order.",
			  ".\nThe second case is when the warp that is prefetching is\nthe last warp of a thread block and the target warp (i.e.\nthread block to which the target warp belongs) has been\nassigned to a different core.",
			  " Inter-thread Prefetching (IP):* One of the main differ-\nences between GPGPU applications and traditional applica-\ntions is that GPGPU applications have a significantly higher\nnumber of thread",
			  "er warp training:* Stream and stride detectors must be\ntrained on a per-warp basis, similar to those in simulta-\nneous multithreading architectures. This aspect is criti-\ncal since many requests from different warps can easily\nconfuse pattern detector",
			  "*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "The key idea behind hardware IP is that when an application\nexhibits a strided memory access pattern across threads at the\nsame PC, one thread generates prefetch requests for another\nthread.",
			  "able*\n*Hardware*\n*Prefetcher*\n*Training:*\nCurrent\nGPGPU applications exhibit largely regular memory access\npatterns, so one might expect traditional stream or stride\nperfetchers to work well. However, because the number of\nthreads is often in the hundreds, traditional training mecha-\nnisms do not scale.",
			  "Here, we describe extensions to the traditional training\npolicies, for program counter (PC) based stride prefetch-\ners [4, 11], that can overcome this limitation. This basic idea\ncan be extended to other types of prefetchers as well.",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  " *Stride promotion:* Since memory access patterns are\nfairly regular in GPGPU applications, we observe that\nwhen a few warps have the same access stride for a given\nPC, all warps will often have the same stride for the\nPC.",
			  "Based on this observation, when at least three PWS\nentries for the same PC have the same stride, we promote\nthe PC stride combination to the *global stride (GS)* table.",
			  "IV. U NDERSTANDING U SEFUL VS . H ARMFUL\nP REFETCHING IN GPGPU",
			  "o as** ***many-thread aware prefetching*** **(MT-prefetching) mecha-*",
			  "nisms.",
			  "access behavior among fine-grained threads.",
			  "For hardware MT-**",
			  "The key ideas behind our MT-prefetching schemes are\n(a) per-warp-training and stride promotion, (b) inter-thread\nprefetching, and (c) adaptive throttling.",
			  "mechanism.",
			  "In some cases, blindly applying prefetching degrades perfor-**",
			  "mance. To reduce such negative effects, we propose an** ***adaptive***",
			  "prefetch throttling*** **scheme, which permits automatic GPGPU**",
			  "application- and hardware-specific adjustment.",
			  "We show that**",
			  "adaptation reduces the negative effects of prefetching and can**",
			  "even improve performance.",
			  "III. P REFETCHING M ECHANISMS FOR GPGPU",
			  "This section describes our *many-thread aware* prefetching",
			  "(MT-prefetching) schemes, which includes both hardware",
			  "and software mechanisms.",
			  "To support these schemes, we\naugment each core of the GPGPUs with a prefetch cache and\na prefetch engine.",
			  "The prefetch cache holds the prefetched\nblocks from memory and the prefetch engine is responsible\nfor throttling prefetch requests (see Section V).",
			  "*A. Software Prefetching*",
			  "We refer to our software prefetching mechanism as *many-*",
			  " refer to our software prefetching mechanism as *many-*\n*thread aware software prefetching* (MT-SWP). MT-SWP con-\nsists of two components: conventional *stride prefetching* and\na newly proposed *inter-thread prefetching* (IP).",
			  "a newly proposed *inter-thread prefetching* (IP).",
			  "*1) Stride Prefetching:* This mechanism is the same as the",
			  "1) Stride Prefetching:* This mechanism is the same as the\ntraditional stride prefetching mechanism. The prefetch cache\nstores any prefetched blocks.",
			  "tions is that GPGPU applications have a significantly higher\nnumber of threads.",
			  "As a result, the execution length of each\nthread is often very short.",
			  "Figure 3 shows a snippet of sequen-\ntial code with prefetch instructions and the equivalent CUDA\ncode without prefetch instructions.",
			  "In the CUDA code, since\nthe loop iterations are parallelized and each thread executes\nonly one (or very few) iteration(s) of the sequential loop, there\nare no (or very few) opportunities to insert prefetch requests",
			  "This can happen because warps are not\nexecuted in strict sequential order.",
			  "For example, when T32\ngenerates a prefetch request for T64, T64 might have already\nissued the demand request corresponding to the prefetch\nrequest generated by T32.",
			  "hese prefetch requests are usu-\nally merged in the memory system since the corresponding\ndemand requests are likely to still be in the memory system",
			  "Unless inter-core merging occurs\nin the DRAM controller, these prefetch requests are useless.",
			  "This problem is similar to the out-of-array-bounds problem\nencountered when prefetching in CPU systems.",
			  "Nevertheless,\nwe find that the benefits of IP far outweigh its negative effects.",
			  "*B. Hardware Prefetching*",
			  "We refer to our hardware prefetching mechanism as\nthe *many-thread aware hardware prefetcher* (MT-HWP).",
			  "T-HWP has (1) enhanced prefetcher training algorithms\nthat provide improved scalability over previously proposed\nstream/stride prefetchers and (2) a hardware-based inter-\nthread prefetching (IP) mechanism.",
			  "This information is stored in a separate table called an\nIP table.",
			  "*3) Implementation:* Figure 6 shows the overall design of\nthe MT-HWP, which consists of the three tables discussed\nearlier: PWS, GS, and IP tables.",
			  " practice, overly aggressive prefetching can have a nega-\ntive effect on performance.",
			  " principal memory latency tolerance mechanism in a\nGPGPU is multithreading. Thus, if a sufficient number of\nwarps and/or enough computation exist, memory latency can"
			]
		  },
		  {
			"title": "CTA-aware Prefetching for GPGPU - Computer Engineering",
			"url": "https://ceng.usc.edu/techreports/2014/Annavaram%20CENG-2014-08.pdf",
			"excerpts": [
			  "Lee et al. [21] proposed a software and hardware\nbased many-thread aware prefetching which basically commands\nthreads to prefetch data for the other threads.",
			  "They exploit the\nfact that the memory addresses are referenced using thread id in\nmany GPU applications."
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/",
			"excerpts": [
			  "To optimize memory access on NVIDIA GPUs, prefetching can be employed in software when excess warps are insufficient to hide memory latency.",
			  "A synchronization within the loopfor example, `syncthreads` constitutes a memory fence and forces the loading of `arr` to complete at that point within the same iteration, not PDIST iterations later. The fix is to use asynchronous loads into shared memory, the simplest version of which is explained in the [Pipeline interface](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) section of the CUDA programmer guide.",
			  "Prefetching can be implemented by unrolling loops, storing prefetched values in registers or shared memory, and using techniques like batched or rolling prefetching, with the latter being more effective when combined with asynchronous memory copies.",
			  "Confirm that not all memory bandwidth is being used.",
			  "Confirm the main reason warps are blocked is **Stall Long Scoreboard** , which means that the SMs are waiting for data from DRAM.",
			  "This leads to the improved shared memory results shown in Figure 2. A prefetch distance of just 6, combined with asynchronous memory copies in a rolling fashion, is sufficient to obtain optimal performance at almost 60% speedup over the original version of the code."
			]
		  },
		  {
			"title": "Near-Side Prefetch Throttling - of Wim Heirman",
			"url": "https://heirman.net/papers/pact2018.pdf",
			"excerpts": [
			  " near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine. The MSHR tracks outstand-\ning cache misses triggered by both demand requests (application\nloads and stores), and prefetches; the extra state allows for detec-\ntion of late prefetches. The state machine periodically computes\nthe fraction of late prefetches, and updates the optimal prefetch\ndistance.\n",
			  "Maintaining a small fraction\n(e.g., 10%) of late prefetches does not harm performance as long as\nthe late prefetches are only late by a small amount, i.e., the demand\naccess is made only just before the prefetch request completes.",
			  "Near-Side Prefetch Throttling",
			  " We show that near-side throttling can be extended to mul-\ntiple prefetchers per core, where it will naturally throttle\nthose prefetchers that yield no useful requests, allowing for\na diverse set of prefetch algorithms to co-exis",
			  "e throttling detects bandwidth\nsaturation locally (through memory latency), so no global coordi-\nnation is needed between the per-core prefetchers in a many-core\narchitecture, or even between multiple prefetch algorithms on a\nsingle core.\n",
			  "ng detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost. This makes near-side throttling superior over traditional\nfar-side throttling as it is able to provide even slightly better per-\nformance (9.6% vs. 9.4%), at a far cheaper implementation cost,\nand is more widely applicable to other use cases such as software\nprefetching and control of multiple hardware prefetchers.",
			  "The basic concept of near-side prefetch throttling (NST) is to\ndetect late prefetches, and tune the prefetcher aggressiveness such\nthat the amount of late prefetches is balanced around a small but\nnon-zero fraction of all prefetches.",
			  "NEAR-SIDE PREFETCH THROTTLING**",
			  "ur\nsolution is cheap to implement in hardware, includes throttling on\noff-chip bandwidth saturation, applies to both hardware and soft-\nware prefetching, and can control multiple concurrent prefetchers\nwhere it will naturally allow the most useful prefetch algorithm\nto generate most of the requests",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "In a many-core processor, the prefetchers in each core can be con-\ntrolled independently based on their own specific late prefetch\nfraction. This allows for heterogenous applications or multi-\nprogramming workloads, and will tune each prefetchers distance\nto the specific access pattern it is experienci",
			  "Our proposed implementation of near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine.",
			  "sing detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Hardware Design of DRAM Memory Prefetching Engine for ...",
			"url": "https://www.mdpi.com/2227-7080/13/10/455",
			"excerpts": [
			  "Inter-warp prefetching mechanisms are based on the detection of stride patterns and base addresses in different warps. A many-thread-aware prefetching mechanism ...Read more"
			]
		  },
		  {
			"title": "APAC: An Accurate and Adaptive Prefetch Framework with ...",
			"url": "https://par.nsf.gov/servlets/purl/10251073",
			"excerpts": [
			  "Near-side prefetch throttling (NST) [9] only adjusts the aggressiveness of prefetching based on the fraction of late prefetchers, which has a relatively small ...Read more"
			]
		  },
		  {
			"title": "PPT - Many-Thread Aware Prefetching Mechanisms for GPGPU Application PowerPoint Presentation - ID:5741796",
			"url": "https://www.slideserve.com/phil/many-thread-aware-prefetching-mechanisms-for-gpgpu-application",
			"excerpts": [
			  "akash\n**[Motivation](https://image3.slideserve.com/5741796/motivation-l.jpg \"motivation\")**  Memory latency hiding through multithread prefetching schemes  Per-warp training and Stride promotion  Inter-thread Prefetching  Adaptive Throttling  Propose software and hardware prefetching mechanisms for a GPGPU architecture  Scalable to large number of threads  Robustness through feedback and throttling mechanisms to avoid degraded performance\n",
			  "rmance\n**[Memory Latency Hiding techniques](https://image3.slideserve.com/5741796/memory-latency-hiding-techniques-l.jpg \"memory latency hiding techniques\")**  Multithreading  Thread level and Warp level context switching  Utilization of complex cache memory hierarchies  Using L1, L2, DRAMs than accessing Global Memory each time  Prefetching  Insufficient thread-level parallelism  Memory request merging Thread1 Thread2 Thread1 Thread3",
			  "MT-HWP  Stride Promotion  Considering the stride pattern is the same across all warps for a given PC, PWS is monitored for three accesses  If found same stride, promote the PWS to Global Stride(GS) table, if not, retain in PWS  Inter-thread Prefetching  Monitor stride pattern across threads at the same PC, for 3 memory accesses  If found same, stride information is stored in the IP table",
			  "MT-HWP  Implementation  When there are hits in both GS and IP, GS is given preference because  Strides"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications | Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2010.44",
			"excerpts": [
			  "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract\nContent:\nWe consider the problem of how to improve memory latency tolerance in massively multithreaded GPGPUs when the thread-level parallelism of an application is not sufficient to hide memory latency. One solution used in conventional CPU systems is prefetching, both in hardware and software. However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously. This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms. Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads. For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, blindly applying prefetching degrades performance. To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment. We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Overall, compared to the state-of-the-art software and hardware prefetching, our MT-prefetching improves performance on average by 16%(software pref.) / 15% (hardware pref.) on our benchmarks.",
			  "Section Title: Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract",
			  "Content:",
			  "However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously.",
			  "This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms.",
			  "Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads.",
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism.",
			  "In some cases, blindly applying prefetching degrades performance.",
			  "To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment.",
			  "We show that adaptation reduces the negative effects of prefetching and can even improve performance."
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.semanticscholar.org/paper/The-adaptive-radix-tree%3A-ARTful-indexing-for-Leis-Kemper/6abf5107efc723c655956f027b4a67565b048799",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Content:",
			  "Content:"
			]
		  },
		  {
			"title": "[PDF] Near-side prefetch throttling: adaptive prefetching for high-performance many-core processors | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Near-side-prefetch-throttling%3A-adaptive-prefetching-Heirman-Bois/39e6013cbe45a288431ddb4269611a483c90bbb9",
			"excerpts": [
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "TLDR"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nDaniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, and Jir Bittner. 2021. A Survey on Bounding Volume Hierarchies for Ray Tracing. Computer Graphics Forum (2021).\n[Go",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Meister%2C+Shinji+Ogaki%2C+Carsten+Benthin%2C+Michael%C2%A0J.+Doyle%2C+Michael+Guthe%2C+and+Jir%C3%AD+Bittner.+2021.+A+Survey+on+Bounding+Volume+Hierarchies+for+Ray+Tracing.+Computer+Graphics+Forum+%282021%29.)",
			  "[32]",
			  "Bochang Moon, Yongyoung Byun, Tae-Joon Kim, Pio Claudio, Hye-Sun Kim, Yun-Ji Ban, Seung Woo Nam, and Sung-Eui Yoon. 2010. Cache-Oblivious Ray Reordering. ACM Transactions on Graphics (TOG) (2010).",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Bochang+Moon%2C+Yongyoung+Byun%2C+Tae-Joon+Kim%2C+Pio+Claudio%2C+Hye-Sun+Kim%2C+Yun-Ji+Ban%2C+Seung%C2%A0Woo+Nam%2C+and+Sung-Eui+Yoon.+2010.+Cache-Oblivious+Ray+Reordering.+ACM+Transactions+on+Graphics+%28TOG%29+%282010%29.)",
			  "[33]",
			  "Paul Arthur Navratil, Donald S. Fussell, Calvin Lin, and William R. Mark. 2007. Dynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In IEEE Symposium on Interactive Ray Tracing. 95104.\n[Dig",
			  "[Digital Library](/doi/10.1109/RT.2007.4342596)",
			  "[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FRT.2007.4342596)",
			  "[34]"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://info.computer.org/csdl/proceedings-article/micro/2010/05695538/12OmNzcPApv",
			"excerpts": [
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, ...Read more"
			]
		  },
		  {
			"title": "(PDF) Adaptive Prefetching for Fine-grain Communication in PGAS Programs",
			"url": "https://www.researchgate.net/publication/381186663_Adaptive_Prefetching_for_Fine-grain_Communication_in_PGAS_Programs",
			"excerpts": [
			  "In this work, we present an adaptive prefetching optimization that can be applied to PGAS programs with irregular memory access patterns. We ...Read more"
			]
		  },
		  {
			"title": "System level cache prefetching algorithms for complex ...",
			"url": "https://lup.lub.lu.se/student-papers/record/9159122/file/9159147.pdf",
			"excerpts": [
			  "These merges occur when a prefetch request is late and a demand request is coming at the same time. The throttling behaviour can be observed in table 1. Early ...Read more"
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching - Technical Blog - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/boosting-application-performance-with-gpu-memory-prefetching/209210",
			"excerpts": [
			  "This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.Read more"
			]
		  },
		  {
			"title": "Many-thread aware hardware prefetcher (MT-HWP). | Download Scientific Diagram",
			"url": "https://www.researchgate.net/figure/Many-thread-aware-hardware-prefetcher-MT-HWP_fig7_221005092",
			"excerpts": [
			  "h consists of the three tables discussed earlier: PWS, GS, and IP tables. The ",
			  "The IP and GS tables are indexed in parallel with a PC address.",
			  "adaptive MT-HWP pro- vides a 29% performance improvement over the baseline."
			]
		  },
		  {
			"title": "Some issues regarding the use of prefetch in the cuda kernel - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/some-issues-regarding-the-use-of-prefetch-in-the-cuda-kernel/334568",
			"excerpts": [
			  "e difference between cp.async and normal loads is the means of re-synchronization of the asynchronous operations.",
			  "For normal loads from global memory, the re-synchronization works with the help of the long scoreboard, as soon as the result registers are used in instructions dependent on the result.",
			  "For cp.async, the results are transferred into shared memory. You have to use cp.async.wait_group or cp.async.wait_all for re-synchronization."
			]
		  },
		  {
			"title": "Combining Local and Global History for High Performance ...",
			"url": "https://jilp.org/dpc/online/papers/00dimitrov.pdf",
			"excerpts": [
			  "To capture delta correlation, a delta buffer is included in the prefetch function, which keeps the delta information when a linked list is traversed in the GHB.Read more"
			]
		  },
		  {
			"title": "L5 - Advanced Memory Prefetching Techniques",
			"url": "https://coconote.app/notes/5551d401-c94c-4d86-bb70-c67c9e8bb912",
			"excerpts": [
			  "Delta correlation prefetchers track repeating delta patterns between accessed addresses to predict future accesses. Correlation-based ...Read more"
			]
		  },
		  {
			"title": "Pointer jumping - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Pointer_jumping",
			"excerpts": [
			  "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs.Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov Predictors",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=joseph_isca97.pdf",
			"excerpts": [
			  "The Markov prefetcher provides the best performance across nll\nthe applications, particularly when applied to both the instruction\nand data caches."
			]
		  },
		  {
			"title": "A Survey on Recent Hardware Data Prefetching ...",
			"url": "https://arxiv.org/pdf/2009.00715",
			"excerpts": [
			  "99] D. Joseph and D. Grunwald, Prefetching Using Markov Predictors, in *Proceedings of the International Symposium on*\n*Computer Architecture (ISCA)* , pp. 252263, 1997"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1999/jmp-ptr.isca.pdf",
			"excerpts": [
			  "bstract**\n*Current techniques for prefetching linked data structures*\n*(LDS) exploit the work available in one loop iteration or*\n*recursive call to overlap pointer chasing latency. Jump-*\n*pointers, which provide direct access to non-adjacent*\n*nodes, can be used for prefetching when loop and recur-*\n*sive procedure bodies are small and do not have sufficient*\n*work to overlap a long latency. This paper describes a*\n*framework for jump-pointer prefetching (JPP) that sup-*\n*ports four prefetching idioms: queue, full, chain, and root*\n*jumping and three implementations: software-only, hard-*\n*ware-only, and a cooperative software/hardware tech-*\n*nique.",
			  "n a suite of pointer intensive programs, jump-*\n*pointer prefetching reduces memory stall time by 72% for*\n*software, 83% for cooperative and 55% for hardware, pro-*\n*ducing speedups of 15%, 20% and 22% respecti",
			  "general purpose technique for tol-\nerating serialized latencies that result from LDS tra-\nversal.\nBy storing explicit jump-pointers to nodes\nseveral hops away, JPP overcomes the pointer-chasing\nproblem.\nIt is able to generate prefetch addresses\ndirectly, rather than in a serial fashion, and is effective\neven in situations where not enough work is available\nto hide latencies by scheduling.\n**",
			  "P implementations: software-only,\nhardware-only, and cooperative.\nFor those programs\nwith appreciable memory latency components, these\nimplementations reduce overall observed memory\nlatency by 72%, 55%, and 83%, respectively and\nachieve speedups of 15%, 22%, and 20%.\n",
			  "**Abstract**",
			  "*Current techniques for prefetching linked data structures*",
			  "\n*(LDS) exploit the work available in one loop iteration or*",
			  "*recursive call to overlap pointer chasing latency. Jump-*",
			  "*pointers, which provide direct access to non-adjacent*",
			  "*nodes, can be used for prefetching when loop and recur-*",
			  "*sive procedure bodies are small and do not have sufficient*",
			  "*work to overlap a long latency. This paper describes a*",
			  "*framework for jump-pointer prefetching (JPP) that sup-*",
			  "*ports four prefetching idioms: queue, full, chain, and root*",
			  "*jumping and three implementations: software-only, hard-*",
			  "*ware-only, and a cooperative software/hardware tech-*",
			  "*nique.*",
			  "*On a suite of pointer intensive programs, jump-*",
			  "*pointer prefetching reduces memory stall time by 72% for*",
			  "*software, 83% for cooperative and 55% for hardware, pro-*",
			  "*ducing speedups of 15%, 20% and 22% respectively.*",
			  "*\n**1 Introduction**",
			  "Linked data structures (LDS) are common in many appli-",
			  "cations, and their importance is growing with the spread of",
			  "object-oriented programming.",
			  "The popularity of LDS",
			  "stems from their flexibility, not their performance. LDS",
			  "access, often referred to as *pointer-chasing* , entails chains",
			  "of data dependent loads that serialize address generation",
			  "and memory access.",
			  "In traversing an LDS, these loads",
			  "often form the programs critical path.\nC",
			  "Consequently,",
			  "when they miss in the cache, they can severely limit paral-",
			  "lelism and degrade performance.",
			  "Prefetching is one way to hide LDS load latency and",
			  " ... \nefficiently [6] and to parallelize searches and reductions on",
			  "lists [9].",
			  "Discussions of maintaining recursion-avoiding",
			  "traversal *threads* in non-linear data structures can be found",
			  "in data structures literature [18]. As noted earlier, Luk and",
			  "Mowry [11] suggested the use of programmer controlled",
			  "jump-pointers for prefetching. We are not aware of any",
			  "implementations, actual or proposed,\nof hardware or",
			  "cooperative jump-pointer prefetching.",
			  "\n**6 Summary and Future Directions**\n",
			  "In this paper, we describe the general technique of jump-",
			  "pointer prefetching (JPP) for tolerating linked structure",
			  "(LDS) access latency. JPP is effective when limited work",
			  "is available between successive dependent accesses (e.g., a",
			  "***Figure***",
			  "***7.***",
			  "***Tolerating***",
			  "***longer***",
			  "***memory***",
			  "***latencies.***",
			  "*Execution times for health: the first group of bars uses*",
			  "*the base configuration (70 cycle memory latency), the*",
			  "*second and third simulate long memory latency (280*",
			  "cycles).*",
			  "*In terms of prefetching, the first two*",
			  "*configurations use a jump interval (the distance*",
			  "*\n*between a jump-pointers home and target nodes) of 8,*",
			  "the third uses an interval of 16.*",
			  "0.0",
			  "0.5",
			  "1.0",
			  "1.5",
			  "2.0",
			  "2.5",
			  "3.0",
			  "MemLat=70",
			  "Interval=8",
			  "Interval=8",
			  "BDSCH",
			  "BDSCH",
			  "BDSCH",
			  "MemLat=280",
			  "MemLat=280",
			  "Interval=16",
			  "Normalized Execution Time",
			  "memory latency",
			  "Compute time",
			  "**Legend: B:** Base",
			  "**D:** DBP",
			  "**S:** Software JPP",
			  "**C:** Cooperative JPP",
			  "**H:** Hardware JPP",
			  "tight pointer chasing loop) to enable aggressive scheduling"
			]
		  },
		  {
			"title": "The Efficacy of Software Prefetching and Locality ...",
			"url": "https://jilp.org/vol6/v6paper7.pdf",
			"excerpts": [
			  "In jump pointer prefetching, additional pointers are inserted into a dynamic data structure\nto connect non-consecutive link elements.",
			  "These jump pointers allow prefetch instructions to\nname link elements further down the pointer chain ( i.e. a prefetch distance, PD , away which\nis computed as in Sections 3.1 and 3.2) without sequentially traversing the intermediate links.",
			  "Consequently, prefetch instructions can overlap the fetch of multiple link elements simultaneously\nby issuing prefetches through the memory addresses stored in the jump pointers.",
			  "Jump pointer prefetching, however, cannot prefetch the first PD link nodes in a linked list\nbecause there are no jump pointers that point to these early nodes.",
			  "To enable prefetching of early\nnodes, jump pointer prefetching can be extended with prefetch arrays [7]. In this technique, an\narray of prefetch pointers is added to every linked list to point to the first PD link nodes.",
			  "Hence,\nprefetches can be issued through the memory addresses in the prefetch arrays before traversing\neach linked list to cover the early nodes, much like the prologue loops in affine array and indexed\narray prefetching prefetch the first PD array elements.",
			  "Figure 5 Part(A) illustrates the addition\nof a prologue loop that performs prefetching through a prefetch array.",
			  "6.3 ccmalloc and Prefetching",
			  "Software prefetching for pointer-chasing codes suffers high overhead to create and manage jump\npointers, as described in Section 5.2.",
			  "owever, jump pointers may not be necessary when prefetch-\ning is combined with ccmalloc memory allocation",
			  "Since intelligent allocation places link nodes\ncontiguously in memory, prefetch instructions can access future link nodes by simple indexing, just\nas for affine array accesses.",
			  "Figure 9 shows the effect of ccmalloc on nodes linked together by\npointers.",
			  "From the right-hand part of the figure, it is intuitive that a compiler can insert prefetches\nfor list nodes further down the list using the size of a node and the location of the first node.",
			  "This approach, which we call index prefetching [1, 37], was originally proposed in [8].",
			  "With in-\ndex prefetching, the jump pointers can be removed, thus eliminating all the overhead associated\nwith jump pointer prefetchin",
			  "To quantify this benefit, we created index prefetching versions for\nHealth and MST , and show the results for these benchmarks in Figure 18.",
			  "(We did not create an\nindex prefetching version for EM3D , our third pointer-chasing benchmark, since it already achieves\nhigh performance with normal software prefetching as shown in Figures 10 and 13)."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta- ...",
			"url": "https://jilp.org/vol13/v13paper2.pdf",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " Correlating Prediction\nTables (DCPT). DCPT builds upon two previously proposed prefetcher techniques, com-\nbining them and refining their ideas to achieve better perf"
			]
		  },
		  {
			"title": "Prefetching",
			"url": "https://ece752.ece.wisc.edu/lect15-prefetching.pdf",
			"excerpts": [
			  "Markov prefetching forms address correlations",
			  "Joseph and Grunwald (ISCA 97)",
			  "Uses global memory addresses as states in the Markov graph",
			  "Correlation Table *approximates* Markov graph",
			  "obal History Buffer (GHB)\n Holds miss address\nhistory in FIFO order",
			  "Global History Buffer",
			  "Delta-based prefetching leads to much smaller table than\nclassical Markov Prefetching",
			  "Delta-based prefetching can remove compulsory misses"
			]
		  },
		  {
			"title": "Data prefetching on in-order processors",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/2268c547-2c42-49d5-9e73-7578ebe3758e/content",
			"excerpts": [
			  "[14] K. J. Nesbit and J. E. Smith, Data cache prefetching using a global history buffer, Software, IEE Proceedings-, 2004. [15] S. Srinath, O. Mutlu, H ...Read more"
			]
		  },
		  {
			"title": "Feedback Mechanisms for Improving Probabilistic Memory ...",
			"url": "https://www.cs.utexas.edu/~lin/papers/hpca09.pdf",
			"excerpts": [
			  "The efficiency of stream prefetching has been improved by Nesbit and. Smith [18], who introduce the Global History Buffer to im- prove prefetch effectiveness ...Read more"
			]
		  },
		  {
			"title": "Data Access History Cache and Associated Data Prefetching ...",
			"url": "http://www.cs.iit.edu/~scs/assets/files/SC07_DAHC.pdf",
			"excerpts": [
			  "Nesbit and Smith proposed a global history buffer for data prefetching in [14] and. [15]. The similarity between their work and our work is that both attempt ..."
			]
		  },
		  {
			"title": "TDT4260 Computer Architecture Mini-Project",
			"url": "https://www.nichele.eu/files/nichele_tdt4260.pdf",
			"excerpts": [
			  "[14] M. Grannaes, M. Jahre and L. Natvig. Multi-level Hardware Prefetching. Using Low Complexity Delta Correlating Prediction Tables with Partial. Matching.Read more"
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer",
			"url": "https://www.researchgate.net/publication/3215463_Data_Cache_Prefetching_Using_a_Global_History_Buffer",
			"excerpts": [
			  "This research is to design a history table-based linear analysis ... This paper studies hardware prefetching for second-level (L2) caches."
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | Proceedings of the 10th International Symposium on High Performance Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1109/HPCA.2004.10030",
			"excerpts": [
			  "A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction.",
			  "The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.",
			  "Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods.",
			  "Global History Buffer prefetching can increase correlation prefetching performance by 20% and cut its memory traffic by 90%. Furthermore, the Global History Buffer can make correlations within a loads address stream, which can increase stride prefetching performance by 6%. "
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | IEEE Micro",
			"url": "https://dl.acm.org/doi/abs/10.1109/MM.2005.6",
			"excerpts": [
			  "By organizing data cache prefetch information in a new way, a GHB supports existing prefetch algorithms more effectively than conventional prefetch tables. It reduces stale table data, improving accuracy and reducing memory traffic. It contains a more complete picture of cache miss history and is smaller than conventional tables",
			  "The structure is based on a Global History Buffer that holds the most\nrecent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer",
			  "HPCA '04: Proceedings of the 10th International Symposium on High Performance Computer ArchitectureA new structure for implementing data cache prefetching is proposed and analyzed via",
			  "A new structure for implementing data cache prefetching is proposed and analyzed via"
			]
		  },
		  {
			"title": "DATA CACHE PREFETCHING USING A GLOBAL ...",
			"url": "https://minds.wisconsin.edu/bitstream/1793/11158/1/file_1.pdf",
			"excerpts": [
			  "As a circular buffer, the GHB prefetching\nstructure eliminates many problems associat-\ned with conventional tables. First, the GHB\nFIFO naturally gives table space priority to\nthe most recent history, thus eliminating the\nstale-data problem.",
			  "dex table entries contain point-\ners into the GHB.",
			  "The GHB is larger, with a size chosen to hold\na representative portion of the miss address\nstream. Last, and perhaps most important, a\ndesigner can use the ordered global history to\ncreate more-sophisticated prefetching meth-\nods than conventional stride and correlation\nprefetchin"
			]
		  },
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%.",
			  "In\nfigure 7 we show the average portion of deltas that can be\nrepresented with a given amount of bits across all SPEC2006\nbenchmarks.",
			  "Although the coverage steadily increases with the amount\nof bits used, speedup has a distinct knee at around 7 bits.",
			  "In figure 8 we show the geometric mean of speedups as\na function of the number of deltas per table entry.",
			  "One of the main differences between DCPT and PC/DC is\nthat DCPT stores deltas, while PC/DC stores entire addresses\nin its GHB.",
			  "the deltas are usually quite small, fewer\nbits are needed to represent a delta than a full address."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov predictors | Proceedings of the 24th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/264107.264207",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the *Markov prefetcher.* This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching *multiple reference predictions* from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "Prefetching using Markov predictors for ISCA 1997 - IBM Research",
			"url": "https://research.ibm.com/publications/prefetching-using-markov-predictors--1",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems.",
			  "In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs.",
			  "The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://people.ucsc.edu/~hlitz/papers/crisp.pdf",
			"excerpts": [
			  " Prefetching using Markov predictors.\n",
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using Markov predictors.",
			  "Tempo-\nral prefetchers [ 8 , 49 , 54 , 119 , 122 ] track the temporal order of\ncache line accesses based on Markov prefetching [ 55 ] introducing\nsignificant storage overheads in the order of megabytes in con-\ntrast to CRISP",
			  " Runahead prefetchers [ 6 , 33 , 48 , 82  84 , 89 , 95 ] and\nhelper threads [ 21 , 23 , 24 , 70 , 73 , 74 , 110 , 117 , 123 , 124 ] prefetch\nirregular memory accesses as in linked-list traversals, however,\nthey introduce significant hardware complexity or consume sepa-\nrate SMT-threads [ 34 ] whereas CRISP requires only minimal hard-\nware modifications. Bra",
			  "CRISP can be\ncombined with these prior approaches to increase coverage by\nreducing the miss penalty of irregular memory accesses. T"
			]
		  },
		  {
			"title": "Perceptron-Based Prefetch Filtering - Engineering People Site",
			"url": "https://people.engr.tamu.edu/djimenez/pdfs/ppf_isca2019.pdf",
			"excerpts": [
			  "7.2\nLookahead Prefetchers\nUnlike spatial prefetchers, lookahead prefetchers take program order\ninto account when they make predictions. Shevgoor et al. propose\nthe Variable Length Delta Prefetcher (VLDP) [ 35 ], which correlates\nhistories of deltas between cache line accesses within memory pages\nwith the next delta within that page. SPP [ 2 ] and KPCs prefetching\ncomponent [ 36 ] are more recent examples of lookahead prefetchers.\n",
			  "Ishii et al. propose the Access Map Pattern\nMatching prefetcher (AMPM) [ 11 ], which creates a map of all ac-\ncessed lines within a region of memory, and then analyzes this map\non every access to see if any fixed-stride pattern can be identified\nand prefetched that is centered on the current access.",
			  "In a single core configuration, PPF increases performance by\n3.78% compared to the underlying prefetcher, SPP.",
			  "In a multi-core\nsystem running a mixes of memory intensive SPEC CPU 2017 traces,\nPPF saw an improvement of 11.4% over SPP for a 4-core system,\nand 9.65% for an 8-core system.",
			  "Michaud proposes the\nBest-Offset Prefetcher [ 34 ], which determines the optimal offset to\nprefetch while considering memory latency and prefetch timeliness.",
			  "DRAM-Aware\nAMPM (DA-AMPM) [ 32 ] is a variant of AMPM that delays some\nprefetches so they can be issued together with others in the same\nDRAM row, increasing bandwidth utilization.",
			  "Pugsley et al. pro-\npose the Sandbox Prefetcher [ 33 ], which analyzes candidate fixed-\noffset prefetchers in a sandboxed environment to determine which is\nmost suitable for the current program phase."
			]
		  },
		  {
			"title": "Building Efficient Neural Prefetcher",
			"url": "https://www.memsys.io/wp-content/uploads/2023/09/3.pdf",
			"excerpts": [
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using markov predictors. In\n*Proceedings of the 24th annual international symposium on Computer architecture* .\n252",
			  "chun Kim, Seth H Pugsley, Paul V Gratz, AL Narasimha Reddy, Chris Wilker-\nson, and Zeshan Chishti. 2016. Path confidence based lookahead prefetching.\nIn *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture*\n*(MICRO)* . IEEE, 112."
			]
		  },
		  {
			"title": "Arsenal of Hardware Prefetchers",
			"url": "https://www.arxiv.org/pdf/1911.10349v1",
			"excerpts": [
			  "ature Path Prefetcher** (SPP) [ 5 ] stores the stride pat-\nterns in a compressed form in the signature table (ST). Each\nentry in the ST is used to index into the pattern table (PT),\nwhich is used to predict the next stride and also contains the\nconfidence for the current prefetch. The signature is then up-\ndated with the latest stride and is used to recursively lookup\nthe PT to predict more strides. This goes on until the confi-\ndence, which is multiplied with the last prefetch confidenc"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching For Linked Data ...",
			"url": "https://www.scribd.com/document/861770035/9",
			"excerpts": [
			  "This paper presents a framework for jump-pointer prefetching (JPP) aimed at improving the performance of linked data structures (LDS) by ...Read more"
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "Jump pointers, which provide direct access to non-adjacent nodes, can be used for prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures",
			  "New technique: Jump Pointer Prefetching",
			  "Creates parallelism",
			  "Hides arbitrary latency",
			  "Choice of implementation: software, hardware, cooperative",
			  "Problem: Pointer chasing latency",
			  "pointer loads",
			  "Jump pointer prefetching:\nOverlap pointer loads with each other anyway!"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  " uses the underlying data of the application, and provides an 11.3% speedup using no additional processor state. By adding less than 1/2% space 2 overhead to the second level cache, performance can be further increased to 12.6% across a range of \"real world\" applications.\n*",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems.",
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "There are a number of ways to\nidentify \"likely\" addresses.",
			  "Roth\net al.\nintroduced dependence-based techniques for capturing\nproducer-consumer load pairs [ 12].",
			  "his paper investigates a technique that predicts addresses in\npointer-intensive applications using a hardware only technique with\nno built-in biases toward the layout of the recursive data struc-\nSection Title: A state",
			  "ability to \"run ahead\" of an application has been shown to be a re-\nquirement for pointer-intensive applications [12], which tradition-\nally do not provide sufficient computational work for masking the\nprefetch latency.",
			  "me hybrid prefetch engines [13] do have the\nability to run several instances ahead of the processor, but require\napriori\nknowledge of the layout of the data structure, and in some\ncases, the traversal order of the structu",
			  "es. Content-based prefetching works by examining the content of data as it is moved from memory to the caches. Data values that are likely to be addresses are then translated and pushed to a prefetch buffer. Con",
			  "N\nIn early processor designs, the performance of the processor and\nmemory were comparable, but in the last 20 years their relative\nperformances have steadily diverged [4], with the performance im-\nprovemen"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  "Content-Directed Data Prefetching, a data*\nprefetching architecture that exploits the memory allocation used\nby operating systems and runtime systems to improve the perfor-\nmance of pointer-intensive applications constructed using modem\nlanguage systems. This technique is modeled after conservative\ngarbage collection, and prefetches \"likely\" virtual addresses ob-\nserved in memory references",
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "Prefetching:- - CSE IITM",
			"url": "https://www.cse.iitm.ac.in/~pritam/prefetching.pdf",
			"excerpts": [
			  "**PRITAM MAJUMDER, PACE LAB, CSE DEPT., IIT MADRAS** **9**\nCollins Jamison\nCalder Brad\nTullsen Dean M\nPointer Cache Assisted Prefetching\n2002\nMICRO"
			]
		  },
		  {
			"title": "CS 473 Homework 3 (due 12/20/03) Fall 2004",
			"url": "https://jeffe.cs.illinois.edu/teaching/473/hw3final.pdf",
			"excerpts": [
			  "Storing a complete binary search tree in the van Emde Boas layout allows us to perform any search in O(logB N) memory operations in the cache-oblivious model.Read more"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://diglib.eg.org/bitstreams/c41b2a27-076a-4252-a346-584b2deac5fd/download",
			"excerpts": [
			  "Yoon et al. [YM06] proposed a cache-oblivious. BVH layout for collision detection which applied to a k- d tree for ray tracing, resulted ...Read more",
			  "The common DFS layout performed worst for all node layouts in both memory areas. Excluding layouts that use statistics the equally simple to construct BFS ...Read more",
			  "BVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and p",
			  "reelet based DFS/BFS (TDFS/TBFS):** A treelet is a\nconnected sub-tree of a BVH. For this layout treelets of\nnodes that were accessed above a certain threshold are\nbuilt",
			  "The internal memory layout of a treelet can be cho-\nsen freely. By always adding nodes just to the front or the\nback of the merge queue we automatically obtain a treelet\nin DFS or BFS order. Finally the node order of the whole\ntree is obtained by lining up the nodes of all treelets.",
			  "A32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines)",
			  "A16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache line",
			  "**Table 1:** *Scenes used for benchmarking",
			  "ivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and par",
			  " cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.",
			  "**5.1. Node Layouts**",
			  "e classic BVH node data structure stores a bounding vol-\nume along with pointers to its children. We follow Aila et\nal. [ AL09 ], i.e., a node does not store its bounding box, but\nthe bounding boxes of its children. Both children are fetched\nand tested together, which is more efficient for GPUs due\nto increased instruction level parallelism and allows rough\nfront to back traversal. Depending on the data layout, the\nsize of such a node is at least 56 bytes (2 float values for\nminimum/maximum per dimension and child plus pointers).",
			  "*AoS:** 64 bytes, including 8 bytes padding (fitting 2 nodes\nin one 128B cache line",
			  "*SoA32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines",
			  "*SoA16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache lin",
			  "We also analyzed an SoA8 layout which fitted 16 nodes in\n7 cache lines. As it consistently performed much worse than\nthe other layouts, we excluded it from our experiments.",
			  "**5.2. Tree Layouts**",
			  "A tree layout describes how nodes are grouped in memory.",
			  "We analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:",
			  "DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf",
			  "BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches",
			  "vEB):** A cache-oblivious tree layout\n[ vEB75 ]",
			  "ext we describe our two proposed layouts depending on\nnode access statistics which use a threshold *p* ",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "We analyzed six different tree layouts. The first four layouts are two common layouts and two cache-efficient layouts. We further propose two more layouts.Read more"
			]
		  },
		  {
			"title": "[2209.09166] Cache-Oblivious Representation of B-Tree Structures",
			"url": "https://arxiv.org/abs/2209.09166",
			"excerpts": [
			  "CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout with packed memory array.",
			  "In the use of the vEB layout mostly search complexity was considered, so far.",
			  "We describe how to build an arbitrary tree in vEB layout if we can simulate its depth-first search.",
			  " The data structure allows searching with an optimal I/O complexity \\mathcal{O}(\\log_B{N}) and is stored in linear space. ",
			  "ave an amortized I/O complexity \\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)",
			  " amortized time complexity \\mathcal{O}(S\\cdot\\log^2 N) , where S is the size of the subtree and N the size of the whole stored tree. R",
			  "Rebuilding an existing subtree saves the multiplicative \\mathcal{O}(\\log^2 N) in both complexities if the number of vertices on individual tree levels is not changed; it is paid only for the inserted/removed vertices otherwise.",
			  "We propose a general data structure CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout ...Read more"
			]
		  },
		  {
			"title": "Cache-oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/paper.pdf",
			"excerpts": [
			  "The van Emde Boas layout proceeds recursively. Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree. Suppose first that *h* is\na power of 2. Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + 1. This breaks the tree into the *top recursive subtree* *A* of\nheight *h/* 2, and several *bottom recursive subtrees* *B* 1 , *B* 2 , . . . , *B* ** , each of height *h/* 2",
			  "eaf nodes have the same number of children, then the recursive subtrees all\nhave size roughly\n**\n*N* , and ** is roughly\n**\n*N* . The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2",
			  " memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail* . Each level of detail is a partition of the tree into disjoint recursive\nsubtrees. In the finest level of detail, 0, each node forms its own recursive subtree. In\nthe coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subt",
			  "ur layout is a modified version of Prokops layout for a complete binary tree\nwhose height is a power of 2 [40, pp. 6162]. We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48]. ",
			  "One useful consequence of our rounding scheme is the following.",
			  "Lemma 2.1. *At level of detail* *k* *all recursive subtrees except the one containing*\n ... \ntree, even though the relative order of elements changes very little. Instead we use a\npacked-memory array from Section 2.3 to store the van Emde Boas layout, allowing\nus to make room for changes in the tree.",
			  "n additional technical complication arises in maintaining pointers to nodes that\nmove during an update. We search in the top tree by following pointers from nodes\nto their children, represented by indices into the packed-memory array. When we\ninsert or delete an element in the packed-memory array, an amortized *O* (log 2 *N* ) ele-\nments move. Any element that is moved must let its parent know where it has go",
			  "Thus, each node must have a pointer to its parent, and so each node must also let\nits children know where it has moved.",
			  " nodes that move can have\n*O* (log 2 *N* ) children scattered throughout the packed-memory array, each in separate\nmemory blocks. Thus an insertion or deletion can potentially induce *O* (log 2 *N* ) mem-\nory transfers to update these disparately located pointers. We can afford this large\nCACHE-OBLIVIOUS B-TREES\n11\ncost in an amortized sense by adding another level of (log *N* ",
			  "verall structure of our cache-oblivious B-tree therefore has three levels. The\ntop level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array. The middle level is a collection o",
			  "The van Emde Boas layout proceeds recursively.",
			  "Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree.",
			  "Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + ",
			  "The memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail",
			  "Each level of detail is a partition of the tree into disjoint recursive\nsubtrees.",
			  "In the finest level of detail, 0, each node forms its own recursive subtree.",
			  "the coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subtree.",
			  "The key property of the van Emde Boas layout is that, at any\nlevel of detail, each recursive subtree is stored in a contiguous block of memory.",
			  "cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.",
			  "op level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array",
			  "The middle level is a collection of",
			  "We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.\nI",
			  " The cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors."
			]
		  },
		  {
			"title": "Cache Oblivious Search Trees via Binary ...",
			"url": "https://www.cs.au.dk/~gerth/papers/soda02.pdf",
			"excerpts": [
			  "The basic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22]. The static structures are maintained\nby global rebuilding, i.e. they are rebuilt each time the\ndynamic tree has doubled or halved in size.",
			  "ion.\nFor storing n elements, our proposal uses (1 +  ) n\ntimes the element size of memory, and performs searches\nin worst case O (log B n ) memory transfers, updates\nin amortized O ((log 2 n ) / ( B )) memory transfers, and\nrange queries in worst case O (log B n + k/B ) memory\ntransfers, where k is the size of the output.",
			  " For random searches, we\ncan expect the top levels of the trees to reside in cache.\nFor the remaining levels, a cache fault should happen at\nevery level for the BFS layout, approximately at every\nsecond level for the DFS layout (most nodes reside in the\nsame cache line as their left child), and every (log B n )\nlevels for the van Emde Boas layout.",
			  "In the last part of this paper, we try to assess\nmore systematically the impact of the memory layout\nof search trees by comparing experimentally the effi-\nciency of the cache-oblivious van Emde Boas layout with\na cache-aware layout based on multiway trees, and with\nclassical layouts such as Breath First Search (BFS),\nDepth First Search (DFS), and inorde",
			  " trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "Inside main memory, the BFS is best, but looses by\na factor of five outside. The tree optimized for page size\nis the best outside main memory, but looses by a factor\nof two inside. Remarkably, the van Emde Boas layout\nis on par with the best throughout the range.",
			  "4.3\nConclusion.\nFrom the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "It also appears that in the area of search trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  ".\nFigure 4 compares the time for random searches in\nimplicit layouts. For sizes up to cache size ( n = 2 16 ), it\nappears that the higher instruction count for navigating\nin an implicit layout dominates the running times: most\ngraphs are slightly higher than corresponding graphs\nin Figure 3, and the van Emde Boas layout (most\ncomplicated address arithmetic) is the slowest while the\nBFS layout (simplest address arithmetic) is fastest. ",
			  "In particular, our data structure avoids the\nuse of weight balanced B -trees, and can be implemented\nas just a single array of data elements, without the use of\npointers.",
			  "sic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22",
			  " For storing n elements, our data structure uses\n(1 +  ) n times the element size of memory. ",
			  "Searches are\nperformed in worst case O (log B n ) memory transfers,\nupdates in amortized O ((log 2 n ) / ( B )) memory trans-\nfers, and range queries in worst case O (log B n + k/B )\nmemory transfers, where k is the size of the output.",
			  "From the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "he van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "The basic idea of our data structure is to maintain a\ndynamic binary tree of height log n + O (1) using existing\nmethods, embed this tree in a static binary tree, which\nin turn is embedded in an array in a cache oblivious\nfashion, using the van Emde Boas layout of Prokop.",
			  "he\nvan Emde Boas layout , was proposed by Prokop [19,\nSection 10.2], and is related to a data structure of\nvan Emde Boas [21, 22]."
			]
		  },
		  {
			"title": "Cache-Oblivious Dynamic Search Trees Zardosht Kasheff",
			"url": "https://people.csail.mit.edu/bradley/papers/Kasheff04.pdf",
			"excerpts": [
			  "The tree is laid out in memory in a recursive\nfashion. Let *h* be the height of the binary tree. For simplicity, assume *h* is a power of 2. Let\n*N* be the number of nodes in the tree. We divide the tree into two sections. The first section\nis the top half containing a subtree, sharing the same root as the tree, of height *h/* 2 with\n**\n*N* nodes. The second section is the bottom half containing 2 *h/* 2 subtrees of height *h/* 2,\neach containing about\n**\n*N* nodes. This represents subtree *A* in Figure 2-3. The idea is to\n19\nfirst layout the top half recursively. Then layout the remaining 2 *h/* 2 subtrees recursively in\norder. This represents subtrees *B* 1 *, B* 2 *, . . . , B* *l* in Figure 2-3. In memory, the entire subtree\n*A* would be laid out first, followed by *B* 1 *, . . . , B* *l* . We assume the binary tree is full and\nbalanced. If *h* is not a power of 2, the bottom half of the tree is chosen such that its height\nis a power of 2. Figure 2-3 shows the layout of a binary tree with height 5.\n",
			  "The locations of the children of node *i* are 2 *i* and 2 *i* + 1. Thus, the\nlocation of children may be implicitly calculated. Implicit calculations of children makes the\n35",
			  "The tree is represented in memory as an array. The value at location *i* of the array\ncorresponds to some node of the tree. We need a way of computing the location of the left\nand right children of node *i* . One solution is to have the array store pointers, but pointers\ncost space. Instead, we wish to have an array such that the root of the tree is the first\nelement of the array, and for a given node located at array location *i* , the locations of the\nnodes two children are easily found. This chapter provides details.",
			  "ch that the tree\nmay be traversed with *O* (1+log *B* ( *N* )) memory transfers, which is asymptotically optimal [7].\n**"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  "The Eurographics Association and Blackwell Publishing 2006.",
			  "e compare our cache-efficient layouts with other layouts in the*\n*context of collision detection and ray tracing.",
			  "r benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applicatio",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  "he resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmost.",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C",
			  "Our layout*\n*computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the",
			  "The COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH.",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime appli",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applic",
			  ".\nHavran analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.2.\n**Layouts of geometric meshes:** Many algor",
			  "8.2. Comparison with Cache-Oblivious Mesh Layouts",
			  "We have tested the performance of the OBB-tree collision\n ... \ngraph. The edge creation methods for BVHs described in\nYoon et al. [YLPM05] do not adequately represent access\npatterns of the travers",
			  "Our greedy algorithm is\nbased on greedy heuristics to compute cache-coherent layouts\nbased on parent-child locality. T",
			  "There are several areas for future work. We would like to\nextend our probability formulation that predicts runtime data\naccess patterns of collision queries to consider other proximity\nqueries such as minimum separation distance. W",
			  " The Eurographics Association and Blackwell Publishing 2006.\n",
			  "an analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.",
			  "outs of geometric meshes:** Many algorithms and repre-\nsentations have been proposed to compute coherent layouts for\nspecialized applications. Rendering sequences (e.g., triangle\nstrips) [Dee95,Hop99] are used to improve rendering through-\nput by optimizing the vertex cache hits in the GPU. Isenburg\nand Gumhold [IG03] propose processing sequences, includ-\ning streaming meshes [IL04], as an extension of rendering se-\nquences for large-data processing. In these cases, global mesh\naccess is restricted to a fixed traversal order. Many algorithms\nuse space filling curves [Sag94] to compute cache-friendly\nlayouts of volumetric grids or height fields. These layouts\nare widely used to improve performance of image process-\ning [VG91] and terrain or volume visualization [PF01,L",
			  "We introduce a new probabilistic\nmodel to predict the runtime access patterns of BVHs based on\nlocalities.",
			  "ecifically, we utilize two types of localities during\ntraversal of a BVH: parent-child and spatial localities between\nthe accessed node",
			  "In this section, we define two localities that are used to com-\npute a cache-efficient layout of a BV",
			  "to achieve this\ngoal, we recursively compute the clusters. We first decompose\nthe BVH into a set of clusters and recursively decompose each\ncluster. In this case, the cache block boundaries can lie any-\nwhere within a layout that corresponds to the nodes of these\nclusters. Therefore, we need to compute a cache-efficient or-\ndering of the clusters computed at each level of recursion.\nOur algorithm has two different components that handle\nparent-child and spatial localities. In particular, the first part\nof our algorithm decomposes a BVH into a set of clusters that\nminimize the cache misses for parent-child locality. The clus-\nters are classified as a root cluster and child clusters. The root\ncluster contains the root node of the BVH and child clusters\nare created for each node outside the root cluster whose par-\nent node is in the root cluster (see the middle image in Fig.\n4). The second part of the algorithm computes an ordering of\nthe clusters and stores the root cluster at the beginning of the\nordering. The ordering of child clusters is computed by con-\nsidering their spatial locality. Then, we can merge two child\nclusters if it can further decrease the size of the working set.\nWe recursively apply this two-fold procedure to compute an\nordering of all the BVs in the BVH"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://diglib.eg.org/bitstream/handle/10.1111/cgf142662/v40i2pp683-712.pdf",
			"excerpts": [
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged.",
			  "he latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order",
			  " authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH. The key difference is the use of two different types of\nclusters to further reduce bandwidth and cache misses.",
			  "s first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit withi",
			  "In this report, we review the basic principles of bounding volume hierarchies as well as advanced state of the art methods with a focus on the construction and ...Read more"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies | Request PDF",
			"url": "https://www.researchgate.net/publication/220507680_Cache-Efficient_Layouts_of_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "Yoon and Manocha [YM06] proposed a node layout algorithm known as cache-oblivious BVH (COLBVH) that recursively decomposes clusters of nodes and works without prior knowledge of the cache, such as the block size.",
			  "In initialization, each node is assigned the probability that the node is accessed, given that the cluster's root is already accessed."
			]
		  },
		  {
			"title": "CacheEfficient Layouts of Bounding Volume Hierarchies - Yoon - 2006 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2006.00970.x",
			"excerpts": [
			  "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			  "Section Title: Cache-Efficient Layouts of Bounding Volume Hierarchies > Abstract",
			  "*We present a novel algorithm to compute cache-efficient layouts of bounding volume hierarchies (BVHs) of polygonal models. Our approach does not make any assumptions about the cache parameters or block sizes of the memory hierarchy. We introduce a new probabilistic model to predict the runtime access patterns of a BVH. Our layout computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the number of cache misses and the size of the working set. Our algorithm also works well for spatial partitioning hierarchies including kd-trees. We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of millions of triangles. We compare our cache-efficient layouts with other layouts in the context of collision detection and ray tracing. In our benchmarks, our layouts consistently show better performance over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the underlying algorithms or runtime applications.*",
			  "Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Hierarchy and Geometric Transformations"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "The VEB layout is complex\nto setup and maintain for trees and difficult to apply to graphs in\ngeneral.",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "Figure 2 shows graphically how this might be done. Assume\nan algorithm *P* *i* that aims to copy a tree while traversing it, into\nblocks that fit into the cache at level *i* . Using breadth first search,\nit can discover the entire subtree that fits into a block at level *i*",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "A third class of techniques (including the one in this paper) are\nused at runtime. One approach is to control memory allocation.",
			  "The van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7].",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "ble 1.** Cachegrind miss rates",
			  "Figure 4.** Binary Search Tree Performan",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.",
			  "For storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "The source code of the programs, our scripts and tools, and the data we present here are available online under ftp.brics.dk/RS/01/36/Experiments/.",
			  "Section Title: Cache Oblivious Search Trees via Binary Trees of Small Height > Abstract",
			  "Content:\nWe propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.\nFor storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.\nThe basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.\nWe also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "Van Emde Boas tree",
			"url": "https://en.wikipedia.org/wiki/Van_Emde_Boas_tree",
			"excerpts": [
			  "A van Emde Boas tree also known as a vEB tree or van Emde Boas priority queue, is a tree data structure which implements an associative array with m-bit ...Read more"
			]
		  },
		  {
			"title": "The Cost of Cache-Oblivious Searching",
			"url": "https://www3.cs.stonybrook.edu/~bender/newpub/2011-algorithmica-BenderBrFa-co-searching.pdf",
			"excerpts": [
			  "atic cache-oblivious search tree is built as follows: Embed a complete binary tree with\nN nodes in memory, conceptually splitting the tree at half its height, thus obtaining (\n\nN ) subtrees each\nwith (\n\nN ) nodes. Lay out each of these trees contiguously, storing each recursively in memory. This type\nof recursive layout is commonly referred to in the literature as a van Emde Boas layout because it is remi-\nniscent of the recursive structure of the van Emde Boas tree [37,38]. The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26",
			  " The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26]. ",
			  "We present the following results:",
			  "We present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.",
			  " Intuitively, the improvement of uneven splitting, as compared to the even splitting in the standard van\nEmde Boas layout, is likely to be due to the generation of a variety of subtree sizes at each recursive\nlevel of the layout. Such a variety will on any search path reduce the number of subtrees that can have\nparticularly bad sizes compared to the block size B",
			  "Finally, we demonstrate that it is harder to search in the cache-oblivious model than in the DAM model.\nPreviously the only lower bound for searching in the cache oblivious model was the log B N lower bound\nfrom the DAM model. We prove a lower bound of lg e log B N memory transfers for searching in the\naverage case in the cache-oblivious model.",
			  "\n We then present a class of generalized van Emde Boas layouts that optimizes performance through\nthe use of uneven splits on the height of the tree. For any constant  > 0, we optimize the layout\nachieving a performance of [lg e +  + O (lg lg B/ lg B )] log B N + O (1) expected memory transfers. ",
			  "In this section we give a tight analysis of the cost of searching in a binary tree stored using the van Emde. Boas layout [31]. As mentioned earlier, in the vEB ...Read more",
			  "s.\nWe present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.\n ",
			  "in the vEB layout, the tree is split evenly by height, except for\nroundoff. Thus, a tree of height h is split into a top tree of height  h/ 2  and bottom tree of height  h/ 2  . It\nis known [15,22] that the number of memory transfers for a search is 4 log B N in the worst case ; we give a\nmatching configuration showing that this analysis is tight. We then consider the average-case performance\nover all starting positions of the tree in memory, and we show that the expected search cost is 2(1 +\n3 /\n\nB ) log B N + O (1) memory transfers, which is tight within a 1 + o (1) factor. We assume that the data\nstructure begins at a random position in memory; if there is not enough space, then the data structure\nwraps around to the first location in memory.\nA",
			  "trees.\nThe generalized vEB layout is as follows: Suppose the complete binary tree contains N  1 = 2 h  1\nnodes and has height h = lg N . Let a and b be constants such that 0 < a < 1 and b = 1  a . Conceptually\nwe split the tree at the edges below the nodes of depth  ah  . This splits the tree into a top recursive subtree\nof height  ah  , and k = 2  ah  bottom recursive subtrees of height  bh  . Thus, there are roughly N a bottom\nrecursive subtrees and each bottom recursive subtree contains roughly N b nodes. We map the nodes of the\ntree into positions in the array by recursively laying out the subtrees contiguously in memory. The base case\nis reached when the trees have one node, as in the standard vEB layout.\nWe",
			  "We find the values of a and b that yield a layout whose memory-transfer cost is arbitrarily close to\n[lg e + O (lg lg B/ lg B )] log B N + O (1) for a = 1 / 2   and large enough N . We focus our analysis on the first\nlevel of detail where recursive subtrees have size at most the block size B . In our analysis memory transfers\ncan be classified in two types. There are V path-length memory transfers , which are caused by accessing\ndifferent recursive subtrees in the level of detail of the analysis, and there are C page-boundary memory"
			]
		  },
		  {
			"title": "Memory Layouts for Binary Search",
			"url": "https://cglab.ca/~morin/misc/arraylayout/",
			"excerpts": [
			  "`veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature.",
			  "In many settings B-trees (with a properly\nchosen value of B) are best. In others, the Eytzinger layout\nwins. In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "In many settings B-trees (with a properly\nchosen value of B) are best.",
			  "In others, the Eytzinger layout\nwins.",
			  "In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "Which of these array memory layouts is fastest?",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "For an example, consider the following two graphs, generated\nby running the same code on two different Intel machines.",
			  "In\nthe left graph, the Eytzinger layout is almost as slow as a\nplain sorted array while the van Emde Boas and B-tree layouts\nare more than twice as fast.",
			  "In the right graph, the Eytzinger layout and b-tree are the\nfastest, the sorted array is still the slowest, and the vEB layout\nis somewhere in betweeen (for array sizes).",
			  "veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature"
			]
		  },
		  {
			"title": "Cache oblivious search trees via binary trees of small height | Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms",
			"url": "https://dl.acm.org/doi/10.5555/545381.545386",
			"excerpts": [
			  "Section Title: Cache oblivious search trees via binary trees of small height",
			  "ng Brodal](# \"Gerth Stlting Brodal\") Gerth Stlting Brodal\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81409594931) , [Rolf Fagerberg](# \"Rolf Fagerberg\") Rolf Fagerberg\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100166398) , [Riko Jacob](# \"Riko Jacob\") Riko Jacob\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100438116) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386)\n[SODA '02: Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms](/doi/proceedings/10.5555/545381)\nPages 39 - 48\nPublished : 06 January 2002 [Publication History](#) To view Publication History, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386) [](# \"Check for updates on crossmark\")\n... citation ... Downloads\n[](#) To get citation alerts, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F54",
			  "For storing *n* elements, our proposal uses (1 + ) *n* times the element size of memory, and performs searches in worst case *O* (log *B* *n* ) memory transfers, updates in amortized *O* ((log 2 *n* )/( *B* )) memory transfers, and range queries in worst case *O* (log *B* *n + k/B* ) memory transfers, where *k* is the size of the output.The ",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log *n+O* (1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "We also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "[PDF] Cache Oblivious Search Trees via Binary Trees of Small Height | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache-Oblivious-Search-Trees-via-Binary-Trees-of-Brodal-Fagerberg/da35c4651414b03abe079b5c8454948c59f372c0",
			"excerpts": [
			  "Cache Oblivious Search Trees via Binary Trees of Small Height",
			  "A version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds is proposed, and can be implemented as just a single array of data elements, without the use of pointers.",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/",
			"excerpts": [
			  "We present a novel algorithm to compute cache-efficient layouts of\nbounding volume hierarchies (BVHs) of polygonal\nmodels.",
			  "We does not make any\nassumptions about the cache parameters or block sizes of the memory hierarchy.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH.",
			  "Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%\nwithout any modification of the underlying algorithms or runtime\napplications.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing.",
			  "Paper: [Cache-Efficient Layouts of Bounding Volume Hierarchies](CELBVH.pdf) , Computer graphics forum (Eurographics), volume 25, issue 3, 2006, pp. 507-516",
			  "Section Title: by [Sung-Eui Yoon](http://jupiter.kaist.ac.kr/~sungeui/) and [Dinesh Manocha](http://www.cs.unc.edu/~dm/) .",
			  "llision Detection between Hugo and 1M Power Plant Models:** The hugo robot model is placed inside the power plant model, whose\noverall shape is shown on the right. We are able to achieve 35%--2600%\nperformance improvement in collision detection by using our cache-efficient\nlayouts of the OBBTree over other tested layouts.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH. Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing. In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%",
			  "without any modification of the underlying algorithms or runtime\napplications.",
			  "We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of\nmillions of triangles.",
			  " In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%"
			]
		  },
		  {
			"title": "[PDF] CacheEfficient Layouts of Bounding Volume Hierarchies | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache%E2%80%90Efficient-Layouts-of-Bounding-Volume-Yoon-Manocha/77fdc59d6f28a3eb5da7c9fc134205a8f498f306",
			"excerpts": [
			  "A novel algorithm to compute cacheefficient layouts of bounding volume hierarchies (BVHs) of polygonal models and a new probabilistic model to predict the runtime access patterns of a BVH is introduced",
			  "Published in Computer graphics forum 1 September 2006\n",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/",
			"excerpts": [
			  "Demaine, and Martin Farach-Colton, Cache-Oblivious B-Trees, SIAM Journal on Computing, volume 35, number 2, 2005, pages 341358. Abstract: This paper presents ...Read more"
			]
		  },
		  {
			"title": "Concurrent Cache-Oblivious B-Trees - People",
			"url": "https://people.csail.mit.edu/bradley/papers/BenderFiGi05.pdf",
			"excerpts": [
			  "e first review the performance models used to analyze cache-\nefficient data structures, and then review three variations on serial\ncache-oblivious B-trees.",
			  "ernal-memory data structures, such as B-trees, are tradition-\nally analyzed in the *disk-access model (DAM)* [1], in which internal\nmemory has size *M* and is divided into blocks of size *B* , and exter-\nnal memory (disk) is arbitrarily larg",
			  "Performance in the DAM\nmodel is measured in terms of the number of block transfers.",
			  "Thus\nB-trees implement searches asymptotically optimally in the DAM\nmodel.",
			  "The *cache-oblivious model* [15,26] is like the DAM model in that\nthe objective is to minimize the number of data transfers between\ntwo levels.",
			  "A CO B-tree [8] achieves nearly optimal locality of reference at\nevery level of the memory hierarchy. It optimizes simultaneously\nfor first- and second-level cache misses, page faults, TLB misses,\ndata prefetching, and locality in the disk subsystem."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*",
			  "To mitigate\nthis, we modified the memory allocator implementation in\nEmbree [8] to allocate BVH in a single 1GB hugepage, which\nled to an end-to-end rendering speedup of 6.4%.",
			  "Divergent memory access is a major source of latency.",
			  "d stall* cycles reveals that most of the memory stalls are\ncaused by TLB misses. This is possible due to the virtually-\nindexed physically-tagged nature of L1 data cache, where TLB\nmisses can delay tag access latency and thereby the hit latency\nof the cache.",
			  "Ray sorting improves memory\ndivergence by mapping coherent rays to adjacent threads, and\nthis is evidenced by the fewer number of L1 sectors per\nrequest.",
			  "IMT execution divergence** severely limits per-\nformance of software-based GPU ray tracing. For our CUDA\nworkload, 44% of the threads in a single SIMD unit is inactive\nduring execution. This is because threads mapped to divergent\nrays unavoidably execute divergent branches or terminate early\nduring traversal, causing them to become inactive due to the\nlockstep execution of SI"
			]
		  },
		  {
			"title": "OPTIMIZING QUERY TIME IN A BOUNDING VOLUME ...",
			"url": "https://benedikt-bitterli.me/bvh-report.pdf",
			"excerpts": [
			  "To reduce the likelihood of this occurring, we implemented\na special tree layout from literature, the van Emde Boas\nordering[ **?** ], which is a cache-oblivious layout designed to\nkeep certain subtrees close together in memory.",
			  "The van Emde Boas ordering of a single node is the node\nitself.",
			  "The van Emde Boas ordering of a tree *T* of depth *d* *T*\nis the van Emde Boas ordering of the top subtree ending at\ndepth ** *d* *T*\n2 ** followed by the van Emde Boas ordering of all\nchild subtrees rooted at depth ** *d* *T*\n2 ** + 1 .\nIn",
			  ".\nInformally, this guarantees that any subtree is likely to\nbe stored in a contiguous memory segment; in other words,\nthe traversal algorithm is likely to work in a locally contigu-\nous memory segment for many traversal steps before mak-\ning a large jump through memory.",
			  "Although this should increase performance in theory, in\npractice, no performance improvement can be observed. It\nappears that TLB misses do not play a significant role in the\ntraversal performance."
			]
		  },
		  {
			"title": "Cache-Oblivious Algorithms and Data Structures",
			"url": "https://erikdemaine.org/papers/BRICS2002/paper.pdf",
			"excerpts": [
			  "A comparison of cache aware and cache oblivious static search trees using program in- strumentation. In Experimental Algorithmics: From Algorithm Design to.Read more"
			]
		  },
		  {
			"title": "Cache-oblivious algorithms and data structures",
			"url": "https://scispace.com/pdf/cache-oblivious-algorithms-and-data-structures-jjcrutokhi.pdf",
			"excerpts": [
			  "Prokop in [60] proposed static cache-oblivious search trees with search cost\nO (log B N ) I/Os, matching the search cost of standard (cache-aware) B-trees [17].",
			  "\nThe search trees of Prokop are related to a data structure of van Emde Boas [67,\n68], since the recursive layout of a search tree generated by Prokops scheme re-\nsembles the layout of the search trees of van Emde Boas.",
			  "The constant in the\nO (log B N ) search cost was studied in [21], where it is proved that no cache-\noblivious algorithm can achieve a performance better than log 2 e  log B N I/Os,\ni.e. a factor  1 . 44 slower than a cache-aware algorithm.",
			  "Dynamic B-trees were first presented by Bender et al. [22] achieving searches\nin O (log B N ) I/Os and updates requiring amortized O (log B N ) I/Os.",
			  "A cache-oblivious dictionary based on exponential search trees was presented\nin [19]."
			]
		  },
		  {
			"title": "Interactive Visualization and Collision Detection using ...",
			"url": "https://sgvr.kaist.ac.kr/~sungeui/thesis/phd_thesis_yoon_2005.pdf",
			"excerpts": [
			  "We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layou",
			  "erall, our approach offers the following\nbenefits:\n1. **Generality:** Our algorithm is general and applicable to all kind of BVHs. It\ndoes not require any knowledge of cache parameters or block sizes of a memory\nhierarchy.\n159\n2. **Applicability:** Our algorithm does not require any modification of BVH-based\nalgorithms or the runtime application. We simply compute cache-oblivious lay-\nouts of BVHs without making any assumptions about the applications.\n3. **Improved performance:** Our layouts reduce the number of cache misses during\ntraversals of BVHs. We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layouts. Main improve"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.\nNext we describe our ",
			  " Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  "In global memory we have\nachieved a runtime reduction by 1%6%. We gained a 30%\n40% runtime reduction compared to the baseline in global\nmemory when the BVH is stored in texture memory simi-\nlar to Aila et al. [ ALK12",
			  " **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  "*Address Cluster (AC):** A continuous address space that can be\nreferenced by a small pointer. If the original BVH can address\n2 *n* nodes, the maximum size of an AC is 2 *n* */* 2 .",
			  "We achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address spac",
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "4. Two-Level Clustering",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft",
			  " consider that storing *P* *Le ft* allows less nodes per\ncache line. The size *|* *P* *|* can be up to 4 bytes, which is small\ncompared to conventional BVH encodings used by previous wo",
			  "However, a node pair can be quantized to as small as 8 bytes using\nDFL (Sec. 6.1 ), whereas clustering would require up to 12 bytes.",
			  "C* can maintain the node size of the depth-first layout (or\neven reduce it), while the *CC* reorders nodes within the same *AC* for\nthe best cache utilization.",
			  "5. Algorithm",
			  "In order to effectively reduce the working set, we must carefully\nselect the nodes for each cluster. We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ].",
			  " We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ]. They attempt to order the\nnodes according to the most likely traversal path based on *parent-*\n*child* and *spatial locality* .",
			  "ssuming that all traversal paths start at the root of the tree, their\n*COLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next",
			  "4.1. Glue Nodes",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  "lue nodes* only generate bandwidth when the\nchild node is traversed, not when accessing the parent node.",
			  "Glue Nodes",
			  "7.2. Bandwidth Analysis",
			  "*A novel node layout and addressing scheme*",
			  " achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address space. We\nthen choose the order of nodes inside these clusters to maxi-\nmize the cache line locality. We introduce a new node type to\nreference address-space changes during traversal. This keeps the\nnode sizes uniform, which is more suited for a fixed function\nhardware.",
			  "ct node ordering** schemes can eliminate a few child point-\ners from the BVH. Depth-first layout (DFL) places the left child\ndirectly after the parent node, therefore only the the right pointer\nis required. Alternatively, two sibling nodes can be stored sequen-\ntially [ AL09 ]. Besides compression, these layouts can also improve\ncache locality, since child nodes are often tested together following\nthe parent during traversal. Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent",
			  "ubtree partitioning** methods first decompose the BVH into clus-\nters of nodes, each containing one or more subtrees. By optimizing\nthe node order for multiple traversal paths it can further improve\ncache locality. Moreover, the size of the child pointers within clus-\nters may be reduced. This optimization was presented for BSP trees\nby Havran [ Hav97 ]. Gil and Itai [ GI99 ] showed that cache local-\nity for tree traversal can be significantly improved if the clusters\nof nodes are generated top-down, by greedily merging the children\nwith the highest probability. Yoon et al. [ YM06 ] applied this theory\nto kd-tree based ray travers",
			  "**8-byte Internal Node:**",
			  "6 x 6 bits per plane",
			  "6 bits",
			  "2 bits",
			  "2 x 10 bits",
			  "Figure 3: A 2D illustration of our quantized storage of sibling nodes\nwith parent-plane sharing (top). The layout of our internal nodes\n(bottom). We store 2 bits to indicate leaves, one low-precision\npointer per child, and 6 plane offsets (z-axis not shown). Finally,\nour reuse mask is set to 1 if the corresponding plane belongs to the\nleft child.",
			  "Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent.\n**",
			  " compare our method (green) with the standard ODFL (gray).\nWhen scaling the L2 cache with a fixed L1, we see a different\ntrend: as the capacity of L2 increases, the reduction achieved by our\nmethod slowly diminishes. Our explanation is that the outstanding\nmisses from L2 become less and less coherent and since more of the\nfrequently traversed nodes reside inside L2, the clustering heuris-\ntic cannot predict the outgoing access pattern anymore. There is\nanother interesting trend regarding the utilization of the traversal\nunit, which increases with the L2 capacity.",
			  "hen describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footprint* . In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft* can be omit",
			  "ache Cluster (CC):** A small set of nodes that fits within a\ncache line, created within an *AC* ",
			  " Two-Level Clustering*",
			  " Two-Level Clustering*"
			]
		  },
		  {
			"title": "The Ultimate Guide to Bounding Volume Hierarchies",
			"url": "http://www.lufei.ca/posts/BVH.html",
			"excerpts": [
			  "Alternatively, proposals including Cache-Oblivious BVH (COLBVH) 19 and Swapped Subtrees (SWST) 20 organize the tree into subtree clusters in memory.Read more"
			]
		  },
		  {
			"title": "Further Reading",
			"url": "https://pbr-book.org/4ed/Primitives_and_Intersection_Acceleration/Further_Reading",
			"excerpts": [
			  "Vaidyanathan et al. ( [2016](:Vaidyanathan2016) ), who introduced a\nreduced-precision representation of the BVH that still guarantees\nconservative intersection tests with respect to the original BVH.",
			  "Liktor\nand Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node\nrepresentation based on clustering nodes that improves cache performance\nand reduces storage requirements for child node pointers.",
			  "Ylitie et\nal. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs\ninto wider BVHs with more children at each node, from which they derived a\ncompressed BVH representation that shows a substantial bandwidth reduction\nwith incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )\ndeveloped an algorithm for efficiently traversing such wide BVHs using a\nsmall stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing\nsets of adjacent leaf nodes of BVHs under the principle that most of the\nmemory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described\nan approach that saves both computation and storage by taking advantage of\nshared planes among the bounds of the children of a BVH node.",
			  "Other work in the area of space-efficient BVHs includes that of",
			  "reduced-precision representation of the BVH that still guarantees",
			  "conservative intersection tests with respect to the original BVH.",
			  "and reduces storage requirements for child node pointers. Ylitie et",
			  "with incoherent rays. Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "small stack. Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "Section Title: Further Reading > Grids > Bounding Volume Hierarchies",
			  "Liktor",
			  "Liktor",
			  "and Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node",
			  "representation based on clustering nodes that improves cache performance",
			  "representation based on clustering nodes that improves cache performance",
			  "and reduces storage requirements for child node pointers.",
			  "Ylitie et",
			  "al. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "with incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "developed an algorithm for efficiently traversing such wide BVHs using a",
			  "small stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "sets of adjacent leaf nodes of BVHs under the principle that most of the",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "an approach that saves both computation and storage by taking advantage of",
			  "an approach that saves both computation and storage by taking advantage of",
			  "shared planes among the bounds of the children of a BVH node.",
			  "shared planes among the bounds of the children of a BVH node."
			]
		  },
		  {
			"title": "High-Performance Graphics 2016",
			"url": "https://diglib.eg.org/collections/c76a314b-e5ee-4fc4-8162-19c780bda7db",
			"excerpts": [
			  "Reduced precision bounding volume ... Moreover, as BVH nodes become comparably small to practical cache line sizes, the BVH is cached less efficiently.Read more"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://www.researchgate.net/publication/284233414_Performance_Comparison_of_Bounding_Volume_Hierarchies_and_Kd-Trees_for_GPU_Ray_Tracing",
			"excerpts": [
			  "The BVH node takes 64 bytes: 24 bytes for each of the children axis-aligned bounding boxes (AABB), two 4 byte offsets to the left and right ...Read more"
			]
		  },
		  {
			"title": "[PDF] Bandwidth-efficient BVH layout for incremental hardware traversal | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Bandwidth-efficient-BVH-layout-for-incremental-Liktor-Vaidyanathan/23e7f72b23dbbc376dd46dd681e0edcde34a6037",
			"excerpts": [
			  "This paper introduces a novel memory layout and node addressing scheme and map it to a system architecture for fixed-function ray traversal and demonstrates a significant reduction in memory bandwidth, compared to previous approaches."
			]
		  },
		  {
			"title": "What is a GPU warp? | Modular",
			"url": "https://docs.modular.com/glossary/gpu/warp",
			"excerpts": [
			  "Threads in a warp can access contiguous memory locations efficiently through memory coalescing. The hardware automatically synchronizes threads within a warp ..."
			]
		  },
		  {
			"title": "Introduction to the HIP programming model  HIP 7.1.52802 Documentation",
			"url": "https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html",
			"excerpts": [
			  "Coalescing memory accesses means aligning and organizing these accesses so that multiple threads in a warp can combine their memory requests into the fewest ...Read more"
			]
		  },
		  {
			"title": "definition - In CUDA, what is memory coalescing, and how is it achieved? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/5041328/in-cuda-what-is-memory-coalescing-and-how-is-it-achieved",
			"excerpts": [
			  "A coalesced memory transaction is one in which all of the threads in a half-warp access global memory at the same time.Read more"
			]
		  },
		  {
			"title": "Accessing same global memory address within warps - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/accessing-same-global-memory-address-within-warps/66574",
			"excerpts": [
			  "If a warp accesses the same addresses several times, then the memory instruction is coalesced. Internally, NVIDIA GPUs only support gather instructions.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://meistdan.github.io/publications/raysorting/paper.pdf",
			"excerpts": [
			  " employed techniques such as path tracing produce increasingly\nincoherent ray sets due to scattering on diffuse and glossy surfaces.",
			  "\nTracing incoherent rays is much more costly than tracing coherent\nones due to higher memory bandwidth, higher cache miss rate,\nand computational divergence. ",
			  ". A number of techniques were pro-\nposed to mitigate this issue that usually use the wavefront path\ntracing combined with ray reordering, packet tracing, or ray space\nhierarchies.",
			  "The core of the ray tracing based algorithms is evaluating ray\nscene intersections, which is often referred to as the trace kernel. In\nour paper, we revisit the basic problem of sorting rays to produce\ncoherent subsets of rays in order to accelerate the trace kernel. We",
			  "We\nfocus on methods that are fully agnostic to the particular trace ker-\nnel and the employed acceleration data structure.",
			  "Such techniques\nalready appeared in the literature [Aila and Karras 2010; Costa et al .\nI3D 20, May 57, 2020, San Francisco, CA, USA\n",
			  "USA\nMeister, Bokansk, Guthe, and Bittner\n2015; Moon et al . 2010; Reis et al . 2017], but we feel there is a need\nfor their thorough comparison and deeper analysis.",
			  "\nWe aim at the following contributions:\n We summarize previously published methods for ray reorder-\ning suitable for GPU ray tracing.",
			  "We propose a method for sorting key computation that aims\nto maximize ray coherence by using a novel termination\npoint estimation technique.\n",
			  "We show the current limits of the trace acceleration using an\n ...",
			  "\nRays in three-dimensional space can be represented as points in\na five-dimensional space (ray space), where three dimensions rep-\nresent ray origins, and two dimensions represent ray directions.",
			  "cing on GPUs, Aila and\nLaine [2009] also evaluated a hash-based sorting criterion based on\ninterleaving ray origin and normalized ray direction. At that time,\nthe sorting overhead was too large to improve the overall rendering\ntime. Ano",
			  ".\nIn a case when thread divergence occurs on GPU, the whole warp\nof threads is blocked until all its rays finish the traversal. Aila and\nLaine [2009] proposed to increase SIMD efficiency by replacing al-\nready finished rays with new ones from a global queue.",
			  " Techniques\nsuch as speculative traversal slightly increase the redundancy of ray\nintersection tests because they work on possibly terminated rays.\n",
			  "Moon et al .\n[2010]. They propose to sort rays using an estimated termination\npoint that is calculated by ray tracing a simplified scene that fits\ninto the main memory. The approach is, however, only suitable for\nout-of-core ray tracing due to the expensive hit point estimation.",
			  "the sorting overhead was too large to improve the overall rendering\ntime."
			]
		  },
		  {
			"title": "Designing Fast Architecture-Sensitive Tree Search on",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2043652.2043655",
			"excerpts": [
			  "We explore latency hiding techniques for CPUs and GPUs to improve instruction throughput, resulting in better SIMD utilization. This article is an extended ...Read more"
			]
		  },
		  {
			"title": "Cpu Cache",
			"url": "https://paul.bone.id.au/blog/2019/05/01/cpu-cache/",
			"excerpts": [
			  "In other words, the cache manages 64-byte long (and aligned) blocks, or lines of memory. Managing cache in lines improves its use, since if you ...Read more"
			]
		  },
		  {
			"title": "Global memory access - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/global-memory-access/44288",
			"excerpts": [
			  "In general if you can have each thread load a 128 bit segment(16 bytes) then this will usually be faster than a 32 bit(4 bytes) or 64(8 bytes) bit word per ...Read more"
			]
		  },
		  {
			"title": "Memory transaction size - CUDA",
			"url": "https://forums.developer.nvidia.com/t/memory-transaction-size/8856",
			"excerpts": [
			  "In device 1.2+ (G200), you can use a transaction size as small as 32 bytes as long as each thread accesses memory by only 8-bit words.Read more"
			]
		  },
		  {
			"title": "Adapting Tree Structures for Processing with SIMD ...",
			"url": "https://openproceedings.org/2014/conf/edbt/ZeuchFH14.pdf",
			"excerpts": [
			  "We adapt the. B+-Tree and prefix B-Tree (trie) by changing the search al- gorithm on inner nodes from binary search to k-ary search. The k-ary search enables ...Read more",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search",
			  "The remainder of this paper is structured as follows. Sec-\ntion 2 covers preliminaries of our work. First, we discuss the\nSIMD chipset extension of modern processors and their op-\nportunities. Furthermore, we outline the *k-ary search* idea\nas the foundation for our work. Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search.",
			  "The optimized Seg-\nTrie provides a constant 14 fold speedup independently of\ntree depth and an eight fold reduced memory consumption\ncompared to the original *B* + -Tree.",
			  "Like the *B* + -Tree using binary search, the Seg-Tree adds one\nnode to the traversal path for each increase in tree depth.",
			  "The smallest data type that can currently be processed by\nthe *SIMD Extensions* is 8-bit [2]. This restriction limits a\nfurther increase in tree depth."
			]
		  },
		  {
			"title": "A High Throughput B+tree for SIMD Architectures",
			"url": "https://www.ece.lsu.edu/lpeng/papers/tpds-20-1.pdf",
			"excerpts": [
			  "AbstractB+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent.Read more",
			  "armonia, a\nnovel B+tree structure, to bridge the gaps between B+tree\nand SIMD architectures.",
			  "The key region stores the nodes with its keys in a breadth-\nfirst order",
			  "The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region.",
			  "Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality.",
			  "Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "aluations on a 28-core INTELCPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than\nthat of CPU-based HB+Tree [1], a recent state-of-the-art solution. And on a Volta TITAN VGPU, it can achieve up to 3.6 billion queries per\nsecond, which is about 3.4X faster than that of GPU-based HB+Tree.",
			  "The key region stores the nodes with its keys in a\nbreadth-first order.",
			  "To make it more efficient, Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "ap in Memory Access Requirement. Each B+tree query\nneeds to traverse the tree from root to leaf. This traversal\nbrings lots of indirect memory accesses, which is propor-\ntional to tree height",
			  "Gap in Memory Divergence. Since the target leaf node of a\nquery is generally random, multiple queries may traverse the",
			  "e\ngaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodes with its keys in a\nbreadth-first order. The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region. Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality. To",
			  "Partially-Sorted Aggregation (PSA)\n",
			  "narrowed thread-group traversal (NTG)."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "fast architecture sensitive tree search on modern CPUs ...",
			"url": "http://kaldewey.com/pubs/FAST__SIGMOD10.pdf",
			"excerpts": [
			  "t **FAST** (Fast Architecture Sensitive Tree)\nsearch algorithm that exploits high compute in modern processors\nfor index tree traversal. FAST is a binary tree, managed as a hier-\narchical tree whose elements are rearranged based on architecture\nfeatures like page size, cache line size, and SIMD width of underly-\ning hardware. We",
			  "eliminate the impact of latency with\nhierarchically blocked tree, software pipelining, and prefetches.",
			  "Having eliminated memory latency impact, we show how to ex-",
			  "cache line have the minimum number of cache misses, they found",
			  "that TLB misses are much higher than on trees with large node\nsizes, thus favoring large node sizes. Chen at al. [9] also concluded\nthat having a B+-tree node size larger than a cache line performs\nbetter and proposed pB+-trees, which tries to minimize the increase\nof cache misses of larger nodes by inserting software prefetches.",
			  "In order to efficiently use the compute performance of processors,\nit is imperative to eliminate the latency stalls, and store/access trees\nin a SIMD friendly fashion to further speedup the run-time.",
			  "*Hierarchical Blocking**\nWe advocate building binary trees (using the keys of the tuple)\nas the index structure, with a layout optimized for the specific ar-\nchitectural features.",
			  "For tree sizes larger than the LLC, the per-\nformance is dictated by the number of cache lines loaded from the\nmemory, and the hardware features available to hide the latenc",
			  "architecture features like page size, cache\nline size, and SIMD width of the underlying hardware.",
			  "this paper, we present FAST, an extremely fast architecture\nsensitive layout of the index tree. FAST is a binary tree logically\norganized to optimize for architecture features like page size, cache\nline size, and SIMD width of the underlying hardware. FAST elimi-\nnates impact of memory latency, and exploits thread-level and data-\nlevel parallelism on both CPUs and GPUs to achieve 50 million\n(CPU) and 85 million (GPU) queries per second, 5X (CPU) and\n1.7X (GPU) faster than the best previously reported performance\non the same architectures.",
			  "ompression techniques have been\nused to overcome disk I/O bottleneck by increasing the effective\nmemory capacity [15, 17, 20]. The transfer unit between mem-\nory and processor cores is a cache line. Compression allows each\ncache line to pack more data and increases the effective memory\nbandwidth. This increased memory bandwidth can improve query\nprocessing speed as long as decompression overhead is kept mini-\nmal [19, 3"
			]
		  },
		  {
			"title": "(PDF) FAST: fast architecture sensitive tree search on modern CPUs and GPUs",
			"url": "https://www.researchgate.net/publication/221213860_FAST_fast_architecture_sensitive_tree_search_on_modern_CPUs_and_GPUs",
			"excerpts": [
			  "FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware.",
			  " FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures.",
			  "FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.",
			  "In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree."
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "The memory bus of your GPU isn't simply 48 bytes wide (which would be quite cumbersome as it is not a power of 2). Instead, it is composed of 6 memory channels of 8 bytes (64 bits) each. Memory transactions are usually much wider than the channel width, in order to take advantage of the memory's burst mode. Good transaction sizes start from 64 bytes to produce a size-8 burst, which matches nicely with 16 32-bit words of a half-warp on compute capability 1.x devices.",
			  "128 byte wide transactions are still a bit faster, and match the warp-wide 32-bit word accesses of compute capability 2.0 (and higher) devices. Cache lines are also 128 bytes wide to match. Note that all of these accesses must be aligned on a multiple of the transaction width in order to map to a single memory transaction.",
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM.",
			  "When you are operating in caching mode, you can't really control the granularity of requests to global memory below 128 bytes at a time, anyway."
			]
		  },
		  {
			"title": "The granularity of L1 and L2 caches - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/the-granularity-of-l1-and-l2-caches/290065",
			"excerpts": [
			  ")\nI am currently studying CUDA.\nAcorrding to the 2022 CUDA C Programming Guide, A cache line is 128 bytes and maps to a 128 byte aligned segment in device memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte memory transactions, whereas memory accesses that are cached in L2 only are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.",
			  "In modern GPUs (say, Pascal and newer) both the L1 and L2 cache can be populated sector-by-sector. The minimum granularity is 1 sector or 32 bytes. The cache line tag, however, applies to 4 sectors (in each case) that comprise the 128-byte cache line. You can adjust L2 cache granularity."
			]
		  },
		  {
			"title": "Coalesced Memory Read Question - CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/coalesced-memory-read-question/41565",
			"excerpts": [
			  "From CUDA Best Programming Guide I know that GPU reading 128 byte word for one warp transaction. Thats mean, each warp(32 threads) can easy read any 4 byte data type in coalesced way during one cycle.",
			  "Im no expert but I think youre right. If you had 32 float3s, it would take 3 warp cycles, no matter how you arrange the data.",
			  "SoA approach would still take 3 warp cycles as well. No matter how you slice it, youre getting a full read.",
			  "Yes, multiple transactions will be issued.",
			  "SoA as a general recommendation is a good idea, but there shouldnt be any problem (no difference in efficiency) with loading a float4 per thread."
			]
		  },
		  {
			"title": "CUDA Performance Optimization",
			"url": "https://juser.fz-juelich.de/record/915940/files/04-mhrywniak-perf_opt.pdf",
			"excerpts": [
			  "To achieve coalesced global memory access: Usually: Fix your access pattern. Try to use shared memory (but first, check cache behavior). Look for different way ..."
			]
		  },
		  {
			"title": "An Evaluation of B-tree Compression Techniques | The VLDB Journal | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1007/s00778-025-00950-8",
			"excerpts": [
			  "Among them, B-tree compression is an important technique introduced as early as the 1970s to improve both space efficiency and query performance ...Read more"
			]
		  },
		  {
			"title": "fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu- ...",
			"url": "https://scispace.com/pdf/fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu-42t1vrahpt.pdf",
			"excerpts": [
			  "e present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive int",
			  "r breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack. ",
			  "This algorithm amortizes the cost of node access\npattern among the rays.",
			  "Ray sorting timings are not taken\ninto account when we evaluate [ AL09 ] performance."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2013/Laine-MegakernelsConsideredHarmful.pdf",
			"excerpts": [
			  "llows further optimizations**\n**Collecting requests in operation-specific queues and scheduling**\n**them individually**\n**This is the big one!** **Really hard to do in the megakernel approach**\n****\n**Path state must reside in memory**\n**A simple loop-based method can keep it in registers**\n**Not as bad as it sounds if we use a good memory layout (SOA",
			  "**Step 2: Per-Operation Queues**\n**Allocate a queue for each primitive operation request**\n**Extension ray casts**\n**Shadow ray casts**\n**New path generation**\n**Material evaluations**\n***With separate queues for individual materials***\n**Place requests compactly (i.e., no gaps) into queues**\n**When executing, use one thread per request**\n**Every thread will have an item to work on**\n**Every thread will be doing the same thing, so theres very**\n**little execution divergence!**"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "It is shown that this type of hash table can benefit from the\ncache locality and multi-threading more than ordinary hash tables.",
			  "Moreover, the batch design\nfor hash tables is amenable to using advanced features of modern processors such as prefetching\nand SIMD vectorization.",
			  "While state-of-the-art research and open-source projects on batch hash\ntables made efforts to propose improved designs by better usage of mentioned hardware features,\ntheir approaches still do not fully exploit the existing opportunities for performance improvements.",
			  "Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing.",
			  "To allow developers to fully take\nadvantage of its performance, we recommend a high-level batch API design.",
			  "Our experimental\nresults show the superiority and competitiveness of this approach in comparison with the alternative\nimplementations and state-of-the-art for the data-intensive workloads of relational join processing,\nset operations, and sparse vector processing.",
			  "The SIMD is a hardware\nfeature that allows the simultaneous execution of an operation on a vector of values.",
			  "On\nthe other hand, prefetching is a hardware feature that allows the program to request future\nmemory accesses in advance and asynchronous to the other computations.",
			  "We will cover the\nmore-detailed definitions of these two concepts later in this section.",
			  " 1 ], Horton [ 9 ] and Cuckoo++[ 23 ] have focused on improving the\nperformance of batch hash tables by applying SIMD and prefetching techniques to a specific\ntype of SIMD-aware batch hash table designs called Bucketized Cuckoo Hash Tables (BCHTs)",
			  " Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing. ",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "Modern CPUs support hardware and software prefetch-\ning. Prefetching improves the performance of a program by amortizing the costs of memory\naccess over tim",
			  "In hash tables, regardless of the hashing scheme, accessing entries is based on the value\nof the computed hash for each provided key.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable.",
			  " \nFigure 6 depicts a generic and high-level algorithm for combining prefetching with vertical\nvectorization (based on the assumption that we take the group-prefetching approach instead\nof standard prefetching",
			  "By having a group of keys as input, before starting the vertical\nvectorization, we define a loop over the group keys (prefetching loop).",
			  "foreach\ngroup in array by GROUP_SIZE {",
			  "// prefetching\nstage",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "Conditions of coalescing global memory into few transactions - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/conditions-of-coalescing-global-memory-into-few-transactions/109481",
			"excerpts": [
			  "The (L2) cache can act as a coalescing buffer by collecting write activity from multiple instructions, before it is written out to DRAM, in presumably a minimized set of transactions. This is possible in part because L2 has write-back, not write-through, behavior.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp."
			]
		  },
		  {
			"title": "CUDA on WSL User Guide  CUDA C++ Best Practices Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/",
			"excerpts": [
			  "A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.",
			  "On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.",
			  "On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.",
			  "The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the *k* -th thread accesses the *k* -th word in a 32-byte aligned array. Not all threads need to participate."
			]
		  },
		  {
			"title": "Memory Latency - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/memory-latency",
			"excerpts": [
			  "eturning to Little's Law, we notice that it assumes that the full bandwidth be utilized, meaning, that all 64 bytes transferred with each memory block are useful bytes actually requested by an application, and not bytes that are transferred just because they belong to the same memory block. When any amount of data is accessed, with a minimum of one single byte, the entire 64-byte block that the data belongs to is actually transferred. To make sure that all bytes transferred are useful, it is necessary that accesses are *coalesced* , i.e. requests from different threads are presented to the [memory management unit](/topics/computer-science/memory-management-unit \"Learn more about memory management unit from ScienceDirect's AI-generated Topic Pages\") (MMU) in such a way that they can be packed into accesses that will use an entire 64-byte block",
			  " Typical latencies are 4 cycles for the L1 cache, 12 cycles for the [L2 cache](/topics/computer-science/l2-cache \"Learn more about L2 cache from ScienceDirect's AI-generated Topic Pages\") , and roughly 150-200 cycles for main memory. This memory hierarchy enhances memory performance in two ways. On one hand, it reduces the memory latency for recently used data. On the other hand, it reduces the number of accesses to the main memory, thereby limiting the usage of the network interconnect and the bandwidth demand. Indeed, accesses to L1 and L2 caches do not require network activation because they are part of the [processor socket](/topics/computer-science/processor-socket \"Learn more about processor socket from ScienceDirect's AI-generated Topic Pages\")",
			  "The size of memory transactions varies significantly between Fermi and the older versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction size would start off at 128 bytes per memory access. This would then be reduced to 64 or 32 bytes if the total region being accessed by the coalesced threads was small enough and within the same 32-byte aligned block. This memory was not cached, so if threads did not access consecutive memory addresses, it led to a rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2, 3, 4, , 31 and thread 1 reads addresses32, 32, 34, , 63, they will not be coalesced. In fact, the hardware will issue one read request of at least 32 bytes for each thread. The bytes not used will be fetched from memory and simply be discarded. Thus, without careful consideration of how memory is used, you can easily receive a tiny fraction of the actual bandwidth available on the device.",
			  "The situation in Fermi and Kepler is much improved from this perspective. Fermi, unlike compute 1.x devices, fetches memory in transactions of either 32 or 128 bytes. A 64-byte fetch is not supported. By default every memory transaction is a 128-byte cache line fetch. Thus, one crucial difference is that access by a stride other than one, but within 128 bytes, now results in cached access instead of another memory fetch. This makes the GPU model from Fermi onwards considerably easier to program than previous generations.",
			  "The 16 LSUs distributes 64 of the 128 bytes to the registers used by the first half-warp of warp 0. In the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register used by the other half-warp. However, warp 0 still can not progress as it has only one of the two operands it needs for the multiply. It thus does not execute and the subsequent bytes arriving from the coalesced read of a for the other warps are distributed to the relevant registers for those warps.",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data. For most programs, the data brought into the cache will then allow a cache hit on the next [loop iteration](/topics/computer-science/loop-iteration \"Learn more about loop iteration from ScienceDirect's AI-generated Topic Pages\") .",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data.",
			  "ple, the MMU can only find 10 threads that read 10 4-byte words from the same block, 40 bytes will actually be used and 24 will be discarded. It is clear that coalescing is extremely important to achieve high memory utilization, and that it is much easier when the access pattern is regular and contiguous. Th"
			]
		  },
		  {
			"title": "CUDA C++ Programming Guide (Legacy)  CUDA C++ Programming Guide",
			"url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/",
			"excerpts": [
			  "By default page-locked host memory is allocated as cacheable. It can optionally be allocated as *write-combining* instead by passing flag `cudaHostAllocWriteCombined` to `cudaHostAlloc()` . Write-combining memory frees up the hosts L1 and L2 cache resources, making more cache available to the rest of the application. In addition, write-combining memory is not snooped during transfers across the PCI Express bus, which can improve transfer performance by up to 40%.",
			  "ng from write-combining memory from the host is prohibitively slow, so write-combining memory should in general be used for memory that the host only writ",
			  "Using CPU atomic instructions on WC memory should be avoided because not all CPU implementations guarantee that functionality.",
			  "An access policy window specifies a contiguous region of global memory and a persistence property in the L2 cache for accesses within that region.",
			  "The code example below shows how to set an L2 persisting access window using a CUDA Stream.",
			  "When a kernel subsequently executes in CUDA `stream` , memory accesses within the global memory extent `[ptr..ptr+num_bytes)` are more likely to persist in the L2 cache than accesses to other global memory locations.",
			  "The `hitRatio` parameter can be used to specify the fraction of accesses that receive the `hitProp` property.",
			  "For example, if the L2 set-aside cache size is 16KB and the `num_bytes` in the `accessPolicyWindow` is 32KB:",
			  "With a `hitRatio` of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.",
			  "Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://www.researchgate.net/publication/2623917_Making_B-Trees_Cache_Conscious_in_Main_Memory",
			"excerpts": [
			  "CSB+Tree (Rao and Ross 2000) improves key density and reduces cache accesses and misses by storing only the address of the first child and ...Read more"
			]
		  },
		  {
			"title": "TRAVERSING A BVH CUT TO EXPLOIT RAY COHERENCE",
			"url": "https://www.scitepress.org/Papers/2011/33634/33634.pdf",
			"excerpts": [
			  "algorithms used for traversing a subtree are due to. (Aila and Laine, 2009). They are the persistent packet and the persistent while-while and will be ..."
			]
		  },
		  {
			"title": "How do cache lines work?",
			"url": "https://stackoverflow.com/questions/3928995/how-do-cache-lines-work",
			"excerpts": [
			  "Modern PC memory modules transfer 64 bits (8 bytes) at a time, in a burst of eight transfers, so one command triggers a read or write of a full cache line from ...Read more"
			]
		  },
		  {
			"title": "Search Lookaside Buffer: Efficient Caching for Index Data ...",
			"url": "https://wuxb45.github.io/papers/slb.pdf",
			"excerpts": [
			  "The CPU cache can leverage access locality to keep the most frequently used part of an index in it for fast access. However, the traversal on the index to a ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Proceedings of the 2001 ACM symposium on Applied computing",
			"url": "https://dl.acm.org/doi/10.1145/372202.372329",
			"excerpts": [
			  "The B+-Tree is the most popular index structure in database systems. In this paper, we present a fast B+-Tree ... Read More  Storage systems for movies-on ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Batch-construction-of-B%2B-trees-Kim-Won/a4f0cb5e06927a905cccb25056730b2a95e48d06",
			"excerpts": [
			  "An algorithm for batchconstructing the B+-tree, the most widely-used index structure in database systems, which achieves up to 28 times performance gain ..."
			]
		  },
		  {
			"title": "Fast Divergent Ray Traversal by Batching Rays in a BVH",
			"url": "https://jbikker.github.io/literature/Fast%20Divergent%20Ray%20Traversal%20by%20Batching%20Rays%20in%20a%20BVH%20-%202016.pdf",
			"excerpts": [
			  "In this work we propose a batching traversal scheme\ncalled RayCrawler.",
			  "Our scheme operates on a hierarchy of\nBVHs by splitting an existing BVH into two separate layers,\ncreating a top-level tree and multiple small trees that fit in the\nL2 cache of modern CPUs.",
			  "The Top-BVH traversal stack of the ray is\nstored to resume traversal later on.",
			  "he traversal algorithm starts by first traversing each\nray depth-first through the Top-BVH; once the ray reaches\na leaf node of the Top-BVH, the ray is batched at the Leaf-\nBVH that the leaf node is pointing to and the traversal of the\nray is suspended.",
			  "This system tries to amortize the cost of retrieving a Leaf-\nBVH from memory by traversing many batched rays through\nthe Leaf-BVH once it has been loaded into cache",
			  "hed rays. The scheme\nachieves modest speedups compared to a single-ray traver-\nsal algorithm for secondary rays and proves that a batching\nscheme can outperform a naive single-ray traversal approach\nfor highly divergent rays",
			  "he comparisons in this work are made with the algo-\nrithms implemented in the Embree framework version 2.7.1",
			  "s section gives a short overview of the data structure used\nthroughout the paper and an overview of the traversal al-\ngorithm. The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched",
			  "We achieve this by splitting a regular 4-wide BVH in two\nlayers. The rays are batched in the leaf nodes of the top\nlayer before traversing the bottom layer of the data struc-\ntur",
			  " The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched rays."
			]
		  },
		  {
			"title": "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/3170492.3136044",
			"excerpts": [
			  "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms",
			  "GPCE 2017: Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and ExperiencesIn order to achieve the highest possible performance, the ray traversal and intersection\nroutines at the core of every high-performance ray tracer are usually hand-coded,\nheavily optimized, and implemented separately for each hardware platformeven ...[Read More](# \"",
			  "Stackless traversal algorithms for ray tracing acceleration structures require significantly\nless storage per ray than ordinary stack-based ones. This advantage is important for\nmassively parallel rendering methods, where there are many rays in flight. ...[Read More]",
			  "Efficient stack-less BVH traversal for ray tracing",
			  "SCCG '11: Proceedings of the 27th Spring Conference on Computer GraphicsWe propose a new, completely iterative traversal algorithm for ray tracing bounding\nvolume hierarchies that is based on storing a parent pointer with each node, and on\nusing simple state logic to infer which node to traverse next."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://minkhollow.ca/Courses/461/Notes/Trees/Resources/rao.pdf",
			"excerpts": [
			  "we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "CSS-Trees augment binary search\nby storing a directory structure on top of the\nsorted list of elements.",
			  "We proposed a new index\nstructure called Cache-Sensitive Search Trees\n(CSS-Tree) that has even better cache behavior\nthan a B + -Tree. ",
			  "With a large amount of RAM, most of the\nindexes can be memory resident.",
			  "Our experiments are performed for 4-byte keys\nand 4-byte child pointers. Theoretically, B + -Trees\nwill have 30% more cache misses than CSB + -Trees.",
			  "In this paper, we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "As the gap between CPU and memory speed is\nwidening, CSB + -Trees should be considered as a re-\nplacement for B + -Trees in main memory database"
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf",
			"excerpts": [
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees."
			]
		  },
		  {
			"title": "Cache Craftiness for Fast Multicore Key-Value Storage",
			"url": "https://pdos.csail.mit.edu/papers/masstree:eurosys12.pdf",
			"excerpts": [
			  "Masstree uses a combination of old and new techniques\nto achieve high performance [8, 11, 13, 20, 2729]. It\nachieves fast concurrent operation using a scheme inspired\nby OLFIT [11], Bronson *et al.* [9], and read-copy up-\ndate [28]. Lookups use no locks or interlocked instructions,\nand thus operate without invalidating shared cache lines and\nin parallel with most inserts and update",
			  "Masstree shares a single tree among all cores to avoid load\nimbalances that can occur in partitioned designs.",
			  "The tree\nis a trie-like concatenation of B + -trees, and provides high\nperformance even for long common key prefixes, an area in\nwhich other tree designs have trouble.",
			  "Masstree uses a\nwide-fanout tree to reduce the tree depth, prefetches nodes\nfrom DRAM to overlap fetch latencies, and carefully lays\nout data in cache lines to reduce the amount of data needed\nper node.",
			  "Masstree achieves six to ten million operations per\nsecond on parts AC of the benchmark, more than 30 ** as\nfast as VoltDB [5] or MongoDB [2",
			  "e contributions of this paper are as follows. First, an\nin-memory concurrent tree that supports keys with shared\nprefixes efficiently. Second, a set of techniques for laying\nout the data of each tree node, and accessing it, that reduces\nthe time spent waiting for DRAM while descending the tree.\nThird, a demonstration that a single tree shared among mul-\ntiple cores can provide higher performance than a partitioned\ndesign for some workloads. Fourth, a complete design that\naddresses all bottlenecks in the way of million-query-per-\nsecond performance",
			  "Masstree provides high\nconcurrency from the start."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory",
			"url": "https://dl.acm.org/doi/pdf/10.1145/335191.335449",
			"excerpts": [
			  "ache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a\nvariant of B + -Trees that stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node.",
			  "We propose a new indexing technique called\nCache Sensitive B + -Trees (CSB + -Trees).",
			  "t stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node. The rest of the children can\nbe found by adding an offset to that address. ",
			  "Since only\none child pointer is stored explicitly, the utilization of\na cache line is high.",
			  "In Section 2 we survey related work on cache\noptimization.\nIn Section 3 we introduce our\nnew CSB + -Tree and its variants.",
			  "Full CSB + -Trees are better than B + -Tree in all\naspects except for space.\nWhen space overhead\nis not a big concern,\nFull CSB + -Tree is the\nbes"
			]
		  },
		  {
			"title": "Cache craftiness for fast multicore key-value storage | Proceedings of the 7th ACM european conference on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/2168836.2168855",
			"excerpts": [
			  "J. Rao and K. A. Ross. Making B+-trees cache conscious in main memory. SIGMOD Record, 29:475--486, May 2000.",
			  ". Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: A cache-sensitive parallel external sort. The VLDB Journal, 4(4):603--627, 1995.\n"
			]
		  },
		  {
			"title": "What is Memory Coalescing? | GPU Glossary",
			"url": "https://modal.com/gpu-glossary/perf/memory-coalescing",
			"excerpts": [
			  "Memory coalescing takes advantage of the internals of DRAM technology to enable\nfull bandwidth utilization for certain access patterns. Each time a DRAM address\nis accessed, multiple consecutive addresses are fetched together in parallel in\na single clock. For a bit more detail, see Section 6.1 of [the 4th edition of Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311) ;\nfor comprehensive detail, see Ulrich Drepper's excellent article [*What Every Programmer Should Know About Memory*](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf) .\nThe access and transfer of these consecutive memory locations is referred to as\na *DRAM burst* . If multiple concurrent logical accesses are serviced by a single\nphysical burst, the access is said to be *coalesced* . Note that a physical\naccess is part of a memory transaction, terminology you may see elsewhere in\ndescriptions of memory coalescing.\nOn CPUs, a similar mapping of bursts onto cache lines improves access\nefficiency. As is common in GPU programming, what is automatic cache behavior in\nCPUs is here programmer-managed."
			]
		  },
		  {
			"title": "Request Hedging vs Request Coalescing: A Software Engineers Guide to Optimizing Distributed Systems | by Sourav Chaurasia | Medium",
			"url": "https://medium.com/@mr.sourav.raj/request-hedging-vs-request-coalescing-a-software-engineers-guide-to-optimizing-distributed-fdcc6590ba9d",
			"excerpts": [
			  "**Request Coalescing** , also known as request deduplication or the singleflight pattern, is a resource optimization technique that merges multiple identical concurrent requests into a single execution. When multiple clients request the same resource simultaneously, only one actual request is executed, and all requesters share the result.",
			  "**Benefits:**",
			  "**Significant resource savings** : Can reduce duplicate executions by 7090% in high-concurrency scenarios.",
			  "**Improved cache efficiency** : Better hit rates when multiple requests need the same data.",
			  "**Prevents thundering herd** : Avoids overwhelming backend services during spikes.",
			  "**Lower infrastructure costs** : Reduced CPU, memory, and network usage.",
			  "**Drawbacks:**",
			  "**No individual latency improvement** : Doesnt help single request performance.",
			  "**Implementation complexity** : Requires careful key generation and cleanup.",
			  "**Memory overhead** : Must track pending requests and their futures.",
			  "**Potential bottlenecks** : Shared operations can become single points of failure.",
			  "**Discords Message Storage:** Discord uses request coalescing to manage its trillion-message database efficiently, preventing duplicate queries for the same message data.",
			  "**CDN Cache Filling:** Content delivery networks coalesce multiple requests for the same uncached resource, fetching it once and serving all waiting clients.",
			  "**Authentication Token Refresh:** When multiple requests need to refresh an expired token simultaneously, coalescing ensures only one refresh operation occurs.",
			  "**Database Query Optimization:** High-traffic applications coalesce identical database queries to reduce load and improve response times.",
			  "**Memory vs CPU Balance** Coalescing trades memory (for tracking pending requests) against CPU and network resources (avoiding duplicate work).",
			  "**Key Strategy Impact:** The effectiveness depends heavily on your key generation strategy:\nToo specific: Minimal coalescing benefit.\nToo general: Risk of sharing inappropriate results.\nOptimal: Balance between specificity and reuse.",
			  "\n**Cleanup Strategies:** Implement proper cleanup to prevent memory leaks",
			  "**Resource efficiency matters** : Infrastructure costs or capacity are constraints.",
			  "**High concurrency** : Many clients are requesting identical data simultaneously.",
			  "**Expensive operations** : Computationally intensive or slow database queries.",
			  "**Cache scenarios** : Filling caches or warming up cold data.",
			  "Request hedging and request coalescing represent two fundamental approaches to optimizing distributed systems. Hedging prioritizes user experience through latency reduction, while coalescing prioritizes system efficiency through resource optimization."
			]
		  },
		  {
			"title": "Two-Minute Tech Tuesdays - Request Coalescing - Resources",
			"url": "https://info.varnish-software.com/blog/two-minutes-tech-tuesdays-request-coalescing",
			"excerpts": [
			  "This episode of Two Minute Tech Tuesdays is about request coalescing, a core feature in Varnish that is used to reduce the stress on origin servers when multiple requests are trying to fetch the same uncached content.",
			  "What happens when multiple clients request the same uncached content from Varnish? Does Varnish open up the same amount of requests to the origin, and potentially destabilize the entire platform under heavy load (the Thundering Herd effect)? Luckily Varnish is not sensitive to the Thundering Herd. It identifies requests to the same uncached resource, queues them on a waiting list, and only sends a single request to the origin. As the origin responds, Varnish will satisfy the entire waiting list in parallel, so there's no [head-of-line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking) : everyone gets the content at exactly the same time. So request coalescing will effectively merge multiple potential requests to the origin into a single request.",
			  "The advantages are pretty straightforward: * Less pressure on the origin server",
			  "Less latency for queued clients",
			  "In terms of domains of application, request coalescing is useful for: * Long tail content that doesn't always end up in the cache",
			  "This only applies to cacheable content. Uncacheable content cannot take advantage of request coalescing. With uncacheable content we mean content that uses set cookie headers, or that has cache control response headers that deliberately bypass the cache.",
			  "With serialization we mean items on the waiting list being processed serially, rather than in parallel.",
			  "doing so we avoid potential serialization. With serialization we mean items on the waiting list being processed serially, rather than in parallel. This has a very detrimental effect on the performance and the quality of experience for the user, because in this case there actually is head-of-line blocking. Luc",
			  "Luckily Varnish Configuration Language (VCL) has provisions for that:",
			  "set beresp.ttl = 120s;",
			  "set beresp.uncacheable = true;"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)",
			  "Decoupled Access/Execute (DAE)",
			  "Idea: Decouple operand\naccess and execution via\ntwo separate instruction\nstreams that communicate\nvia ISA-visible queues .",
			  "Smith,  Decoupled Access/Execute\nComputer Architectures ,  ISCA 1982,\nACM TOCS 1984.",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Advantages:\n+ Execute stream can run ahead of the access stream and vice\nversa\n+ If A is waiting for memory, E can perform useful work\n+ If A hits in cache, it supplies data to lagging E\n+ Queues reduce the number of required registers\n+ Limited out-of-order execution without wakeup/select complexi",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream.",
			  "Control flow synchronization (like branches) is handled via a separate branch queue.",
			  "The compiler analyzes the program, identifies operations belonging to each stream, and generates two distinct instruction sequences that explicitly communicate via queue operations inserted by the compiler.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  }
		],
		"reasoning": "The provided JSON describes several hardware accelerator-inspired designs and their software analogues. The 'hardware_concept' field refers to a specific hardware feature or architecture, while 'inspiration_source_accelerator' indicates the hardware that inspired the concept. 'software_analogue_description' explains how this concept can be implemented in software, and 'expected_benefit_in_software' details the performance improvements expected from such an implementation. These descriptions are directly relevant to the user's query about optimizing parallel tree traversal with techniques that exploit hardware features for irregular memory access patterns. The excerpts discuss concepts like Decoupled Access/Execute (DAE), multi-bank caches, software-managed scratchpad memory, computation migration to data, fine-grained memory access, and their software analogues, which are all pertinent to the user's goal of optimizing scattered memory access.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.0",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines)."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  }
		],
		"reasoning": "The finegrained field value centers on a Decoupled Access/Execute (DAE) design as the hardware-inspired approach, including an Access unit that generates memory addresses and issues loads, and an Execute unit that performs computation, with communication via queues. Several excerpts explicitly define and describe this architecture: the official description of Decoupled Access/Execute Computer Architectures presents a two-processor division into an Access processor (A-processor) and an Execute processor (E-processor), each with separate responsibilities, and the concept of communication via queues between them. This establishes the core hardware concept the field value names. Additional excerpts discuss that the Access processor handles address calculation and memory requests, while the Execute processor handles arithmetic work, with the two cores communicating through queue structures, which matches the software analogue described in the field value (DSWP with software queues). The material also highlights how such a decoupled design enables latency tolerance by overlapping memory accesses with computation, which aligns with the expected software benefit stated in the field value. Further excerpts elaborate on the queue-based communication model (AEQ/EAQ) and on the potential performance impact, such as tolerating memory latency by overlapping accesses and computations, which directly corroborates the described benefit of hiding latency and enabling batching/scheduling opportunities. Taken together, these excerpts strongly support the proposed field values claim about a DA-style hardware/software decoupling with queue-based communication, used as a latency-hiding strategy and as a basis for a software-pipelined approach (DSWP).\n",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.4",
		"citations": [
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "Section Title: Cache Oblivious Search Trees via Binary Trees of Small Height > Abstract",
			  "Content:\nWe propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.\nFor storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.\nThe basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.\nWe also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory.",
			  "For storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.",
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization."
			]
		  },
		  {
			"title": "The Cost of Cache-Oblivious Searching",
			"url": "https://www3.cs.stonybrook.edu/~bender/newpub/2011-algorithmica-BenderBrFa-co-searching.pdf",
			"excerpts": [
			  "atic cache-oblivious search tree is built as follows: Embed a complete binary tree with\nN nodes in memory, conceptually splitting the tree at half its height, thus obtaining (\n\nN ) subtrees each\nwith (\n\nN ) nodes. Lay out each of these trees contiguously, storing each recursively in memory. This type\nof recursive layout is commonly referred to in the literature as a van Emde Boas layout because it is remi-\nniscent of the recursive structure of the van Emde Boas tree [37,38]. The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26",
			  " The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26]. ",
			  "We present the following results:",
			  "\n We then present a class of generalized van Emde Boas layouts that optimizes performance through\nthe use of uneven splits on the height of the tree. For any constant  > 0, we optimize the layout\nachieving a performance of [lg e +  + O (lg lg B/ lg B )] log B N + O (1) expected memory transfers. ",
			  "In this section we give a tight analysis of the cost of searching in a binary tree stored using the van Emde. Boas layout [31]. As mentioned earlier, in the vEB ...Read more",
			  "s.\nWe present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.\n "
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  "The Eurographics Association and Blackwell Publishing 2006.",
			  "r benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applicatio",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C"
			]
		  },
		  {
			"title": "CacheEfficient Layouts of Bounding Volume Hierarchies - Yoon - 2006 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2006.00970.x",
			"excerpts": [
			  "Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Hierarchy and Geometric Transformations"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "The VEB layout is complex\nto setup and maintain for trees and difficult to apply to graphs in\ngeneral.",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "Figure 2 shows graphically how this might be done. Assume\nan algorithm *P* *i* that aims to copy a tree while traversing it, into\nblocks that fit into the cache at level *i* . Using breadth first search,\nit can discover the entire subtree that fits into a block at level *i*",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "A third class of techniques (including the one in this paper) are\nused at runtime. One approach is to control memory allocation.",
			  "The van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7].",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "ble 1.** Cachegrind miss rates",
			  "Figure 4.** Binary Search Tree Performan",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  }
		],
		"reasoning": "The fine-grained field value centers on hardware-inspired data-density and memory-access density strategies to maximize effective bandwidth when traversing data structures, using concepts like compact layouts (e.g., van Emde Boas) and density-optimized storage to ensure every memory fetch yields meaningful data. The most directly relevant excerpts discuss cache-oblivious or cache-efficient tree layouts and density-conscious layouts (van Emde Boas and related approaches) that minimize misses by organizing nodes so that accessed data stay close together in memory. The passages that explicitly describe van Emde Boas (vEB) layouts, their recursive decomposition, and their impact on memory transfers provide concrete evidence for the strategy of storing tree nodes in cache-friendly contiguous blocks and for structuring trees to reduce bandwidth and misses. Additionally, broader treatments of cache-oblivious BVHs and bandwidth-optimized layouts illustrate how reordering and clustering can cluster accesses, which aligns with the Emu Chick-inspired goal of high data-density per fetch and predictable memory traffic. Quotes describing vEB layout as a recursive, cache-oblivious arrangement and the associated reductions in memory transfers support the claim that such layouts increase effective bandwidth by improving locality and reducing wasted data. Other excerpts discuss cache-efficient layouts (COLBVH, ODFL, treelet-based layouts) and the general principle of exploiting parent-child and spatial locality to reduce misses, which reinforces the hypothesis that density-packed layouts and topology-aware arrangements can uplift the performance of irregular memory access patterns. Collectively, these excerpts substantiate the idea that a software analogue could implement pointer-compression, density-packed node representations, and layout choices (e.g., van Emde Boas-based or COLBVH-style) to increase memory-efficiency and near-coalesced-like access patterns in irregular tree traversals, paralleling the Emu Chick-inspired design goals.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.3",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues.",
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  }
		],
		"reasoning": "The strongest alignment is with excerpts that explicitly present a decoupled access/execute (DAE) model, where one stream handles memory/address operations and another handles execution/computation, communicating via queues. These excerpts describe A-processors that generate addresses and memory requests and E-processors that perform arithmetic/logic, with data flowing between them through dedicated queues (AEQ/EAQ). This directly mirrors the notion of Computation Migration to Data in a NUMA-aware setting: migrate or split the workload so that data movement and computation are decoupled, enabling the compute to execute closer to data locality and enabling explicit scheduling of fetch and execute stages. In these excerpts, the memory/address generation tasks are executed by a dedicated Access processor, memory requests are issued, and an Execute processor performs the computational work, with the two streams communicating through queues. This provides a concrete architectural mechanism that echoes the idea of computation moving toward data locality on NUMA nodes, and it maps well to Emu Chick-style inspiration about co-locating compute with where data resides. The surrounding excerpts reinforce the same theme by describing how such a decoupled approach yields latency tolerance, ordered data movement, and streamlined coordination between memory operations and compute tasks, which are central to the concept of Computation Migration to Data. The connection is that the discussed architecture demonstrates practical realizations of decoupled memory-access and computation pipelines, which embody the same principle as migrating compute to data locality on NUMA-enabled systems and as inspired by Emu Chick style accelerators. The other excerpts, while informative on memory layouts, prefetching, and BVH/tree traversal optimizations, provide contextual grounding on related memory-coalescing and locality techniques but do not directly instantiate the requested fine-grained field value as clearly as the DA/EA and queue-based discussions.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.1",
		"citations": [
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB"
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*"
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM."
			]
		  }
		],
		"reasoning": "The target field describes a hardware-inspired cache strategy (Multi-Bank On-Chip Caches / Inter-Bank Dispersion) and a software analogue (page coloring) to emulate multi-bank behavior and reduce LLC conflicts during scattered, irregular tree-walker accesses. The most relevant excerpts directly discuss memory-layout and data-organization strategies that mimic the effect of distributing data across banks to minimize conflicts and improve locality:\n- The Bandwidth-Efficient BVH Layout for Incremental Hardware Traversal introduces a two-level clustering (ACs and CCs) and glue nodes to compress pointers and cluster traversal data, achieving explicit memory bandwidth reductions and improved cache locality. This aligns with the idea of distributing data access patterns to reduce cache conflicts and improve effective bandwidth, which is the essence of inter-bank dispersion in software terms.\n- The same Bandwidth-Efficient BVH Layout document details how clustering and reordering nodes by locality reduces bandwidth and improves cache utilization, which is a software-analogue to distributing data across banks to minimize contention.\n- The discussion of glue nodes and two-level clustering provides concrete mechanisms to cluster frequently traversed data so multiple walkers access memory in a more coherent pattern, effectively emulating inter-bank dispersion at the software/layout level.\n- The Cache-Efficient Layouts of BVHs and COLBVH discussions (Yoon & Manocha, and related COLBVH work) emphasize arranging data to exploit parent-child and spatial locality, reducing cache misses and bandwidth pressure, which mirrors the goal of distributing memory requests across banks to minimize conflicts in a single LLC. These sources provide foundational justification for software strategies (like page coloring) to emulate hardware-wide dispersion effects.\n- Additional context on memory-coalescing and cache-line alignment (coalescing discussions and 128-byte line granularity) supports the notion that data layout and allocation strategies can significantly impact effective memory bandwidth and conflict behavior, which is central to emulating multi-bank dispersion in software.\n\nCollectively, the strongest support comes from the exact techniques described for bandwidth-reducing BVH layouts and treelet-like clustering that group memory accesses, thereby reducing conflicts and improving coherence across divergent walkers. These excerpts map directly to the idea of creating software mechanisms (via data layout or page coloring-inspired allocation) that mimic hardware multi-bank dispersion to reduce LLC conflicts and improve effective bandwidth for irregular, data-dependent traversal patterns. The cache-oblivious and cache-efficient BVH literature further reinforces that carefully designed data layouts can achieve substantial gains in locality and bandwidth without hardware changes, which fits the requested software analogue approach.\n",
		"confidence": "medium"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.0.inspiration_source_accelerator",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "queues",
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines)."
			]
		  }
		],
		"reasoning": "The field value points to architectures that separate the memory-access aspect from the compute core, enabling higher memory throughput and better scheduling for irregular or data-dependent access patterns. The most relevant excerpts describe a decoupled access/execute design where an A-processor handles address generation and memory requests while an E-processor performs arithmetic and data processing, with communication via dedicated queues (the AEQ/EAQ structure). This separation supports keeping memory access flowing independently from computation, which is beneficial when many walkers diverge early but share an initial root. Additional sources discuss latency tolerance through asynchronous streams and queue-based communication, which aligns with batching opportunities and early-stage temporal locality when many walkers start at the same root. Other excerpts cover specialization of the two processors and high-level architecture comparisons, reinforcing how a two-stream, decoupled approach can yield throughput gains on memory-bound traversal tasks. In summary, the excerpts collectively support applying a DEA-like or Graphicionado-inspired design to the 256-walker tree traversal problem, where a decoupled memory-access path can better utilize limited memory bandwidth and exploit shared initial traversal phases before divergence. Key connecting points include (a) the concept of two independent instruction streams with inter-processor communication, (b) the use of queues to decouple access and execution, (c) latency-tolerant and parallelizable memory access patterns, and (d) architectural specialization to optimize either address generation or processing throughput. These elements collectively explain how the target design could emulate batching in the early rounds and manage irregular access through decoupled pipelines, matching the users search goals for SIMD-friendly and memory-efficient tree traversal techniques.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.0.expected_benefit_in_software",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines)."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  "**DEA vs. CRAY-1**",
			  "queues"
			]
		  }
		],
		"reasoning": "The finegrained field value describes hiding memory latency by overlapping memory access with computation, tolerating latency up to a bound set by walkers and load slots, and centralizing memory requests to enable batching and scheduling optimizations. Excerpts that introduce the Decoupled Access/Execute paradigm explicitly state that latency tolerance arises when the Access stream can run ahead of the Execute stream, enabling tolerance to memory latency and growth opportunities through asynchronous, decoupled processing. This supports the notion of overlapping memory access with computation in a software-system context. Further, the notes emphasize queue-based communication between the Access and Execute streams, which is a straightforward mechanism to hide latency by decoupling memory access from computation and allow scheduling and batching of memory requests. The discussion of specializationwhere the Access processor handles address calculations and memory requests while the Execute processor handles arithmeticreinforces the architectural separation that enables such latency-hiding techniques in software/hardware co-design. Additional excerpts describe the functional split and the flow of memory operations through dedicated queues, which underpins centralized memory request handling and potential batching. Collectively, these points directly support the proposed benefit as a software-visible outcome of a hardware-accelerator-inspired, decoupled design approach. The strongest connections are explicit statements about latency tolerance due to decoupled streams and queue-based communication, which map well to the idea of overlapping memory access with computation and centralizing memory requests for batching and scheduling optimizations. The remaining excerpts provide corroborating details about the architectures components and memory-flow characteristics that reinforce how such latency-hiding behavior could be achieved in practice.",
		"confidence": "medium"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.0.software_analogue_description",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines)."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  }
		],
		"reasoning": "The Decoupled Access/Execute (DAE) paradigm is precisely the architectural idea behind DSWP: separate the memory-issuing stage from the computation stage, enabling one unit to prefetch and issue memory requests while the other performs computation. The excerpts describe two coordinated processors with distinct responsibilitiesan Access (A) processor that handles address generation and memory requests, and an Execute (E) processor that performs arithmetic and logic operations and consumes memory data from the queues. This separation naturally supports dedicating one or more cores (or GPU warps) as an Access Unit that issues memory loads for the walkers, while other cores serve as Execute Units that compute the hash and control flow, communicating through dedicated queues (AEQ or similar) to transfer data/results. The architecture emphasizes latency tolerance by allowing the Access stream to run ahead of the Execute stream, which aligns with the goal of hiding memory latency in a heterogeneous, software-managed pipeline. The idea of queue-based communication reduces synchronization complexity and supports lock-free data exchange, which is crucial for maintaining throughput when multiple walkers diverge. The material also discusses specialization (e.g., one unit optimized for address calculation and memory requests, another for arithmetic pipelines), which mirrors the DSWP design where A-processor handles memory bandwidth and prefetching while E-processor handles computation. Together, these excerpts provide concrete references to decoupled processing, memory-request scheduling via dedicated channels (queues), and cross-unit coordinationcore components needed to implement the proposed DSWP solution with software-managed prefetching and shared-memory queues for the walkers.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.0.hardware_concept",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines)."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "queues",
			  "MAP-200",
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)"
			]
		  }
		],
		"reasoning": "The most directly relevant excerpts explicitly discuss the Decoupled Access/Execute (DAE) paradigm, describing an architecture with two separate instruction streams (Access and Execute), separate processors, and communication between them via queues or FIFOs. These passages articulate how memory access and computation are decoupled, with memory requests handled by an Access processor and data processed by an Execute processor, including how results flow through dedicated queues. Such descriptions directly map to the finegrained field value of Decoupled Access/Execute (DAE) as a design concept intended to improve throughput in memory-latency-bound or irregular access scenarios, like batched, data-dependent tree traversal where multiple walkers could benefit from overlapping memory access with computation. Additional excerpts expand on the specialized roles of the A-processor and E-processor, including address generation, memory requests, and the use of AEQ/EAQ queues to coordinate data movement, which reinforces the practical realization of DAE in hardware or accelerator-like designs. The surrounding notes on latency tolerance and potential specialization further support the idea that decoupled execution enables more flexible scheduling and higher throughput when memory bandwidth is a limiting factor. The presence of historical and architectural context around decoupled designs (e.g., references to DA/EA queues, decoupled architectures in computer architecture literature) provides corroborating evidence that DAE is a well-established approach relevant to the users problem of coordinating 256 parallel walkers with limited memory bandwidth. Secondary excerpts that discuss queue-based communication and the structural separation of A- and E-processing units reinforce how DAE can be leveraged to aggregate memory accesses and overlap them with computation, which is precisely the kind of batching and temporal locality the user is seeking to exploit in early rounds when walkers share a root and diverge. Overall, the gathered content consistently supports interpreting Decoupled Access/Execute as the central finegrained field value and provides architectural rationale for its applicability to the described workload.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.4.expected_benefit_in_software",
		"citations": [
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively."
			]
		  }
		],
		"reasoning": "The target field value concerns improving effective memory bandwidth so that each available load (out of a limited number of memory slots per cycle) yields more useful data. Excerpts that discuss cache-oblivious and cache-aware layouts for trees and hierarchies describe concrete strategies to minimize memory transfers and improve data locality when traversing irregular structures. For example, discussions of Cache-Efficient Layouts of Bounding Volume Hierarchies enumerate layout options (including van Emde Boas) and explicitly note how different layouts yield performance improvements by structuring data to improve cache efficiency and reduce costly memory transfers. This directly supports the idea that arranging data to maximize locality can increase the impact of memory loads, aligning with the notion of higher effective bandwidth per load. Additional material on memory-hierarchy sensitive graph layouts explains recursive layout techniques (like VEB) that place contiguous subtrees in memory to boost spatial locality, which again serves to reduce useless data transfers during traversal and thus enhances the usefulness of each load. The cache-oblivious searching and BVH-related discussions likewise emphasize strategies to minimize cache misses and memory transfers, offering concrete evidence that thoughtful data layouts can increase the effective bandwidth experienced by parallel walkers accessing irregular memory patterns. Together, these sources provide concrete support for the claim that data-layout and memory-organization choices can meaningfully raise the utility of each memory access under bandwidth-constrained, data-dependent traversal. ",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.3.software_analogue_description",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  }
		],
		"reasoning": "The most directly related concept is the decoupled access/execute model, which separates memory operations from computation and can enable scheduling techniques that tolerate memory latency and streamline memory-bound workloads. This aligns with the idea of reorganizing execution to place computation close to data, by treating the memory access as a separate stream that can be managed to optimize locality and bandwidth. Discussions that present decoupled architectures emphasize how memory latency can be hidden through parallelism and queues, which supports the notion of migrating or pinning tasks to the NUMA node housing the data to reduce interconnect traffic and latency. References describing memory-system considerations, such as miss-penalty reduction and cache/memory system design, provide context for why locality-aware scheduling would help in a scenario where many walkers access data with irregular patterns yet share initial root locality. The warp and instruction-stream specialization discussions, while focused on divergence and GPU-specific scheduling, offer complementary ideas about restructuring execution to minimize cross-domain stalls and improve throughput under irregular access, which can inform a NUMA-aware orchestration strategy. Finally, mentions of queue-based communication, specialized access/execute units, and misalignment between access and compute streams collectively underscore a broader architectural theme: separating concerns (memory vs compute) and scheduling work to maximize data locality and memory bandwidth, which is exactly the spirit of migrating a task to the NUMA node where its data resides. This collection of excerpts supports the applicability of NUMA-aware task migration and data placement policies as a viable optimization in the described parallel tree-walker workload.",
		"confidence": "medium"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.3.hardware_concept",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams."
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  }
		],
		"reasoning": "The most directly supportive excerpts describe the Decoupled Access-Execute (DAE) paradigm, which explicitly separates memory access from compute to tolerate memory latency and improve throughput when data movement dominates. This aligns with the notion of migrating computation patterns toward data or adaptively scheduling work around where data resides, effectively enabling computation to occur in a data-local context. The core idea across these excerpts is to split the workload into an address/loads phase and a compute phase, enabling specialized hardware or streamlined pipelines to keep the data flowing while computation proceeds, which is a concrete instantiation of \"Computation Migration to Data\" in practice. Additionally, warp specialization and related discussions show how to restructure execution around memory-access events (loader vs. compute stages) to reduce divergence penalties and improve SIMD efficiency in irregular access scenarios, which complements the concept of computation moving closer to the data or coalescing memory-heavy portions of work. Supporting details describe queue-based coordination between specialized streams, potential specialization of access vs. compute units, and the architectural benefits of decoupled execution. These points together illustrate a coherent approach where computation is effectively driven by data locality and staged memory access, which matches the finegrained field value about migrating computation toward data in hardware-aware designs. The included notes also touch on practical aspects such as handling memory latency, branch synchronization between streams, and the possibility of prefetching or specialized pipelinesall of which reinforce the interpretation of computation moving toward data to sustain high throughput under bandwidth constraints. In summary, the excerpts collectively substantiate a framework where computation is restructured to align with data availability and memory access patterns, embodying the essence of computation migration to data in hardware-centric optimization contexts.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.4.software_analogue_description",
		"citations": [
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph."
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL"
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization."
			]
		  }
		],
		"reasoning": "The finegrained field value advocates designing data structures with maximum density by using implicit layouts such as Eytzinger and packing multiple logical nodes or metadata into a single cache line to improve memory utilization. The excerpts directly address these themes: Eytzinger and van Emde Boas are presented as cache-friendly tree layouts that improve spatial locality and reduce memory transfers. Several sources discuss cache-oblivious layouts (VEB) for trees and BVHs, including how to recursively layout subtrees to improve cache performance and reduce memory traffic. These works also describe the performance benefits of dense, layout-aware structures (e.g., embedding trees in arrays, avoiding pointers, and using recursive VEB-like stratagems). The connection to the field value is that by adopting an implicit Eytzinger-like array layout or a cache-oblivious VEB layout, one can achieve higher data density per memory fetch and better utilize fetched cache lines, which matches the idea of packing more information per 64-byte line and reducing inefficiency from scattered pointers. Additionally, the discussion of memory-hierarchy-aware graph layouts reinforces the principle that careful layout choice (dense, implicit, cache-friendly) can materially impact memory bandwidth usage in traversal workloads.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.4.hardware_concept",
		"citations": [
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB"
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization."
			]
		  }
		],
		"reasoning": "The requested fine-grained field value centers on Fine-Grained Memory Access, i.e., strategies to organize and access memory efficiently at a fine-grained level under heavy parallelism. The most relevant excerpts discuss memory-hierarchy-aware and cache-oblivious layouts for trees and graphs, which directly address how data should be laid out and accessed to improve memory bandwidth usage and reduce cache/memory stalls during irregular or irregular-but-predictable traversal. For example, the memory-hierarchy sensitive graph layout describes laying out a graph as a tree component and recursively organizing subtrees to improve locality, which directly informs how to structure data to maximize temporal locality when 256 walkers diverge early but share common prefixes. The discussions of van Emde Boas layouts and cache-oblivious layouts for bounding volume hierarchies provide concrete layouts (VEB, DFL/BFL, COML) that reduce memory transfers and improve spatial locality, which are core techniques for fine-grained memory access optimization. Descriptions of explicitly arranging a tree using a VEB or related cache-oblivious approach, and the notion of preparing a sub-graph in the form of a tree to fit memory hierarchy characteristics, further support the idea of orchestrating memory accesses to minimize random/scattered accesses and exploit locality across batched or divergent traversals. Additional notes on memory hierarchy sensitivity (e.g., ensuring that data structures maintain locality across levels of cache and memory subsystems) reinforce the connection to fine-grained memory access improvements, particularly in the context of traversing hierarchical structures like trees or BVHs where access patterns can be coalesced or batched. Collectively, these excerpts provide concrete, architecture-aware strategies (layout choices, recursive subdivision, and hierarchy-aware placement) that underpin fine-grained memory access optimizations in parallel, diverging-walker scenarios.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.1.hardware_concept",
		"citations": [
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM.",
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*"
			]
		  },
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next."
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB"
			]
		  }
		],
		"reasoning": "The most directly relevant excerpts discuss how memory access patterns and cache behavior influence performance in traversals and irregular access scenarios. A note on coalescing memory accesses emphasizes that aligning and coalescing accesses reduces traffic to the memory subsystem and makes better use of the memory controller, which is a core concern when designing or leveraging multi-bank caches that can service multiple streams in parallel. The discussion of coalescing and memory controller behavior, including the importance of aligned, contiguous access patterns, maps closely to the concerns of inter-bank dispersion where bank conflicts and bandwidth utilization are critical for keeping many parallel walkers fed with data. Additionally, several excerpts describe cache-aware or cache-oblivious BVH layouts and their impact on bandwidth and cache locality. These layouts aim to organize data so that spatial and temporal locality are exploited, which is exactly what multi-bank on-chip caches and dispersion strategies try to achieve: reduce cross-bank traffic, improve cache hit rates, and maintain throughput when traversal paths diverge. The bit about back-end stalls and memory bottlenecks in the divergence-cost context highlights that memory bandwidth and cache interactions are the dominant constraints, reinforcing the relevance of multi-bank cache architectures to mitigate such bottlenecks. The discussions of cache-oblivious and cache-aware layouts (e.g., COLBVH, van Emde Boas, DFS/BFS variants) illustrate design space where data placement and access patterns influence cache performance, which aligns with the goals of inter-bank dispersion to balance load and minimize cross-bank traffic. Together, these excerpts provide evidence that cache-aware strategies, memory coalescing, and layout choices are pertinent to enabling high-throughput, irregular, data-dependent traversals on architectures with multi-bank on-chip caches.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.3.expected_benefit_in_software",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  }
		],
		"reasoning": "The finegrained field value asserts that an accelerator-inspired design eliminates remote NUMA latency and guarantees local, fast memory accesses for a latency-bound workload. Excerpts describing decoupled access/execute architectures directly address this core idea by proposing a split where memory operations (Load/Store, address calculation) are handled by an Access stream and computations by an Execute stream, effectively tolerating memory latency and enabling ahead-of-time data fetching. The notion of decoupled instruction streams and queue-based communication demonstrates a concrete mechanism by which memory accesses can be organized to appear local and predictable, which aligns with removing remote NUMA penalties. Additional excerpts highlight that decoupling allows latency to be hidden via overlapping memory access with computation, and that specialized hardware paths or warp-level memory management can further improve memory access efficiency. Discussions of miss-penalty reduction and memory-traffic-oriented GPU techniques provide complementary evidence that sophisticated memory scheduling and data delivery strategies can further reduce effective latency, which supports the claim that a hardware/software design can achieve local, fast memory access for latency-bound workloads. Overall, the strongest support comes from explicit descriptions of decoupled access/execute paradigms and their latency-tolerance benefits; supporting points include dedicated memory-access vs compute streams, queue-based inter-stream communication, and the notion of specializing units to optimize memory traffic. The other excerpts supplement by illustrating related concepts like warp specialization and kernel-level memory management, which reinforce the general direction of improving memory access locality and scheduling in parallel hardware contexts.",
		"confidence": "high"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.1.software_analogue_description",
		"citations": [
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*"
			]
		  }
		],
		"reasoning": "To support the goal of emulating conflict-avoidance via memory allocation strategies, the most relevant content demonstrates that reordering or reorganizing a tree's layout can significantly affect bandwidth and cache locality during traversal. One excerpt describes a novel BVH memory layout that improves cache locality and compresses child pointers, directly addressing how data placement can reduce bandwidth- and cache-related penalties during traversal. Another excerpt discusses COLBVH and other cache-oblivious layouts that recursively decompose the tree into clusters and order nodes by access probability, which aligns with the concept of distributing or grouping data to minimize cross-walker eviction and to exploit temporal locality. Additional excerpts lay out the broader context that the way nodes are laid out (DFS, BFS, vEB, COLBVH) strongly influences how well traversals with irregular access patterns behave on real hardware, reinforcing that careful data placement and layout choices are viable strategies for reducing eviction contention among many walkers. The discussion of the cost of divergence and bandwidth requirements provides practical justification for why such cache-conscious strategies matter in high-throughput traversal scenarios. Finally, references to memory bandwidth and cache considerations in traversal contexts offer corroborating context for applying page coloring or OS-level mapping ideas to distribute data across LLC sets, since these strategies aim to reduce contention and improve locality at scale. Overall, the strongest support comes from explicit cache-locality-focused layout approaches, with secondary support from the broader context of memory bandwidth and divergence costs.",
		"confidence": "medium"
	  },
	  {
		"field": "hardware_accelerator_inspired_designs.1.expected_benefit_in_software",
		"citations": [
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing.",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th"
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*"
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM."
			]
		  }
		],
		"reasoning": "The most relevant material directly ties memory layout and traversal locality to bandwidth savings and cache efficiency. A novel BVH memory layout that improves cache locality and compresses child pointers directly supports the idea that reorganizing data structures can reduce memory traffic and contention, which aligns with reducing LLC conflicts and improving cache hit rate under constrained bandwidth. An approach that uses a two-level clustering scheme to reorder nodes while keeping compact child pointers further substantiates how data layout can reduce pointer-chasing overhead and improve spatial locality during traversal. Cache-oblivious and cluster-based layout concepts (including van Emde Boas style decompositions and COLBVH) illustrate strategies to maintain cache efficiency across hierarchical memory, which is exactly the kind of technique that would mitigate irregular access penalties and lower effective memory latency. Discussion of how traversing the same set of nodes with different layouts impacts bandwidth reinforces the central claim that structure-aware layouts can reduce bandwidth pressure during traversal. Additional excerpts discussing explicit cache-oblivious layouts and their role in collision detection or ray tracing provide concrete evidence that such layouts are practical under memory bandwidth constraints and can improve locality across bursts of traversal. Content on memory access patterns such as divergence cost and back-end stalls highlights the overall memory bottlenecks that these layout strategies aim to alleviate, which further supports the idea that reducing LLC conflicts and improving cache utilization translates into higher effective bandwidth. Peripheral notes on coalescing memory accesses in CUDA contexts illustrate analogous benefits from aligning access patterns and memory transactions, offering broader validation that memory layout and access cohesion improve performance in irregular traversals. The cited material collectively builds a proof-of-concept that cache-friendly, clustered, or cache-oblivious layouts can reduce memory traffic and improve cache efficiency for irregular traversal patterns, thereby supporting the claimed benefit of reduced LLC conflicts and improved effective bandwidth.",
		"confidence": "high"
	  },
	  {
		"field": "executive_summary",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "This is\nachieved by sharing the traversal stack among the rays in a warp-\nsized packet, which implies rays will visit (potentially many) nodes\nthey do not intersect but also that memory accesses are very co-\nherent because each node is fetched only once per pack",
			  "We discuss the mapping of elementary ray tracing operations\nacceleration structure traversal and primitive intersectiononto\nwide SIMD/SIMT machines",
			  "Trees with branching factor higher than two can make memory\nfetches more coherent and possibly improve the SIMD efficiency,\nand may therefore improve performance on some platforms.",
			  "Wide trees**\nTrees with branching factor higher than two can make memory\nfetches more coherent and possibly improve the SIMD efficiency,\nand may therefore improve performance on some platforms. We\nimplemented a kernel that supported branching factors 4, 8, 16 and\n32, by assigning a ray to the respective number of adjacent threads"
			]
		  },
		  {
			"title": "Embree: A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://cseweb.ucsd.edu/~ravir/274/15/papers/a143-wald.pdf",
			"excerpts": [
			  "Single-Ray Vectorization**\nOther work has focused on using vectorization to accelerate spa-\ntial data structure traversal for individual rays. For example, multi-\nbranching BVH structures enable multiple nodes or primitives to\nbe tested for intersection against the same ray in parallel. Quad-\nbranching BVH (BVH4) data structures perform well with 4-wide\nand 16-wide vector units [Ernst and Greiner 2008; Dammertz et al.\n2008; Benthin et al. 2012], but higher branching factors offer di-\nminishing returns [Wald et al. 2008].\nGenerally speaking, single ray vectorization is faster than packet\ntracing for incoherent ray distributions (and can be used within a\nscalar renderer), but is slower for coherent rays. For this reason, hy-\nbrid techniques have been developed which can dynamically switch\nbetween packet tracing where rays are coherent, and single-ray vec-\ntorization where not [Benthin et al. 2012",
			  "cket tracing is conceptually simple compared to single-ray SIMD\ntraversal and intersection. In classical packet tracing, rays in a given\npacket are intersected with the same BVH node or triangle at each\nstep of traversal. However, it is also possible to implement packet\ntracing following a single program multiple data (SPMD) program-\nming model. Here, the rays of a packet are independently traversed\nthrough the BVH, and each ray is potentially tested against different\nBVH nodes or triangles [Aila and Laine 2009].",
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "Yet\nthe memory storage order for a BVH optimized for packets may be\nsuboptimal for single-ray methods (the converse may also be true).",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "**Hybrid Traversal and Intersection**"
			]
		  },
		  {
			"title": "General Transformations for GPU Execution of Tree ...",
			"url": "https://engineering.purdue.edu/~milind/docs/sc13.pdf",
			"excerpts": [
			  "Tree traversal algorithms represent an interesting target for GPU\nparallelization. As naturally parallel algorithms (the points traver-\nsals of the tree are independent), they exhibit the massive paral-\nlelism that GPUs excel at.",
			  "However, because the tree structures\nare irregular, and the points traversals are input-dependent, sim-\nply running multiple traversals simultaneously on the GPU can-\nnot take advantage of efficient memory accesses, seriously hinder-\ning performance (Section 2.2 discusses GPU architectures and the\nGPU performance model in more detail).",
			  "chieve this\nthroughput, the GPU memory controller requires threads within the\nsame warp to access contiguous regions of memory so that a sin-\ngle wide block of memory can be transferred in a single transac-\ntion."
			]
		  },
		  {
			"title": "Generalizing Ray Tracing Accelerators for Tree Traversals ...",
			"url": "https://intra.engr.ucr.edu/~htseng/files/2024MICRO-TTA.pdf",
			"excerpts": [
			  ") RTAs use a dedicated hardware memory scheduler**\n**to improve memory bandwidth utilization.** This scheduler\ncoalesces node requests between threads when possible and\narbitrates for one memory request into the GPU memory system\nper cycle. The dedicated memory scheduler better handles\nirregular memory access patterns by only focusing on node\nrequests, allowing the scheduler to track more concurrent\ntraversals and increase DRAM utilization by nearly ",
			  "Prior works have explored accelerating tree-based appli-\ncations on FPGAs and ASICs, such as k-nearest neighbor\nsearch [ 56 ], [ 86 ], dynamic search tree [ 98 ], and ray tracing [ 48 ],\n[ 66 "
			]
		  },
		  {
			"title": "REGTT: Accelerating Tree Traversals on GPUs by ...",
			"url": "https://cgi.cse.unsw.edu.au/~jingling/papers/icpp16.pdf",
			"excerpts": [
			  "There are three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The thre",
			  "re three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access overhead.\n** **Load Imbalance** Different threads in the same warp may\nhave different workloads due to query-dependent",
			  "e three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access over"
			]
		  },
		  {
			"title": "SIMD Parallelization of Applications that Traverse Irregular ...",
			"url": "https://www.cs.wm.edu/~bren/files/papers/CGO13.pdf",
			"excerpts": [
			  "In order to make our approach fast, we demonstrate\nseveral optimizations including a stream compaction method\nthat aids with control flow in SIMD, a set of layouts that\nreduce memory latency, and a tiling approach that enables\nmore effective prefetching."
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays | Research",
			"url": "https://research.nvidia.com/publication/2010-06_architecture-considerations-tracing-incoherent-rays",
			"excerpts": [
			  "Furthermore the architecture allows submitting rays in an arbitrary order with practically no performance penalty."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  ". Aila et al. [ 2 ] proposed group-\ning rays into ray packets and traversing rays together, improving\nmemory coherence. ",
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr",
			  "Acceleration Structure Optimizations.* Some works optimize the"
			]
		  },
		  {
			"title": "Batching of divergent rays on GPU architectures",
			"url": "https://studenttheses.uu.nl/bitstream/handle/20.500.12932/41250/thesis_final.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "A number of advanced traversal algorithms have been devised in recent years that increase ray coherence by bundling active rays together dynamically as they ...Read more",
			  "Benthin et al.[ Ben+12 ] devised a hybrid traversal model that combines ray packets and single ray\ntraversal."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for GPU Ray Tracing | Request PDF",
			"url": "https://www.researchgate.net/publication/220507044_Fast_Ray_Sorting_and_Breadth-First_Packet_Traversal_for_GPU_Ray_Tracing",
			"excerpts": [
			  "Garanzha and Loop (2010) devised breadth-first packet traversal algorithms using fast ray sorting via a compress-sort-decompress scheme to generate ray packet bundles."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level."
			]
		  },
		  {
			"title": "Treelet Accelerated Ray Tracing on GPUs",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/chou.asplos2025.pdf",
			"excerpts": [
			  "a et al. [ 5 ]\nproposed to subdivide the BVH tree into smaller subtrees, or\ntreelets, that fit in the processors cache to reduce memory\ntraff",
			  "Treelet queues essentially achieve a similar goal by grouping\nup rays based on their accessed treelet, but without the high\noverhead."
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://meistdan.github.io/publications/bvh_star/paper.pdf",
			"excerpts": [
			  " BVH is divided into *treelets* (small subtrees within the to-\ntal BVH) which are set to the size of either the L1 or L2 cache. The\narchitecture maintains a set of ray *queues* , with queues assigned\nto treelets at runtime. Rays begin tracing at the root treelet, and\nas rays cross treelet boundaries, they are placed in the ray queues\nto be processed later when their required treelet is present in the\ncache. The architecture thus attempts to maximize the number of\nrays which are processed each time a treelet is loaded on-chip, re-\nducing memory bandwidth",
			  "odniok et al. [ WSWG13 ] proposed new layouts: *swapped*\n*subtrees* (SWST) and *treelet-based depth-first-search/breadth-first-*\n*search* (TDFS/TBFS). These layouts are determined based on the\nnode access statistics obtained by casting a small number of sample\nrays in a preprocessing step. SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged. The latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order. The authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.\nHowever, none of these layouts is always better",
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Tree traversal is an intensive pointer-chasing operation, requiring traversing to a node in the tree and finding the child pointers, before being able to find the child node addresses and issue loads.",
			  "With treelet prefetching, as rays traverse the BVH tree and visit the root node of treelets, corresponding treelets can be prefetched to load deeper levels of the tree before they are needed.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulations show treelet based traversal reduces performance slightly by 3.7% over a DFS baseline. However, when combined with treelet prefetching, the overall speedup reaches 32.1% while maintaining the same power consumption.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping rays that traverse similar parts of the AS. Pharr et al. [ [39]() ] reordered ray computation to improve ray coherency and cache utilization. Garanzha and Loop [ [16]() ] sorted rays based on ray origin and direction before processing in packets. Moon et al. [ [32]() ] sorted rays with their final hit points. Meister et al. [ [30]() ] improved sorting heuristics to minimize ray divergence.",
			  "**Acceleration Structure Optimizations.** Ylitie et al. [ [48]() ] explored wide BVH trees to increase SIMD utilization. Lin et al. [ [26]() ] restructured BVH nodes with node splitting, reducing memory footprint. Benthin et al. [ [10]() ] and Liktor et al. [ [25]() ] perform BVH compression for memory bandwidth reduction. BVH optimizations benefit our work as more nodes fit into the same memory footprint, making prefetching more effective.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache. Ray tracing is a pointer-chasing application and memory accesses are divergent and hard to predict. With the treelet based traversal algorithm introduced previously, memory accesses are now clustered as individual treelets, making it possible to prefetch easily.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "... BVH tree statistics for each scene are outlined in Table 2. The scene ... We use the concept of treelets which are connected subpartitions of a BVH tree.Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "We form treelets by grouping connected BVH nodes to maximize the size of each treelet. It is a greedy algorithm that starts from the BVH root node and greedily adds nodes to the current treelet until the maximum treelet size is reached.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "We propose to add a treelet prefetcher that prefetches treelets into the L1 cache of the GPU based on the rays in the warp buffer.",
			  "The threshold comparator generates the prefetch enable signal if the treelet popularity is greater than a manually set threshold which ranges from 0 to the maximum number of rays in the warp buffer ().",
			  "The prefetch enable is ANDed with the upper bits of the treelet root node address to generate the treelet prefetch address and sent to the prefetch queue to be processed ().",
			  "We only require the upper bits of the treelet root node address because the treelets have a fixed maximum size and nodes within a treelet are organized to be packed together in memory.",
			  "The treelet prefetcher also records the address of the last treelet it prefetches to avoid pushing duplicate treelet addresses to the prefetch queue and prefetching the same treelet multiple times in a row.",
			  "We combine treelet prefetching with a treelet based traversal algorithm in the ray tracing accelerator to further reduce ray traversal latenc",
			  "\nRay traversal is typically done by traversing the BVH tree in a depth-first or breadth-first manner",
			  "This section describes our proposed treelet prefetching technique for ray tracing.",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "As a ray visits a treelet root node, its subsequent memory accesses will also be to the nodes in the treelet since accesses to nodes from different treelets are deferred to the *otherTreeletStack",
			  "Thus, we can prefetch the entire treelet to the GPU's cache and reduce the latency of accessing nodes in the current treelet.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "[15] Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First\nPacket Traversal for GPU Ray Tracing. *Computer Graphics Forum* (2010).",
			  "[30] Daniel Meister, Jakub Boksansky, Michael Guthe, and Jiri Bittner. 2020. On Ray\nReordering Techniques for Faster GPU Ray Tracing. In *Proc. ACM SIGGRAPH*\n*Symp. on Interactive 3D Graphics and Games (I3D)* . 19.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "d\nTreelet Prefetching For Ray Tracing\nMICRO 23, October 28November 01, 2023, Toronto, ON, Canada\nmore frequently than the lower levels. In the next sections, we pro-\npose a treelet based ray traversal algorithm with an accompanying\nprefetcher.",
			  "s a ray visits a treelet root node, its subse-\nquent memory accesses will also be to the nodes in the treelet since\naccesses to nodes from different treelets are deferred to the *oth-*\n*erTreelet",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "refetching techniques can\nbe used to improve memory latency tolerance in GPGPU applica-\ntions [ 22 , 24 , 42 ]",
			  "a treelet prefetcher to the RT unit to speed\nup ray traversal along with a prefetch queue to hold the issued\nprefetch addresses, both of which are highlighted in red in Figur",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "In Vulkan-Sim [ 41 ], the simulator we use\nfor evaluation, the ray tracing accelerator is referred to as the RT\nunit. When a warp issues a trace ray instruction, it enters the RT\nunit during the pipelines execute stage and is queued in the warp\nbuffer, which holds ray metadata for all 32 threads of the warp.",
			  "This work presents a treelet prefetching scheme to improve ray\ntraversal performance.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS. Pharr et al. [ 39 ] reordered\nray computation to improve ray coherency and cache utilization.",
			  "**7.3**\n**Data Structure Partitioning**\nFeng et al. [ 13 ] take advantage of data structure partitioning to ex-\nploit parallelism. Within parallel regions during traversal identified\nby the programmer with compiler pragmas, they subdivide graphs\nand trees and distribute each partition to be processed by different\ncores to improve parallelism for CPUs. Locality improves by hav-\ning the same cores process the same partitions repeatedly.",
			  "**7.4**\n**Treelet Based Ray Tracing Techniques**\nNavratil et al. [ 33 ] tackled incoherent rays by collecting rays into\n ... \nstaging buffer which might require non-trivial shader modifications\nto realize on a GPU and are not discussed in their paper.",
			  "**7.5**\n**Ray Traversal Acceleration Techniques**\n**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "We propose to add\na treelet prefetcher that prefetches treelets into the L1 cache of\nthe GPU based on the rays in the warp buffer.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict.",
			  "With 512B treelets, the treelet\nbased memory layout performs best with a 31.9% speedup over\nthe baseline.",
			  "Loose and Strict Wait represent a rough upper and\nlower bound performance when using an unmodified BVH tree that\nrequires mapping table loads, falling behind the repacked BVH with\nonly a 29.7% speedup and 2.5% slowdown respectively.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The original BVH node\nlayout is allocated to be 64 bytes and stores the bounding box,\npointers, and other metadata for the child nodes of a 6-wide BVH\ntree.",
			  "It is a greedy algorithm that starts from the BVH\nroot node and greedily adds nodes to the current treelet until the\nmaximum treelet size is reached. T",
			  "Figure 3 shows\nan example of a two-wide BVH tree partitioned into treelets with a\nmaximum size of 4 nodes each.",
			  "reelet formation initializes the *remainingBytes* to the maximum\ntreelet size and adds the BVH root address to the *pendingTreelets*\nqueue and traversal sta"
			]
		  },
		  {
			"title": "Efficient SIMD Single-Ray Traversal using Multi-branching ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/wald08_widebvh.pdf",
			"excerpts": [
			  "In this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.\nNote that this forces every multi-node to contain 16 node slots even\nif the parent node has less than 16 children; any unused nodes are\nflagged as invalid. ",
			  "\n**M** **ETHOD** **O** **VERVIEW**\nIn this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.",
			  "The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  "the optimal branching factor is somewhere between 4 and 8.",
			  "Concurrently to this paper, the idea of BVHs with branching fac-\ntors higher than two has also been investigated by Dammertz et\nal. [4]",
			  "The only difference to the standard SAH is that the cost function\nfor leaves and inner nodes is slightly different. Since we always\nperform 16 triangle tests respectively 16 axis-aligned box tests in\nparallel for the same ray, intersecting 16 triangles in a leaf now\ncosts roughly as much as intersecting a single triangle; the same\nargument holds true for box tests in inner nodes.",
			  "For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16.",
			  "**B** **UILDING** **G** **OOD** **M** **ULTI** **-** **BRANCHING** **BVH** **S**\nThough an efficient traversal routine is key to performance, the ac-\ntual way the data structure is built often can have a similar per-\nformance impact.",
			  "onsequently, if there were a method that would use the 16-wide\nSIMD to intersect less than 16 triangles (say, four), and that would\ndo that *faster* than we currently perform 16 triangle intersections,\nthen a BVH with a less extreme branching factor might be even\nfaster",
			  "We also introduce an efficient SIMD traversal technique for these\nBVHs that produces a strict front-to-back traversal with a minimum\nof scalar code to determine the traversal order.",
			  "ber of inner nodes drops even further, by consistently\naround 50 *x* . Though every node now is 16 *x* as large as a traditional\nnode, the net savings is still significant (we had, in fact, expected a\n*higher* memory consumption",
			  " unused nodes are\nflagged as invalid. The actual true hierarchy is governed by a sur-\nface area heuristic that takes the modified traversal and intersection\ncost into account. The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  " traditional assumptions for the surface area heuris-\ntic [6, 7], the probability of any node *n* being traversed by a random\nray is proportional to the nodes surface area *SA* ( *n* ) . Thus, the ag-\ngregate cost in SIMD triangle intersections and SIMD box tests for\na given multi-BVH can be estimated a",
			  "We demonstrate that with a properly built bounding volume hierarchy (BVH) and a front-to- back traversal algorithm, this approach is somewhat slower than."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://users.aalto.fi/~laines9/publications/ylitie2017hpg_paper.pdf",
			"excerpts": [
			  "Specifically, we start\nby building a binary BVH using an existing high-quality builder,\nand then convert it into an 8-wide BVH in a SAH-optimal fashion.\nWe also employ octant-aware fixed-order traversal [Garanzha and\nLoop 2010], and present an improved method for ordering the child\nnodes at build time to obtain a better traversal order.",
			  "We present a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "Wide BVHs.* Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instead. Shallower hierar-\nchies reduce the number of visited nodes and memory accesses\nduring traversal, but they typically increase the number of ray-box\nand ray-triangle intersection tests [Afra 2013]. Wider BVHs also",
			  "The high branching factor amortizes the memory fetch latencies\nover 8 bounding box intersection tests and increases instruction\nlevel parallelism during internal node tests.",
			  "he memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code.",
			  "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, U",
			  "In addition, we present an algorithmically efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "e also encode the quantiza-\ntion grid and child node indexing information in a compact for",
			  "We compare the performance of our method ( **Ours** ) to four pre-\nviously published GPU-based methods: the traversal kernels by\nAila et al. [2012] ( **Baseline** ), latency-optimized four-wide traver-\nsal by Guthe [2014] ( **4-wide** ), stackless traversal by Binder and\nKeller [2016] ( **Stackless** ), and irregular grids by Prard-Gayot et\nal. [2017] ( **IrrGrid** ). ",
			  " ). We use the authors original implementations\nfor all comparison methods, with no changes to the traversal code.\nFor *",
			  "**5.1**",
			  "**Memory usage**",
			  "Table 6 shows the amount of memory consumed by each method.",
			  "We only report the memory usage of the acceleration structure\nitself, excluding the triangle data to make the comparison as fair as\npossible.",
			  "icient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs\nHPG 17, July 28-30, 2017, Los Angeles, CA, USA\nRay-node tests\nRay-triangle tests\nBounce\n0\n1\n4\n0\n1\n4\nFull precision\n14.30\n14.54\n14.62\n6.45\n7.64\n8.61\n8 bits\n14.69\n14.95\n15.02\n6.59\n7.98\n9.03\n7 bits\n15.04\n15.32\n15.38\n6.73\n8.23\n9.31\n6 bits\n15.69\n16.03\n16.06\n7.01\n8.70\n9.81\n5 bits\n17.18\n17.53\n17.50\n7.62\n9.56\n10.76\n4 bits\n20.56\n20.86\n20.67\n8.94\n11.36\n12.71\n**Table 1: Effect of AABB quantization on intersection test**\n**counts. The numbers represent arithmetic means over our**\n**test scenes, excluding Pow",
			  "resent a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed B",
			  "Similarly to previous methods [Keely 2014; Mahovsky and Wyvill\n2006; Segovia and Ernst 2010; Vaidyanathan et al . 2016], we quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "an 8-wide BVH, and we\n ... \ninternal nodes require only ten bytes per child, obtaining 1 : 3 . 2\ncompression ratio compared to the standard uncompressed BVH\nnode format.",
			  "y efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "Child Bounding Box Compression",
			  "quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "ficient Incoherent Ray Traversal on GPUs Through**\n**Compressed Wide BV",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, USA",
			  "Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instea",
			  "Our acceleration structure is a compressed 8-wide BVH that uses\naxis-aligned bounding boxes (AABBs) as the bounding volumes.",
			  "Wide BVHs.",
			  "Managing a full traversal stack is costly in GPU ray tracers,\nbecause the caches in GPUs are too small to capture the stack traffic.\nThis tends to lead to high DRAM traffic from the traversal stacks\nonly.",
			  "We compress both bounding boxes and child pointers so that our"
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs | Research",
			"url": "https://research.nvidia.com/publication/2017-07_efficient-incoherent-ray-traversal-gpus-through-compressed-wide-bvhs",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal.",
			  "the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion",
			  "Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art.",
			  "Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CPUStyleSIMDRayTraversal.pdf",
			"excerpts": [
			  "**Same Old, Same Old**",
			  "**Same Old, Same Old**",
			  "Binary BVH\n",
			  "Binary BVH\n",
			  "Stack based\n",
			  " Root node on stack",
			  "Test for intersections\n",
			  "* Check distanc",
			  "* Push onto stac",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**Usual Suspects**",
			  "SIMT / GPU"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://jcgt.org/published/0011/04/01/paper.pdf",
			"excerpts": [
			  "odern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes",
			  "ald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *,*\n(",
			  "**3.**\n**Implementation**\nIn this section, we provide implementations details of the evaluated methods. Most\nof the tested methods have publicly available implementations. However, to provide\na fair comparison, we have to use the same settings for all methods, which might be\ndifficult in different frameworks. Thus, we created a single unified framework with\npublicly available implementations of the algorithm integrated into it. Our framework\nis based on Ailas ray tracing framework [ Aila and Laine 2009 ].\nAilas framework contains high-performance traversal kernels for binary BVHs.\nTo support wide BVHs, we adopted the publicly available implementations of the\nmethod proposed by Lier et al. [ 2018 ]. In the original version of these kernels, the",
			  " exploit.\nModern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes. Triangles in leaves are processed\nin a similar fashion, and thus it is desirable to have the number of triangles aligned\nwith the SIMD width as the number of executed intersection tests is equal to the SIMD\nwidth. Motivated by this fact, Wald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *",
			  "Traversal on the GPU might be challenging due to warp divergence and incoherent\nmemory accesses.",
			  "Aila and Laine [ 2009 ] proposed a stack-based traversal algorithm\nwith persistent warps and dynamic fetch.",
			  "To prevent warp divergence, the traversal\nis divided into two independent loops processing interior and leaf nodes separately\n(i.e., the *while-while* traversal).",
			  "Dynamic fetch allows to fetch new rays if a certain number of threads are\ninactive to keep parallel resources occupied enough."
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through compressed wide BVHs",
			"url": "https://www.researchgate.net/publication/318730238_Efficient_incoherent_ray_traversal_on_GPUs_through_compressed_wide_BVHs",
			"excerpts": [
			  "Wald et al. [WBB08] presented the way to convert a binary BVH to a wide BVH.",
			  "de BVHs on the GPU On GPUs, Guthe [Gut14] found that tracing a binary hierarchy was latency-bound on their hardware and that using a 4-wide BVH increases tracing performance",
			  ". In practice, we build a binary BVH (all the aforementioned methods produce binary BVHs), and then we convert it to a wide BVH by pulling child nodes to the parent nodes in a top-down manner [Wald et al. 2014]. Pinto [2010] and Ylitie et al. [2017] proposed an algorithm based on dynamic programming that performs this conversion optimally with regard to the BVH cost.",
			  "Ylitie et al. showed how to do the conversion in a SAH-optimal fashion and presented a compressed wide layout [YKL17] ",
			  "The direct construction of wide BVHs is considered difficult, remaining as an open problem.",
			  "wide BVHs are the state-of-the-art acceleration structure for GPU raytracing, therefore we want our construction algo-rithm to output the nodes in a compressed format. We use a representation similar to the one described by Ylitie et al. [YKL17] : ",
			  "By quantizing the coordinates of child bounding boxes to a single byte they reduce the size of an 8-wide node to 80 bytes.",
			  "The presented 8-wide BVH ray stream implementation reduces memory traffic to only 18% of traditional approaches by using 8-bit quantization for box and triangle coordinates and directly ray tracing these quantized structures.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH."
			]
		  },
		  {
			"title": "Wide BVH Traversal with a Short Stack",
			"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/wide-bvh-traversal-with-a-short-stack-837099.pdf",
			"excerpts": [
			  "In this paper we introduce an algorithm for wide bounding volume hierarchy (BVH) traversal that uses a short stack of just a few entries. This stack can be ...Read more"
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231583/07-1038-lier.pdf",
			"excerpts": [
			  "In this paper we describe and evaluate an implementation of CPU-\nstyle SIMD ray traversal on the GPU. We show how spreading\nmoderately wide BVHs (up to a branching factor of eight) across\nmultiple threads in a warp can improve performance while not\nrequiring expensive pre-processing. Te presented ray-traversal\nmethod exhibits improved traversal performance especially for\nincreasingly incoherent rays.",
			  "Te foundation of our approach is teaming multiple lanes of a warp\nand leting them traverse the BVH together for one single ray. Tis\nconcept mimics a regular SIMD-based BVH traversal known from\nmethods utilizing SSE and AVX extension on the CPU [ Dammertz\net al . 2008 ; Ernst and Greiner 2008 ; Wald et al . 2014 ]. But in con-\ntrast to CPUs, switching between vector instruction (e.g. parallel\nintersection tests) and scalar instruction (e.g. stack management) is\nnot easily (or even efciently) possible on GPUs. In our case, some\noperations (e.g. loading, storing, and stack management) have to\nbe handled individually and at times redundantly on each lane.\nTerefore, we supply each lane with its own copy of the ray data,\nnearest hit information, and stack pointer. However, the stack itself\nresides in shared memory and thus is not redundant.\n",
			  "We start the construction of our acceleration\nstructure with a regular binary BVH. Tere are no limits to the\nconstruction of this BVH, except that we strive for the number of\nleaf primitives to match the width of our target BVH, i.e. 2, 4, or\n8 elements, similarly to CPU implementations [ Ernst and Greiner\n2008 ].",
			  "BVH Construction.* Starting with an efcient binary BVH [ Stich\net al . 2009 ], a similarly efcient wide BVH can be constructed ef-\nciently by pulling up individual nodes to create a BVH of a specifc\nwidth [ Wald et al . 2008 ]. Similarly, fast construction methods for\nbinary BVHs [ Karras 2012 ; Lauterbach et al . 2009 ; Selgrad et al .\n2015 ] can be turned into efcient methods for wide BVHs in the\nsame way",
			  "all our BVHs, we slightly adjust the node layout\n(from Aila et al. [ 2012 ]) in order to improve coalesced memory\naccesses. Te original implementation loads two bounding boxes\nand two child indices at once. Each thread individually fetches four\n*Vec4* elements and both child indices are stored in the last *Vec4*\nelement:\n**int** i = node_index * 4;\nnodes[i+0] = **vec4** (box1.min.x , box1.max.x , box1.min.y , box1.max.y );\nnodes[i+1] = **vec4** (box2.min.x , box2.max.x , box2.min.y , box2.max.y );\nnodes[i+2] = **vec4** (box1.min.z , box1.max.z , box2.min.z , box2.max.z );\nnodes[i+3] = **vec4** (child1.idx , child2.idx , 0\n, 0\n);\nIn our version, nodes are individually loaded in parts by multiple\nadjacent threads. Terefore, it is benefcial to separate individual\nbounding volumes and child indices. In case of a binary BVH, the\nfollowing layout allows coalesced memory access to particular *Vec4*\n ... \nand distributing them among threads was generally slower than\nduplica",
			  "versal.* Te traversal can be classifed as *if-if* -based [ Aila and\nLaine 2009 ]. In each iteration, a node index is fetched from the\nstack. Based on that index, either a box-intersection or a triangle-\nhit test is performed. We do not incorporate a *while-while* setup,\nsince it resulted in slower traversal speed than the straight-forward\napproac",
			  "Triangle intersection is handled similarly by each thread apply-\ning an ofset and loading one single triangle from global memory.\nConsequently, each thread tests only one bounding box or one\nsingle triangle for intersections in each iteration"
			]
		  },
		  {
			"title": "CPU-style SIMD ray traversal on GPUs",
			"url": "https://www.researchgate.net/publication/326762001_CPU-style_SIMD_ray_traversal_on_GPUs",
			"excerpts": [
			  "Lier et al. [2018] proposed a stack-based traversal algorithm for wide BVHs. The idea is to process a single node by multiple threads in a similar manner as ...Read more"
			]
		  },
		  {
			"title": "(PDF) Embree ray tracing kernels: overview and new features",
			"url": "https://www.researchgate.net/publication/305455456_Embree_ray_tracing_kernels_overview_and_new_features",
			"excerpts": [
			  "Embree is an open source ray tracing library consisting of high-performance kernels optimized for modern CPUs with increasingly wide SIMD units.Read more"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree-tutorial.pdf",
			"excerpts": [
			  " Adopt algorithms from Embree into your code.  However Embree internals change frequently!  As a library through the Embree API (recommended).Read more"
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://dl.acm.org/doi/10.1145/3105762.3105773",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH.",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://code.google.com/archive/p/understanding-the-efficiency-of-ray-traversal-on-gpus",
			"excerpts": [
			  "Timo Aila and Samuli Laine, Proc. High-Performance Graphics 2009 http://www.tml.tkk.fi/~timo/publications/aila2009hpg_paper.pdf. In addition to the original ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://github.com/matt77hias/GPURayTraversal",
			"excerpts": [
			  "The results for these GPUs have been published in the following technical report: \"Understanding the Efficiency of Ray Traversal on GPUs - ...Read more"
			]
		  },
		  {
			"title": "Fused Collapsing for Wide BVH Construction",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/cgf.70213?af=R",
			"excerpts": [
			  "We propose a novel approach for constructing wide bounding volume hierarchies on the GPU by integrating a simple bottom-up collapsing ..."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://scispace.com/papers/understanding-the-efficiency-of-ray-traversal-on-gpus-kepler-15z9c6llb5",
			"excerpts": [
			  "This technical report is an addendum to the HPG2009 paper \"Understanding the Efficiency of Ray Traversal on GPUs\", and provides citable performance results ..."
			]
		  },
		  {
			"title": "Compressed Bounding Volume Hierarchies for Efficient ...",
			"url": "https://diglib.eg.org/bitstream/handle/10.2312/vmv20181258/097-102.pdf",
			"excerpts": [
			  "We then build a BVH for these segments with axis-aligned or oriented bounding boxes, avoiding the memory-heavier oriented bounding boxes in the deep levels of ...Read more"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2017/Papers-Session2/HPG2017_AcceleratedSingleRayTracing.pdf",
			"excerpts": [
			  "Encode fixed traversal orders into nodes",
			  "One order per ray signs combination (8)",
			  "Order defined by permute vector",
			  "Map node ordering to a single permute vector operation",
			  "Map stack operation to a single vector compress operation",
			  "Initially nodes are in memory order",
			  "Permute nodes (A)",
			  "Intersection test (B)",
			  "Compress (C)",
			  "Store to stack (D)",
			  "WiVe Node Order",
			  "Binary treelet with split axis labels (a)",
			  "Leaves form BVH8 node cluster",
			  "Split hierarchy determines permute vectors",
			  "BVH8 node cluster in spatial domain (b)",
			  "Ray with +x and -y signs (octant is 10b = 2)",
			  "If signs[split axis] negative: Swap default order",
			  "A)",
			  "x",
			  "x",
			  "x",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "B)",
			  "Permute_vector[2] =",
			  "WiVe Configurations (BVH8-half)",
			  "BVH branching-factor equals half the vector width",
			  "E.g BVH8 with 16-wide vector instructions",
			  "t min and t max values interleaved in register",
			  "Child offset and t min interleaved on stack",
			  "WiVe Configurations (BVH8-full)",
			  "BVH branching-factor equals full vector width",
			  "E.g. BVH8 with 8-wide vector instructions",
			  "t min and t max values in separate registers",
			  "Child offset and t min on separate stacks",
			  "4b",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "a",
			  "b",
			  "c",
			  "d",
			  "\n4b\nn\nn\nn\nn\nt $%&\nt $%&\nt $%&\nt $%&\nt $()\n*\nt $()\n'\nt $()\n+\nt $%&\n*\nt $%&\n'\nt $%&\n+\n**EVALUATION**\nWiVe\n",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2015-Siggraph-Embree-talk.pdf",
			"excerpts": [
			  "Initial AVX512 support\n 16 wide AVX512 traversal kernels\n Full AVX512 optimizations will come when\nhardware available!",
			  "\nEmbree API (C++ and ISPC)\nRay Tracing Kernel Selection\nAccel. structure\nbvh4.triangle4,\nbvh8.triangle8,\nbvh4aos.triangle1,\nbvh4.grid\n\n"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://web.cs.ucdavis.edu/~hamann/FuetterlingLojewskiPfreundtHamannEbertHPG2017PaperFinal06222017.pdf",
			"excerpts": [
			  "The sweet spot for\nthe branching factor appears to be between two and eight, depend-\ning on the hardware architecture and the implementation used. The\ntwo approaches have also been combined for hybrid traversal meth-\nods. For real world applications the most relevant approach by far is\nsingle ray traversal of multi-branching BVHs due to its straightfor-\nward integration into complex shading pipelines.",
			  ".\nOur contribution is a novel single ray traversal algorithm which\nmaps all relevant traversal operations to vector instructions, includ-\ning front-to-back ordering for multi-branching BVHs with branch-\ning factors of 8 (BVH8) or higher. The bene  t is signi  cantly reduced\nalgorithm complexity and constant-time execution, which is ideal\nfor current and future wide vector micro architectures. In ",
			  "We have evaluated our ray traversal algorithm by generating per-\nformance data based on our AVX2 and AVX-512 implementations\non the dual-socket Intel  Xeon  E5 2680v3 @ 2.5GHz (HW) and\nthe Intel  Xeon Phi  7250 @ 1.4GHz (KNL), respectively. We com-\npare our results with those obtained with Embree 2.15.0 [Wald\net al . 2014], the leading high-performance ray tracing library for\nCPUs. In order to ensure comparability of performance data, we\nintegrated our code into the open source Embree benchmark suite\nProtoray [Intel Corporation 2017a], which by default o  ers Embree\nand Nvidia  OptiX  [Parker et al . 2010] kernels. A compariso",
			  "Embree constructs a native\nBVH8 using SAH-based centroid binning[Wald 2007], which we\ndirectly convert to our own data layout retaining the exact same\ntopology. W",
			  "We have disabled spatial splits [Stich et al . 2009] to en-\n ...",
			  ". The result is a simple and highly e  cient technique for\nmulti-branching BVH coherent traversal.",
			  "WiVe promises to accelerate\nsingle ray traversal for multi-branching bounding volume hierar-"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels: Overview and New Features",
			"url": "https://pdfs.semanticscholar.org/4f68/0f47d5b8a052fb6fa698508a42320a7f1cc4.pdf",
			"excerpts": [
			  "Embree features",
			  "Embree API (C++ and ISPC)",
			  "Ray Tracing Kernel Selection",
			  "Acceleration",
			  "structures",
			  "bvh4.triangle4",
			  "bvh8.triangle4",
			  "bvh4.quad4v",
			  "",
			  "builders",
			  "SAH Builder",
			  "Spatial Split Builder",
			  "Morton Builder",
			  "BVH Refitter",
			  "intersection",
			  "Mller-Trumbore",
			  "Single rays, ray packets (4, 8, 16), ray streams (N)",
			  "traversal",
			  "traversal",
			  "Single ray",
			  "Single ray",
			  "Packet/Hybrid",
			  "Packet/Hybrid",
			  "ray stream",
			  "ray stream",
			  "Common Vector and SIMD Library",
			  "Common Vector and SIMD Library",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)"
			]
		  },
		  {
			"title": "8-Wide BVH Ray Stream Implementation",
			"url": "https://www.emergentmind.com/topics/8-wide-bvh-ray-stream-implementation",
			"excerpts": [
			  "An 8-wide BVH (Bounding Volume Hierarchy) ray stream implementation is a memory-efficient, SIMD-friendly approach to accelerating ray tracing ...Read more"
			]
		  },
		  {
			"title": "The Minimal Bounding Volume Hierarchy",
			"url": "https://graphics.tu-bs.de/upload/publications/minimal-bounding-volume-hierarchy.pdf",
			"excerpts": [
			  "In this paper we present the Minimal Bounding Volume Hierarchy (MVH) as a k-ary object partitioning scheme. Similar to BVHs a ray traverses this tree in a top- ...Read more",
			  "... two-level BVH which uses both un- compressed BVH nodes for the top levels and compressed nodes for the rest of the hierarchy. This idea of a two-level.Read more"
			]
		  },
		  {
			"title": "Ray Classification for Accelerated BVH Traversal",
			"url": "https://meistdan.github.io/publications/shafts/paper.pdf",
			"excerpts": [
			  "The green nodes are entry points of our traversal algorithm. The yellow nodes are visited by neither traversal method and only denote the path to candidate list.Read more"
			]
		  },
		  {
			"title": "Using Embree generated BVH trees for GPU raytracing  Interplay of Light",
			"url": "https://interplayoflight.wordpress.com/2020/07/21/using-embree-generated-bvh-trees-for-gpu-raytracing/",
			"excerpts": [
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for SIMD acceleration. Such trees are harder to use during a GPU-based traversal.",
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for ...Read more"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray Tracing of Incoherent Rays",
			"url": "https://www.researchgate.net/publication/220506625_Shallow_Bounding_Volume_Hierarchies_for_Fast_SIMD_Ray_Tracing_of_Incoherent_Rays",
			"excerpts": [
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006; Dammertz et al. 2008; Ernst and Greiner 2008]. Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "A 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008]. Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008 ] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ..",
			  "MBVH has a more compact memory layout than BVH, as noted by Dammertz et al. [Dammertz et al. 2008 ] and illustrated in Figure 1. Consequently, it improves the hit rate of caches and reduces external memory traffic. ...",
			  "Storing nearby nodes grouped into small subtrees closer in memory increases the L1 or L2 cache hit rates, thus reducing bandwidth required to render the scene [Aila and Karras 2010]. ...",
			  "In this paper we describe and evaluate an implementation of CPU-style SIMD ray traversal on the GPU. We show how spreading moderately wide BVHs (up to a branching factor of eight) across multiple threads in a warp can improve performance while not requiring expensive pre-processing. The presented ray-traversal method exhibits improved traversal performance especially for increasingly incoherent rays."
			]
		  },
		  {
			"title": "Multi Bounding Volume Hierarchies",
			"url": "https://www.researchgate.net/publication/4375554_Multi_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ...",
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006;Dammertz et al. 2008; Ernst and Greiner 2008] . Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "In case of a leaf, the active child mask is set to zero, the five-byte index points to a list of triangles, and the number of triangles is saved in the look-up byte. This somewhat flexible structure allows a saving of about 5% of memory compared to the standard 128-byte layout [Dammertz et al. 2008; Ernst and Greiner 2008] and also performs marginally faster during traversal. ...",
			  " 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008] . Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "Quad-BVH structures in which child nodes are stored together in memory, enable efficient single-ray traversal on architectures with a vector width of 4 [Ernst and Greiner 2008; Dammertz et al. 2008]. At each traversal step, the ray is tested for intersection against the 4 child nodes in parallel. ..."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de- compression. Our breadth-first BVH traversal ...Read more",
			  "om-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs.",
			  " stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections",
			  " utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests ",
			  "ur breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  " novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections. We utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de-*\n*compress",
			  "A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack",
			  "Ray sorting is used to store spatially coherent rays in\nconsecutive memory locations. Compared to unsorted rays,\nthe tracing routine for sorted rays has less divergence on a\nwide SIMD machine such as GPU.",
			  "We explicitly maintain ray coherence in our pipeline\nby using this procedure.",
			  "his algorithm amortizes the cost of node access\npattern among the rays. A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  "The binary BVH is\nconstructed on the CPU and 2/3 rds of tree levels are\neliminated and an Octo-BVH is created (all the nodes are\nstored in a breadth-first storage layout).",
			  "he traversal pro-\ncedure we perform intersection tests for each frustum cor-\nresponding to *Qin.FrustumIDs* *i* with all the children of the\nBVH-node specified by *Qin.NodeIDs* *i* (usi",
			  "he new ray tracing pipeline provides the possibility to\ntrace relatively big packets of rays and perform efficient\nview-independent queries using a breadth-first frustum\ntraversal. Memory access patterns for breadth-first traversal\nare coherent as we perform operations in parallel for each\nBVH level (and the BVH is stored in a breadth-first lay-\nout)",
			  "Since we store all the leaf references for all the frustums\nthe memory consumption may be considerable (and we\nalso store the rays). But this consumption may be reduced\nthrough using a screen-space tiling (send reasonably big\ntiles to render on the GPU) or even frustum depth tiling.",
			  "Abstract"
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/10.1145/2601097.2601222",
			"excerpts": [
			  "While each new generation of processors gets larger caches and more compute power, external memory bandwidth capabilities increase at a much lower pace. Additionally, processors are equipped with wide vector units that require low instruction level divergence to be efficiently utilized.",
			  "Our main contribution is an efficient algorithm for traversing large packets of rays against a bounding volume hierarchy in a way that groups coherent rays during traversal.",
			  "Ray tracing algorithms is a mature research field in computer graphics, and despite this, our new technique increases traversal performance by 36--53%, and is applicable to most ray tracers.",
			  "In order to exploit these trends for ray tracing, we present an alternative to traditional depth-first ray traversal that takes advantage of the available cache hierarchy, and provides high SIMD efficiency, while keeping memory bus traffic low.",
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing."
			]
		  },
		  {
			"title": "Dynamic Ray Stream Traversal",
			"url": "http://cseweb.ucsd.edu/~ravir/274/15/papers/drst.pdf",
			"excerpts": [
			  "Divergence leads to underutilization since SIMD lanes will\nneed to be masked out. This is not a trait generally attributed to\ndepth-first traversal. Even when packets of rays are traced in a\nSIMD fashion, rays usually diverge quickly when incoherent, such\nas for diffuse interreflections, for example.",
			  "r algorithm is designed to\nhave a predictable memory access pattern with high data coherence,\nwhich substantially reduces the amount of memory bandwidth us-\nage in our tests.",
			  "Despite this, our results show that total ray tracing performance can\nbe improved by 2237%, while traversal alone is increased by 36\n53%, which is rather remarkable.",
			  "ck-less ray traver-\nsal [Hughes and Lim 2009; Laine 2010; Hapala et al. 2011; Bar-\nringer and Akenine-Moller 2013] is a more recent endeavor that\nwas, at least initially, motivated by the high overhead of main-\ntaining a traversal stack on previous generations of G",
			  "ually, a stack is maintained that contains the next node to be pro-\ncessed during ray traversal. The technique has been combined with\nvarious forms of ray sorting and tracing of whole packets [Wald\net al. 2001] of rays to improve performance.",
			  " rays in the ray stream take the same path in the tree, which decreases\ndivergence and increases SIMD utilization.",
			  ".\nEmbree also includes a set of packet traversal kernels [Wald et al.\n2001], as well as hybrid kernels that starts with packets and\nswitches to single-ray traversal as utilization becomes low [Ben-\nthin et al. 2012]. ",
			  "To use these kernels, it is the responsibility of the\nrenderer to manage ray packets.",
			  "s. The example renderer that supports\nthese kernels constitutes a total rewrite of the single-ray renderer us-\ning *ISPC* [Pharr and Mark 2012], which makes the entire renderer\nvectorized.",
			  "which makes the entire renderer\nvectorized. This makes it a bit difficult to compare performance di-\nrectly with our algorithm and single-ray traversal.",
			  "However, we have chosen to keep the scalar shaders"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving | Research",
			"url": "https://research.nvidia.com/publication/2022-01_gpu-subwarp-interleaving",
			"excerpts": [
			  "In this paper, we present an architectural enhancement called Subwarp Interleaving that exploits thread divergence to hide pipeline stalls in divergent ...Read more"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving - Cloudfront.net",
			"url": "https://d1qx31qr3h6wln.cloudfront.net/publications/Damani_Subwarp_Interleaving_HPCA_IT_2022.pdf",
			"excerpts": [
			  "Subwarp Interleaving allows for fine-grained interleaved execution of diverged paths within a warp with the goal of increasing hardware utilization and reducing ...Read more"
			]
		  },
		  {
			"title": "Dynamic Stackless Binary Tree Traversal",
			"url": "https://jcgt.org/published/0002/01/03/paper.pdf",
			"excerpts": [
			  "We evaluate our algorithm using a ray tracer with a bounding volume hierarchy for which source code is supplied. 1. Introduction. Traversing a ..."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Section Title: Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware > Abstract\nContent:\nRecent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  " key insight is that typical control flow inten-\nsive CUDA applications exhibit sufficient control flow lo-\ncality within the group of scalar threads used for bulk-\nsynchronous parallel execution that full DWF and/or MIMD\nflexibility is not necessary to regain most of the perfor-\nmance loss due to branch diverge",
			  "he compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  " The\nprevious best (on average) policy known, majority, incurs\npoor performance when a small number of threads falls\nbehind the majority of threads",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o",
			  "Dynamic warp formation,\n",
			  " dynamic warp\nformation and scheduling mechanism.",
			  "rn *graphics processing unit* (GPU) can be\nviewed as an example of the latter approach [18, 28, 6].\nEarlier generations of GPUs consisted of fi xed function 3D\nrendering pipelines.\nThis required new hardware to en-\nable new real-time rendering techniques, which impeded\nthe adoption of new graphics algorithms and thus motivated\nthe introduction of programmability, long available in tradi-\ntional of fl ine computer animation [35], into GPU hardwar",
			  "One approach is to add a stack*\n*to allow different SIMD processing elements to execute dis-*\n*tinct program paths after a branch instruc",
			  "Dynamic warp formation improves performance by cre-\nating new thread warps out of diverged warps as the shader\nprogram execute",
			  "Every cycle, the thread scheduler tries to\nform new warps from a pool of ready threads by combining\nscalar threads whose next PC values are the same.",
			  "We\nexplore the design space of this scheduling policy in detail\nin Section 4.3.",
			  "e thread scheduler policy is\ncritical to the performance impact of dynamic warp forma-\ntion in Section 6."
			]
		  },
		  {
			"title": "Thread block compaction for efficient SIMT control flow",
			"url": "https://ieeexplore.ieee.org/document/5749714/",
			"excerpts": [
			  "Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data cores to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22% over a baseline per-warp, stack-based reconvergence mechanism, and 17% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "A survey of architectural approaches for improving GPGPU ...",
			"url": "https://mkhairy.github.io/Docs/jpdc-survey.pdf",
			"excerpts": [
			  "Fung and Aamodt [46] proposed thread block compaction (TBC) that allows a group of warps, that belong to the same thread block, to share the same PDOM stack.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://www.researchgate.net/publication/392717030_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing | Request PDF",
			"url": "https://www.researchgate.net/publication/341162869_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[8] S. Damani, M. Stephenson, R. Rangan, D. Johnson, R. Kulkami, and S. W. Keckler, Gpu subwarp interleaving, in *International Symposium on High-Performance Computer Architecture (HPCA)* , 2022.",
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011",
			  " [14] W. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, Dynamic warp formation and scheduling for efficient gpu control flow, in *International Symposium on Microarchitecture (MICRO)* , 2007. "
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Evaluated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path",
			  "SIMT execution.",
			  "**Acknowledgments**",
			  "The authors would like to thank Wilson Fung, Tayler",
			  "Hetherington, Ali Bakhoda, Timothy G. Rogers, Ayub",
			  "Gubran, Hadi Jooybar and the reviewers for their insightful",
			  "feedback. This research was funded in part by a Four Year",
			  "Doctoral Fellowship from University of British Columbia,",
			  "the Natural Sciences and Engineering Research Council of",
			  "Canada and a grant from Advanced Micro Devices Inc.",
			  "[9] W. Fung, I. Sham, G. Yuan, and T. Aamodt. Dynamic Warp",
			  "Formation and Scheduling for Efficient GPU Control Flow.",
			  "[10] W. W. L. Fung and T. M. Aamodt.",
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages",
			  "2536, 2011.",
			  "[6] S. Collange. Stack-less SIMT Reconvergence at Low Cost.",
			  "hnical Report hal-00622654, ARENAIRE - Inria Greno-\nble Rhne-Alpes / LIP Laboratoire de lInformatique du Par-\nalllisme, 2011",
			  ". Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "W. Fung, I. Sham, G. Yuan, and T. Aamodt.",
			  "Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "407420, 2007.",
			  "407420, 2007.",
			  " Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "valuated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path\nSIMT execution",
			  "**References**"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://arxiv.org/html/2506.11273v1",
			"excerpts": [
			  "Garanzha and Loop (2010) Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First Packet Traversal for GPU Ray Tracing. *Computer Graphics Forum* 29, 2 (2010), 28929",
			  "/2506.11273v1.bib12) ) used breadth-first packet traversal after a ray sorting step. They proposed the idea of sorting rays to reduce divergence in computation using a hash-based method for sorting the rays into coherent packets. T",
			  "In addition, the rays are grouped into frusta, which are further tested against the scene as proposed by Reshetov\net al . ( [200",
			  "This way, the total number of intersection tests is reduced.",
			  "While they report impressive speedups for primary rays and deterministic ray tracing, this does not translate to path tracing because the frusta become too large and intersect most of the scene.",
			  "For production rendering, not only the trace kernel but also shading might be limited by memory bandwidth. Therefore, Eisenacher et al . ( [2013](https://arxiv.org/html/2506.11273v1.bib11) ) proposed to sort termination points to improve shading performance. While this approach is designed for out-of-core path tracing, grouping shading calculations by a material also improves in-core performance for complex shaders. For highly detailed scenes, Hanika\net al . ( [2010](https://arxiv.org/html/2506.11273v1.bib15) ) proposed to use a two-level hierarchy combined with ray sorting to facilitate efficient on the fly micro-polygon tessellation. The rays are traversed through the top-level hierarchy, and they are repeatedly sorted to determine sets of rays traversing the same leaf nodes of the top-level hierarchy.",
			  "When coherence among rays exists, the packet traversal (Gunther\net al . , [2007](https://arxiv.org/html/2506.11273v1.bib14) ) exploits it by forcing a SIMD processing of a group of rays. This, on the other hand, increases inter-thread communication and synchronization. Furthermore, it assumes high ray coherence and is significantly slower than depth-first traversal for incoherent rays. Bikker ( [2012](https://arxiv.org/html/2506.11273v1.bib7) ) proposed a packet traversal algorithm that uses batching to improve data locality.",
			  " \nSection Title: On Ray Reordering Techniques for Faster GPU Ray Tracing > References\nContent:\nDynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In *Proceedings of IEEE Symposium on Interactive Ray Tracing* . 95104. Nimier-David et al . (2019) Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2: A Retargetable Forward and Inverse Renderer. *ACM Transactions on Graphics* 38, 6 (2019), 203. Pharr et al . (1997) Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. 1997. Rendering Complex Scenes with Memory-coherent Ray Tracing. In *Proceedings of International Conference on Computer Graphics and Interactive Techniques* . 101108. Reis et al . (2017) Nuno T. Reis, Vasco S. Costa, and Joo M. Pereira. 2017. Coherent Ray-Space Hierarchy via Ray Hashing and Sorting. In *Proceedings of International Joint Conference on Computer Vision, Imaging, and Computer Graphics Theory and Applications* . Resh",
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal."
			]
		  },
		  {
			"title": "[PDF] Two-level ray tracing with reordering for highly complex scenes | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Two-level-ray-tracing-with-reordering-for-highly-Hanika-Keller/ee5673f3141a61924798d7642f06971dd41d871c",
			"excerpts": [
			  "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision Surfaces",
			  "Nikolaus Binder](/author/Nikolaus-Binder/2231395) [A. Keller](/author/A.-Keller/145661463)",
			  "* 2018",
			  "Besides introducing an optimized method to determine axis aligned bounding boxes of Gregory patches restricted in the parametric domain, several techniques are introduced that accelerate the recursive subdivision process including stackless operation, efficient work distribution, and control flow optimizations.",
			  "TLDR"
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Proceedings of High Performance Graphics",
			"url": "https://dl.acm.org/doi/10.5555/2977336.2977343",
			"excerpts": [
			  "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time ... Stackless multi-BVH traversal for CPU, MIC and GPU ray tracing."
			]
		  },
		  {
			"title": "Extending GPU Ray-Tracing Units for Hierarchical Search ...",
			"url": "https://engineering.purdue.edu/tgrogers/publication/barnes-micro-2024/barnes-micro-2024.pdf",
			"excerpts": [
			  "Binder and A. Keller, Efficient Stackless Hierarchy Traversal on. GPUs with Backtracking in Constant Time, in Proceedings of High. Performance Graphics (HPG).Read more"
			]
		  },
		  {
			"title": "A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees",
			"url": "https://jcgt.org/published/0014/01/03/paper.pdf",
			"excerpts": [
			  "BINDER, N. AND KELLER, A. Efficient stackless hierarchy traversal on GPUs with back- tracking in constant time. In Proceedings of High ...Read more"
			]
		  },
		  {
			"title": "[PDF] Thread block compaction for efficient SIMT control flow",
			"url": "https://www.semanticscholar.org/paper/Thread-block-compaction-for-efficient-SIMT-control-Fung-Aamodt/8bd6f67ef03b3c138c52f3e9b1716aebe937d244",
			"excerpts": [
			  "This paper proposes and evaluates the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, ..."
			]
		  },
		  {
			"title": "HIPRT: A Ray Tracing Framework in HIP",
			"url": "https://gpuopen.com/download/HIPRT-paper.pdf",
			"excerpts": [
			  "Binder and Keller [2016] proposed backtracking in constant time based on a path to the node in a bitset and perfect hashing. 3 HIPRT API OVERVIEW. Similarly to ...Read more",
			  "Later, Ylitie et al. [2017] showed that a compressed. 8-wide BVH can reduce memory traffic even further, improving the overall ray tracing performance.Read more"
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "http://www.danielwong.org/classes/_media/ee260_w17/threadblockcompaction.pdf",
			"excerpts": [
			  ", we propose and*\n*evaluate the benefits of extending the sharing of resources*\n*in a block of warps, already used for scratchpad mem-*\n*ory, to exploit control flow locality among threads (where*\n*such sharing may at first seem detrimental). In our pro-*\n*posal, warps within a thread block share a common block-*\n*wide stack for divergence handling. At a divergent branch,*\n*threads are compacted into new warps in har",
			  "he modifications consist of three major parts: a modi-\nfied branch unit ( 1 ), a new hardware unit called the thread\ncompactor ( 2 ), and a modified instruction buffer called the\nwarp buffer ( 3 ). The branch unit ( 1 ) has a block-wide re-\nconvergence stack for each block. Each entry in the stack\nconsists of the starting PC (PC) of the basic block that cor-\nresponds to the entry, the reconvergence PC (RPC) that in-\ndicates when this entry will be popped from the stack, a\nwarp counter (WCnt) that stores the number of compacted\nwarps this entry contains, and a block-wide activemask that\nrecords which thread is executing the curre",
			  " comparison to DWF [9], thread block compaction ac-\ncomplishes the lookup-and-merge operation of the warp\nLUT and the warp pool [9] with simpler hardware. In\nDWF, an incoming warp is broken down every cycle and\n .."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow | Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2007.12",
			"excerpts": [
			  "We show that a realistic hardware im- plementation that dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes improves performance by an average of 20.7% for an estimated area increase of 4.7%."
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Research",
			"url": "https://research.nvidia.com/publication/2016-06_efficient-stackless-hierarchy-traversal-gpus-backtracking-constant-time",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling and use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or state memory. We show that the next node in such a traversal actually can be determined in constant time and state memory. In fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and outperforms the fastest stack-based algorithms on GPUs. In addition, it reduces memory access during traversal, making it a very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal on GPUs with ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/1c026ceb-0c54-4e20-9fd2-2ff77222894d/content",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling",
			  "nd use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or*\n*state memory",
			  "We show that the next node in such a traversal actually can be determined in constant time and state memory.",
			  "*fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and*\n*outperforms the fastest stack-based algorithms on GPUs.",
			  " addition, it reduces memory access during traversal, making it a*\n*very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal with Backtracking in ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2016/2016-HPG-Binder-StacklessTraversalPerfectHash.pdf",
			"excerpts": [
			  "Efficient Stackless Hierarchy Traversal with Backtracking in Constant Time",
			  "Results: Performance in M rays/s, NVIDIA Titan X, for Primary/Shadow/Diffuse Rays"
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Software Developer's Manual ...",
			"url": "https://kib.kiev.ua/x86docs/Intel/SDMs/326018-062.pdf",
			"excerpts": [
			  "Page 1. Intel 64 and IA-32 Architectures. Software Developer's Manual ... _mm512_i64gather_epi64( __m512i vdx, void * base ... Intel C/C++ Compiler Intrinsic ..."
			]
		  },
		  {
			"title": "\n\tGather of byte/word with avx2 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Gather-of-byte-word-with-avx2/td-p/921687",
			"excerpts": [
			  "1) Use _mm256_i32gather_epi32. We would fetch an extra 16-bits that we do not want for each voxel, and then either mask off the extra bits or ...Read more"
			]
		  },
		  {
			"title": "_mm_i32gather_epi32, _mm256_i32gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-1F275401-A760-49B1-944A-B02C075514D8.htm",
			"excerpts": [
			  "Gathers 2/4 doubleword values from memory referenced by the given base address, dword indices, and scale. The corresponding Intel AVX2 instruction is"
			]
		  },
		  {
			"title": "_mm_i64gather_epi32, _mm256_i64gather_epi32",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/compiler_c/main_cls/intref_cls/common/intref_avx2_mm256_i64gather_epi32.htm",
			"excerpts": [
			  "Gather 2/4 doubleword values from memory referenced by the given base address, qword indices and scale. The corresponding Intel AVX2 instruction is VPGATHERQD."
			]
		  },
		  {
			"title": "An introduction to Arm Scalable Vector Extensions",
			"url": "https://epicure-hpc.eu/wp-content/uploads/2025/03/SVE_Vectorization_Ricardo_Fonseca.pdf",
			"excerpts": [
			  "SVE gather / scatter operations.  Gather operations.  Gather scalar values ...  https://developer.arm.com/documentation/101458/2404/Optimize.  Arm ...Read more"
			]
		  },
		  {
			"title": "Mirror of Intel Intrinsics Guide",
			"url": "https://www.laruence.com/sse/",
			"excerpts": [
			  "Intel. Intrinsics Guide. Technologies. MMX. SSE. SSE2. SSE3. SSSE3. SSE4.1. SSE4.2 ... This intrinsic is provided for conversion between little and big endian ...Read more"
			]
		  },
		  {
			"title": "Intrinsics for Integer Gather and Scatter Operations",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-35D298CC-B89B-4B38-856B-FCD0EBB3AA23.htm",
			"excerpts": [
			  "**_mm512_i32gather_epi32**\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i32gather_epi32**\nSection Title: Intrinsics for Integer Gather and Scatter Operations\nContent:\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i32scatter_epi32**\nScatters int32 from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ).",
			  "**_mm512_mask_i32scatter_epi64**\nScatters int64 from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ) subject to mask k (elements are not stored when the corresponding mask bit is not set).",
			  "**_mm512_i64scatter_epi64**\nScatters int64 from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale )."
			]
		  },
		  {
			"title": "Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel2021.7/HPC/cpp_compiler/cpp_compiler_classic_dev_guide/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.html",
			"excerpts": [
			  "\nThe prototypes for Intel Advanced Vector Extensions 512 (Intel AVX-512) intrinsics are located in the zmmintrin.h header file.\nTo u",
			  "To use these intrinsics, include the immintrin.h file as follows:",
			  "ement.\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Data Types for Intel AVX-512 Intrinsics\nC",
			  "s:\nIntel AVX-512 intrinsics have vector variants that use __m128 , __m128i , __m128d , __m256 , __m256i , __m256d , __m512 , __m512i , and __m512d data types.",
			  ".\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Naming and Usage Syntax\nCon"
			]
		  },
		  {
			"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
			"excerpts": [
			  "**AVX2 has gathers (not scatters), but they're only fast on Skylake and newer** . They're ok on Broadwell, slowish on Haswell, and slow on AMD. (Like one per 12 clocks for Ryzen's `vpgatherqq` ). See http://agner.org/optimize/ and other performance links in [the x86 tag wiki](https://stackoverflow.com/tags/x86/info) .\nIntel's optimization manual has a small section on manual gather / scatter (using insert/extract or `movhps` ) vs. hardware instructions, possibly worth reading. In this case where the indices are runtime variables (not a constant stride or something), I think Skylake can benefit from AVX2 gather instructions here",
			  "**extract indices, manually gather into a vector with `vmovq` / `vmovhps` for a SIMD `vpor` , then scatter back with `vmovq` / `vmovhps`** .Just like a HW gather/scatter, **correctness requires that all indices are unique** , so you'll want to use one of the above options until you get to that point in your algo. (vector conflict detection + fallback would not be worth the cost vs. just always extracting to scalar: [Fallback implementation for conflict detection in AVX2](https://stackoverflow.com/questions/44843518/fallback-implementation-for-conflict-detection-in-avx2) ).See [selectively xor-ing elements of a list with AVX2 instructions](https://stackoverflow.com/questions/50583718/selectively-xor-ing-elements-of-a-list-with-avx2-instructions) for an intrinsics version.",
			  "**AVX2 `vpgatherqq` for the gather ( `_mm256_i64gather_epi64(sieveX, srli_result, 8)` ), then extract indices and manually scatter.** So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather. (Two 128-bit gathers cost more than one 256-bit gather, so you would want to take the instruction-level parallelism hit and gather into a single 256-bit register).",
			  "Possibly a win on Skylake (where `vpgatherqq ymm` is 4 uops / 4c throughput, plus 1 uop of setup), but not even Broadwell (9 uops, one per 6c throughput) and definitely not Haswell (22 uops / 9c throughput). You do need the indices in scalar registers anyway, so you're *only* saving the manual-gather part of the work. That's pretty cheap.",
			  "**manual gather/scatter: 20 uops, 5 cycles of front-end throughput** (Haswell / BDW / Skylake). Also good on Ryzen.",
			  "**Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.** (Worse on any earlier uarch or on AMD).",
			  "\nvextracti128 indices (1 uop for port 5)",
			  "2x vmovq extract (2p0)",
			  "2x vpextrq (4 = 2p0 2p5)",
			  "`vpcmpeqd ymm0,ymm0,ymm0` create an all-ones mask for `vpgatherqq` (p015)",
			  "`vpgatherqq ymm1, [rdi + ymm2*8], ymm0` 4 uops for some ports.",
			  "`vpor ymm` (p015)",
			  "vextracti128 on the OR result (p5)\n",
			  "2x vmovq store (2x 1 micro-fused uop, 2p23 + 2p4). Note no port7, we're using indexed stores.",
			  "2x vmovhps store (2x 1 micro-fused uop, 2p23 + 2p4)."
			]
		  },
		  {
			"title": "c++ - AVX2 Gather Instruction Usage Details - Stack Overflow",
			"url": "https://stackoverflow.com/questions/58832024/avx2-gather-instruction-usage-details",
			"excerpts": [
			  "The offsets in `vindex` are in bytes. Therefore, you gather 32-bit integer values from addresses `{arr, arr+2, arr+4, ...}` .",
			  "Either change these indexes from `{0,2,4...}` to `{0,8,16,...}` , or update the scale factor as:\nThis prints out the expected values."
			]
		  },
		  {
			"title": "Arm C Language Extensions",
			"url": "https://arm-software.github.io/acle/main/acle.html",
			"excerpts": [
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B)",
			  "\nThe SVE ACLE intrinsics have the form:\nwhere the individual parts are as follows:\n**base**\nFor most intrinsics this is the lower-case name of an SVE\ninstruction, but with some adjustments:The most common change is to drop `F` , `S` and `U` if they\nstand for floating-point, signed and unsigned respectively,\nin cases where this would duplicate information in the type\nsuffixes below.Simple non-extending loads and non-truncating stores drop the\nsize suffix ( `B` , `H` , `W` or `D` ), which would always duplicate\ninformation in the suffixes.Conversely, extending loads always specify an explicit extension\ntype, since this information is not available in the suffixes.\nA sign-extending load has the same base as the architectural\ninstruction (for instance, `ld1sb` ) while a zero-extending load replaces\nthe `s` with a `u` (for instance, `ld1ub` for a zero-extending `LD1B` ).\nThus [`svld1ub_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1ub_u32) zero-extends 8-bit data to a vector of `uint32_t` s while [`svld1sb_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1sb_u32) sign-extends 8-bit data to a vector of `uint32_t` s.",
			  "| LD1D (vector plus immediate) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather%5B) |",
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B) ",
			  "| ST1D (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) |",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |"
			]
		  },
		  {
			"title": "Arm Scalable Vector Extension and application to Machine ...",
			"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Arm-scalable-vector-extensions-and-application-to-machine-learning.pdf",
			"excerpts": [
			  "the function svld1_gather_u32base_offset_s32 , with signature\nsvint32_t svld1_gather_u32base_offset_s32(svbool_t pg, svuint32_t bases, int64_t\noffset)\nis a *gather load* ( ld1_gather ) of *signed 32-bit integer* ( _s32 ) from a vector of *unsigned 32-bit integer* base\naddresses ( _u32base ) plus an *offset in bytes* ( _offset ).",
			  "The SVE ACLE are compatible with C++ overloading and C _Generic association, so that the names\ncan be contracted removing those parts that can be derived from the arguments types.",
			  "The SVE ACLE (or ACLE hereafter) is a set of functions and types that exposes the vectorization capabilities of\nSVE at C/C++ level.",
			  "They introduce a set of *size-less* types and *intrinsic functions* that a C/C++ compiler can directly convert into\nSVE assembly.",
			  "An additional svbool_t type is defined to represent predicates for masking operations.",
			  "SVE intrinsic functions",
			  "The naming convention of the intrinsic functions in the SVE ACLE is described in detail in section 4 of the SVE\nACLE document (ARM limited 2017b).",
			  "Most of them are in the form: svbase[_disambiguator][_type0][_type1]...[_predication] .",
			  "For example, the name of the intrinsic svadd_n_u16_m , with signature svuint16_t svadd_n_u16_m(svbool_t\npg, svuint16_t op1, uint16_t op1) , describes a vector *addition* ( add ) of *unsigned 16-bit integer* ( u16 ),\nwhere one of the arguments is a scalar ( _n ) and the predication mode is *merging* ( _m ).",
			  "Some of the functions, like loads and stores, have a different form for the names, with additional tokens that\nspecify the addressing mode.",
			  "All the examples of this document use the short form. For simplicity, we also assume no aliasing, meaning that"
			]
		  },
		  {
			"title": "Arm C Language Extensions for SVE - Version 00bet6",
			"url": "https://rci.stonybrook.edu/sites/default/files/documents/acle_sve_100987_0000_06_en.pdf",
			"excerpts": [
			  "COMPACT: Compact vector and fill with zero",
			  "These functions concatenate the active elements of the input vector, filling any remaining elements with\nzero.",
			  "6.23. Predicate creation",
			  "6.23.1. PTRUE: Return an all-true predicate for a given pattern",
			  "These functions return an all-true predicate for a particular vector pattern and element size. When an\nelement has more than one predicate bit associated with it, only the lowest of those bits is ever true.",
			  "There are two forms: one with a _pat suffix that takes an explicit vector pattern and one without a _pat\nsuffix in which the pattern is implicitly SV_ALL .",
			  "svbool_t **svptrue_b8** ()",
			  "svbool_t **svptrue_b16** ()",
			  "svbool_t **svptrue_b32** ()",
			  "svbool_t **svptrue_b64** ()"
			]
		  },
		  {
			"title": "ARM's Scalable Vector Extensions: A Critical Look at SVE2 ...",
			"url": "https://gist.github.com/zingaburga/805669eb891c820bd220418ee3f0d6bd",
			"excerpts": [
			  "Under ACLE, NEON  SVE value transfer must go through memory. Interestingly ... SVE adds support for gather/scatter operations, which helps vectorize ..."
			]
		  },
		  {
			"title": "_mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-8B147603-6A6A-4F23-8529-1609A13AB784.htm",
			"excerpts": [
			  "extern __m512i __cdecl _mm512_i32gather_epi32(_m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "tent:\n| extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "The non-masked variant of the intrinsic is equivalent to the masked variant with full mask ( k1 =0xffff).",
			  ":\nGather int32 vector with int32 indices. Corresponding instruction is VPGATHERDD . This intrinsic only applies to Intel Many\nIntegrated Core Architecture (Intel MIC Architecture).\n",
			  "Up-converts a set of 16 memory locations pointed by base address mv and int32 index vector index with scale scale , and gathers them into a int32 vector.",
			  "The resulting vector for the masked variant is populated by elements for which the corresponding bit in the writemask vector k1 is set. The remaining elements of the resulting vector for the masked variant is populated by corresponding elements from v1_old .",
			  "Returns the result of the up-convert load operation."
			]
		  },
		  {
			"title": "x86 Intrinsics Cheat Sheet",
			"url": "https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf",
			"excerpts": [
			  "AVX2\n**mi** **i64gather_epi32**\n( **i*** ptr, **mi** a, **i** s)\n**FOR** j := 0 to 1;\ni := j*32\nm := j*64\n**dst** [i+31:i] :=\n*(ptr + a[m+63:m]*s])\n**dst** [MAX:64] := 0\nMask Gather\nmask_ *i32* / *i64* gather\nepi32-64,ps/d\n**NOTE:** Same as gather but takes\nan additional mask and src\nregister. Each element is only\ngathered if the highest\ncorresponding bit in the mask is\nset. Otherwise it is copied from\nsrc. Memory does not need to\nbe aligned.",
			  "AVX2\n**mi mask_i64gather_epi32** ( **mi** src,\n**i*** ptr, **mi** a, **mi** mask, **i32** s)\n**FOR** j := 0 to 1; i:=j*32;m:=j*64\n**IF** mask[i+31]\n**dst** [i+31:i]:=*(ptr+a[i+63:i]*s)\nmask[i+31] := 0\n**ELSE**\n**dst** [i+31:i] := src[i+31:i]\nmask[MAX:64] := 0\n**dst** [MAX:64] := 0\n256bit Insert\ninsertf128\nsi256,ps/d\n**m** **insertf128_ps** ( **m** a, **m** b, **ii** i)\n**dst** [255:0] := a[255:0]\nsel := i*128\n**dst** [sel+15:sel]:=b[127:0]"
			]
		  },
		  {
			"title": "Filtering a Vector with SIMD Instructions (AVX-2 and AVX-512) | Quickwit",
			"url": "https://quickwit.io/blog/simd-range",
			"excerpts": [
			  "Let's start with `compact` .\nAVX2 does not exactly have an instruction for this. In the 128-bits world, [`PSHUFB`](https://www.felixcloutier.com/x86/pshufb.html#:~:text=PSHUFB%20performs%20in%2Dplace%20shuffles,leaving%20the%20shuffle%20mask%20unaffected.) is a powerful instruction that\nlets you apply a permutation over the bytes of your register.",
			  "The equivalent instruction exists and is called `vpshufd` , but there is a catch: it only\napplies two disjoint permutations within the two 128-bits lanes, which is perfectly useless to us.",
			  "This is a very common pattern in AVX2 instructions. Instructions crossing that dreaded 128bit-lane are seldom.",
			  "\nFortunately, applying a permutation over `u32s` (which is what we need) is actually possible,\nvia the `VPERMPS` instruction [__mm256_permutevar8x32_epi32](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-vector-extensions-2/intrinsics-for-permute-operations/mm256-permutevar8x32-epi32.html) .",
			  "Then I did what every sane engineer should do. I asked [Twitter](https://twitter.com/fulmicoton/status/1539534316405161984) (Ok I am lying a bit, at the time of the tweet I was playing with SSE2).",
			  "a single byte. The instruction is called `VMOVMSKPS` . I could not find it because it is presented as a floating point instruction to extract the sign of a bunch of 32-bits floats.\n"
			]
		  },
		  {
			"title": "On the usage of the Arm C Language Extensions for a High ...",
			"url": "https://hal.science/hal-03029933v1/document",
			"excerpts": [
			  "In our case, we\nuse the following code:\nauto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "the gath-\nering step of our kernel, we first need to gather indices of\nthe first point of each element considered.",
			  "At order 4, each\nelement is composed of 125 points.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "In our case, we\nuse the following code:",
			  "auto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "At iteration i , the first element of the vector is at position\ni*svcntw()*125 and we need to duplicate it and add the\nvstrides values to obtain the indices of the first point of\neach element in the vector:"
			]
		  },
		  {
			"title": "Introduction to SVE",
			"url": "https://documentation-service.arm.com/static/67ab35a4091bfc3e0a9478b5?token=",
			"excerpts": [
			  "The ACLE (Arm C Language Extension) for SVE defines which SVE instruction functions\nare available, their parameters and what they do.",
			  " To use the ACLE intrinsics,\nyou must include the header file arm_sve.h, which contains a list of vector types and instruction\nfunctions (for SVE) that can be used in C/C++.",
			  "The following example C code has been manually optimized with SVE intrinsics:\n//intrinsic_example.c\n\\ <arm_sve.h>\nsvuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)\n{\n// widening add of even elements\nsvuint64_t result = svaddlb(Zs1, Zs2);\nreturn result;\n}"
			]
		  },
		  {
			"title": "\n\tAgner's tables show the - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Throughput-MUL-FMA-Broadwell/m-p/1151730/highlight/true",
			"excerpts": [
			  "Agner's tables show the throughput for sequences of independent instructions, and the latency for sequences of dependent instructions."
			]
		  },
		  {
			"title": "Release Notes for Intel Intrinsics Guide",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/release-notes/intrinsics-guide-release-notes.html",
			"excerpts": [
			  "Removed extended gather/scatter intrinsics."
			]
		  },
		  {
			"title": "Surprising new feature in AMD Ryzen 3000 | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24302057",
			"excerpts": [
			  " But the scatter/gather instructions do random access memory operations. You have one SIMD register with a 8 (or whatever the width is) indexes to be applied to a base address in a scalar register, and the hardware then goes and does 8 separate memory operations on your behalf, packing the results into a SIMD register at the end.",
			  "That has to hit the cache 8 times in the general case. It's extremely expensive as a single instruction, though faster than running scalar code to do the same thing.",
			  "I looked Agner's tables, and was curious how Intel fared with it. All numbers are reciprocal throughput. So how many cycles per instruction in throughput. zen 2 has it's gather variants mostly 9 and 6 cycles and one variant with 16. Broadwell has only 6,7 and 5 cycles.",
			  "Skylake has mostly 4 and 2 and one variant with 5.",
			  "Now I was surprised by Agners figures for zen2 LOOP and CALL which both have reciprocal throughput of 2. Being equal to doing with just normal jump instructions."
			]
		  },
		  {
			"title": "4. Instruction tables",
			"url": "https://www.agner.org/optimize/instruction_tables.pdf",
			"excerpts": [
			  "VPGATHERDD\nx,[r+s*x],x\n24\n5\nP0123\nAVX2",
			  "VPGATHERDD\ny,[r+s*y],y\n42\n8",
			  "VPGATHERQQ\nx,[r+s*x],x\n18\n4",
			  "VPGATHERQQ\ny,[r+s*y],y\n24\n5",
			  "VPSCATTERDD\n[r+s*x]{k},x\n27\n6",
			  "VPSCATTERQQ\n[r+s*z]{k},z\n48\n12",
			  "VPCOMPRESSD/Q\nv{k},v\n2\n2",
			  "VPEXPANDB/W/D/Q\nx{k},x\n2\n2"
			]
		  },
		  {
			"title": "(PDF) Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://www.academia.edu/145129609/Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Section Title: Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads\nContent:\n[Karthik Sankaranarayanan](https://independent.academia.edu/KarthikSankaranarayanan)\n2020, ArXiv\nvisibility\n\ndescription See full PDF download Download PDF\nbookmark Save to Library share Share\nclose"
			]
		  },
		  {
			"title": "Prefetching for complex memory access patterns",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-923.pdf",
			"excerpts": [
			  "This thesis makes three contributions. I first contribute an automated software prefetch- ing compiler technique to insert high-performance prefetches into ...Read more"
			]
		  },
		  {
			"title": "On Reusing the Results of Pre-Executed Instructions in a ...",
			"url": "https://dl.acm.org/doi/abs/10.1109/L-CA.2005.1",
			"excerpts": [
			  "Previous research on runahead execution took it for granted as a prefetch-only technique. Even though the results of instructions independent of an L2 miss ..."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/pdf/2505.21669",
			"excerpts": [
			  "Hardware prefetcher designs have been previously created with pointer-chasing access patterns in mind. Some techniques compute and store jump ...",
			  "oth et al. describe a prefetching technique in [ **50** ] that utilizes dependency chains to\ndetermine the shape of a linked data structure node in hardware, and issue requests ac-\ncordingly. While this approach requires no modification of the executable, it cannot learn\nthe full structure of an object without observing accesses to all children. Additionally, the\nauthors limit their prefetcher to only a single node ahead, meaning prefetches are only\ntriggered when the core issues new loads, *and* blocks must be returned from the memory\nsystem before new prefetches can be issuedleading to significantly worse performance\nas effective memory access latencies incr",
			  " [ **37** ] build on dependence based prefetching (DBP) as described in [ **50** ] to en-\nable *timely* prefetches of children; this is needed as increased memory access times require\nprefetches to be issued earlier to avoid stalls."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Observing the backward slice shown in Figure 3a, we see that\nthe one cycle in the graph is comprised of a single instruction\n0x6cf , *i.e.* , the stride address increment, and that it is the only\nloop-carried dependence in this backward slice.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "We duplicate the backward slice code\nand assign new registers to it. By analogy, this code is the carrot\nand the main computation is the horse.",
			  "Prior to the entry into\nthe loop, the carrot is first extended *k* iterations ahead of the horse.\nWe call this phase in the dynamic execution the *head start* phase.",
			  "After the entry into the loop, the carrot locks steps with the horse\nand stays a constant *k* iterations ahead. We call this phase in the\ndynamic execution the *stay ahead* phase.",
			  "During the last *k* iterations\nof the loop, the carrot ceases to stay ahead and merges with the\nhorse. We call this phase of dynamic execution the *join* phase.",
			  "**4**",
			  "**M** **ETHOD**",
			  "In the previous section, we explained the problem of memory-\nbound DILs through a hash table example and outlined the\nchallenges in implementing a prefetcher with helper threads",
			  "Here,\nwe will outline our approach to a solution, with a reminder that we\nwant to create a prefetcher implementation without threads.",
			  "We exclude\nsuch scenarios by design for two reasons: first, such situations\nare rare and second, prefetcher complexity increases tremendously\nin such cases.",
			  "To see why, let us consider the example of the\nbinary tree where both the paths are equally likely. If we want to\nprefetch *k* iterations ahead, then there are 2 *k* possible addresses\nto prefetch.",
			  "We have the option of either prefetching all of those\naddresses or implementing a software-based branch predictor to\nselect one of the addresses to prefetch."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "Content:\nAugust 2020\nDOI: [10.48550/arXiv.2009.00202](https://doi.org/10.48550/arXiv.2009.00202)",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/html/2505.21669v1",
			"excerpts": [
			  "[52] Sankaranarayanan, K., Lin, C.-K., and Chinya, G. N. Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads. ArXiv (Sept. 2020).",
			  "These two benchmarks ( bintree_dfs and bintree_bfs ) perform common traversals of binary tree data structures to compute a sum.",
			  "The DFS benchmark is implemented recursively, while the BFS benchmark uses a std::queue from the C++ STL.",
			  "This tree is staticonly lookups are performed.",
			  "In [ [61](https://arxiv.org/html/2505.21669v1.bib61) ] , Wang et al. describe a prefetch engine that utilizes hints from a compiler analysis to inform issued requests.",
			  "Notably, the analysis detects common cases of recursive pointers as used in linked-list traversals, which hardware then exploits to issue prefetches of the LDS up to six levels deep.",
			  "However, layout information is not provided to the hardware prefetch engineit instead speculates on values in a cache block being pointers, falling victim to the same cache pollution flaws as other CDPs.",
			  "Nodes are also assumed to be smaller than two cache blocks, which may not hold across real-world applications.",
			  "The researchers also state the technique does not perform well on trees.",
			  "Most importantly, prefetch requests for an LDS are issued sequentially, and thus performance degrades with increased effective memory access times."
			]
		  },
		  {
			"title": "(PDF) On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor",
			"url": "https://www.academia.edu/126501330/On_Reusing_the_Results_of_Pre_Executed_Instructions_in_a_Runahead_Execution_Processor",
			"excerpts": [
			  "Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs.",
			  "runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient.",
			  "In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance."
			]
		  },
		  {
			"title": "Pointer Cache Assisted Prefetching - Computer Science",
			"url": "https://cseweb.ucsd.edu/~calder/papers/MICRO-02-PCache.pdf",
			"excerpts": [
			  ".\nSpeculative precomputation [6] works by identifying the\nsmall number of static loads, known as delinquent loads, that\nare responsible for the vast majority of memory stall cycles.",
			  "\nPrecomputation slices (p-slices), sequences of dependent in-\nstructions which, when executed, produce the address of a\nfuture delinquent load, are extracted from the program be-\ning accelerated. ",
			  " When an instruction in the non-speculative\nthread that has been identified as a trigger instruction reaches\nsome point in the pipeline (typically commit or rename),\nthe corresponding p-slice is spawned into an available SMT\nthread context.\n",
			  "Speculative slices [28] focus largely on the use of precom-\nputation to predict future branch outcomes and to correlate\npredictions to future branch instances in the non-speculative\nthread, but they also support load prefetching.",
			  "\nSoftware controlled pre-execution [13] focuses on the use\nof specialized, compiler inserted code that is executed in\n",
			  "e trace to extract data reference sequences that fre-\nquently repeat in the same order. At this point, the system\ninserts prefetch instructions to detect and prefetch these fre-\nquent data references.",
			  "The sampling and optimization are\ndone dynamically at runtime with very low overhead.\n*",
			  "The Pointer Cache holds mappings between heap point-\ners and the address of the heap object they point to.",
			  "\nThe primary function of the pointer cache is to break the\nserial dependence chains in pointer chasing code. ",
			  ". When one\nload depends on the data loaded by another, a cache miss by\nthe first load forces the second load to stall until the first load\ncompletes.",
			  "When executing a long sequence of such depen-\ndent pointer-chasing loads, instructions can only be executed\nat the speed of the serial accesses to memory.",
			  "*\nOnly pointer loads are candidates to be inserted into the\npointer cache.",
			  "The\nprograms static instructions are analyzed in the reverse order\nof execution from a delinquent load, building up a slice of in-\nstructions the load is directly and indirectly dependent upon.",
			  "Slice construction terminates when an-\nalyzing an instruction far enough from the delinquent load\nthat a spawned thread can provide a timely prefetch, or when\nfurther analysis will add additional instructions to the slice\nwithout providing further performance benefits.",
			  ". In this form,\na slice consists of a sequence of instructions in the order they\nwere analyzed.",
			  "The single path slices constructed in this work are trig-",
			  "Jump pointers are a software technique for prefetching\nlinked data structures.",
			  "Artificial jump pointers are extra\npointers stored into an object that point to other objects some\ndistance ahead in the traversal order.",
			  "Natural jump pointers are existing pointers in the\ndata structure used for prefetching.",
			  "These techniques were introduced by\nLuk and Mowry [12] and refined in [11] and [17].",
			  "Chilimbi and Hirzel [4] proposed an automated\nsoftware approach based on correlation. Their scheme first\ngathers a data reference profile via sampling. Next, they pro-",
			  "Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer ...Read more",
			  "However, traditional prefetching techniques have diffi- culty with sequences of irregular accesses. A common ex- ample of this type of access is pointer chains, ..."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Speculative precomputation: long-range prefetching of delinquentloads",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Deep-Learning-Driven Prefetching for Far Memory",
			"url": "https://arxiv.org/html/2506.00384v1",
			"excerpts": [
			  "Section Title: Deep-Learning-Driven Prefetching for Far Memory > 1. Introduction",
			  "Many data-center workloads including graph processing ( [PageRank ,](https://arxiv.org/html/2506.00384v1.bib25) ; [han2024graph ,](https://arxiv.org/html/2506.00384v1.bib20) ) , tree and index structures ( [guttman1984r ,](https://arxiv.org/html/2506.00384v1.bib19) ; [gusfield1997algorithms ,](https://arxiv.org/html/2506.00384v1.bib18) ) , pointer chasing ( [hsieh2017implementing ,](https://arxiv.org/html/2506.00384v1.bib24) ) , and recursive data structures ( [harold2004xml ,](https://arxiv.org/html/2506.00384v1.bib21) ) exhibit memory access patterns that defy rule-based prefetching.",
			  "If these access patterns could be learned and predicted accurately, far-memory systems could proactively fetch data and mitigate the performance penalties associated with remote access, even in the absence of new hardware.",
			  "Figure 1. FarSight Achieving Three Key Goals Together. FarSight, FastSwap ( [fastswap ,](https://arxiv.org/html/2506.00384v1.bib2) ) , and Hermit ( [hermit ,](https://arxiv.org/html/2506.00384v1.bib35) ) are far-memory systems that run in the Linux kernel. Voyager ( [voyager ,](https://arxiv.org/html/2506.00384v1.bib39) ) , Hashemi etal. ( [pmlr-v80-hashemi18a ,](https://arxiv.org/html/2506.00384v1.bib22) ) , and Twilight ( [duong2024twilight ,](https://arxiv.org/html/2506.00384v1.bib15) ) are micro-architecture CPU cache prefetchers implemented in simulation or with offline traces.",
			  "Content:"
			]
		  },
		  {
			"title": "An Event-Triggered Programmable Prefetcher for Irregular ...",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/programmableprefetcher.pdf",
			"excerpts": [
			  "Ainsworth and T. M. Jones. Graph prefetching using data structure knowledge. In ICS, 2016. [2] S. Ainsworth and T. M. Jones. Software prefetching for indirect ...Read more"
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads ...",
			"url": "https://www.bohrium.com/paper-details/helper-without-threads-customized-prefetching-for-delinquent-irregular-loads/867745821129441724-108609",
			"excerpts": [
			  "Download the full PDF of Helper Without Threads: Customized Prefetching for Delinquent. Includes comprehensive summary, implementation ..."
			]
		  },
		  {
			"title": "[PDF] Speculative precomputation: long-range prefetching ...",
			"url": "https://www.semanticscholar.org/paper/Speculative-precomputation%3A-long-range-prefetching-Collins-Wang/cd42d31aa8f4d07a41556ee4640cb47d3401b9ef",
			"excerpts": [
			  "This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of ..."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~tmj32/papers/docs/ainsworth19-tocs.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automatically generate software prefetches for indirect\nmemory accesses, a special class of irregular memory accesses often seen in high-performance workloads.",
			  "Across a set of memory-bound benchmarks, our automated pass achieves average\nspeedups of 1.3  for an Intel Haswell processor, 1.1  for both an ARM Cortex-A57 and Qualcomm Kryo,\n1.2  for a Cortex-72 and an Intel Kaby Lake, and 1.35  for an Intel Xeon Phi Knights Landing, each of which\nis an out-of-order core, and performance improvements of 2.1  and 2.7  for the in-order ARM Cortex-A53\nand first generation Intel Xeon Phi.",
			  "Hardware prefetchers in real systems focus on stride patterns [ 10 , 12 , 18 ,\n37 , 40 ]. These pick up and predict regular access patterns, such as those in dense-matrix and array\niteration, based on observation of previous addresses being accessed."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses | Department of Computer Science and Technology",
			"url": "https://www.cst.cam.ac.uk/blog/tmj32/software-prefetching-indirect-memory-accesses",
			"excerpts": [
			  "In the paper we create a compiler pass that will automatically identify opportunities to insert prefetches where we find these memory-indirect accesses.",
			  "One of these is that there must be an induction variable within the transitive closure of the source operand (which, for a load, is the operand that calculates the address to load from).",
			  "This means we search backwards through the data dependence graph, starting at the load, until we find this induction variable.",
			  "When we have identified loads that need to be prefetched then we duplicate all of the necessary computation to calculate the address and insert the software prefetch instructions.",
			  "We also have to add code around any loads that are part of this address computation to prevent them from causing errors at runtime if they calculate an invalid address, for example running beyond the end of an array.",
			  "There are more details in the paper including information about how we schedule the prefetches so that data is available immediately before being used."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automat-\nically generate software prefetches for indirect memory\naccesses, a special class of irregular memory accesses of-\nten seen in high-performance workloads. We evaluate this\nacross a wide set of systems, all of which gain benefit from\nthe technique. We then evaluate the extent to which good\nprefetch instructions are architecture dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Categ",
			  " dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Cat"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective: ACM Transactions on Computer Systems: Vol 36, No 3",
			"url": "https://dl.acm.org/doi/10.1145/3319393",
			"excerpts": [
			  "CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being required.",
			  ")Indirect memory accesses have irregular access patterns that limit the performance\nof conventional software and hardware-based prefetchers. To address this problem,\nwe propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect\nmemory ...",
			  "Software prefetching for indirect memory accesses\")CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being require"
			]
		  },
		  {
			"title": "Filtered runahead execution with a runahead buffer | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830812",
			"excerpts": [
			  "Section Title: Filtered runahead execution with a runahead buffer",
			  "Section Title: Filtered runahead execution with a runahead buffer > References",
			  "Authors : [Milad Hashemi](# \"Milad Hashemi\") Milad Hashemi",
			  "Milad Hashemi",
			  "The University of Texas at Austin",
			  "Pages 358 - 369",
			  "https://doi.org/10.1145/2830772.2830812"
			]
		  },
		  {
			"title": "Copyright by Milad Olia Hashemi 2016",
			"url": "https://repositories.lib.utexas.edu/bitstreams/4d988cbc-f809-418f-972d-8202b4c72bf4/download",
			"excerpts": [
			  "Jeffery A. Brown, Hong Wang, George Chrysos, Perry H. Wang, and John P.\nShen.\nSpeculative precomputation on chip multiprocessors.\nIn *Workshop on*\n*Multithreaded Execution, Architecture, and Compilation* , 2001.",
			  "Luis Ceze, James Tuck, Josep Torrellas, and Calin Cascaval. Bulk disambiguation\nof speculative threads in multiprocessors. In *ISCA* , 2006.",
			  "Murali Annavaram, Jignesh M. Patel, and Edward S. Davidson. Data prefetching\nby dependence graph precomputation. In *ISCA* , 2001."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "putation*\n*enables*\n*effective*\n*cache*\n*prefetching for even irregular memory access behavior, by*\n*using an alternate thread on a multithreaded or multi-core*\n*architecture. This paper describes a system that constructs*\n*and runs precomputation based prefetching threads via*\n*event-driven dynamic optimization. Precomputation threads*\n*are dynamically constructed by a runtime compiler from the*\n*programs frequently executed hot traces, and are adapted*\n*to the memory behavior automatically. Both construction*\n*and execution of the prefetching threads happen in another*\n*thread, imposing little overhead on the main thread. This*\n*paper also presents several techniques to accelerate the pre-*\n*computation threads, including colocation of p-threads with*\n*hot traces, dynamic stride prediction, and automatic adap-*\n*tation of runahead and jumpstart distan",
			  "While in-\nlined prefetches are typically effective for simple addressing\npatterns (e.g., strided addresses), p-thread based prefetching\nhas the potential to handle more complex address patterns\n(e.g. pointer chasing), or accesses embedded in more com-\nplex control flow. This is because the prefetching address is\ncomputed via actual code extracted from the main thread.",
			  "ead.\nA successful precomputation-based prefetcher must ad-\ndress several challenges. It must be able to determine the\nproper distance by which the prefetching thread should lead\nthe main thread, and it should have the ability to control that\ndistance. It must create lightweight threads that can actually\nproceed faster than the main thread, so that they stay out in\nfront. It must prevent p-threads from diverging from the ad-\ndress stream of the main thread, or at least detect when it\nhas happened. This divergence may be the result of control\nflow or address value speculation in the p-thread. Runaway\nprefetching may unnecessarily displace useful data, resulting\nin more data cache m",
			  " sophistication of slice creation.\nMore recent work by Lu, et al. [12] dynamically con-\nstructs p-slices via a runtime optimizer running on an idle\ncore. A single user-level thread is multiplexed to detect the\nprograms phases, construct the p-thread code, and perform\nprecomputation prefetching.",
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "**Loop Re-rolling**  A hot trace may contain multiple\ncopies of the same code due to loop unrolling done during\nstatic compilation. We perform loop *re-rolling* for the p-\nslice (i.e. removing the redundant loop copies) to reduce\nduplicated computation inside a p-slice. This optimization\nincreases the granularity at which we can set the prefetch\nrun-ahead distance, since the prefetch distance is always an\nintegral number of iterations.",
			  "**Object-Based Prefetching**  We perform same-object\nbased prefetching, as in our prior work on inline prefetch-\ning [26]. Same-object prefetching clusters prefetches falling",
			  "**P-Thread Jump Starting**\nSometimes, the only way to get the prefetch thread ahead\nof the main thread is to give it a head start. Existing dy-\nnamic precomputation schemes (e.g.\n[12]) typically start\np-threads from the same starting point (same iteration) as the\nmain thread.",
			  "**6**\n**Results**\nThis section evaluates the cost and performance of our dy-\nnamically generated precomputation based prefetching tech-\nnique.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches).",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "Our approach is complementary to and does not interfere with existing hardware prefetchers since we target only delinquent irregular load instructions (those with no constant or striding address patterns).",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support.",
			  "Delinquent Irregular Loads",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://hlitz.github.io/papers/crisp.pdf",
			"excerpts": [
			  "We also compare CRISP to a hardware-only design referred to as IBDA which performs load slice extraction via iterative backwards dependency analysis. IBDA ...Read more"
			]
		  },
		  {
			"title": "Caching and Performance of CPUs - Jyotiprakash's Blog",
			"url": "https://blog.jyotiprakash.org/caching-and-performance-of-cpus",
			"excerpts": [
			  "Nonblocking Caches to Handle Multiple Misses:** Nonblocking or \"lockup-free\" caches take cache optimization a step further by allowing out-of-order execution. With a nonblocking cache, the CPU doesn't need to stall on a cache miss; instead, it can continue to fetch other instructions while waiting for the missing data. This technique, referred to as \"hit under miss\" or \"miss under miss,\" reduces effective miss penalties by overlapping misses and allowing more efficient cache utilizatio",
			  "ltibanked Caches to Increase Cache Bandwidth:** Instead of treating the cache as a single large block, **multibanked caches** split it into independent banks that can support simultaneous accesses. This approach effectively increases cache bandwidth by allowing multiple memory accesses at the same time.",
			  "For example, modern processors like the Intel Core i7 use multiple banks in their L1 cache, enabling up to two memory accesses per clock cycle. By using **sequential interleaving** , addresses are spread evenly across different banks, ensuring that memory accesses are well distributed, reducing contention, and improving cache performance."
			]
		  },
		  {
			"title": "Understanding the Backward Slices of Performance ...",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/2000/slice.isca.pdf",
			"excerpts": [
			  "backward slice (the subset of the program that relates to*\n*the instruction) of these performance degrading instructions, if*\n*small compared to the whole dynamic instruction stream, can be*\n*pre-executed to hide the instructions latenc",
			  " be effective with respect to a given instruction, a pre-execu-\ntion technique needs three things.",
			  "First, at an *initiation point* ahead\nof the instructions execution, the pre-execution technique needs to\nknow that the performance degrading instruction *will* be executed.",
			  "Second, it has to know which other instructions contribute to the\nperformance degrading instruction.",
			  "-fetches.\nPre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. U",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "The key to answering all of these questions lies in the backward\nslice of the performance degrading instruction.",
			  "Pre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. Unlike the previous two cases, this\npre-executed branch outcome (and perhaps target) needs to be\nbound to a particular dynamic branch instance to fully benefit from\nthe pre-execution.",
			  "n general, pre-execution amounts\nto guessing the existence of a future performance degrading\ninstruction and executing it (or what we think it will be) some time\nprior to its actual encounter in the machine, thereby at least par-\ntially hiding its latency"
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "In this section requirements for\neffective direct pre-execution are examined and hardware\nmechanisms supporting these requirements are described.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://llvm.org/devmtg/2017-03/assets/slides/software_prefetching_for_indirect_memory_accesses.pdf",
			"excerpts": [
			  "Software Prefetching for Indirect. Memory Accesses. Sam Ainsworth and Timothy M. Jones. Computer Laboratory. Page 2. What should we software prefetch? Stride ...Read more"
			]
		  },
		  {
			"title": "Long-range Prefetching of Delinquent Loads",
			"url": "http://cseweb.ucsd.edu/~tullsen/isca2001.pdf",
			"excerpts": [
			  "Speculative Precomputation, a tech-*\n*nique that uses idle thread contexts in a multithreaded ar-*\n*chitecture to improve performance of single-threaded appli",
			  "ecula-\ntive threads are spawned under one of two conditions: when\nencounteringa basic trigger, which occurs when a designated\ninstruction in the main thread reaches a particular pipeline\nstage (such as the commit stage), or a chaining trigger, when\none speculative thread explicitly spawns another.",
			  "A speculative thread is spawned by allocating a hardware\nthread context, copying necessary live-in values into its reg-\nister file, and providing the thread context with the address of\nthe first instruction of the threa",
			  "If a free hardware context\nis not available the spawn request is ignored.",
			  "Necessary live-in values are always copied into the thread\ncontext when a speculative thread is spawned.",
			  "peculative threads execute precomputation slices (p-\nslices), which are sequences of dependent instructions which\nhave been extracted from the non-speculative thread and\ncompute the address accessed by delinquent loads.",
			  "When\na speculative thread is spawned, it precomputes the address\nexpected to be accessed by a future delinquent load, and\nprefetches the data.",
			  "wo primary forms of Speculative Precom-*\n*putation are evaluat",
			  "Delinquent Loads",
			  "We find that in most programs the set of delinquent\nloads is quite small; commonly 10 or fewer static loads cause\nmore than 80% of L1 data cache misses.",
			  " precomputa-\ntion slices used by our work are constructed within an in-\n ...",
			  "ulative precom-\nputation could be thought of as a special prefetch mech-\nanism that effectively targets load instructions that tradi-\ntionally have been difficult to handle via prefetching, such\nas loads that do not exhibit predictable access patterns and\nchains of dependent load",
			  "Speculative threads can be spawned",
			  "hardware structure is called the Outstanding Slice\nCounter (OSC). This structure tracks, for a subset of delin-\nquent loads, the number of instances of delinquent loads\nfor which a speculative thread has been spawned but for\nwhich the main thread has not yet committed the corre-\nsponding load."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3319393",
			"excerpts": [
			  "S. Ainsworth and Timothy M. Jones. 2017. Software prefetching for indirect memory accesses. In *Proceedings of the International Symposium on Code Generation and Optimization (CGO17)* . Navigate to citation 1 citation 2",
			  "Sam Ainsworth and Timothy M. Jones. 2018. An event-triggered programmable prefetcher for irregular workloads. In *Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)* . Navigate to citation 1"
			]
		  },
		  {
			"title": "Orchestrated Scheduling and Prefetching for GPGPUs",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/orchestrated-gpgpu-scheduling-prefetching_isca13.pdf",
			"excerpts": [
			  "In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General. Purpose Graphics Processing Unit (GPGPU) ...Read more"
			]
		  },
		  {
			"title": "Evaluating and Mitigating Bandwidth Bottlenecks Across ...",
			"url": "https://users.cs.utah.edu/~vijay/papers/ispass17.pdf",
			"excerpts": [
			  "For instance, we observe that to prevent throttling of L1 cache, increasing the L1 bandwidth by increasing the MSHRs to handle more outstanding misses can lead ..."
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://hparch.gatech.edu/papers/lee_micro10.pdf",
			"excerpts": [
			  " **In some cases, blindly applying prefetching degrades perfor-**\n**mance. To reduce such negative effects, we propose an** ***adaptive***\n***prefetch throttling*** **scheme, which permits automatic GPGPU**\n**application- and hardware-specific adjustment. We show that**\n**adaptation reduces the negative effects of prefetching and can**\n**even improve performance. Overall, compared to the state-of-**\n**the-art software and hardware prefetching, our MT-prefetching**\n**improves performance on average by 16% (software pref.) / 15%**\n**(hardware pref.) on our benchmarks.**",
			  "IP may not be useful in two cases. The first case is when\ndemand requests corresponding to prefetch requests have al-\nready been generated. This can happen because warps are not\nexecuted in strict sequential order.",
			  ".\nThe second case is when the warp that is prefetching is\nthe last warp of a thread block and the target warp (i.e.\nthread block to which the target warp belongs) has been\nassigned to a different core.",
			  " Inter-thread Prefetching (IP):* One of the main differ-\nences between GPGPU applications and traditional applica-\ntions is that GPGPU applications have a significantly higher\nnumber of thread",
			  "er warp training:* Stream and stride detectors must be\ntrained on a per-warp basis, similar to those in simulta-\nneous multithreading architectures. This aspect is criti-\ncal since many requests from different warps can easily\nconfuse pattern detector",
			  "*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "The key idea behind hardware IP is that when an application\nexhibits a strided memory access pattern across threads at the\nsame PC, one thread generates prefetch requests for another\nthread.",
			  "able*\n*Hardware*\n*Prefetcher*\n*Training:*\nCurrent\nGPGPU applications exhibit largely regular memory access\npatterns, so one might expect traditional stream or stride\nperfetchers to work well. However, because the number of\nthreads is often in the hundreds, traditional training mecha-\nnisms do not scale.",
			  "Here, we describe extensions to the traditional training\npolicies, for program counter (PC) based stride prefetch-\ners [4, 11], that can overcome this limitation. This basic idea\ncan be extended to other types of prefetchers as well.",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  " *Stride promotion:* Since memory access patterns are\nfairly regular in GPGPU applications, we observe that\nwhen a few warps have the same access stride for a given\nPC, all warps will often have the same stride for the\nPC.",
			  "Based on this observation, when at least three PWS\nentries for the same PC have the same stride, we promote\nthe PC stride combination to the *global stride (GS)* table.",
			  "IV. U NDERSTANDING U SEFUL VS . H ARMFUL\nP REFETCHING IN GPGPU",
			  "o as** ***many-thread aware prefetching*** **(MT-prefetching) mecha-*",
			  "nisms.",
			  "access behavior among fine-grained threads.",
			  "For hardware MT-**",
			  "The key ideas behind our MT-prefetching schemes are\n(a) per-warp-training and stride promotion, (b) inter-thread\nprefetching, and (c) adaptive throttling.",
			  "mechanism.",
			  "In some cases, blindly applying prefetching degrades perfor-**",
			  "mance. To reduce such negative effects, we propose an** ***adaptive***",
			  "prefetch throttling*** **scheme, which permits automatic GPGPU**",
			  "application- and hardware-specific adjustment.",
			  "We show that**",
			  "adaptation reduces the negative effects of prefetching and can**",
			  "even improve performance.",
			  "III. P REFETCHING M ECHANISMS FOR GPGPU",
			  "This section describes our *many-thread aware* prefetching",
			  "(MT-prefetching) schemes, which includes both hardware",
			  "and software mechanisms.",
			  "To support these schemes, we\naugment each core of the GPGPUs with a prefetch cache and\na prefetch engine.",
			  "The prefetch cache holds the prefetched\nblocks from memory and the prefetch engine is responsible\nfor throttling prefetch requests (see Section V).",
			  "*A. Software Prefetching*",
			  "We refer to our software prefetching mechanism as *many-*",
			  " refer to our software prefetching mechanism as *many-*\n*thread aware software prefetching* (MT-SWP). MT-SWP con-\nsists of two components: conventional *stride prefetching* and\na newly proposed *inter-thread prefetching* (IP).",
			  "a newly proposed *inter-thread prefetching* (IP).",
			  "*1) Stride Prefetching:* This mechanism is the same as the",
			  "1) Stride Prefetching:* This mechanism is the same as the\ntraditional stride prefetching mechanism. The prefetch cache\nstores any prefetched blocks.",
			  "tions is that GPGPU applications have a significantly higher\nnumber of threads.",
			  "As a result, the execution length of each\nthread is often very short.",
			  "Figure 3 shows a snippet of sequen-\ntial code with prefetch instructions and the equivalent CUDA\ncode without prefetch instructions.",
			  "In the CUDA code, since\nthe loop iterations are parallelized and each thread executes\nonly one (or very few) iteration(s) of the sequential loop, there\nare no (or very few) opportunities to insert prefetch requests",
			  "This can happen because warps are not\nexecuted in strict sequential order.",
			  "For example, when T32\ngenerates a prefetch request for T64, T64 might have already\nissued the demand request corresponding to the prefetch\nrequest generated by T32.",
			  "hese prefetch requests are usu-\nally merged in the memory system since the corresponding\ndemand requests are likely to still be in the memory system",
			  "Unless inter-core merging occurs\nin the DRAM controller, these prefetch requests are useless.",
			  "This problem is similar to the out-of-array-bounds problem\nencountered when prefetching in CPU systems.",
			  "Nevertheless,\nwe find that the benefits of IP far outweigh its negative effects.",
			  "*B. Hardware Prefetching*",
			  "We refer to our hardware prefetching mechanism as\nthe *many-thread aware hardware prefetcher* (MT-HWP).",
			  "T-HWP has (1) enhanced prefetcher training algorithms\nthat provide improved scalability over previously proposed\nstream/stride prefetchers and (2) a hardware-based inter-\nthread prefetching (IP) mechanism.",
			  "This information is stored in a separate table called an\nIP table.",
			  "*3) Implementation:* Figure 6 shows the overall design of\nthe MT-HWP, which consists of the three tables discussed\nearlier: PWS, GS, and IP tables.",
			  " practice, overly aggressive prefetching can have a nega-\ntive effect on performance.",
			  " principal memory latency tolerance mechanism in a\nGPGPU is multithreading. Thus, if a sufficient number of\nwarps and/or enough computation exist, memory latency can"
			]
		  },
		  {
			"title": "CTA-aware Prefetching for GPGPU - Computer Engineering",
			"url": "https://ceng.usc.edu/techreports/2014/Annavaram%20CENG-2014-08.pdf",
			"excerpts": [
			  "Lee et al. [21] proposed a software and hardware\nbased many-thread aware prefetching which basically commands\nthreads to prefetch data for the other threads.",
			  "They exploit the\nfact that the memory addresses are referenced using thread id in\nmany GPU applications."
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/",
			"excerpts": [
			  "To optimize memory access on NVIDIA GPUs, prefetching can be employed in software when excess warps are insufficient to hide memory latency.",
			  "A synchronization within the loopfor example, `syncthreads` constitutes a memory fence and forces the loading of `arr` to complete at that point within the same iteration, not PDIST iterations later. The fix is to use asynchronous loads into shared memory, the simplest version of which is explained in the [Pipeline interface](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) section of the CUDA programmer guide.",
			  "Prefetching can be implemented by unrolling loops, storing prefetched values in registers or shared memory, and using techniques like batched or rolling prefetching, with the latter being more effective when combined with asynchronous memory copies.",
			  "Confirm that not all memory bandwidth is being used.",
			  "Confirm the main reason warps are blocked is **Stall Long Scoreboard** , which means that the SMs are waiting for data from DRAM.",
			  "This leads to the improved shared memory results shown in Figure 2. A prefetch distance of just 6, combined with asynchronous memory copies in a rolling fashion, is sufficient to obtain optimal performance at almost 60% speedup over the original version of the code."
			]
		  },
		  {
			"title": "Near-Side Prefetch Throttling - of Wim Heirman",
			"url": "https://heirman.net/papers/pact2018.pdf",
			"excerpts": [
			  " near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine. The MSHR tracks outstand-\ning cache misses triggered by both demand requests (application\nloads and stores), and prefetches; the extra state allows for detec-\ntion of late prefetches. The state machine periodically computes\nthe fraction of late prefetches, and updates the optimal prefetch\ndistance.\n",
			  "Maintaining a small fraction\n(e.g., 10%) of late prefetches does not harm performance as long as\nthe late prefetches are only late by a small amount, i.e., the demand\naccess is made only just before the prefetch request completes.",
			  "Near-Side Prefetch Throttling",
			  " We show that near-side throttling can be extended to mul-\ntiple prefetchers per core, where it will naturally throttle\nthose prefetchers that yield no useful requests, allowing for\na diverse set of prefetch algorithms to co-exis",
			  "e throttling detects bandwidth\nsaturation locally (through memory latency), so no global coordi-\nnation is needed between the per-core prefetchers in a many-core\narchitecture, or even between multiple prefetch algorithms on a\nsingle core.\n",
			  "ng detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost. This makes near-side throttling superior over traditional\nfar-side throttling as it is able to provide even slightly better per-\nformance (9.6% vs. 9.4%), at a far cheaper implementation cost,\nand is more widely applicable to other use cases such as software\nprefetching and control of multiple hardware prefetchers.",
			  "The basic concept of near-side prefetch throttling (NST) is to\ndetect late prefetches, and tune the prefetcher aggressiveness such\nthat the amount of late prefetches is balanced around a small but\nnon-zero fraction of all prefetches.",
			  "NEAR-SIDE PREFETCH THROTTLING**",
			  "ur\nsolution is cheap to implement in hardware, includes throttling on\noff-chip bandwidth saturation, applies to both hardware and soft-\nware prefetching, and can control multiple concurrent prefetchers\nwhere it will naturally allow the most useful prefetch algorithm\nto generate most of the requests",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "In a many-core processor, the prefetchers in each core can be con-\ntrolled independently based on their own specific late prefetch\nfraction. This allows for heterogenous applications or multi-\nprogramming workloads, and will tune each prefetchers distance\nto the specific access pattern it is experienci",
			  "Our proposed implementation of near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine.",
			  "sing detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Hardware Design of DRAM Memory Prefetching Engine for ...",
			"url": "https://www.mdpi.com/2227-7080/13/10/455",
			"excerpts": [
			  "Inter-warp prefetching mechanisms are based on the detection of stride patterns and base addresses in different warps. A many-thread-aware prefetching mechanism ...Read more"
			]
		  },
		  {
			"title": "APAC: An Accurate and Adaptive Prefetch Framework with ...",
			"url": "https://par.nsf.gov/servlets/purl/10251073",
			"excerpts": [
			  "Near-side prefetch throttling (NST) [9] only adjusts the aggressiveness of prefetching based on the fraction of late prefetchers, which has a relatively small ...Read more"
			]
		  },
		  {
			"title": "PPT - Many-Thread Aware Prefetching Mechanisms for GPGPU Application PowerPoint Presentation - ID:5741796",
			"url": "https://www.slideserve.com/phil/many-thread-aware-prefetching-mechanisms-for-gpgpu-application",
			"excerpts": [
			  "akash\n**[Motivation](https://image3.slideserve.com/5741796/motivation-l.jpg \"motivation\")**  Memory latency hiding through multithread prefetching schemes  Per-warp training and Stride promotion  Inter-thread Prefetching  Adaptive Throttling  Propose software and hardware prefetching mechanisms for a GPGPU architecture  Scalable to large number of threads  Robustness through feedback and throttling mechanisms to avoid degraded performance\n",
			  "rmance\n**[Memory Latency Hiding techniques](https://image3.slideserve.com/5741796/memory-latency-hiding-techniques-l.jpg \"memory latency hiding techniques\")**  Multithreading  Thread level and Warp level context switching  Utilization of complex cache memory hierarchies  Using L1, L2, DRAMs than accessing Global Memory each time  Prefetching  Insufficient thread-level parallelism  Memory request merging Thread1 Thread2 Thread1 Thread3",
			  "MT-HWP  Stride Promotion  Considering the stride pattern is the same across all warps for a given PC, PWS is monitored for three accesses  If found same stride, promote the PWS to Global Stride(GS) table, if not, retain in PWS  Inter-thread Prefetching  Monitor stride pattern across threads at the same PC, for 3 memory accesses  If found same, stride information is stored in the IP table",
			  "MT-HWP  Implementation  When there are hits in both GS and IP, GS is given preference because  Strides"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications | Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2010.44",
			"excerpts": [
			  "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract\nContent:\nWe consider the problem of how to improve memory latency tolerance in massively multithreaded GPGPUs when the thread-level parallelism of an application is not sufficient to hide memory latency. One solution used in conventional CPU systems is prefetching, both in hardware and software. However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously. This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms. Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads. For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, blindly applying prefetching degrades performance. To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment. We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Overall, compared to the state-of-the-art software and hardware prefetching, our MT-prefetching improves performance on average by 16%(software pref.) / 15% (hardware pref.) on our benchmarks.",
			  "Section Title: Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract",
			  "However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously.",
			  "This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms.",
			  "Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads.",
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism.",
			  "In some cases, blindly applying prefetching degrades performance.",
			  "To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment.",
			  "We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Content:"
			]
		  },
		  {
			"title": "[PDF] Near-side prefetch throttling: adaptive prefetching for high-performance many-core processors | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Near-side-prefetch-throttling%3A-adaptive-prefetching-Heirman-Bois/39e6013cbe45a288431ddb4269611a483c90bbb9",
			"excerpts": [
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "TLDR"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nDaniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, and Jir Bittner. 2021. A Survey on Bounding Volume Hierarchies for Ray Tracing. Computer Graphics Forum (2021).\n[Go",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Meister%2C+Shinji+Ogaki%2C+Carsten+Benthin%2C+Michael%C2%A0J.+Doyle%2C+Michael+Guthe%2C+and+Jir%C3%AD+Bittner.+2021.+A+Survey+on+Bounding+Volume+Hierarchies+for+Ray+Tracing.+Computer+Graphics+Forum+%282021%29.)",
			  "[32]",
			  "Bochang Moon, Yongyoung Byun, Tae-Joon Kim, Pio Claudio, Hye-Sun Kim, Yun-Ji Ban, Seung Woo Nam, and Sung-Eui Yoon. 2010. Cache-Oblivious Ray Reordering. ACM Transactions on Graphics (TOG) (2010).",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Bochang+Moon%2C+Yongyoung+Byun%2C+Tae-Joon+Kim%2C+Pio+Claudio%2C+Hye-Sun+Kim%2C+Yun-Ji+Ban%2C+Seung%C2%A0Woo+Nam%2C+and+Sung-Eui+Yoon.+2010.+Cache-Oblivious+Ray+Reordering.+ACM+Transactions+on+Graphics+%28TOG%29+%282010%29.)",
			  "[33]",
			  "Paul Arthur Navratil, Donald S. Fussell, Calvin Lin, and William R. Mark. 2007. Dynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In IEEE Symposium on Interactive Ray Tracing. 95104.\n[Dig",
			  "[Digital Library](/doi/10.1109/RT.2007.4342596)",
			  "[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FRT.2007.4342596)",
			  "[34]"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://info.computer.org/csdl/proceedings-article/micro/2010/05695538/12OmNzcPApv",
			"excerpts": [
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, ...Read more"
			]
		  },
		  {
			"title": "(PDF) Adaptive Prefetching for Fine-grain Communication in PGAS Programs",
			"url": "https://www.researchgate.net/publication/381186663_Adaptive_Prefetching_for_Fine-grain_Communication_in_PGAS_Programs",
			"excerpts": [
			  "In this work, we present an adaptive prefetching optimization that can be applied to PGAS programs with irregular memory access patterns. We ...Read more"
			]
		  },
		  {
			"title": "System level cache prefetching algorithms for complex ...",
			"url": "https://lup.lub.lu.se/student-papers/record/9159122/file/9159147.pdf",
			"excerpts": [
			  "These merges occur when a prefetch request is late and a demand request is coming at the same time. The throttling behaviour can be observed in table 1. Early ...Read more"
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching - Technical Blog - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/boosting-application-performance-with-gpu-memory-prefetching/209210",
			"excerpts": [
			  "This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.Read more"
			]
		  },
		  {
			"title": "Many-thread aware hardware prefetcher (MT-HWP). | Download Scientific Diagram",
			"url": "https://www.researchgate.net/figure/Many-thread-aware-hardware-prefetcher-MT-HWP_fig7_221005092",
			"excerpts": [
			  "h consists of the three tables discussed earlier: PWS, GS, and IP tables. The ",
			  "The IP and GS tables are indexed in parallel with a PC address.",
			  "adaptive MT-HWP pro- vides a 29% performance improvement over the baseline."
			]
		  },
		  {
			"title": "Some issues regarding the use of prefetch in the cuda kernel - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/some-issues-regarding-the-use-of-prefetch-in-the-cuda-kernel/334568",
			"excerpts": [
			  "e difference between cp.async and normal loads is the means of re-synchronization of the asynchronous operations.",
			  "For normal loads from global memory, the re-synchronization works with the help of the long scoreboard, as soon as the result registers are used in instructions dependent on the result.",
			  "For cp.async, the results are transferred into shared memory. You have to use cp.async.wait_group or cp.async.wait_all for re-synchronization."
			]
		  },
		  {
			"title": "Combining Local and Global History for High Performance ...",
			"url": "https://jilp.org/dpc/online/papers/00dimitrov.pdf",
			"excerpts": [
			  "To capture delta correlation, a delta buffer is included in the prefetch function, which keeps the delta information when a linked list is traversed in the GHB.Read more"
			]
		  },
		  {
			"title": "L5 - Advanced Memory Prefetching Techniques",
			"url": "https://coconote.app/notes/5551d401-c94c-4d86-bb70-c67c9e8bb912",
			"excerpts": [
			  "Delta correlation prefetchers track repeating delta patterns between accessed addresses to predict future accesses. Correlation-based ...Read more"
			]
		  },
		  {
			"title": "Pointer jumping - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Pointer_jumping",
			"excerpts": [
			  "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs.Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov Predictors",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=joseph_isca97.pdf",
			"excerpts": [
			  "The Markov prefetcher provides the best performance across nll\nthe applications, particularly when applied to both the instruction\nand data caches."
			]
		  },
		  {
			"title": "A Survey on Recent Hardware Data Prefetching ...",
			"url": "https://arxiv.org/pdf/2009.00715",
			"excerpts": [
			  "99] D. Joseph and D. Grunwald, Prefetching Using Markov Predictors, in *Proceedings of the International Symposium on*\n*Computer Architecture (ISCA)* , pp. 252263, 1997"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1999/jmp-ptr.isca.pdf",
			"excerpts": [
			  "bstract**\n*Current techniques for prefetching linked data structures*\n*(LDS) exploit the work available in one loop iteration or*\n*recursive call to overlap pointer chasing latency. Jump-*\n*pointers, which provide direct access to non-adjacent*\n*nodes, can be used for prefetching when loop and recur-*\n*sive procedure bodies are small and do not have sufficient*\n*work to overlap a long latency. This paper describes a*\n*framework for jump-pointer prefetching (JPP) that sup-*\n*ports four prefetching idioms: queue, full, chain, and root*\n*jumping and three implementations: software-only, hard-*\n*ware-only, and a cooperative software/hardware tech-*\n*nique.",
			  "n a suite of pointer intensive programs, jump-*\n*pointer prefetching reduces memory stall time by 72% for*\n*software, 83% for cooperative and 55% for hardware, pro-*\n*ducing speedups of 15%, 20% and 22% respecti",
			  "general purpose technique for tol-\nerating serialized latencies that result from LDS tra-\nversal.\nBy storing explicit jump-pointers to nodes\nseveral hops away, JPP overcomes the pointer-chasing\nproblem.\nIt is able to generate prefetch addresses\ndirectly, rather than in a serial fashion, and is effective\neven in situations where not enough work is available\nto hide latencies by scheduling.\n**",
			  "P implementations: software-only,\nhardware-only, and cooperative.\nFor those programs\nwith appreciable memory latency components, these\nimplementations reduce overall observed memory\nlatency by 72%, 55%, and 83%, respectively and\nachieve speedups of 15%, 22%, and 20%.\n",
			  "**Abstract**",
			  "*Current techniques for prefetching linked data structures*",
			  "\n*(LDS) exploit the work available in one loop iteration or*",
			  "*recursive call to overlap pointer chasing latency. Jump-*",
			  "*pointers, which provide direct access to non-adjacent*",
			  "*nodes, can be used for prefetching when loop and recur-*",
			  "*sive procedure bodies are small and do not have sufficient*",
			  "*work to overlap a long latency. This paper describes a*",
			  "*framework for jump-pointer prefetching (JPP) that sup-*",
			  "*ports four prefetching idioms: queue, full, chain, and root*",
			  "*jumping and three implementations: software-only, hard-*",
			  "*ware-only, and a cooperative software/hardware tech-*",
			  "*nique.*",
			  "*On a suite of pointer intensive programs, jump-*",
			  "*pointer prefetching reduces memory stall time by 72% for*",
			  "*software, 83% for cooperative and 55% for hardware, pro-*",
			  "*ducing speedups of 15%, 20% and 22% respectively.*",
			  "*\n**1 Introduction**",
			  "Linked data structures (LDS) are common in many appli-",
			  "cations, and their importance is growing with the spread of",
			  "object-oriented programming.",
			  "The popularity of LDS",
			  "stems from their flexibility, not their performance. LDS",
			  "access, often referred to as *pointer-chasing* , entails chains",
			  "of data dependent loads that serialize address generation",
			  "and memory access.",
			  "In traversing an LDS, these loads",
			  "often form the programs critical path.\nC",
			  "Consequently,",
			  "when they miss in the cache, they can severely limit paral-",
			  "lelism and degrade performance.",
			  "Prefetching is one way to hide LDS load latency and",
			  " ... \nefficiently [6] and to parallelize searches and reductions on",
			  "lists [9].",
			  "Discussions of maintaining recursion-avoiding",
			  "traversal *threads* in non-linear data structures can be found",
			  "in data structures literature [18]. As noted earlier, Luk and",
			  "Mowry [11] suggested the use of programmer controlled",
			  "jump-pointers for prefetching. We are not aware of any",
			  "implementations, actual or proposed,\nof hardware or",
			  "cooperative jump-pointer prefetching.",
			  "\n**6 Summary and Future Directions**\n",
			  "In this paper, we describe the general technique of jump-",
			  "pointer prefetching (JPP) for tolerating linked structure",
			  "(LDS) access latency. JPP is effective when limited work",
			  "is available between successive dependent accesses (e.g., a",
			  "***Figure***",
			  "***7.***",
			  "***Tolerating***",
			  "***longer***",
			  "***memory***",
			  "***latencies.***",
			  "*Execution times for health: the first group of bars uses*",
			  "*the base configuration (70 cycle memory latency), the*",
			  "*second and third simulate long memory latency (280*",
			  "cycles).*",
			  "*In terms of prefetching, the first two*",
			  "*configurations use a jump interval (the distance*",
			  "*\n*between a jump-pointers home and target nodes) of 8,*",
			  "the third uses an interval of 16.*",
			  "0.0",
			  "0.5",
			  "1.0",
			  "1.5",
			  "2.0",
			  "2.5",
			  "3.0",
			  "MemLat=70",
			  "Interval=8",
			  "Interval=8",
			  "BDSCH",
			  "BDSCH",
			  "BDSCH",
			  "MemLat=280",
			  "MemLat=280",
			  "Interval=16",
			  "Normalized Execution Time",
			  "memory latency",
			  "Compute time",
			  "**Legend: B:** Base",
			  "**D:** DBP",
			  "**S:** Software JPP",
			  "**C:** Cooperative JPP",
			  "**H:** Hardware JPP",
			  "tight pointer chasing loop) to enable aggressive scheduling"
			]
		  },
		  {
			"title": "The Efficacy of Software Prefetching and Locality ...",
			"url": "https://jilp.org/vol6/v6paper7.pdf",
			"excerpts": [
			  "In jump pointer prefetching, additional pointers are inserted into a dynamic data structure\nto connect non-consecutive link elements.",
			  "These jump pointers allow prefetch instructions to\nname link elements further down the pointer chain ( i.e. a prefetch distance, PD , away which\nis computed as in Sections 3.1 and 3.2) without sequentially traversing the intermediate links.",
			  "Consequently, prefetch instructions can overlap the fetch of multiple link elements simultaneously\nby issuing prefetches through the memory addresses stored in the jump pointers.",
			  "Jump pointer prefetching, however, cannot prefetch the first PD link nodes in a linked list\nbecause there are no jump pointers that point to these early nodes.",
			  "To enable prefetching of early\nnodes, jump pointer prefetching can be extended with prefetch arrays [7]. In this technique, an\narray of prefetch pointers is added to every linked list to point to the first PD link nodes.",
			  "Hence,\nprefetches can be issued through the memory addresses in the prefetch arrays before traversing\neach linked list to cover the early nodes, much like the prologue loops in affine array and indexed\narray prefetching prefetch the first PD array elements.",
			  "Figure 5 Part(A) illustrates the addition\nof a prologue loop that performs prefetching through a prefetch array.",
			  "6.3 ccmalloc and Prefetching",
			  "Software prefetching for pointer-chasing codes suffers high overhead to create and manage jump\npointers, as described in Section 5.2.",
			  "owever, jump pointers may not be necessary when prefetch-\ning is combined with ccmalloc memory allocation",
			  "Since intelligent allocation places link nodes\ncontiguously in memory, prefetch instructions can access future link nodes by simple indexing, just\nas for affine array accesses.",
			  "Figure 9 shows the effect of ccmalloc on nodes linked together by\npointers.",
			  "From the right-hand part of the figure, it is intuitive that a compiler can insert prefetches\nfor list nodes further down the list using the size of a node and the location of the first node.",
			  "This approach, which we call index prefetching [1, 37], was originally proposed in [8].",
			  "With in-\ndex prefetching, the jump pointers can be removed, thus eliminating all the overhead associated\nwith jump pointer prefetchin",
			  "To quantify this benefit, we created index prefetching versions for\nHealth and MST , and show the results for these benchmarks in Figure 18.",
			  "(We did not create an\nindex prefetching version for EM3D , our third pointer-chasing benchmark, since it already achieves\nhigh performance with normal software prefetching as shown in Figures 10 and 13)."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta- ...",
			"url": "https://jilp.org/vol13/v13paper2.pdf",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " Correlating Prediction\nTables (DCPT). DCPT builds upon two previously proposed prefetcher techniques, com-\nbining them and refining their ideas to achieve better perf"
			]
		  },
		  {
			"title": "Prefetching",
			"url": "https://ece752.ece.wisc.edu/lect15-prefetching.pdf",
			"excerpts": [
			  "Markov prefetching forms address correlations",
			  "Joseph and Grunwald (ISCA 97)",
			  "Uses global memory addresses as states in the Markov graph",
			  "Correlation Table *approximates* Markov graph",
			  "obal History Buffer (GHB)\n Holds miss address\nhistory in FIFO order",
			  "Global History Buffer",
			  "Delta-based prefetching leads to much smaller table than\nclassical Markov Prefetching",
			  "Delta-based prefetching can remove compulsory misses"
			]
		  },
		  {
			"title": "Data prefetching on in-order processors",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/2268c547-2c42-49d5-9e73-7578ebe3758e/content",
			"excerpts": [
			  "[14] K. J. Nesbit and J. E. Smith, Data cache prefetching using a global history buffer, Software, IEE Proceedings-, 2004. [15] S. Srinath, O. Mutlu, H ...Read more"
			]
		  },
		  {
			"title": "Feedback Mechanisms for Improving Probabilistic Memory ...",
			"url": "https://www.cs.utexas.edu/~lin/papers/hpca09.pdf",
			"excerpts": [
			  "The efficiency of stream prefetching has been improved by Nesbit and. Smith [18], who introduce the Global History Buffer to im- prove prefetch effectiveness ...Read more"
			]
		  },
		  {
			"title": "Data Access History Cache and Associated Data Prefetching ...",
			"url": "http://www.cs.iit.edu/~scs/assets/files/SC07_DAHC.pdf",
			"excerpts": [
			  "Nesbit and Smith proposed a global history buffer for data prefetching in [14] and. [15]. The similarity between their work and our work is that both attempt ..."
			]
		  },
		  {
			"title": "TDT4260 Computer Architecture Mini-Project",
			"url": "https://www.nichele.eu/files/nichele_tdt4260.pdf",
			"excerpts": [
			  "[14] M. Grannaes, M. Jahre and L. Natvig. Multi-level Hardware Prefetching. Using Low Complexity Delta Correlating Prediction Tables with Partial. Matching.Read more"
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer",
			"url": "https://www.researchgate.net/publication/3215463_Data_Cache_Prefetching_Using_a_Global_History_Buffer",
			"excerpts": [
			  "This research is to design a history table-based linear analysis ... This paper studies hardware prefetching for second-level (L2) caches."
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | Proceedings of the 10th International Symposium on High Performance Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1109/HPCA.2004.10030",
			"excerpts": [
			  "A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction.",
			  "The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.",
			  "Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods.",
			  "Global History Buffer prefetching can increase correlation prefetching performance by 20% and cut its memory traffic by 90%. Furthermore, the Global History Buffer can make correlations within a loads address stream, which can increase stride prefetching performance by 6%. "
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | IEEE Micro",
			"url": "https://dl.acm.org/doi/abs/10.1109/MM.2005.6",
			"excerpts": [
			  "By organizing data cache prefetch information in a new way, a GHB supports existing prefetch algorithms more effectively than conventional prefetch tables. It reduces stale table data, improving accuracy and reducing memory traffic. It contains a more complete picture of cache miss history and is smaller than conventional tables",
			  "The structure is based on a Global History Buffer that holds the most\nrecent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer",
			  "HPCA '04: Proceedings of the 10th International Symposium on High Performance Computer ArchitectureA new structure for implementing data cache prefetching is proposed and analyzed via",
			  "A new structure for implementing data cache prefetching is proposed and analyzed via"
			]
		  },
		  {
			"title": "DATA CACHE PREFETCHING USING A GLOBAL ...",
			"url": "https://minds.wisconsin.edu/bitstream/1793/11158/1/file_1.pdf",
			"excerpts": [
			  "As a circular buffer, the GHB prefetching\nstructure eliminates many problems associat-\ned with conventional tables. First, the GHB\nFIFO naturally gives table space priority to\nthe most recent history, thus eliminating the\nstale-data problem.",
			  "dex table entries contain point-\ners into the GHB.",
			  "The GHB is larger, with a size chosen to hold\na representative portion of the miss address\nstream. Last, and perhaps most important, a\ndesigner can use the ordered global history to\ncreate more-sophisticated prefetching meth-\nods than conventional stride and correlation\nprefetchin"
			]
		  },
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%.",
			  "In\nfigure 7 we show the average portion of deltas that can be\nrepresented with a given amount of bits across all SPEC2006\nbenchmarks.",
			  "Although the coverage steadily increases with the amount\nof bits used, speedup has a distinct knee at around 7 bits.",
			  "In figure 8 we show the geometric mean of speedups as\na function of the number of deltas per table entry.",
			  "One of the main differences between DCPT and PC/DC is\nthat DCPT stores deltas, while PC/DC stores entire addresses\nin its GHB.",
			  "the deltas are usually quite small, fewer\nbits are needed to represent a delta than a full address."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov predictors | Proceedings of the 24th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/264107.264207",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the *Markov prefetcher.* This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching *multiple reference predictions* from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "Prefetching using Markov predictors for ISCA 1997 - IBM Research",
			"url": "https://research.ibm.com/publications/prefetching-using-markov-predictors--1",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems.",
			  "In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs.",
			  "The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://people.ucsc.edu/~hlitz/papers/crisp.pdf",
			"excerpts": [
			  " Prefetching using Markov predictors.\n",
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using Markov predictors.",
			  "Tempo-\nral prefetchers [ 8 , 49 , 54 , 119 , 122 ] track the temporal order of\ncache line accesses based on Markov prefetching [ 55 ] introducing\nsignificant storage overheads in the order of megabytes in con-\ntrast to CRISP",
			  " Runahead prefetchers [ 6 , 33 , 48 , 82  84 , 89 , 95 ] and\nhelper threads [ 21 , 23 , 24 , 70 , 73 , 74 , 110 , 117 , 123 , 124 ] prefetch\nirregular memory accesses as in linked-list traversals, however,\nthey introduce significant hardware complexity or consume sepa-\nrate SMT-threads [ 34 ] whereas CRISP requires only minimal hard-\nware modifications. Bra",
			  "CRISP can be\ncombined with these prior approaches to increase coverage by\nreducing the miss penalty of irregular memory accesses. T"
			]
		  },
		  {
			"title": "Perceptron-Based Prefetch Filtering - Engineering People Site",
			"url": "https://people.engr.tamu.edu/djimenez/pdfs/ppf_isca2019.pdf",
			"excerpts": [
			  "7.2\nLookahead Prefetchers\nUnlike spatial prefetchers, lookahead prefetchers take program order\ninto account when they make predictions. Shevgoor et al. propose\nthe Variable Length Delta Prefetcher (VLDP) [ 35 ], which correlates\nhistories of deltas between cache line accesses within memory pages\nwith the next delta within that page. SPP [ 2 ] and KPCs prefetching\ncomponent [ 36 ] are more recent examples of lookahead prefetchers.\n",
			  "Ishii et al. propose the Access Map Pattern\nMatching prefetcher (AMPM) [ 11 ], which creates a map of all ac-\ncessed lines within a region of memory, and then analyzes this map\non every access to see if any fixed-stride pattern can be identified\nand prefetched that is centered on the current access.",
			  "In a single core configuration, PPF increases performance by\n3.78% compared to the underlying prefetcher, SPP.",
			  "In a multi-core\nsystem running a mixes of memory intensive SPEC CPU 2017 traces,\nPPF saw an improvement of 11.4% over SPP for a 4-core system,\nand 9.65% for an 8-core system.",
			  "Michaud proposes the\nBest-Offset Prefetcher [ 34 ], which determines the optimal offset to\nprefetch while considering memory latency and prefetch timeliness.",
			  "DRAM-Aware\nAMPM (DA-AMPM) [ 32 ] is a variant of AMPM that delays some\nprefetches so they can be issued together with others in the same\nDRAM row, increasing bandwidth utilization.",
			  "Pugsley et al. pro-\npose the Sandbox Prefetcher [ 33 ], which analyzes candidate fixed-\noffset prefetchers in a sandboxed environment to determine which is\nmost suitable for the current program phase."
			]
		  },
		  {
			"title": "Building Efficient Neural Prefetcher",
			"url": "https://www.memsys.io/wp-content/uploads/2023/09/3.pdf",
			"excerpts": [
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using markov predictors. In\n*Proceedings of the 24th annual international symposium on Computer architecture* .\n252",
			  "chun Kim, Seth H Pugsley, Paul V Gratz, AL Narasimha Reddy, Chris Wilker-\nson, and Zeshan Chishti. 2016. Path confidence based lookahead prefetching.\nIn *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture*\n*(MICRO)* . IEEE, 112."
			]
		  },
		  {
			"title": "Arsenal of Hardware Prefetchers",
			"url": "https://www.arxiv.org/pdf/1911.10349v1",
			"excerpts": [
			  "ature Path Prefetcher** (SPP) [ 5 ] stores the stride pat-\nterns in a compressed form in the signature table (ST). Each\nentry in the ST is used to index into the pattern table (PT),\nwhich is used to predict the next stride and also contains the\nconfidence for the current prefetch. The signature is then up-\ndated with the latest stride and is used to recursively lookup\nthe PT to predict more strides. This goes on until the confi-\ndence, which is multiplied with the last prefetch confidenc"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching For Linked Data ...",
			"url": "https://www.scribd.com/document/861770035/9",
			"excerpts": [
			  "This paper presents a framework for jump-pointer prefetching (JPP) aimed at improving the performance of linked data structures (LDS) by ...Read more"
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "Jump pointers, which provide direct access to non-adjacent nodes, can be used for prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures",
			  "New technique: Jump Pointer Prefetching",
			  "Creates parallelism",
			  "Hides arbitrary latency",
			  "Choice of implementation: software, hardware, cooperative",
			  "Problem: Pointer chasing latency",
			  "pointer loads",
			  "Jump pointer prefetching:\nOverlap pointer loads with each other anyway!"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  " uses the underlying data of the application, and provides an 11.3% speedup using no additional processor state. By adding less than 1/2% space 2 overhead to the second level cache, performance can be further increased to 12.6% across a range of \"real world\" applications.\n*",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems.",
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "There are a number of ways to\nidentify \"likely\" addresses.",
			  "Roth\net al.\nintroduced dependence-based techniques for capturing\nproducer-consumer load pairs [ 12].",
			  "his paper investigates a technique that predicts addresses in\npointer-intensive applications using a hardware only technique with\nno built-in biases toward the layout of the recursive data struc-\nSection Title: A state",
			  "ability to \"run ahead\" of an application has been shown to be a re-\nquirement for pointer-intensive applications [12], which tradition-\nally do not provide sufficient computational work for masking the\nprefetch latency.",
			  "me hybrid prefetch engines [13] do have the\nability to run several instances ahead of the processor, but require\napriori\nknowledge of the layout of the data structure, and in some\ncases, the traversal order of the structu",
			  "es. Content-based prefetching works by examining the content of data as it is moved from memory to the caches. Data values that are likely to be addresses are then translated and pushed to a prefetch buffer. Con",
			  "N\nIn early processor designs, the performance of the processor and\nmemory were comparable, but in the last 20 years their relative\nperformances have steadily diverged [4], with the performance im-\nprovemen"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  "Content-Directed Data Prefetching, a data*\nprefetching architecture that exploits the memory allocation used\nby operating systems and runtime systems to improve the perfor-\nmance of pointer-intensive applications constructed using modem\nlanguage systems. This technique is modeled after conservative\ngarbage collection, and prefetches \"likely\" virtual addresses ob-\nserved in memory references",
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "Prefetching:- - CSE IITM",
			"url": "https://www.cse.iitm.ac.in/~pritam/prefetching.pdf",
			"excerpts": [
			  "**PRITAM MAJUMDER, PACE LAB, CSE DEPT., IIT MADRAS** **9**\nCollins Jamison\nCalder Brad\nTullsen Dean M\nPointer Cache Assisted Prefetching\n2002\nMICRO"
			]
		  },
		  {
			"title": "CS 473 Homework 3 (due 12/20/03) Fall 2004",
			"url": "https://jeffe.cs.illinois.edu/teaching/473/hw3final.pdf",
			"excerpts": [
			  "Storing a complete binary search tree in the van Emde Boas layout allows us to perform any search in O(logB N) memory operations in the cache-oblivious model.Read more"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://diglib.eg.org/bitstreams/c41b2a27-076a-4252-a346-584b2deac5fd/download",
			"excerpts": [
			  "Yoon et al. [YM06] proposed a cache-oblivious. BVH layout for collision detection which applied to a k- d tree for ray tracing, resulted ...Read more",
			  "The common DFS layout performed worst for all node layouts in both memory areas. Excluding layouts that use statistics the equally simple to construct BFS ...Read more",
			  "BVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and p",
			  "reelet based DFS/BFS (TDFS/TBFS):** A treelet is a\nconnected sub-tree of a BVH. For this layout treelets of\nnodes that were accessed above a certain threshold are\nbuilt",
			  "The internal memory layout of a treelet can be cho-\nsen freely. By always adding nodes just to the front or the\nback of the merge queue we automatically obtain a treelet\nin DFS or BFS order. Finally the node order of the whole\ntree is obtained by lining up the nodes of all treelets.",
			  "A32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines)",
			  "A16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache line",
			  "**Table 1:** *Scenes used for benchmarking",
			  "ivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and par",
			  " cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.",
			  "**5.1. Node Layouts**",
			  "e classic BVH node data structure stores a bounding vol-\nume along with pointers to its children. We follow Aila et\nal. [ AL09 ], i.e., a node does not store its bounding box, but\nthe bounding boxes of its children. Both children are fetched\nand tested together, which is more efficient for GPUs due\nto increased instruction level parallelism and allows rough\nfront to back traversal. Depending on the data layout, the\nsize of such a node is at least 56 bytes (2 float values for\nminimum/maximum per dimension and child plus pointers).",
			  "*AoS:** 64 bytes, including 8 bytes padding (fitting 2 nodes\nin one 128B cache line",
			  "*SoA32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines",
			  "*SoA16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache lin",
			  "We also analyzed an SoA8 layout which fitted 16 nodes in\n7 cache lines. As it consistently performed much worse than\nthe other layouts, we excluded it from our experiments.",
			  "**5.2. Tree Layouts**",
			  "A tree layout describes how nodes are grouped in memory.",
			  "We analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:",
			  "DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf",
			  "BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches",
			  "vEB):** A cache-oblivious tree layout\n[ vEB75 ]",
			  "ext we describe our two proposed layouts depending on\nnode access statistics which use a threshold *p* ",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "We analyzed six different tree layouts. The first four layouts are two common layouts and two cache-efficient layouts. We further propose two more layouts.Read more"
			]
		  },
		  {
			"title": "[2209.09166] Cache-Oblivious Representation of B-Tree Structures",
			"url": "https://arxiv.org/abs/2209.09166",
			"excerpts": [
			  "CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout with packed memory array.",
			  "In the use of the vEB layout mostly search complexity was considered, so far.",
			  "We describe how to build an arbitrary tree in vEB layout if we can simulate its depth-first search.",
			  " The data structure allows searching with an optimal I/O complexity \\mathcal{O}(\\log_B{N}) and is stored in linear space. ",
			  "ave an amortized I/O complexity \\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)",
			  " amortized time complexity \\mathcal{O}(S\\cdot\\log^2 N) , where S is the size of the subtree and N the size of the whole stored tree. R",
			  "Rebuilding an existing subtree saves the multiplicative \\mathcal{O}(\\log^2 N) in both complexities if the number of vertices on individual tree levels is not changed; it is paid only for the inserted/removed vertices otherwise.",
			  "We propose a general data structure CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout ...Read more"
			]
		  },
		  {
			"title": "Cache-oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/paper.pdf",
			"excerpts": [
			  "The van Emde Boas layout proceeds recursively. Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree. Suppose first that *h* is\na power of 2. Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + 1. This breaks the tree into the *top recursive subtree* *A* of\nheight *h/* 2, and several *bottom recursive subtrees* *B* 1 , *B* 2 , . . . , *B* ** , each of height *h/* 2",
			  "eaf nodes have the same number of children, then the recursive subtrees all\nhave size roughly\n**\n*N* , and ** is roughly\n**\n*N* . The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2",
			  " memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail* . Each level of detail is a partition of the tree into disjoint recursive\nsubtrees. In the finest level of detail, 0, each node forms its own recursive subtree. In\nthe coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subt",
			  "ur layout is a modified version of Prokops layout for a complete binary tree\nwhose height is a power of 2 [40, pp. 6162]. We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48]. ",
			  "One useful consequence of our rounding scheme is the following.",
			  "Lemma 2.1. *At level of detail* *k* *all recursive subtrees except the one containing*\n ... \ntree, even though the relative order of elements changes very little. Instead we use a\npacked-memory array from Section 2.3 to store the van Emde Boas layout, allowing\nus to make room for changes in the tree.",
			  "n additional technical complication arises in maintaining pointers to nodes that\nmove during an update. We search in the top tree by following pointers from nodes\nto their children, represented by indices into the packed-memory array. When we\ninsert or delete an element in the packed-memory array, an amortized *O* (log 2 *N* ) ele-\nments move. Any element that is moved must let its parent know where it has go",
			  "Thus, each node must have a pointer to its parent, and so each node must also let\nits children know where it has moved.",
			  " nodes that move can have\n*O* (log 2 *N* ) children scattered throughout the packed-memory array, each in separate\nmemory blocks. Thus an insertion or deletion can potentially induce *O* (log 2 *N* ) mem-\nory transfers to update these disparately located pointers. We can afford this large\nCACHE-OBLIVIOUS B-TREES\n11\ncost in an amortized sense by adding another level of (log *N* ",
			  "verall structure of our cache-oblivious B-tree therefore has three levels. The\ntop level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array. The middle level is a collection o",
			  "The van Emde Boas layout proceeds recursively.",
			  "Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree.",
			  "Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + ",
			  "The memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail",
			  "Each level of detail is a partition of the tree into disjoint recursive\nsubtrees.",
			  "In the finest level of detail, 0, each node forms its own recursive subtree.",
			  "the coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subtree.",
			  "The key property of the van Emde Boas layout is that, at any\nlevel of detail, each recursive subtree is stored in a contiguous block of memory.",
			  "cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.",
			  "op level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array",
			  "The middle level is a collection of",
			  "We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.\nI",
			  " The cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors."
			]
		  },
		  {
			"title": "Cache Oblivious Search Trees via Binary ...",
			"url": "https://www.cs.au.dk/~gerth/papers/soda02.pdf",
			"excerpts": [
			  "The basic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22]. The static structures are maintained\nby global rebuilding, i.e. they are rebuilt each time the\ndynamic tree has doubled or halved in size.",
			  "ion.\nFor storing n elements, our proposal uses (1 +  ) n\ntimes the element size of memory, and performs searches\nin worst case O (log B n ) memory transfers, updates\nin amortized O ((log 2 n ) / ( B )) memory transfers, and\nrange queries in worst case O (log B n + k/B ) memory\ntransfers, where k is the size of the output.",
			  " For random searches, we\ncan expect the top levels of the trees to reside in cache.\nFor the remaining levels, a cache fault should happen at\nevery level for the BFS layout, approximately at every\nsecond level for the DFS layout (most nodes reside in the\nsame cache line as their left child), and every (log B n )\nlevels for the van Emde Boas layout.",
			  "In the last part of this paper, we try to assess\nmore systematically the impact of the memory layout\nof search trees by comparing experimentally the effi-\nciency of the cache-oblivious van Emde Boas layout with\na cache-aware layout based on multiway trees, and with\nclassical layouts such as Breath First Search (BFS),\nDepth First Search (DFS), and inorde",
			  " trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "Inside main memory, the BFS is best, but looses by\na factor of five outside. The tree optimized for page size\nis the best outside main memory, but looses by a factor\nof two inside. Remarkably, the van Emde Boas layout\nis on par with the best throughout the range.",
			  "4.3\nConclusion.\nFrom the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "It also appears that in the area of search trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  ".\nFigure 4 compares the time for random searches in\nimplicit layouts. For sizes up to cache size ( n = 2 16 ), it\nappears that the higher instruction count for navigating\nin an implicit layout dominates the running times: most\ngraphs are slightly higher than corresponding graphs\nin Figure 3, and the van Emde Boas layout (most\ncomplicated address arithmetic) is the slowest while the\nBFS layout (simplest address arithmetic) is fastest. ",
			  "In particular, our data structure avoids the\nuse of weight balanced B -trees, and can be implemented\nas just a single array of data elements, without the use of\npointers.",
			  "sic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22",
			  " For storing n elements, our data structure uses\n(1 +  ) n times the element size of memory. ",
			  "Searches are\nperformed in worst case O (log B n ) memory transfers,\nupdates in amortized O ((log 2 n ) / ( B )) memory trans-\nfers, and range queries in worst case O (log B n + k/B )\nmemory transfers, where k is the size of the output.",
			  "From the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "he van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "The basic idea of our data structure is to maintain a\ndynamic binary tree of height log n + O (1) using existing\nmethods, embed this tree in a static binary tree, which\nin turn is embedded in an array in a cache oblivious\nfashion, using the van Emde Boas layout of Prokop.",
			  "he\nvan Emde Boas layout , was proposed by Prokop [19,\nSection 10.2], and is related to a data structure of\nvan Emde Boas [21, 22]."
			]
		  },
		  {
			"title": "Cache-Oblivious Dynamic Search Trees Zardosht Kasheff",
			"url": "https://people.csail.mit.edu/bradley/papers/Kasheff04.pdf",
			"excerpts": [
			  "The tree is laid out in memory in a recursive\nfashion. Let *h* be the height of the binary tree. For simplicity, assume *h* is a power of 2. Let\n*N* be the number of nodes in the tree. We divide the tree into two sections. The first section\nis the top half containing a subtree, sharing the same root as the tree, of height *h/* 2 with\n**\n*N* nodes. The second section is the bottom half containing 2 *h/* 2 subtrees of height *h/* 2,\neach containing about\n**\n*N* nodes. This represents subtree *A* in Figure 2-3. The idea is to\n19\nfirst layout the top half recursively. Then layout the remaining 2 *h/* 2 subtrees recursively in\norder. This represents subtrees *B* 1 *, B* 2 *, . . . , B* *l* in Figure 2-3. In memory, the entire subtree\n*A* would be laid out first, followed by *B* 1 *, . . . , B* *l* . We assume the binary tree is full and\nbalanced. If *h* is not a power of 2, the bottom half of the tree is chosen such that its height\nis a power of 2. Figure 2-3 shows the layout of a binary tree with height 5.\n",
			  "The locations of the children of node *i* are 2 *i* and 2 *i* + 1. Thus, the\nlocation of children may be implicitly calculated. Implicit calculations of children makes the\n35",
			  "The tree is represented in memory as an array. The value at location *i* of the array\ncorresponds to some node of the tree. We need a way of computing the location of the left\nand right children of node *i* . One solution is to have the array store pointers, but pointers\ncost space. Instead, we wish to have an array such that the root of the tree is the first\nelement of the array, and for a given node located at array location *i* , the locations of the\nnodes two children are easily found. This chapter provides details.",
			  "ch that the tree\nmay be traversed with *O* (1+log *B* ( *N* )) memory transfers, which is asymptotically optimal [7].\n**"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  "The Eurographics Association and Blackwell Publishing 2006.",
			  "e compare our cache-efficient layouts with other layouts in the*\n*context of collision detection and ray tracing.",
			  "r benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applicatio",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  "he resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmost.",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C",
			  "Our layout*\n*computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the",
			  "The COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH.",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime appli",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applic",
			  ".\nHavran analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.2.\n**Layouts of geometric meshes:** Many algor",
			  "8.2. Comparison with Cache-Oblivious Mesh Layouts",
			  "We have tested the performance of the OBB-tree collision\n ... \ngraph. The edge creation methods for BVHs described in\nYoon et al. [YLPM05] do not adequately represent access\npatterns of the travers",
			  "Our greedy algorithm is\nbased on greedy heuristics to compute cache-coherent layouts\nbased on parent-child locality. T",
			  "There are several areas for future work. We would like to\nextend our probability formulation that predicts runtime data\naccess patterns of collision queries to consider other proximity\nqueries such as minimum separation distance. W",
			  " The Eurographics Association and Blackwell Publishing 2006.\n",
			  "an analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.",
			  "outs of geometric meshes:** Many algorithms and repre-\nsentations have been proposed to compute coherent layouts for\nspecialized applications. Rendering sequences (e.g., triangle\nstrips) [Dee95,Hop99] are used to improve rendering through-\nput by optimizing the vertex cache hits in the GPU. Isenburg\nand Gumhold [IG03] propose processing sequences, includ-\ning streaming meshes [IL04], as an extension of rendering se-\nquences for large-data processing. In these cases, global mesh\naccess is restricted to a fixed traversal order. Many algorithms\nuse space filling curves [Sag94] to compute cache-friendly\nlayouts of volumetric grids or height fields. These layouts\nare widely used to improve performance of image process-\ning [VG91] and terrain or volume visualization [PF01,L",
			  "We introduce a new probabilistic\nmodel to predict the runtime access patterns of BVHs based on\nlocalities.",
			  "ecifically, we utilize two types of localities during\ntraversal of a BVH: parent-child and spatial localities between\nthe accessed node",
			  "In this section, we define two localities that are used to com-\npute a cache-efficient layout of a BV",
			  "to achieve this\ngoal, we recursively compute the clusters. We first decompose\nthe BVH into a set of clusters and recursively decompose each\ncluster. In this case, the cache block boundaries can lie any-\nwhere within a layout that corresponds to the nodes of these\nclusters. Therefore, we need to compute a cache-efficient or-\ndering of the clusters computed at each level of recursion.\nOur algorithm has two different components that handle\nparent-child and spatial localities. In particular, the first part\nof our algorithm decomposes a BVH into a set of clusters that\nminimize the cache misses for parent-child locality. The clus-\nters are classified as a root cluster and child clusters. The root\ncluster contains the root node of the BVH and child clusters\nare created for each node outside the root cluster whose par-\nent node is in the root cluster (see the middle image in Fig.\n4). The second part of the algorithm computes an ordering of\nthe clusters and stores the root cluster at the beginning of the\nordering. The ordering of child clusters is computed by con-\nsidering their spatial locality. Then, we can merge two child\nclusters if it can further decrease the size of the working set.\nWe recursively apply this two-fold procedure to compute an\nordering of all the BVs in the BVH"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://diglib.eg.org/bitstream/handle/10.1111/cgf142662/v40i2pp683-712.pdf",
			"excerpts": [
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged.",
			  "he latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order",
			  " authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH. The key difference is the use of two different types of\nclusters to further reduce bandwidth and cache misses.",
			  "s first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit withi",
			  "In this report, we review the basic principles of bounding volume hierarchies as well as advanced state of the art methods with a focus on the construction and ...Read more"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies | Request PDF",
			"url": "https://www.researchgate.net/publication/220507680_Cache-Efficient_Layouts_of_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "Yoon and Manocha [YM06] proposed a node layout algorithm known as cache-oblivious BVH (COLBVH) that recursively decomposes clusters of nodes and works without prior knowledge of the cache, such as the block size.",
			  "In initialization, each node is assigned the probability that the node is accessed, given that the cluster's root is already accessed."
			]
		  },
		  {
			"title": "CacheEfficient Layouts of Bounding Volume Hierarchies - Yoon - 2006 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2006.00970.x",
			"excerpts": [
			  "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			  "Section Title: Cache-Efficient Layouts of Bounding Volume Hierarchies > Abstract",
			  "*We present a novel algorithm to compute cache-efficient layouts of bounding volume hierarchies (BVHs) of polygonal models. Our approach does not make any assumptions about the cache parameters or block sizes of the memory hierarchy. We introduce a new probabilistic model to predict the runtime access patterns of a BVH. Our layout computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the number of cache misses and the size of the working set. Our algorithm also works well for spatial partitioning hierarchies including kd-trees. We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of millions of triangles. We compare our cache-efficient layouts with other layouts in the context of collision detection and ray tracing. In our benchmarks, our layouts consistently show better performance over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the underlying algorithms or runtime applications.*",
			  "Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Hierarchy and Geometric Transformations"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "The VEB layout is complex\nto setup and maintain for trees and difficult to apply to graphs in\ngeneral.",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "Figure 2 shows graphically how this might be done. Assume\nan algorithm *P* *i* that aims to copy a tree while traversing it, into\nblocks that fit into the cache at level *i* . Using breadth first search,\nit can discover the entire subtree that fits into a block at level *i*",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "A third class of techniques (including the one in this paper) are\nused at runtime. One approach is to control memory allocation.",
			  "The van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7].",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "ble 1.** Cachegrind miss rates",
			  "Figure 4.** Binary Search Tree Performan",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.",
			  "For storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "The source code of the programs, our scripts and tools, and the data we present here are available online under ftp.brics.dk/RS/01/36/Experiments/.",
			  "Section Title: Cache Oblivious Search Trees via Binary Trees of Small Height > Abstract",
			  "Content:\nWe propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.\nFor storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.\nThe basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.\nWe also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "Van Emde Boas tree",
			"url": "https://en.wikipedia.org/wiki/Van_Emde_Boas_tree",
			"excerpts": [
			  "A van Emde Boas tree also known as a vEB tree or van Emde Boas priority queue, is a tree data structure which implements an associative array with m-bit ...Read more"
			]
		  },
		  {
			"title": "The Cost of Cache-Oblivious Searching",
			"url": "https://www3.cs.stonybrook.edu/~bender/newpub/2011-algorithmica-BenderBrFa-co-searching.pdf",
			"excerpts": [
			  "atic cache-oblivious search tree is built as follows: Embed a complete binary tree with\nN nodes in memory, conceptually splitting the tree at half its height, thus obtaining (\n\nN ) subtrees each\nwith (\n\nN ) nodes. Lay out each of these trees contiguously, storing each recursively in memory. This type\nof recursive layout is commonly referred to in the literature as a van Emde Boas layout because it is remi-\nniscent of the recursive structure of the van Emde Boas tree [37,38]. The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26",
			  " The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26]. ",
			  "We present the following results:",
			  "We present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.",
			  " Intuitively, the improvement of uneven splitting, as compared to the even splitting in the standard van\nEmde Boas layout, is likely to be due to the generation of a variety of subtree sizes at each recursive\nlevel of the layout. Such a variety will on any search path reduce the number of subtrees that can have\nparticularly bad sizes compared to the block size B",
			  "Finally, we demonstrate that it is harder to search in the cache-oblivious model than in the DAM model.\nPreviously the only lower bound for searching in the cache oblivious model was the log B N lower bound\nfrom the DAM model. We prove a lower bound of lg e log B N memory transfers for searching in the\naverage case in the cache-oblivious model.",
			  "\n We then present a class of generalized van Emde Boas layouts that optimizes performance through\nthe use of uneven splits on the height of the tree. For any constant  > 0, we optimize the layout\nachieving a performance of [lg e +  + O (lg lg B/ lg B )] log B N + O (1) expected memory transfers. ",
			  "In this section we give a tight analysis of the cost of searching in a binary tree stored using the van Emde. Boas layout [31]. As mentioned earlier, in the vEB ...Read more",
			  "s.\nWe present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.\n ",
			  "in the vEB layout, the tree is split evenly by height, except for\nroundoff. Thus, a tree of height h is split into a top tree of height  h/ 2  and bottom tree of height  h/ 2  . It\nis known [15,22] that the number of memory transfers for a search is 4 log B N in the worst case ; we give a\nmatching configuration showing that this analysis is tight. We then consider the average-case performance\nover all starting positions of the tree in memory, and we show that the expected search cost is 2(1 +\n3 /\n\nB ) log B N + O (1) memory transfers, which is tight within a 1 + o (1) factor. We assume that the data\nstructure begins at a random position in memory; if there is not enough space, then the data structure\nwraps around to the first location in memory.\nA",
			  "trees.\nThe generalized vEB layout is as follows: Suppose the complete binary tree contains N  1 = 2 h  1\nnodes and has height h = lg N . Let a and b be constants such that 0 < a < 1 and b = 1  a . Conceptually\nwe split the tree at the edges below the nodes of depth  ah  . This splits the tree into a top recursive subtree\nof height  ah  , and k = 2  ah  bottom recursive subtrees of height  bh  . Thus, there are roughly N a bottom\nrecursive subtrees and each bottom recursive subtree contains roughly N b nodes. We map the nodes of the\ntree into positions in the array by recursively laying out the subtrees contiguously in memory. The base case\nis reached when the trees have one node, as in the standard vEB layout.\nWe",
			  "We find the values of a and b that yield a layout whose memory-transfer cost is arbitrarily close to\n[lg e + O (lg lg B/ lg B )] log B N + O (1) for a = 1 / 2   and large enough N . We focus our analysis on the first\nlevel of detail where recursive subtrees have size at most the block size B . In our analysis memory transfers\ncan be classified in two types. There are V path-length memory transfers , which are caused by accessing\ndifferent recursive subtrees in the level of detail of the analysis, and there are C page-boundary memory"
			]
		  },
		  {
			"title": "Memory Layouts for Binary Search",
			"url": "https://cglab.ca/~morin/misc/arraylayout/",
			"excerpts": [
			  "`veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature.",
			  "In many settings B-trees (with a properly\nchosen value of B) are best. In others, the Eytzinger layout\nwins. In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "In many settings B-trees (with a properly\nchosen value of B) are best.",
			  "In others, the Eytzinger layout\nwins.",
			  "In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "Which of these array memory layouts is fastest?",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "For an example, consider the following two graphs, generated\nby running the same code on two different Intel machines.",
			  "In\nthe left graph, the Eytzinger layout is almost as slow as a\nplain sorted array while the van Emde Boas and B-tree layouts\nare more than twice as fast.",
			  "In the right graph, the Eytzinger layout and b-tree are the\nfastest, the sorted array is still the slowest, and the vEB layout\nis somewhere in betweeen (for array sizes).",
			  "veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature"
			]
		  },
		  {
			"title": "Cache oblivious search trees via binary trees of small height | Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms",
			"url": "https://dl.acm.org/doi/10.5555/545381.545386",
			"excerpts": [
			  "Section Title: Cache oblivious search trees via binary trees of small height",
			  "ng Brodal](# \"Gerth Stlting Brodal\") Gerth Stlting Brodal\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81409594931) , [Rolf Fagerberg](# \"Rolf Fagerberg\") Rolf Fagerberg\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100166398) , [Riko Jacob](# \"Riko Jacob\") Riko Jacob\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100438116) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386)\n[SODA '02: Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms](/doi/proceedings/10.5555/545381)\nPages 39 - 48\nPublished : 06 January 2002 [Publication History](#) To view Publication History, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386) [](# \"Check for updates on crossmark\")\n... citation ... Downloads\n[](#) To get citation alerts, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F54",
			  "For storing *n* elements, our proposal uses (1 + ) *n* times the element size of memory, and performs searches in worst case *O* (log *B* *n* ) memory transfers, updates in amortized *O* ((log 2 *n* )/( *B* )) memory transfers, and range queries in worst case *O* (log *B* *n + k/B* ) memory transfers, where *k* is the size of the output.The ",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log *n+O* (1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "We also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "[PDF] Cache Oblivious Search Trees via Binary Trees of Small Height | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache-Oblivious-Search-Trees-via-Binary-Trees-of-Brodal-Fagerberg/da35c4651414b03abe079b5c8454948c59f372c0",
			"excerpts": [
			  "Cache Oblivious Search Trees via Binary Trees of Small Height",
			  "A version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds is proposed, and can be implemented as just a single array of data elements, without the use of pointers.",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/",
			"excerpts": [
			  "We present a novel algorithm to compute cache-efficient layouts of\nbounding volume hierarchies (BVHs) of polygonal\nmodels.",
			  "We does not make any\nassumptions about the cache parameters or block sizes of the memory hierarchy.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH.",
			  "Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%\nwithout any modification of the underlying algorithms or runtime\napplications.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing.",
			  "Paper: [Cache-Efficient Layouts of Bounding Volume Hierarchies](CELBVH.pdf) , Computer graphics forum (Eurographics), volume 25, issue 3, 2006, pp. 507-516",
			  "Section Title: by [Sung-Eui Yoon](http://jupiter.kaist.ac.kr/~sungeui/) and [Dinesh Manocha](http://www.cs.unc.edu/~dm/) .",
			  "llision Detection between Hugo and 1M Power Plant Models:** The hugo robot model is placed inside the power plant model, whose\noverall shape is shown on the right. We are able to achieve 35%--2600%\nperformance improvement in collision detection by using our cache-efficient\nlayouts of the OBBTree over other tested layouts.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH. Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing. In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%",
			  "without any modification of the underlying algorithms or runtime\napplications.",
			  "We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of\nmillions of triangles.",
			  " In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%"
			]
		  },
		  {
			"title": "[PDF] CacheEfficient Layouts of Bounding Volume Hierarchies | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache%E2%80%90Efficient-Layouts-of-Bounding-Volume-Yoon-Manocha/77fdc59d6f28a3eb5da7c9fc134205a8f498f306",
			"excerpts": [
			  "A novel algorithm to compute cacheefficient layouts of bounding volume hierarchies (BVHs) of polygonal models and a new probabilistic model to predict the runtime access patterns of a BVH is introduced",
			  "Published in Computer graphics forum 1 September 2006\n",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/",
			"excerpts": [
			  "Demaine, and Martin Farach-Colton, Cache-Oblivious B-Trees, SIAM Journal on Computing, volume 35, number 2, 2005, pages 341358. Abstract: This paper presents ...Read more"
			]
		  },
		  {
			"title": "Concurrent Cache-Oblivious B-Trees - People",
			"url": "https://people.csail.mit.edu/bradley/papers/BenderFiGi05.pdf",
			"excerpts": [
			  "e first review the performance models used to analyze cache-\nefficient data structures, and then review three variations on serial\ncache-oblivious B-trees.",
			  "ernal-memory data structures, such as B-trees, are tradition-\nally analyzed in the *disk-access model (DAM)* [1], in which internal\nmemory has size *M* and is divided into blocks of size *B* , and exter-\nnal memory (disk) is arbitrarily larg",
			  "Performance in the DAM\nmodel is measured in terms of the number of block transfers.",
			  "Thus\nB-trees implement searches asymptotically optimally in the DAM\nmodel.",
			  "The *cache-oblivious model* [15,26] is like the DAM model in that\nthe objective is to minimize the number of data transfers between\ntwo levels.",
			  "A CO B-tree [8] achieves nearly optimal locality of reference at\nevery level of the memory hierarchy. It optimizes simultaneously\nfor first- and second-level cache misses, page faults, TLB misses,\ndata prefetching, and locality in the disk subsystem."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*",
			  "To mitigate\nthis, we modified the memory allocator implementation in\nEmbree [8] to allocate BVH in a single 1GB hugepage, which\nled to an end-to-end rendering speedup of 6.4%.",
			  "Divergent memory access is a major source of latency.",
			  "d stall* cycles reveals that most of the memory stalls are\ncaused by TLB misses. This is possible due to the virtually-\nindexed physically-tagged nature of L1 data cache, where TLB\nmisses can delay tag access latency and thereby the hit latency\nof the cache.",
			  "Ray sorting improves memory\ndivergence by mapping coherent rays to adjacent threads, and\nthis is evidenced by the fewer number of L1 sectors per\nrequest.",
			  "IMT execution divergence** severely limits per-\nformance of software-based GPU ray tracing. For our CUDA\nworkload, 44% of the threads in a single SIMD unit is inactive\nduring execution. This is because threads mapped to divergent\nrays unavoidably execute divergent branches or terminate early\nduring traversal, causing them to become inactive due to the\nlockstep execution of SI"
			]
		  },
		  {
			"title": "OPTIMIZING QUERY TIME IN A BOUNDING VOLUME ...",
			"url": "https://benedikt-bitterli.me/bvh-report.pdf",
			"excerpts": [
			  "To reduce the likelihood of this occurring, we implemented\na special tree layout from literature, the van Emde Boas\nordering[ **?** ], which is a cache-oblivious layout designed to\nkeep certain subtrees close together in memory.",
			  "The van Emde Boas ordering of a single node is the node\nitself.",
			  "The van Emde Boas ordering of a tree *T* of depth *d* *T*\nis the van Emde Boas ordering of the top subtree ending at\ndepth ** *d* *T*\n2 ** followed by the van Emde Boas ordering of all\nchild subtrees rooted at depth ** *d* *T*\n2 ** + 1 .\nIn",
			  ".\nInformally, this guarantees that any subtree is likely to\nbe stored in a contiguous memory segment; in other words,\nthe traversal algorithm is likely to work in a locally contigu-\nous memory segment for many traversal steps before mak-\ning a large jump through memory.",
			  "Although this should increase performance in theory, in\npractice, no performance improvement can be observed. It\nappears that TLB misses do not play a significant role in the\ntraversal performance."
			]
		  },
		  {
			"title": "Cache-Oblivious Algorithms and Data Structures",
			"url": "https://erikdemaine.org/papers/BRICS2002/paper.pdf",
			"excerpts": [
			  "A comparison of cache aware and cache oblivious static search trees using program in- strumentation. In Experimental Algorithmics: From Algorithm Design to.Read more"
			]
		  },
		  {
			"title": "Cache-oblivious algorithms and data structures",
			"url": "https://scispace.com/pdf/cache-oblivious-algorithms-and-data-structures-jjcrutokhi.pdf",
			"excerpts": [
			  "Prokop in [60] proposed static cache-oblivious search trees with search cost\nO (log B N ) I/Os, matching the search cost of standard (cache-aware) B-trees [17].",
			  "\nThe search trees of Prokop are related to a data structure of van Emde Boas [67,\n68], since the recursive layout of a search tree generated by Prokops scheme re-\nsembles the layout of the search trees of van Emde Boas.",
			  "The constant in the\nO (log B N ) search cost was studied in [21], where it is proved that no cache-\noblivious algorithm can achieve a performance better than log 2 e  log B N I/Os,\ni.e. a factor  1 . 44 slower than a cache-aware algorithm.",
			  "Dynamic B-trees were first presented by Bender et al. [22] achieving searches\nin O (log B N ) I/Os and updates requiring amortized O (log B N ) I/Os.",
			  "A cache-oblivious dictionary based on exponential search trees was presented\nin [19]."
			]
		  },
		  {
			"title": "Interactive Visualization and Collision Detection using ...",
			"url": "https://sgvr.kaist.ac.kr/~sungeui/thesis/phd_thesis_yoon_2005.pdf",
			"excerpts": [
			  "We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layou",
			  "erall, our approach offers the following\nbenefits:\n1. **Generality:** Our algorithm is general and applicable to all kind of BVHs. It\ndoes not require any knowledge of cache parameters or block sizes of a memory\nhierarchy.\n159\n2. **Applicability:** Our algorithm does not require any modification of BVH-based\nalgorithms or the runtime application. We simply compute cache-oblivious lay-\nouts of BVHs without making any assumptions about the applications.\n3. **Improved performance:** Our layouts reduce the number of cache misses during\ntraversals of BVHs. We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layouts. Main improve"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.\nNext we describe our ",
			  " Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  "In global memory we have\nachieved a runtime reduction by 1%6%. We gained a 30%\n40% runtime reduction compared to the baseline in global\nmemory when the BVH is stored in texture memory simi-\nlar to Aila et al. [ ALK12",
			  " **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  "*Address Cluster (AC):** A continuous address space that can be\nreferenced by a small pointer. If the original BVH can address\n2 *n* nodes, the maximum size of an AC is 2 *n* */* 2 .",
			  "We achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address spac",
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "4. Two-Level Clustering",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft",
			  " consider that storing *P* *Le ft* allows less nodes per\ncache line. The size *|* *P* *|* can be up to 4 bytes, which is small\ncompared to conventional BVH encodings used by previous wo",
			  "However, a node pair can be quantized to as small as 8 bytes using\nDFL (Sec. 6.1 ), whereas clustering would require up to 12 bytes.",
			  "C* can maintain the node size of the depth-first layout (or\neven reduce it), while the *CC* reorders nodes within the same *AC* for\nthe best cache utilization.",
			  "5. Algorithm",
			  "In order to effectively reduce the working set, we must carefully\nselect the nodes for each cluster. We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ].",
			  " We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ]. They attempt to order the\nnodes according to the most likely traversal path based on *parent-*\n*child* and *spatial locality* .",
			  "ssuming that all traversal paths start at the root of the tree, their\n*COLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next",
			  "4.1. Glue Nodes",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  "lue nodes* only generate bandwidth when the\nchild node is traversed, not when accessing the parent node.",
			  "Glue Nodes",
			  "7.2. Bandwidth Analysis",
			  "*A novel node layout and addressing scheme*",
			  " achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address space. We\nthen choose the order of nodes inside these clusters to maxi-\nmize the cache line locality. We introduce a new node type to\nreference address-space changes during traversal. This keeps the\nnode sizes uniform, which is more suited for a fixed function\nhardware.",
			  "ct node ordering** schemes can eliminate a few child point-\ners from the BVH. Depth-first layout (DFL) places the left child\ndirectly after the parent node, therefore only the the right pointer\nis required. Alternatively, two sibling nodes can be stored sequen-\ntially [ AL09 ]. Besides compression, these layouts can also improve\ncache locality, since child nodes are often tested together following\nthe parent during traversal. Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent",
			  "ubtree partitioning** methods first decompose the BVH into clus-\nters of nodes, each containing one or more subtrees. By optimizing\nthe node order for multiple traversal paths it can further improve\ncache locality. Moreover, the size of the child pointers within clus-\nters may be reduced. This optimization was presented for BSP trees\nby Havran [ Hav97 ]. Gil and Itai [ GI99 ] showed that cache local-\nity for tree traversal can be significantly improved if the clusters\nof nodes are generated top-down, by greedily merging the children\nwith the highest probability. Yoon et al. [ YM06 ] applied this theory\nto kd-tree based ray travers",
			  "**8-byte Internal Node:**",
			  "6 x 6 bits per plane",
			  "6 bits",
			  "2 bits",
			  "2 x 10 bits",
			  "Figure 3: A 2D illustration of our quantized storage of sibling nodes\nwith parent-plane sharing (top). The layout of our internal nodes\n(bottom). We store 2 bits to indicate leaves, one low-precision\npointer per child, and 6 plane offsets (z-axis not shown). Finally,\nour reuse mask is set to 1 if the corresponding plane belongs to the\nleft child.",
			  "Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent.\n**",
			  " compare our method (green) with the standard ODFL (gray).\nWhen scaling the L2 cache with a fixed L1, we see a different\ntrend: as the capacity of L2 increases, the reduction achieved by our\nmethod slowly diminishes. Our explanation is that the outstanding\nmisses from L2 become less and less coherent and since more of the\nfrequently traversed nodes reside inside L2, the clustering heuris-\ntic cannot predict the outgoing access pattern anymore. There is\nanother interesting trend regarding the utilization of the traversal\nunit, which increases with the L2 capacity.",
			  "hen describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footprint* . In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft* can be omit",
			  "ache Cluster (CC):** A small set of nodes that fits within a\ncache line, created within an *AC* ",
			  " Two-Level Clustering*",
			  " Two-Level Clustering*"
			]
		  },
		  {
			"title": "The Ultimate Guide to Bounding Volume Hierarchies",
			"url": "http://www.lufei.ca/posts/BVH.html",
			"excerpts": [
			  "Alternatively, proposals including Cache-Oblivious BVH (COLBVH) 19 and Swapped Subtrees (SWST) 20 organize the tree into subtree clusters in memory.Read more"
			]
		  },
		  {
			"title": "Further Reading",
			"url": "https://pbr-book.org/4ed/Primitives_and_Intersection_Acceleration/Further_Reading",
			"excerpts": [
			  "Vaidyanathan et al. ( [2016](:Vaidyanathan2016) ), who introduced a\nreduced-precision representation of the BVH that still guarantees\nconservative intersection tests with respect to the original BVH.",
			  "Liktor\nand Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node\nrepresentation based on clustering nodes that improves cache performance\nand reduces storage requirements for child node pointers.",
			  "Ylitie et\nal. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs\ninto wider BVHs with more children at each node, from which they derived a\ncompressed BVH representation that shows a substantial bandwidth reduction\nwith incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )\ndeveloped an algorithm for efficiently traversing such wide BVHs using a\nsmall stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing\nsets of adjacent leaf nodes of BVHs under the principle that most of the\nmemory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described\nan approach that saves both computation and storage by taking advantage of\nshared planes among the bounds of the children of a BVH node.",
			  "Other work in the area of space-efficient BVHs includes that of",
			  "reduced-precision representation of the BVH that still guarantees",
			  "conservative intersection tests with respect to the original BVH.",
			  "and reduces storage requirements for child node pointers. Ylitie et",
			  "with incoherent rays. Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "small stack. Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "Section Title: Further Reading > Grids > Bounding Volume Hierarchies",
			  "Liktor",
			  "Liktor",
			  "and Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node",
			  "representation based on clustering nodes that improves cache performance",
			  "representation based on clustering nodes that improves cache performance",
			  "and reduces storage requirements for child node pointers.",
			  "Ylitie et",
			  "al. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "with incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "developed an algorithm for efficiently traversing such wide BVHs using a",
			  "small stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "sets of adjacent leaf nodes of BVHs under the principle that most of the",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "an approach that saves both computation and storage by taking advantage of",
			  "an approach that saves both computation and storage by taking advantage of",
			  "shared planes among the bounds of the children of a BVH node.",
			  "shared planes among the bounds of the children of a BVH node."
			]
		  },
		  {
			"title": "High-Performance Graphics 2016",
			"url": "https://diglib.eg.org/collections/c76a314b-e5ee-4fc4-8162-19c780bda7db",
			"excerpts": [
			  "Reduced precision bounding volume ... Moreover, as BVH nodes become comparably small to practical cache line sizes, the BVH is cached less efficiently.Read more"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://www.researchgate.net/publication/284233414_Performance_Comparison_of_Bounding_Volume_Hierarchies_and_Kd-Trees_for_GPU_Ray_Tracing",
			"excerpts": [
			  "The BVH node takes 64 bytes: 24 bytes for each of the children axis-aligned bounding boxes (AABB), two 4 byte offsets to the left and right ...Read more"
			]
		  },
		  {
			"title": "[PDF] Bandwidth-efficient BVH layout for incremental hardware traversal | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Bandwidth-efficient-BVH-layout-for-incremental-Liktor-Vaidyanathan/23e7f72b23dbbc376dd46dd681e0edcde34a6037",
			"excerpts": [
			  "This paper introduces a novel memory layout and node addressing scheme and map it to a system architecture for fixed-function ray traversal and demonstrates a significant reduction in memory bandwidth, compared to previous approaches."
			]
		  },
		  {
			"title": "What is a GPU warp? | Modular",
			"url": "https://docs.modular.com/glossary/gpu/warp",
			"excerpts": [
			  "Threads in a warp can access contiguous memory locations efficiently through memory coalescing. The hardware automatically synchronizes threads within a warp ..."
			]
		  },
		  {
			"title": "Introduction to the HIP programming model  HIP 7.1.52802 Documentation",
			"url": "https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html",
			"excerpts": [
			  "Coalescing memory accesses means aligning and organizing these accesses so that multiple threads in a warp can combine their memory requests into the fewest ...Read more"
			]
		  },
		  {
			"title": "definition - In CUDA, what is memory coalescing, and how is it achieved? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/5041328/in-cuda-what-is-memory-coalescing-and-how-is-it-achieved",
			"excerpts": [
			  "A coalesced memory transaction is one in which all of the threads in a half-warp access global memory at the same time.Read more"
			]
		  },
		  {
			"title": "Accessing same global memory address within warps - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/accessing-same-global-memory-address-within-warps/66574",
			"excerpts": [
			  "If a warp accesses the same addresses several times, then the memory instruction is coalesced. Internally, NVIDIA GPUs only support gather instructions.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://meistdan.github.io/publications/raysorting/paper.pdf",
			"excerpts": [
			  " employed techniques such as path tracing produce increasingly\nincoherent ray sets due to scattering on diffuse and glossy surfaces.",
			  "\nTracing incoherent rays is much more costly than tracing coherent\nones due to higher memory bandwidth, higher cache miss rate,\nand computational divergence. ",
			  ". A number of techniques were pro-\nposed to mitigate this issue that usually use the wavefront path\ntracing combined with ray reordering, packet tracing, or ray space\nhierarchies.",
			  "The core of the ray tracing based algorithms is evaluating ray\nscene intersections, which is often referred to as the trace kernel. In\nour paper, we revisit the basic problem of sorting rays to produce\ncoherent subsets of rays in order to accelerate the trace kernel. We",
			  "We\nfocus on methods that are fully agnostic to the particular trace ker-\nnel and the employed acceleration data structure.",
			  "Such techniques\nalready appeared in the literature [Aila and Karras 2010; Costa et al .\nI3D 20, May 57, 2020, San Francisco, CA, USA\n",
			  "USA\nMeister, Bokansk, Guthe, and Bittner\n2015; Moon et al . 2010; Reis et al . 2017], but we feel there is a need\nfor their thorough comparison and deeper analysis.",
			  "\nWe aim at the following contributions:\n We summarize previously published methods for ray reorder-\ning suitable for GPU ray tracing.",
			  "We propose a method for sorting key computation that aims\nto maximize ray coherence by using a novel termination\npoint estimation technique.\n",
			  "We show the current limits of the trace acceleration using an\n ...",
			  "\nRays in three-dimensional space can be represented as points in\na five-dimensional space (ray space), where three dimensions rep-\nresent ray origins, and two dimensions represent ray directions.",
			  "cing on GPUs, Aila and\nLaine [2009] also evaluated a hash-based sorting criterion based on\ninterleaving ray origin and normalized ray direction. At that time,\nthe sorting overhead was too large to improve the overall rendering\ntime. Ano",
			  ".\nIn a case when thread divergence occurs on GPU, the whole warp\nof threads is blocked until all its rays finish the traversal. Aila and\nLaine [2009] proposed to increase SIMD efficiency by replacing al-\nready finished rays with new ones from a global queue.",
			  " Techniques\nsuch as speculative traversal slightly increase the redundancy of ray\nintersection tests because they work on possibly terminated rays.\n",
			  "Moon et al .\n[2010]. They propose to sort rays using an estimated termination\npoint that is calculated by ray tracing a simplified scene that fits\ninto the main memory. The approach is, however, only suitable for\nout-of-core ray tracing due to the expensive hit point estimation.",
			  "the sorting overhead was too large to improve the overall rendering\ntime."
			]
		  },
		  {
			"title": "Designing Fast Architecture-Sensitive Tree Search on",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2043652.2043655",
			"excerpts": [
			  "We explore latency hiding techniques for CPUs and GPUs to improve instruction throughput, resulting in better SIMD utilization. This article is an extended ...Read more"
			]
		  },
		  {
			"title": "Cpu Cache",
			"url": "https://paul.bone.id.au/blog/2019/05/01/cpu-cache/",
			"excerpts": [
			  "In other words, the cache manages 64-byte long (and aligned) blocks, or lines of memory. Managing cache in lines improves its use, since if you ...Read more"
			]
		  },
		  {
			"title": "Global memory access - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/global-memory-access/44288",
			"excerpts": [
			  "In general if you can have each thread load a 128 bit segment(16 bytes) then this will usually be faster than a 32 bit(4 bytes) or 64(8 bytes) bit word per ...Read more"
			]
		  },
		  {
			"title": "Memory transaction size - CUDA",
			"url": "https://forums.developer.nvidia.com/t/memory-transaction-size/8856",
			"excerpts": [
			  "In device 1.2+ (G200), you can use a transaction size as small as 32 bytes as long as each thread accesses memory by only 8-bit words.Read more"
			]
		  },
		  {
			"title": "Adapting Tree Structures for Processing with SIMD ...",
			"url": "https://openproceedings.org/2014/conf/edbt/ZeuchFH14.pdf",
			"excerpts": [
			  "We adapt the. B+-Tree and prefix B-Tree (trie) by changing the search al- gorithm on inner nodes from binary search to k-ary search. The k-ary search enables ...Read more",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search",
			  "The remainder of this paper is structured as follows. Sec-\ntion 2 covers preliminaries of our work. First, we discuss the\nSIMD chipset extension of modern processors and their op-\nportunities. Furthermore, we outline the *k-ary search* idea\nas the foundation for our work. Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search.",
			  "The optimized Seg-\nTrie provides a constant 14 fold speedup independently of\ntree depth and an eight fold reduced memory consumption\ncompared to the original *B* + -Tree.",
			  "Like the *B* + -Tree using binary search, the Seg-Tree adds one\nnode to the traversal path for each increase in tree depth.",
			  "The smallest data type that can currently be processed by\nthe *SIMD Extensions* is 8-bit [2]. This restriction limits a\nfurther increase in tree depth."
			]
		  },
		  {
			"title": "A High Throughput B+tree for SIMD Architectures",
			"url": "https://www.ece.lsu.edu/lpeng/papers/tpds-20-1.pdf",
			"excerpts": [
			  "AbstractB+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent.Read more",
			  "armonia, a\nnovel B+tree structure, to bridge the gaps between B+tree\nand SIMD architectures.",
			  "The key region stores the nodes with its keys in a breadth-\nfirst order",
			  "The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region.",
			  "Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality.",
			  "Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "aluations on a 28-core INTELCPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than\nthat of CPU-based HB+Tree [1], a recent state-of-the-art solution. And on a Volta TITAN VGPU, it can achieve up to 3.6 billion queries per\nsecond, which is about 3.4X faster than that of GPU-based HB+Tree.",
			  "The key region stores the nodes with its keys in a\nbreadth-first order.",
			  "To make it more efficient, Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "ap in Memory Access Requirement. Each B+tree query\nneeds to traverse the tree from root to leaf. This traversal\nbrings lots of indirect memory accesses, which is propor-\ntional to tree height",
			  "Gap in Memory Divergence. Since the target leaf node of a\nquery is generally random, multiple queries may traverse the",
			  "e\ngaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodes with its keys in a\nbreadth-first order. The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region. Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality. To",
			  "Partially-Sorted Aggregation (PSA)\n",
			  "narrowed thread-group traversal (NTG)."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "fast architecture sensitive tree search on modern CPUs ...",
			"url": "http://kaldewey.com/pubs/FAST__SIGMOD10.pdf",
			"excerpts": [
			  "t **FAST** (Fast Architecture Sensitive Tree)\nsearch algorithm that exploits high compute in modern processors\nfor index tree traversal. FAST is a binary tree, managed as a hier-\narchical tree whose elements are rearranged based on architecture\nfeatures like page size, cache line size, and SIMD width of underly-\ning hardware. We",
			  "eliminate the impact of latency with\nhierarchically blocked tree, software pipelining, and prefetches.",
			  "Having eliminated memory latency impact, we show how to ex-",
			  "cache line have the minimum number of cache misses, they found",
			  "that TLB misses are much higher than on trees with large node\nsizes, thus favoring large node sizes. Chen at al. [9] also concluded\nthat having a B+-tree node size larger than a cache line performs\nbetter and proposed pB+-trees, which tries to minimize the increase\nof cache misses of larger nodes by inserting software prefetches.",
			  "In order to efficiently use the compute performance of processors,\nit is imperative to eliminate the latency stalls, and store/access trees\nin a SIMD friendly fashion to further speedup the run-time.",
			  "*Hierarchical Blocking**\nWe advocate building binary trees (using the keys of the tuple)\nas the index structure, with a layout optimized for the specific ar-\nchitectural features.",
			  "For tree sizes larger than the LLC, the per-\nformance is dictated by the number of cache lines loaded from the\nmemory, and the hardware features available to hide the latenc",
			  "architecture features like page size, cache\nline size, and SIMD width of the underlying hardware.",
			  "this paper, we present FAST, an extremely fast architecture\nsensitive layout of the index tree. FAST is a binary tree logically\norganized to optimize for architecture features like page size, cache\nline size, and SIMD width of the underlying hardware. FAST elimi-\nnates impact of memory latency, and exploits thread-level and data-\nlevel parallelism on both CPUs and GPUs to achieve 50 million\n(CPU) and 85 million (GPU) queries per second, 5X (CPU) and\n1.7X (GPU) faster than the best previously reported performance\non the same architectures.",
			  "ompression techniques have been\nused to overcome disk I/O bottleneck by increasing the effective\nmemory capacity [15, 17, 20]. The transfer unit between mem-\nory and processor cores is a cache line. Compression allows each\ncache line to pack more data and increases the effective memory\nbandwidth. This increased memory bandwidth can improve query\nprocessing speed as long as decompression overhead is kept mini-\nmal [19, 3"
			]
		  },
		  {
			"title": "(PDF) FAST: fast architecture sensitive tree search on modern CPUs and GPUs",
			"url": "https://www.researchgate.net/publication/221213860_FAST_fast_architecture_sensitive_tree_search_on_modern_CPUs_and_GPUs",
			"excerpts": [
			  "FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware.",
			  " FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures.",
			  "FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.",
			  "In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree."
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "The memory bus of your GPU isn't simply 48 bytes wide (which would be quite cumbersome as it is not a power of 2). Instead, it is composed of 6 memory channels of 8 bytes (64 bits) each. Memory transactions are usually much wider than the channel width, in order to take advantage of the memory's burst mode. Good transaction sizes start from 64 bytes to produce a size-8 burst, which matches nicely with 16 32-bit words of a half-warp on compute capability 1.x devices.",
			  "128 byte wide transactions are still a bit faster, and match the warp-wide 32-bit word accesses of compute capability 2.0 (and higher) devices. Cache lines are also 128 bytes wide to match. Note that all of these accesses must be aligned on a multiple of the transaction width in order to map to a single memory transaction.",
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM.",
			  "When you are operating in caching mode, you can't really control the granularity of requests to global memory below 128 bytes at a time, anyway."
			]
		  },
		  {
			"title": "The granularity of L1 and L2 caches - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/the-granularity-of-l1-and-l2-caches/290065",
			"excerpts": [
			  ")\nI am currently studying CUDA.\nAcorrding to the 2022 CUDA C Programming Guide, A cache line is 128 bytes and maps to a 128 byte aligned segment in device memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte memory transactions, whereas memory accesses that are cached in L2 only are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.",
			  "In modern GPUs (say, Pascal and newer) both the L1 and L2 cache can be populated sector-by-sector. The minimum granularity is 1 sector or 32 bytes. The cache line tag, however, applies to 4 sectors (in each case) that comprise the 128-byte cache line. You can adjust L2 cache granularity."
			]
		  },
		  {
			"title": "Coalesced Memory Read Question - CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/coalesced-memory-read-question/41565",
			"excerpts": [
			  "From CUDA Best Programming Guide I know that GPU reading 128 byte word for one warp transaction. Thats mean, each warp(32 threads) can easy read any 4 byte data type in coalesced way during one cycle.",
			  "Im no expert but I think youre right. If you had 32 float3s, it would take 3 warp cycles, no matter how you arrange the data.",
			  "SoA approach would still take 3 warp cycles as well. No matter how you slice it, youre getting a full read.",
			  "Yes, multiple transactions will be issued.",
			  "SoA as a general recommendation is a good idea, but there shouldnt be any problem (no difference in efficiency) with loading a float4 per thread."
			]
		  },
		  {
			"title": "CUDA Performance Optimization",
			"url": "https://juser.fz-juelich.de/record/915940/files/04-mhrywniak-perf_opt.pdf",
			"excerpts": [
			  "To achieve coalesced global memory access: Usually: Fix your access pattern. Try to use shared memory (but first, check cache behavior). Look for different way ..."
			]
		  },
		  {
			"title": "An Evaluation of B-tree Compression Techniques | The VLDB Journal | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1007/s00778-025-00950-8",
			"excerpts": [
			  "Among them, B-tree compression is an important technique introduced as early as the 1970s to improve both space efficiency and query performance ...Read more"
			]
		  },
		  {
			"title": "fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu- ...",
			"url": "https://scispace.com/pdf/fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu-42t1vrahpt.pdf",
			"excerpts": [
			  "e present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive int",
			  "r breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack. ",
			  "This algorithm amortizes the cost of node access\npattern among the rays.",
			  "Ray sorting timings are not taken\ninto account when we evaluate [ AL09 ] performance."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2013/Laine-MegakernelsConsideredHarmful.pdf",
			"excerpts": [
			  "llows further optimizations**\n**Collecting requests in operation-specific queues and scheduling**\n**them individually**\n**This is the big one!** **Really hard to do in the megakernel approach**\n****\n**Path state must reside in memory**\n**A simple loop-based method can keep it in registers**\n**Not as bad as it sounds if we use a good memory layout (SOA",
			  "**Step 2: Per-Operation Queues**\n**Allocate a queue for each primitive operation request**\n**Extension ray casts**\n**Shadow ray casts**\n**New path generation**\n**Material evaluations**\n***With separate queues for individual materials***\n**Place requests compactly (i.e., no gaps) into queues**\n**When executing, use one thread per request**\n**Every thread will have an item to work on**\n**Every thread will be doing the same thing, so theres very**\n**little execution divergence!**"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "It is shown that this type of hash table can benefit from the\ncache locality and multi-threading more than ordinary hash tables.",
			  "Moreover, the batch design\nfor hash tables is amenable to using advanced features of modern processors such as prefetching\nand SIMD vectorization.",
			  "While state-of-the-art research and open-source projects on batch hash\ntables made efforts to propose improved designs by better usage of mentioned hardware features,\ntheir approaches still do not fully exploit the existing opportunities for performance improvements.",
			  "Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing.",
			  "To allow developers to fully take\nadvantage of its performance, we recommend a high-level batch API design.",
			  "Our experimental\nresults show the superiority and competitiveness of this approach in comparison with the alternative\nimplementations and state-of-the-art for the data-intensive workloads of relational join processing,\nset operations, and sparse vector processing.",
			  "The SIMD is a hardware\nfeature that allows the simultaneous execution of an operation on a vector of values.",
			  "On\nthe other hand, prefetching is a hardware feature that allows the program to request future\nmemory accesses in advance and asynchronous to the other computations.",
			  "We will cover the\nmore-detailed definitions of these two concepts later in this section.",
			  " 1 ], Horton [ 9 ] and Cuckoo++[ 23 ] have focused on improving the\nperformance of batch hash tables by applying SIMD and prefetching techniques to a specific\ntype of SIMD-aware batch hash table designs called Bucketized Cuckoo Hash Tables (BCHTs)",
			  " Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing. ",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "Modern CPUs support hardware and software prefetch-\ning. Prefetching improves the performance of a program by amortizing the costs of memory\naccess over tim",
			  "In hash tables, regardless of the hashing scheme, accessing entries is based on the value\nof the computed hash for each provided key.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable.",
			  " \nFigure 6 depicts a generic and high-level algorithm for combining prefetching with vertical\nvectorization (based on the assumption that we take the group-prefetching approach instead\nof standard prefetching",
			  "By having a group of keys as input, before starting the vertical\nvectorization, we define a loop over the group keys (prefetching loop).",
			  "foreach\ngroup in array by GROUP_SIZE {",
			  "// prefetching\nstage",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "Conditions of coalescing global memory into few transactions - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/conditions-of-coalescing-global-memory-into-few-transactions/109481",
			"excerpts": [
			  "The (L2) cache can act as a coalescing buffer by collecting write activity from multiple instructions, before it is written out to DRAM, in presumably a minimized set of transactions. This is possible in part because L2 has write-back, not write-through, behavior.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp."
			]
		  },
		  {
			"title": "CUDA on WSL User Guide  CUDA C++ Best Practices Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/",
			"excerpts": [
			  "A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.",
			  "On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.",
			  "On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.",
			  "The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the *k* -th thread accesses the *k* -th word in a 32-byte aligned array. Not all threads need to participate."
			]
		  },
		  {
			"title": "Memory Latency - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/memory-latency",
			"excerpts": [
			  "eturning to Little's Law, we notice that it assumes that the full bandwidth be utilized, meaning, that all 64 bytes transferred with each memory block are useful bytes actually requested by an application, and not bytes that are transferred just because they belong to the same memory block. When any amount of data is accessed, with a minimum of one single byte, the entire 64-byte block that the data belongs to is actually transferred. To make sure that all bytes transferred are useful, it is necessary that accesses are *coalesced* , i.e. requests from different threads are presented to the [memory management unit](/topics/computer-science/memory-management-unit \"Learn more about memory management unit from ScienceDirect's AI-generated Topic Pages\") (MMU) in such a way that they can be packed into accesses that will use an entire 64-byte block",
			  " Typical latencies are 4 cycles for the L1 cache, 12 cycles for the [L2 cache](/topics/computer-science/l2-cache \"Learn more about L2 cache from ScienceDirect's AI-generated Topic Pages\") , and roughly 150-200 cycles for main memory. This memory hierarchy enhances memory performance in two ways. On one hand, it reduces the memory latency for recently used data. On the other hand, it reduces the number of accesses to the main memory, thereby limiting the usage of the network interconnect and the bandwidth demand. Indeed, accesses to L1 and L2 caches do not require network activation because they are part of the [processor socket](/topics/computer-science/processor-socket \"Learn more about processor socket from ScienceDirect's AI-generated Topic Pages\")",
			  "The size of memory transactions varies significantly between Fermi and the older versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction size would start off at 128 bytes per memory access. This would then be reduced to 64 or 32 bytes if the total region being accessed by the coalesced threads was small enough and within the same 32-byte aligned block. This memory was not cached, so if threads did not access consecutive memory addresses, it led to a rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2, 3, 4, , 31 and thread 1 reads addresses32, 32, 34, , 63, they will not be coalesced. In fact, the hardware will issue one read request of at least 32 bytes for each thread. The bytes not used will be fetched from memory and simply be discarded. Thus, without careful consideration of how memory is used, you can easily receive a tiny fraction of the actual bandwidth available on the device.",
			  "The situation in Fermi and Kepler is much improved from this perspective. Fermi, unlike compute 1.x devices, fetches memory in transactions of either 32 or 128 bytes. A 64-byte fetch is not supported. By default every memory transaction is a 128-byte cache line fetch. Thus, one crucial difference is that access by a stride other than one, but within 128 bytes, now results in cached access instead of another memory fetch. This makes the GPU model from Fermi onwards considerably easier to program than previous generations.",
			  "The 16 LSUs distributes 64 of the 128 bytes to the registers used by the first half-warp of warp 0. In the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register used by the other half-warp. However, warp 0 still can not progress as it has only one of the two operands it needs for the multiply. It thus does not execute and the subsequent bytes arriving from the coalesced read of a for the other warps are distributed to the relevant registers for those warps.",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data. For most programs, the data brought into the cache will then allow a cache hit on the next [loop iteration](/topics/computer-science/loop-iteration \"Learn more about loop iteration from ScienceDirect's AI-generated Topic Pages\") .",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data.",
			  "ple, the MMU can only find 10 threads that read 10 4-byte words from the same block, 40 bytes will actually be used and 24 will be discarded. It is clear that coalescing is extremely important to achieve high memory utilization, and that it is much easier when the access pattern is regular and contiguous. Th"
			]
		  },
		  {
			"title": "CUDA C++ Programming Guide (Legacy)  CUDA C++ Programming Guide",
			"url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/",
			"excerpts": [
			  "By default page-locked host memory is allocated as cacheable. It can optionally be allocated as *write-combining* instead by passing flag `cudaHostAllocWriteCombined` to `cudaHostAlloc()` . Write-combining memory frees up the hosts L1 and L2 cache resources, making more cache available to the rest of the application. In addition, write-combining memory is not snooped during transfers across the PCI Express bus, which can improve transfer performance by up to 40%.",
			  "ng from write-combining memory from the host is prohibitively slow, so write-combining memory should in general be used for memory that the host only writ",
			  "Using CPU atomic instructions on WC memory should be avoided because not all CPU implementations guarantee that functionality.",
			  "An access policy window specifies a contiguous region of global memory and a persistence property in the L2 cache for accesses within that region.",
			  "The code example below shows how to set an L2 persisting access window using a CUDA Stream.",
			  "When a kernel subsequently executes in CUDA `stream` , memory accesses within the global memory extent `[ptr..ptr+num_bytes)` are more likely to persist in the L2 cache than accesses to other global memory locations.",
			  "The `hitRatio` parameter can be used to specify the fraction of accesses that receive the `hitProp` property.",
			  "For example, if the L2 set-aside cache size is 16KB and the `num_bytes` in the `accessPolicyWindow` is 32KB:",
			  "With a `hitRatio` of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.",
			  "Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://www.researchgate.net/publication/2623917_Making_B-Trees_Cache_Conscious_in_Main_Memory",
			"excerpts": [
			  "CSB+Tree (Rao and Ross 2000) improves key density and reduces cache accesses and misses by storing only the address of the first child and ...Read more"
			]
		  },
		  {
			"title": "TRAVERSING A BVH CUT TO EXPLOIT RAY COHERENCE",
			"url": "https://www.scitepress.org/Papers/2011/33634/33634.pdf",
			"excerpts": [
			  "algorithms used for traversing a subtree are due to. (Aila and Laine, 2009). They are the persistent packet and the persistent while-while and will be ..."
			]
		  },
		  {
			"title": "How do cache lines work?",
			"url": "https://stackoverflow.com/questions/3928995/how-do-cache-lines-work",
			"excerpts": [
			  "Modern PC memory modules transfer 64 bits (8 bytes) at a time, in a burst of eight transfers, so one command triggers a read or write of a full cache line from ...Read more"
			]
		  },
		  {
			"title": "Search Lookaside Buffer: Efficient Caching for Index Data ...",
			"url": "https://wuxb45.github.io/papers/slb.pdf",
			"excerpts": [
			  "The CPU cache can leverage access locality to keep the most frequently used part of an index in it for fast access. However, the traversal on the index to a ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Proceedings of the 2001 ACM symposium on Applied computing",
			"url": "https://dl.acm.org/doi/10.1145/372202.372329",
			"excerpts": [
			  "The B+-Tree is the most popular index structure in database systems. In this paper, we present a fast B+-Tree ... Read More  Storage systems for movies-on ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Batch-construction-of-B%2B-trees-Kim-Won/a4f0cb5e06927a905cccb25056730b2a95e48d06",
			"excerpts": [
			  "An algorithm for batchconstructing the B+-tree, the most widely-used index structure in database systems, which achieves up to 28 times performance gain ..."
			]
		  },
		  {
			"title": "Fast Divergent Ray Traversal by Batching Rays in a BVH",
			"url": "https://jbikker.github.io/literature/Fast%20Divergent%20Ray%20Traversal%20by%20Batching%20Rays%20in%20a%20BVH%20-%202016.pdf",
			"excerpts": [
			  "In this work we propose a batching traversal scheme\ncalled RayCrawler.",
			  "Our scheme operates on a hierarchy of\nBVHs by splitting an existing BVH into two separate layers,\ncreating a top-level tree and multiple small trees that fit in the\nL2 cache of modern CPUs.",
			  "The Top-BVH traversal stack of the ray is\nstored to resume traversal later on.",
			  "he traversal algorithm starts by first traversing each\nray depth-first through the Top-BVH; once the ray reaches\na leaf node of the Top-BVH, the ray is batched at the Leaf-\nBVH that the leaf node is pointing to and the traversal of the\nray is suspended.",
			  "This system tries to amortize the cost of retrieving a Leaf-\nBVH from memory by traversing many batched rays through\nthe Leaf-BVH once it has been loaded into cache",
			  "hed rays. The scheme\nachieves modest speedups compared to a single-ray traver-\nsal algorithm for secondary rays and proves that a batching\nscheme can outperform a naive single-ray traversal approach\nfor highly divergent rays",
			  "he comparisons in this work are made with the algo-\nrithms implemented in the Embree framework version 2.7.1",
			  "s section gives a short overview of the data structure used\nthroughout the paper and an overview of the traversal al-\ngorithm. The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched",
			  "We achieve this by splitting a regular 4-wide BVH in two\nlayers. The rays are batched in the leaf nodes of the top\nlayer before traversing the bottom layer of the data struc-\ntur",
			  " The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched rays."
			]
		  },
		  {
			"title": "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/3170492.3136044",
			"excerpts": [
			  "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms",
			  "GPCE 2017: Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and ExperiencesIn order to achieve the highest possible performance, the ray traversal and intersection\nroutines at the core of every high-performance ray tracer are usually hand-coded,\nheavily optimized, and implemented separately for each hardware platformeven ...[Read More](# \"",
			  "Stackless traversal algorithms for ray tracing acceleration structures require significantly\nless storage per ray than ordinary stack-based ones. This advantage is important for\nmassively parallel rendering methods, where there are many rays in flight. ...[Read More]",
			  "Efficient stack-less BVH traversal for ray tracing",
			  "SCCG '11: Proceedings of the 27th Spring Conference on Computer GraphicsWe propose a new, completely iterative traversal algorithm for ray tracing bounding\nvolume hierarchies that is based on storing a parent pointer with each node, and on\nusing simple state logic to infer which node to traverse next."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://minkhollow.ca/Courses/461/Notes/Trees/Resources/rao.pdf",
			"excerpts": [
			  "we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "CSS-Trees augment binary search\nby storing a directory structure on top of the\nsorted list of elements.",
			  "We proposed a new index\nstructure called Cache-Sensitive Search Trees\n(CSS-Tree) that has even better cache behavior\nthan a B + -Tree. ",
			  "With a large amount of RAM, most of the\nindexes can be memory resident.",
			  "Our experiments are performed for 4-byte keys\nand 4-byte child pointers. Theoretically, B + -Trees\nwill have 30% more cache misses than CSB + -Trees.",
			  "In this paper, we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "As the gap between CPU and memory speed is\nwidening, CSB + -Trees should be considered as a re-\nplacement for B + -Trees in main memory database",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees."
			]
		  },
		  {
			"title": "Cache Craftiness for Fast Multicore Key-Value Storage",
			"url": "https://pdos.csail.mit.edu/papers/masstree:eurosys12.pdf",
			"excerpts": [
			  "Masstree uses a combination of old and new techniques\nto achieve high performance [8, 11, 13, 20, 2729]. It\nachieves fast concurrent operation using a scheme inspired\nby OLFIT [11], Bronson *et al.* [9], and read-copy up-\ndate [28]. Lookups use no locks or interlocked instructions,\nand thus operate without invalidating shared cache lines and\nin parallel with most inserts and update",
			  "Masstree shares a single tree among all cores to avoid load\nimbalances that can occur in partitioned designs.",
			  "The tree\nis a trie-like concatenation of B + -trees, and provides high\nperformance even for long common key prefixes, an area in\nwhich other tree designs have trouble.",
			  "Masstree uses a\nwide-fanout tree to reduce the tree depth, prefetches nodes\nfrom DRAM to overlap fetch latencies, and carefully lays\nout data in cache lines to reduce the amount of data needed\nper node.",
			  "Masstree achieves six to ten million operations per\nsecond on parts AC of the benchmark, more than 30 ** as\nfast as VoltDB [5] or MongoDB [2",
			  "e contributions of this paper are as follows. First, an\nin-memory concurrent tree that supports keys with shared\nprefixes efficiently. Second, a set of techniques for laying\nout the data of each tree node, and accessing it, that reduces\nthe time spent waiting for DRAM while descending the tree.\nThird, a demonstration that a single tree shared among mul-\ntiple cores can provide higher performance than a partitioned\ndesign for some workloads. Fourth, a complete design that\naddresses all bottlenecks in the way of million-query-per-\nsecond performance",
			  "Masstree provides high\nconcurrency from the start."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory",
			"url": "https://dl.acm.org/doi/pdf/10.1145/335191.335449",
			"excerpts": [
			  "ache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a\nvariant of B + -Trees that stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node.",
			  "We propose a new indexing technique called\nCache Sensitive B + -Trees (CSB + -Trees).",
			  "t stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node. The rest of the children can\nbe found by adding an offset to that address. ",
			  "Since only\none child pointer is stored explicitly, the utilization of\na cache line is high.",
			  "In Section 2 we survey related work on cache\noptimization.\nIn Section 3 we introduce our\nnew CSB + -Tree and its variants.",
			  "Full CSB + -Trees are better than B + -Tree in all\naspects except for space.\nWhen space overhead\nis not a big concern,\nFull CSB + -Tree is the\nbes"
			]
		  },
		  {
			"title": "Cache craftiness for fast multicore key-value storage | Proceedings of the 7th ACM european conference on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/2168836.2168855",
			"excerpts": [
			  "J. Rao and K. A. Ross. Making B+-trees cache conscious in main memory. SIGMOD Record, 29:475--486, May 2000.",
			  ". Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: A cache-sensitive parallel external sort. The VLDB Journal, 4(4):603--627, 1995.\n"
			]
		  },
		  {
			"title": "What is Memory Coalescing? | GPU Glossary",
			"url": "https://modal.com/gpu-glossary/perf/memory-coalescing",
			"excerpts": [
			  "Memory coalescing takes advantage of the internals of DRAM technology to enable\nfull bandwidth utilization for certain access patterns. Each time a DRAM address\nis accessed, multiple consecutive addresses are fetched together in parallel in\na single clock. For a bit more detail, see Section 6.1 of [the 4th edition of Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311) ;\nfor comprehensive detail, see Ulrich Drepper's excellent article [*What Every Programmer Should Know About Memory*](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf) .\nThe access and transfer of these consecutive memory locations is referred to as\na *DRAM burst* . If multiple concurrent logical accesses are serviced by a single\nphysical burst, the access is said to be *coalesced* . Note that a physical\naccess is part of a memory transaction, terminology you may see elsewhere in\ndescriptions of memory coalescing.\nOn CPUs, a similar mapping of bursts onto cache lines improves access\nefficiency. As is common in GPU programming, what is automatic cache behavior in\nCPUs is here programmer-managed."
			]
		  },
		  {
			"title": "Request Hedging vs Request Coalescing: A Software Engineers Guide to Optimizing Distributed Systems | by Sourav Chaurasia | Medium",
			"url": "https://medium.com/@mr.sourav.raj/request-hedging-vs-request-coalescing-a-software-engineers-guide-to-optimizing-distributed-fdcc6590ba9d",
			"excerpts": [
			  "**Request Coalescing** , also known as request deduplication or the singleflight pattern, is a resource optimization technique that merges multiple identical concurrent requests into a single execution. When multiple clients request the same resource simultaneously, only one actual request is executed, and all requesters share the result.",
			  "**Benefits:**",
			  "**Significant resource savings** : Can reduce duplicate executions by 7090% in high-concurrency scenarios.",
			  "**Improved cache efficiency** : Better hit rates when multiple requests need the same data.",
			  "**Prevents thundering herd** : Avoids overwhelming backend services during spikes.",
			  "**Lower infrastructure costs** : Reduced CPU, memory, and network usage.",
			  "**Drawbacks:**",
			  "**No individual latency improvement** : Doesnt help single request performance.",
			  "**Implementation complexity** : Requires careful key generation and cleanup.",
			  "**Memory overhead** : Must track pending requests and their futures.",
			  "**Potential bottlenecks** : Shared operations can become single points of failure.",
			  "**Discords Message Storage:** Discord uses request coalescing to manage its trillion-message database efficiently, preventing duplicate queries for the same message data.",
			  "**CDN Cache Filling:** Content delivery networks coalesce multiple requests for the same uncached resource, fetching it once and serving all waiting clients.",
			  "**Authentication Token Refresh:** When multiple requests need to refresh an expired token simultaneously, coalescing ensures only one refresh operation occurs.",
			  "**Database Query Optimization:** High-traffic applications coalesce identical database queries to reduce load and improve response times.",
			  "**Memory vs CPU Balance** Coalescing trades memory (for tracking pending requests) against CPU and network resources (avoiding duplicate work).",
			  "**Key Strategy Impact:** The effectiveness depends heavily on your key generation strategy:\nToo specific: Minimal coalescing benefit.\nToo general: Risk of sharing inappropriate results.\nOptimal: Balance between specificity and reuse.",
			  "\n**Cleanup Strategies:** Implement proper cleanup to prevent memory leaks",
			  "**Resource efficiency matters** : Infrastructure costs or capacity are constraints.",
			  "**High concurrency** : Many clients are requesting identical data simultaneously.",
			  "**Expensive operations** : Computationally intensive or slow database queries.",
			  "**Cache scenarios** : Filling caches or warming up cold data.",
			  "Request hedging and request coalescing represent two fundamental approaches to optimizing distributed systems. Hedging prioritizes user experience through latency reduction, while coalescing prioritizes system efficiency through resource optimization."
			]
		  },
		  {
			"title": "Two-Minute Tech Tuesdays - Request Coalescing - Resources",
			"url": "https://info.varnish-software.com/blog/two-minutes-tech-tuesdays-request-coalescing",
			"excerpts": [
			  "This episode of Two Minute Tech Tuesdays is about request coalescing, a core feature in Varnish that is used to reduce the stress on origin servers when multiple requests are trying to fetch the same uncached content.",
			  "What happens when multiple clients request the same uncached content from Varnish? Does Varnish open up the same amount of requests to the origin, and potentially destabilize the entire platform under heavy load (the Thundering Herd effect)? Luckily Varnish is not sensitive to the Thundering Herd. It identifies requests to the same uncached resource, queues them on a waiting list, and only sends a single request to the origin. As the origin responds, Varnish will satisfy the entire waiting list in parallel, so there's no [head-of-line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking) : everyone gets the content at exactly the same time. So request coalescing will effectively merge multiple potential requests to the origin into a single request.",
			  "The advantages are pretty straightforward: * Less pressure on the origin server",
			  "Less latency for queued clients",
			  "In terms of domains of application, request coalescing is useful for: * Long tail content that doesn't always end up in the cache",
			  "This only applies to cacheable content. Uncacheable content cannot take advantage of request coalescing. With uncacheable content we mean content that uses set cookie headers, or that has cache control response headers that deliberately bypass the cache.",
			  "With serialization we mean items on the waiting list being processed serially, rather than in parallel.",
			  "doing so we avoid potential serialization. With serialization we mean items on the waiting list being processed serially, rather than in parallel. This has a very detrimental effect on the performance and the quality of experience for the user, because in this case there actually is head-of-line blocking. Luc",
			  "Luckily Varnish Configuration Language (VCL) has provisions for that:",
			  "set beresp.ttl = 120s;",
			  "set beresp.uncacheable = true;"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)",
			  "Decoupled Access/Execute (DAE)",
			  "Idea: Decouple operand\naccess and execution via\ntwo separate instruction\nstreams that communicate\nvia ISA-visible queues .",
			  "Smith,  Decoupled Access/Execute\nComputer Architectures ,  ISCA 1982,\nACM TOCS 1984.",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Advantages:\n+ Execute stream can run ahead of the access stream and vice\nversa\n+ If A is waiting for memory, E can perform useful work\n+ If A hits in cache, it supplies data to lagging E\n+ Queues reduce the number of required registers\n+ Limited out-of-order execution without wakeup/select complexi",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream.",
			  "Control flow synchronization (like branches) is handled via a separate branch queue.",
			  "The compiler analyzes the program, identifies operations belonging to each stream, and generates two distinct instruction sequences that explicitly communicate via queue operations inserted by the compiler.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Stop Crying Over Your Cache Miss Rate: Handling ...",
			"url": "https://www.epfl.ch/labs/lap/wp-content/uploads/2019/06/AsiaticiFeb19_StopCryingOverYourCacheMissRateHandlingEfficientlyThousandsOfOutstandingMissesInFpgas_FPGA19.pdf",
			"excerpts": [
			  "could generate even more\nrequests per cycle with no fundamental limitations on the total num-\nber of in-flight operations",
			  "a shared MHA to maximize the merging opportunities.",
			  "In general, the cache requires 15 block RAMs per 32 kB per cache way, the MSHR buffer requires 1 block RAM per 512 MSHRs per cuckoo hash table for storage plus ...Read more"
			]
		  },
		  {
			"title": "Addressing Isolation Challenges of Non-blocking Caches ...",
			"url": "https://www.ittc.ku.edu/~heechul/papers/taming-rtsj17.pdf",
			"excerpts": [
			  "Miss Status Holding\nRegisters (MSHRs), which track the status of outstanding cache-misses, can be a sig-\nnificant source of contention that is not addressed by conventional cache partitioning.",
			  "e propose to dynamically control the num-\nber of usable MSHRs in the private L1 caches",
			  "We add two\nhardware counters *TargetCount* and *V alidCount* for each L1 cache controller.",
			  "The *V alidCount* tracks the number of total valid MSHR entries (i.e., entries with\noutstanding memory requests) of the cache and is updated by the hardware.",
			  "The\n*TargetCount* defines the maximum number of MSHRs that can be used by the core\nand is set by the system software (OS).",
			  " By control-\nling the value of *TargetCount* , the OS can effectively control the cores local MLP.",
			  "The added area and logic complexity is minimal as we only need two additional\ncounter registers and one comparator logic.",
			  "The degree of parallelism supported by\na memory subsystem is called *Memory-Level Parallelism (MLP)* [13].",
			  "Non-blocking\ncaches are essential to provide high MLP in multicore processors.",
			  "When a cache-miss occurs on a non-blocking cache, the cache controller records\nthe miss on a special register, called Miss Status Holding Register (MSHR) [27],",
			  "The request is managed at a cache-line\ngranularity. Multiple misses to the same cache-line are merged and notified together\nby a single MSHR entry.",
			  "The MSHR entry is cleared when the corresponding mem-\nory request is serviced from the lower-level memory hierarchy. ",
			  " other words, cache hit re-\n ... \ndynamic CPU and memory frequency scaling. Table 1 shows the basic characteristics\nof the five CPU architectures we used in our experime",
			  "3.2 Memory-Level Parallelism\nWe first identify memory-level parallelism (MLP) of the multicore architectures using\nan experimental method described in [11].",
			  "The method uses a pointer-chasing micro-\nbenchmark shown in Figure 3 to identify memory-level parallelism.",
			  "Latency\nis a pointer chasing synthetic benchmark, which accesses a randomly shuffled single\nlinked list. Due to data dependencies, Latency can only generate one outstanding re-\nquest at a time."
			]
		  },
		  {
			"title": "Scalable Cache Miss Handling for High Memory-Level ...",
			"url": "https://iacoma.cs.uiuc.edu/iacoma-papers/micro06_mshr.pdf",
			"excerpts": [
			  " a line is *primary* if there is currently no outstand-\ning miss on the line and, therefore, a new MSHR needs to be allo-\ncated. ",
			  "A miss is *secondary* if there is already a pending miss on the\nline. In this case, the existing MSHR for the line can be augmented to\nrecord the new miss, and no request is issued to memory. In this case,\nthe MSHR for a line keeps information for all outstanding misses on\nthe line.",
			  " each miss, it contains a *subentry* (in contrast to an *en-*\n*try* , which is the MSHR itself",
			  "Once an MHA exhausts its MSHRs or subentries, it\n*locks-up* the cache (or the corresponding cache bank).",
			  "From then on,\nthe cache or cache bank rejects further requests from the processor.",
			  "This may eventually lead to a processor stall.",
			  "They used a design where each cache bank has its own\nMSHR fi le (Figure 1(b)), but did not discuss the MSHR itself"
			]
		  },
		  {
			"title": "CudaDMA | Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis",
			"url": "https://dl.acm.org/doi/10.1145/2063384.2063400",
			"excerpts": [
			  "Section Title: CudaDMA: optimizing GPU memory bandwidth via warp specialization > Abstract\nContent:\nAs the computational power of GPUs continues to scale with Moore's Law, an increasing number of applications are becoming limited by memory bandwidth. We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories. Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms. DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout. To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns. Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Research",
			"url": "https://research.nvidia.com/publication/2011-11_cudadma-optimizing-gpu-memory-bandwidth-warp-specialization",
			"excerpts": [
			  "We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic micro-benchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "Leveraging Warp Specialization for High Performance on GPUs",
			"url": "https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf",
			"excerpts": [
			  "e crucial insight for warp specialization is that while con-\ntrol divergence within a warp results in performance degradation,\ndivergence between warps does not.",
			  "med barriers pro-\nvide two operations: *arrive* and *sync* . Arrive is a non-blocking op-\neration which registers that a warp has arrived at a barrier and then\ncontinues execution. Sync is a blocking operation that waits until\nall the necessary warps have arrived or synced on the barrie",
			  "Warp-specialized partitioning provides a useful mechanism for\nDSL compilers when grappling with computations that exhibit\nboth irregularity and large working sets.",
			  "The mapping compilation stage is responsible for taking in an\narbitrary dataflow graph of operations and mapping it onto the\nspecified number of warps and available GPU memories.",
			  "It is important to\nnote that named barriers support synchronization between arbitrary\nsubsets of warps within a CTA, including allowing synchronization\nbetween a single pair of warps as in this example.",
			  "rtitioning computations us-\ning warp specialization allows Singe to deal efficiently with the\nirregularity in both data access patterns and computation.",
			  "Consumer Warp",
			  "Producer Warp",
			  "(signal begin)",
			  "(wait until ready)",
			  "bar.sync",
			  "bar.sync",
			  "(wait until begin)",
			  "bar.arrive",
			  "bar.arrive",
			  "(signal ready)",
			  "producer-consumer named barrier is used\nto indicate to the QSSA warps when the needed values from the\nnon-QSSA warps have been written into shared memory.",
			  "arp-specialized programs also require more expressive syn-\nchronization mechanisms",
			  "Warp specialization exploits the division\nof a thread block into warps to partition computations into sub-\ncomputations such that each sub-computation is executed by a\ndifferent warp within a thread block.",
			  "However, by using inline PTX statements, a CUDA program\nhas access to a more expressive set of intra-CTA synchronization\nprimitives referred to as *named barriers*",
			  "Using arrive and sync operations, programmers can encode\nproducer-consumer relationships in warp-specialized programs.",
			  "gure 2 illustrates using two named barriers to coordinate move-\nment of data from a producer warp (red) to a consumer warp (blue)\nthrough a buffer in shared memory.",
			  "he\nconsumer warp signals the buffer is ready by performing a non-\nblocking arrive operation.",
			  "Since the arrive is non-blocking, the\nconsumer warp is free to perform additional work while waiting\nfor the buffer to be filled.",
			  "At some point the consumer warp blocks\non the second named barrier waiting for the buffer to be full.",
			  "The\nproducer warp signals when the buffer is full using a non-blocking\narrive operation on the second named barrier.",
			  "Named Barrier 0",
			  "Named Barrier 1"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  " root cause of this entanglement is the require-\nment encouraged by the CUDA programming model that\nthreads of a CTA perform both memory accesses and com-\nputation. By creating specialized warps that perform inde-\npendent compute and memory operations we can tease apart\nthe issues that affect memory performance from those that\naffect compute performan",
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps.",
			  "arp specialization allows subsets of threads within\na CTA to have their behavior tuned for a particular purpose\nwhich enables more efficient consumption of constrained re-\nsources",
			  "If single buffering is not exploiting enough MLP to keep\nthe memory system busy, an alternative is to create two\nbuffers with two sets of DMA warps for transferring data.\nWe call this two-buffer technique double buffering ",
			  "DMA warps also improve programmer productivity\nby decoupling the need for thread array shapes to match\ndata layout.",
			  "Named Barrier 0",
			  "Named Barrier 1",
			  "compute warps",
			  "DMA warps",
			  "start_async_dma()",
			  "wait_for_dma_fnish()",
			  "wait_for_dma_start()",
			  "(bar.sync)",
			  "(bar.sync)",
			  "fnish_async_dma()",
			  "(bar.arrive)",
			  "(bar.arrive)",
			  "The\nproducer/consumer nature of our synchronization mecha-\nnisms allow the programmer to employ a variety of tech-\nniques to overlap communication and computation",
			  "e accomplish fine-grained synchronization by using in-\nlined PTX assembly to express named barriers .",
			  "med\nbarriers are hardware resources that support a barrier oper-\nation for a subset of warps in a CTA and can be identified\nby a unique name (e.g. immediate value in PTX).",
			  "There are\ntwo named barriers associated with every cudaDMA object.\nTwo barriers are required to track whether the data buffer\nin shared memory is full or empty.",
			  "e use the PTX in-\nstruction bar.arrive , which allows a thread to signal its ar-\nrival at a named barrier without blocking the threads execu-\ntion",
			  "This functionality is useful for producer-consumer\nsynchronization by allowing a producer to indicate that a\ndata transfer has finished filling a buffer while permitting\nthe producer thread to continue to perform work.",
			  "Similarly,\na consuming thread can use the same instruction to indicate\nthat a buffer has been read and is now empty.",
			  "For blocking\noperations we use the PTX instruction bar.sync to block\non a named barrier.",
			  "CudaDMA has shown the benefits of emulating an asyn-\nchronous DMA engine in software on a GP",
			  "The CudaDMA approach to GPU programming is therefore\ngeneral enough to be applied to any CUDA program.",
			  "CudaDMA encapsulates this technique in order to make it\nmore generally available to a range of application workloads.",
			  "By decoupling the compute and\nDMA warps, CudaDMA enables this approach without\nsacrificing memory system performance.",
			  "CudaDMA enables the\nprogrammer to decouple the shape of data from how the\ndata is transferred by creating specialized DMA warps for"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more",
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
			"url": "https://arxiv.org/html/2510.14719v2",
			"excerpts": [
			  "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT ...Read more"
			]
		  },
		  {
			"title": "Using Littles Law to Measure System Performance | Kevin Sookocheff",
			"url": "https://sookocheff.com/post/modeling/littles-law/",
			"excerpts": [
			  "Little's Law is a useful tool for software architecture because it provides a simple way to measure the effect of changes to a system.Read more"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "t.\nThere is also a \"Branch From\nQueue\" (BFQ) instruction that is conditional on\nthe branch outcome at the head of the branch queue\ncoming from the\nopposite processor.\n",
			  ".\nThus conditional branches\nappear in the two processors as complementary\npairs.",
			  " deadlock can occur if\nboth the AEQ and EAQ are full and both processors\nare blocked by the full queues, or if both queues\nare empty and both processors are blocked by the\nempty queues.\n"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. ",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Little's Law as Viewed on Its 50th Anniversary",
			"url": "https://people.cs.umass.edu/~emery/classes/cmpsci691st/readings/OS/Littles-Law-50-Years-Later.pdf",
			"excerpts": [
			  ".\nLittles Law says that the average number of items in a\nqueuing system, denoted *L* , equals the average arrival rate\nof items to the system, ** , multiplied by the average waiting\ntime of an item in the system, *W* . Thus,\n*L* = ** *W* *0*\nF",
			  "\nThe two biggest fields in which Littles Law regularly\ncomes into play are operations management (OM) and\ncomputer architecture (computers).",
			  "Latency is a key performance measure for computers\n(low is good) and has an unspoken average implicitly\nattached, but *response time* seems more meaningful for a\nlayperson.",
			  "e\ncomputer architect wants to understand what is going on so\nas best to design the system.",
			  "Computer memory comes in various *tiers* with different\ninherent response times. The tiers range from CPU cache\n(fast), to DRAM (dynamic random access memory) (not\nquite as fast), to RAM (random access memory) (a lit-\ntle slower), all the way to solid-state drives and finally to\nnetwork attached storage (banks of hard disk drives).",
			  "The server CPU contains microprocessors, each of which\ncontains cores, perhaps 8 or 16 of them. Each is a von\nNeumann computer. Now we are coming to the queues. The\npiece of code the computer is executing is aptly called a\nthread (of instructions). The author visualizes it as a list of\ninstructions running down his penciled code sheet. Oh, oh,\nthe instruction calls for a piece of data that is somewhere\nelse in memory, electronically far away from the thread.\nThe thread STOPS and sends out a request for that data."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://dl.acm.org/doi/abs/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982. ... 1984. ISSN ...Read more",
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings ... James E Smith profile image James E. Smith. Department of Electrical and ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Overview and Code Examples",
			"url": "https://www.nvidia.com/content/PDF/GDC2011/Brucek_KhailanySC11.pdf",
			"excerpts": [
			  "Warp Specialization. CudaDMA enables warp specialization: DMA warps. Maximize MLP. Compute warps. No stalls due to memory. CudaDMA objects manage warp ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://alastairreid.github.io/RelatedWork/papers/smith:tocs:1984/",
			"excerpts": [
			  "Decoupled access/execute computer architectures. James E. Smith [doi] [Google Scholar] [DBLP] [Citeseer]. ACM Transactions on Computer Systems 2(4)Read more"
			]
		  },
		  {
			"title": "(PDF) A decoupled access-execute architecture for ...",
			"url": "https://www.researchgate.net/publication/326637256_A_decoupled_access-execute_architecture_for_reconfigurable_accelerators",
			"excerpts": [
			  "PDF | Mapping computational intensive applications on reconfigurable technology for acceleration requires two main implementation parts: (a) ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114% ( ...Read more"
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "This paper demonstrates significant performance gains on a commodity 8-core multicore machine running a variety of codes transformed with DSWP+. Formats ...Read more"
			]
		  },
		  {
			"title": "Scalable-Grain Pipeline Parallelization Method for Multi- ...",
			"url": "https://inria.hal.science/hal-01513778/file/978-3-642-40820-5_23_Chapter.pdf",
			"excerpts": [
			  "Some approaches partition individual instructions across pro- cessors, such as the decoupled software pipeline (DSWP) method [10], while.Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "1.1 Problem. Decoupled software pipelining [5] presents an easy way to automatically ex- tract thread-level parallelism for general loops in any program.Read more"
			]
		  },
		  {
			"title": "decoupled-software-pipelining-creates-parallelization- ...",
			"url": "https://scispace.com/pdf/decoupled-software-pipelining-creates-parallelization-tyrvhzup00.pdf",
			"excerpts": [
			  "An automatic parallelization technique, DSWP splits the loop body into several stages distributed across multiple threads, and executes them in a pipeline."
			]
		  },
		  {
			"title": "\nParallel Techniques of the Sequential Codes Based on Multi-core",
			"url": "https://scialert.net/fulltext/?doi=itj.2013.1673.1684",
			"excerpts": [
			  "DSWP parallelizes a loop by partitioning the body of the loop into a sequence of pipeline stages. Each stage is then executed by a separate thread. The threads ...Read more"
			]
		  },
		  {
			"title": "PIPELINED MULTITHREADING TRANSFORMATIONS AND ...",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_ram.pdf",
			"excerpts": [
			  "tolerate variable latency and to overcome scope restrictions imposed by single PC ar-\nchitectures, a program transformation called *decoupled software pipelining* (DSWP) is pre-\nsented in this chapter",
			  "SWP avoids heavy hardware usage by attacking the fundamental\nproblem of working within a single-threaded execution model, and moving to a multi-\nthreaded execution model.",
			  "Only useful, about-to-execute instructions are even brought into\nthe core. The rest are conveniently left in the instruction cache or their results committed\nand retired from the processor core.",
			  "Unlike the single-threaded multi-core techniques pre-\nsented in Table 2.1, DSWP is an entirely *non-speculative* techniqu",
			  "Each DSWP thread\nperforms useful work towards program completion.",
			  "The concurrent multithreading model of DSWP means that each participating thread\ncommits its register and memory state concurrently and independently of other threads.",
			  "The current thread model for DSWP is as follows. Execution begins as a single thread,\ncalled *primary thread* . It spawns all necessary *auxiliary threads* at the beginning of a\nprogram",
			  "When the primary thread reaches a DSWPed loop, auxiliary threads are set up\nwith necessary loop live-in values.",
			  "Similarly, upon loop termination, loop live-outs from\nauxiliary threads have to be communicated back to the primary thread."
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization. They also promise much wider flexi-\nbility than data-parallelism-centric methods for processors, aiming\nfor the effective parallelization of general-purpose application",
			  "In this paper, we provide another method to decouple control-\nflow regions of serial programs into concurrent tasks, exposing\npipeline and data parallelism",
			  "The power and simplicity of the\nmethod rely on the restriction that all streams should retain a\nsynchronous semantics [8].",
			  "ics [8]. It amounts to checking the sufficient\ncondition that the source and target of any decoupled dependence\nare control-dependent on the same node in the control dependence\ntree (this assumes structured control flow).",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "For example, when\nthere are no dependences between loop iterations of a DSWP stage,\nthe incoming data can be distributed over multiple data-parallel\nworker threads dedicated to this stage, while the outgoing data can\nbe merged to proceed with downstream pipeline stages."
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformati",
			  "WP operates by partitioning the instructions of a\nloop among a sequence of loops. The new loops are concurrently\nexecuted on different threads, with dependences among them flow-\ning in a single direction, thus forming a pipeline of threads.",
			  "The performance results indicate\nthe potential of this technique to exploit iteration-level parallelism\nin loops that cannot be parallelized as DOALL.",
			  "e evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining with the Synchronization Array",
			"url": "https://liberty.princeton.edu/Publications/pact04_dswp.pdf",
			"excerpts": [
			  " software pipelining (DSWP), a technique that stati-*\n*cally splits a single-threaded sequential loop into multi-*\n*ple non-speculative threads, each of which performs use-*\n*ful computation essential for overall program correctne",
			  "g threads execute on thread-parallel architec-*\n*tures such as simultaneous multithreaded (SMT) cores or*\n*chip multiprocessors (CMP), expose additional instruction*\n*level parallelism, and tolerate latency better than the orig-*\n*inal single-threaded RDS lo",
			  "accomplish this,\nthe paper presents the *synchronization array* which ap-\npears to the ISA as a set of queues capable of supporting\nboth out-of-order execution and speculative issue",
			  "e traversal and computation threads behave as a tra-\nditional decoupled producer-consumer pair.",
			  "DSWP threads these loops\nfor parallel execution on SMT or CMP processors.",
			  "ever, for this threading technique to be effective, a very\nlow overhead communication and synchronization mecha-\nnism between the threads is required. "
			]
		  },
		  {
			"title": "CoroBase: Coroutine-Oriented Main-Memory Database ...",
			"url": "http://vldb.org/pvldb/vol14/p431-he.pdf",
			"excerpts": [
			  "Data stalls are a major overhead in main-memory database engines\ndue to the use of pointer-rich data structures. ",
			  ". Lightweight corou-\ntines ease the implementation of software prefetching to hide data\nstalls by overlapping computation and asynchronous data prefetch-\ning. ",
			  "Coroutine-to-transaction models transactions as coroutines\nand thus enables inter-transaction batching, avoiding application\nchanges but retaining the benefits of prefetching. W",
			  ". We show that\non a 48-core server, CoroBase can perform close to 2  better for\nread-intensive workloads and remain competitive for workloads\nthat inherently do not benefit from software prefetching.\n*",
			  "CoroBase is open-source at https://github.com/sfu-dis/corobase ."
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://medium.com/nlnetlabs/adapting-radix-trees-15fe7d27c894",
			"excerpts": [
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the ...Read more"
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/pf_final.pdf",
			"excerpts": [
			  "For index searches, pB+-Trees reduce this problem by having wider nodes than the natural data transfer size,. e.g., eight vs. one cache lines (or disk pages).",
			  "To accelerate searches, pB+-Trees use prefetching to e ectively create wider nodes than the natural data trans- fer size: e.g., eight vs. one cache lines or ...Read more",
			  "prefetching pointer-linked data structures (i.e. linked-lists, trees, etc.) in general-purpose applications. Assuming that three nodes worth of computation ..."
			]
		  },
		  {
			"title": "The Taming of the B-Trees - ScyllaDB",
			"url": "https://www.scylladb.com/2021/11/23/the-taming-of-the-b-trees/",
			"excerpts": [
			  "There is a good and pretty cheap optimization to mitigate this spike that we've called linear root. The leaf root node grows on demand, ...Read more"
			]
		  },
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "The [first]() is based on the memory layout of a B-tree, and, depending on the array size, it is up to 8x faster than `std::lower_bound` while using the same space as the array and only requiring a permutation of its eleme",
			  "[second]() is based on the memory layout of a B+ tree, and it is up to 15x faster than `std::lower_bound` while using just 6-7% more memory  or 6-7% **of** the memory if we can keep the original sorted array.",
			  "o distinguish them from B-trees  the structures with pointers, hundreds to thousands of keys per node, and empty spaces in them  we will use the names *S-tree* and *S+ tree* respectively to refer to these particular memory layouts [1](:1) .",
			  ":\nTo find the lower bound, we need to fetch the $B$ keys in a node, find the first key $a_i$ not less than $x$, descend to the $i$-th child  and continue until we reach a leaf node. There is some variability in how to find that first key. For example, we could do a tiny internal binary search that makes $O(\\log B)$ iterations, or maybe just compare each key sequentially in $O(B)$ time until we find the local lower bound, hopefully exiting from the loop a bit early.",
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://blog.nlnetlabs.nl/adapting-radix-trees/",
			"excerpts": [
			  "The Adaptive Radix Tree (ART)",
			  "Section Title: The Adaptive Radix Tree (ART) > *Adaptively sized nodes*",
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the respective number of references to child nodes, and are grown/shrunk as needed.",
			  "Fixed sizes are used to minimize the number of memory (de)allocations.",
			  "Nodes of sizes up to 48, map keys onto edges using two separate vectors.",
			  "Nodes of size 16 make use of 128-bit SIMD instructions ( [SSE2](https://en.wikipedia.org/wiki/SSE2) and [NEON](https://en.wikipedia.org/wiki/ARM_architecture(Neon)) ) to map a key onto an edge to improve performance.\n",
			  "Section Title: The Adaptive Radix Tree (ART) > Path compression",
			  "Inner nodes that have only one child are merged with their parent and each node reserves a fixed number of bytes to store the prefix.",
			  "If more space is required, lookups simply skip the remaining number of bytes and compare the search key to the key of the leaf once it arrives there.",
			  "Section Title: The Adaptive Radix Tree (ART) > Lazy expansion",
			  "Inner nodes are created only to distinguish at least two leaf nodes. Paths are truncated. Pointer tagging is used to tell inner nodes apart from leaf nodes."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://db.in.tum.de/~leis/papers/ART.pdf",
			"excerpts": [
			  "**ode16:** This node type is used for storing between 5 and\n16 child pointers. Like the Node4 , the keys and pointers\nare stored in separate arrays at corresponding positions, but\nboth arrays have space for 16 entries.",
			  "ART adapts the\nrepresentation of every individual node, as exemplified in\nFigure 1. By adapting each inner node *locally* , it optimizes\n*global* space utilization and access efficiency at the same ",
			  "Radix trees consist of two types of nodes: Inner nodes,\nwhich map partial keys to other nodes, and leaf nodes, which\nstore the values corresponding to the keys. The most efficient\nrepresentation of an inner node is as an array of 2 *s* pointer",
			  "A useful property of radix trees is that the order of the keys\nis not random as in hash tables; rather, the keys are ordered\nbitwise lexicographically.",
			  "Two additional techniques, path\ncompression and lazy expansion, allow ART to efficiently\nindex long keys by collapsing nodes and thereby decreasing\nthe tree height.",
			  "e use a small number of node types, each with a different\nfanout. Depending on the number of non-null children, the\nappropriate node type is used. ",
			  "the space consumption per key is bounded\nto 52 bytes, even for arbitrarily long keys. We show\nexperimentally, that the space consumption is much lower\nin practice, often as low as 8.1 bytes per key.",
			  " **ode4:** The smallest node type can store up to 4 child\npointers and uses an array of length 4 for keys and another\narray of the same length for pointers. The keys and pointers\nare stored at corresponding positions and the keys are sorted.",
			  "The height (and complexity) of radix trees depends on\nthe length of the keys but in general not on the number\nof elements in the tree.",
			  "Instead of using\na list of key/value pairs, we split the list into one key part\nand one pointer part. This allows to keep the representation\ncompact while permitting efficient search:",
			  "**ode48:** As the number of entries in a node increases,\nsearching the key array becomes expensive. Therefore, nodes\nwith more than 16 pointers do not store the keys explicitly.\n",
			  " lookup**\n**performance surpasses highly tuned, read-only search trees, while**\n**supporting very efficient insertions and deletions as we",
			  "en though ARTs performance**\n**is comparable to hash tables, it maintains the data in sorted**\n**order, which enables additional operations like range scan and**\n**pref"
			]
		  },
		  {
			"title": "Adaptive Radix Tree",
			"url": "https://pages.cs.wisc.edu/~yxy/cs764-f22/slides/L16.pdf",
			"excerpts": [
			  "**Key idea** : Use a small node type\nwhen only a small number of\nchildren pointers exist",
			  "t\nKey Idea: Adaptive Radix Tree",
			  "**Node4** and **Node16**\n**Node48**\n**Node256**\n 256 child pointers indexed with\npartial key byte directly\n",
			  "Inner Node Structure",
			  "\n**Node4** and **Node16**\n Store up to 4 (16) partial keys\nand the corresponding pointers\n Each partial key is one byte\n Use SIMD instructions to\naccelerate key search\n**No",
			  "**Node48**",
			  "**Node256**",
			  "256 entries indexed with partial\nkey byte directly",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "**Lazy expansion** : remove path to\nsingle leaf",
			  "**Path compression** : merge one-way\nnode into child node",
			  "ART requires at most **52 bytes** of memory to index a key"
			]
		  },
		  {
			"title": " Beating hash tables with trees? The ART-ful radix trie | Paper Trail ",
			"url": "https://www.the-paper-trail.org/post/art-paper-notes/",
			"excerpts": [
			  "The most significant change that ART makes to the standard trie structure is that it introduces the\nability to change the datastructure used for each internal node depending on how many children the\nnode actually has, rather than how many it might have.",
			  ".\nPointers are assumed to be 8 bytes, so a single `Node4` is 36 bytes, so sits in a single cache\nline. The search loop can also be unrolled. Finally, by not early-exiting from the loop, we can hint\nto the compiler that it need not use a full branch, but can just use a conditional `cmov` [predicated instruction]",
			  "Nodes with from 5 to 16 children have an identical layout to `Node4` , just with 16 children per node:\nKeys in a `Node16` are stored sorted, so binary search could be used to find a particular key. Since\nthere are only 16 of them, its also possible to search all the keys in parallel using SIMD. What\nfollows is an annotated version of the algorithm presented in the papers Fig 8.\nThis is superior to binary-search: no branches (except for the test when bitfield is 0), and all the\ncomparisons are done in parallel.",
			  "The next node can hold up to three times as many keys as a `Node16` . As the paper says, when there\nare more than 16 children, searching for the key can become expensive, so instead the keys are\nstored implicitly in an array of 256 indexes. The entries in that array index a separate array of up\nto 48 pointers.\nThe idea here is that this is superior to just storing an array of 256 `Node` pointers because you\ncan store 48 children in 640 bytes (where 256 pointers would take 2k). Looking up the pointer does\ntake an extra indirection:\nThe paper notes that in fact only 6 bytes (i.e. \\(log_2(48)\\)) are needed for each index; both in\nthe paper and here its simpler to use a byte per index to avoid any shifting and masking.",
			  " final node type is the traditional trie node, used when a node has between 49 and 256 children.\nLooking up child pointers is obviously very efficient - the most efficient of all the node types -\nand when occupancy is at least 49 children the wasted space is less significant (although not 0 by\nany stretch of the imagination)"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://www.eecs.umich.edu/techreports/cse/02/CSE-TR-468-02.pdf",
			"excerpts": [
			  "A design decision that is consistently used in cache-conscious tree-based indices is defining the node size\nto be equal to the size of the L2 data cache line.",
			  "nalogous\nto the traditional B+-tree where a node size is equal to a disk page to minimize the number of page ac-\ncesses during a search, the node size for the CSB\n -tree is set equal to a processor cache line to minimize\nthe number of cache miss",
			  "The work by Rao and Ross has been extended in recent years in a number\nof different ways, including handling variable key length attributes efficiently [4] and for architectures that\nsupport prefetching [9].",
			  "In a recent paper, Chen, Gibbons and Mowry [9] examined the cache behavior of B+trees and CSB\n -\ntrees. They conclude that the CSB\n -trees produce very deep trees which cause many cache misses as a\nsearch traverses down the tree. They propose a prefetching-based solution, in which the node size of a\nB+tree is larger than the cache line size, and special prefetching instructions are manually inserted into th",
			  ". The CSB\n -tree eliminates child node pointers in the non-leaf\nnodes, allowing additional keys to be stored in a node which improves cache line utilization. An"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://pages.cs.wisc.edu/~jignesh/publ/cci.pdf",
			"excerpts": [
			  "uthors also investi-\ngated dynamic indexing techniques in the main-memory environ-\nment in [27], proposing a cache-conscious variation of the tradi-\ntional B+-tree, called the CSB + -tree. The CSB + -tree eliminates\nchild node pointers in the non-leaf nodes, allowing additional keys\nto be stored in a node which improves cache line utilizatio",
			  "the\nnode size for the CSB + -tree is set equal to a processor cache line to\nminimize the number of cache misses.",
			  "In a recent paper, Chen, Gibbons and Mowry [10] examined\nthe cache behavior of B+trees and CSB + -trees. They conclude\nthat the CSB + -trees produce very deep trees which cause many\ncache misses as a search traverses down the tree. They propose a\nprefetching-based solution, in which the node size of a B+tree is\nlarger than the cache line size, and special prefetching instructions\nare manually inserted into the B+tree code to prefetch cache lines\nand avoid stalling the processor.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  "We also recommend larger node\nsizes for the CSB + -tree, but our recommendation is not based on\nusing special hardware prefetch instructions. Rather, we recognize\nthat node size influences a number of different factors besides cache\nmisses, and that overall performance is improved by carefully con-\nsidering the effect of node size on all these factor",
			  " [27] is an adaptation of the ubiquitous B + -tree for\nmain memory databases [27]. The CSB + -tree is an important data\nstructure for memory-resident databases as it has been shown to\noutperform other cache-conscious, tree-based indices as well as tra-\nditional, tree-based indices in memory-resident databases",
			  "sing a first-order analytical model of the search performance,\nwe show that the conventional choice of setting the node size\nequal to the cache line size is often suboptimal. This design\nchoice focuses on reducing the number of cache misses, but\nignores the effect on the number of instructions that are exe-\ncuted, the number of conditional branches mispredicted, and\nthe number of TLB misses",
			  "The work by Rao and Ross\nhas been extended in recent years in a number of different ways,\nincluding handling variable key length attributes efficiently [5] and\nfor architectures that support prefetching [10].",
			  "Chen, Gibbons, Mowry, and Valentin also propose a version of\ntheir prefetching B+-tree optimized for disk pages, called Fractal\npB+-trees\n[11]."
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/238799599_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "As the speed gap between main memory and modern processors continues to widen, the cache behavior becomes more important for main memory database systems (MMDBs).",
			  "Indexing technique is a key component of MMDBs. Unfortunately, the predominant indexes  B+-trees and T-trees  have been shown to utilize cache poorly, which triggers the development of many cache-conscious indexes, such as CSB+-trees and pB+-trees.",
			  "The J+-tree stores all the keys in its leaf nodes and keeps the reference values of leaf nodes in a Judy structure, which makes J+-tree not only hold the advantages of Judy (such as fast single value search) but also outperform it in other aspects.",
			  "For example, J+-trees can achieve better performance on range queries than Judy. The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans.",
			  "asstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary l",
			  "Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. O"
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.semanticscholar.org/paper/The-adaptive-radix-tree%3A-ARTful-indexing-for-Leis-Kemper/6abf5107efc723c655956f027b4a67565b048799",
			"excerpts": [
			  "\nMain memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree Expand",
			  "Section Title: The adaptive radix tree: ARTful indexing for main-memory databases",
			  "[Viktor Leis](/author/Viktor-Leis/1787012) , [A. Kemper](/author/A.-Kemper/144122431) , [Thomas Neumann](/author/Thomas-Neumann/143993045)",
			  "Computer Science",
			  "[View on IEEE](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812 \"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812\")",
			  "[www-db.in.tum.de](http://www-db.in.tum.de/%7Eleis/papers/ART.pdf \"http://www-db.in.tum.de/%7Eleis/papers/ART.pdf\")",
			  "Save to Library Save",
			  "Create Alert Alert",
			  "Cite",
			  "Share",
			  "442 Citations",
			  "Content:"
			]
		  },
		  {
			"title": "Beautiful branchless binary search | Hacker News",
			"url": "https://news.ycombinator.com/item?id=35737862",
			"excerpts": [
			  "A significant speedup of a classic BS could be achieved by switching to vectorized search when the remaining range has a width of 3-4 SIMD lines."
			]
		  },
		  {
			"title": "An Eight-Dimensional Systematic Evaluation of Optimized ...",
			"url": "http://www.vldb.org/pvldb/vol11/p1550-schulz.pdf",
			"excerpts": [
			  "e implemented the algorithms described above and opti-\nmized the resulting code by eliminating branches, unrolling\nloops and adding software prefetching",
			  " vec-\ntorized the sequential, binary and uniform k-ary search us-\ning AVX2 instruction",
			  "One way to eliminate branches is by branch predication,\ni.e., both sides of a branch are executed and only the effects\nof the side that was actually needed are kept.",
			  "he x86 ar-\nchitecture supports branch predication via conditional move\ninstructions ( cmov )",
			  "el et al. [12] propose to use SIMD to parallelize in-\ndex computations and key comparisons of a k-ary search by\nfitting the *k* ** 1 separator elements and their indices in a\nsingle SIMD regist",
			  "Assuming AVX2 with 32-bit indices and keys, *k* = 9.",
			  "e vectorized implementa-\ntions were again tuned like the scalar variants.",
			  "The sequential search is vectorized by simply loading and\ncomparing *m* consecutive keys in parallel each iteration,\nwhere *m* is the number of keys per SIMD w",
			  "The binary search can be vectorized by viewing the array\nas a sequence of SIMD word sized blocks."
			]
		  },
		  {
			"title": "Avx2/branch-optimized binary search in .NET  GitHub",
			"url": "https://gist.github.com/buybackoff/f403be01486220baba8a9d4fe22c3cf6",
			"excerpts": [
			  "Binary search is theoretically optimal, but it's possible to speed it up substantially using AVX2 and branchless code even in .NET Core.",
			  "Memory access is the limiting factor for binary search. When we access each element for comparison a cache line is loaded, so we could load a 32-byte vector almost free, check if it contains the target value, and if not - reduce the search space by `32/sizeof(T)` elements instead of 1 element. This gives quite good performance improvement (code in `BinarySearch1.cs` and results in the table 1 below).",
			  "The linear search was not using AVX2, and for linear AVX2 should definitely work, shouldn't it!? With vectorized linear search and some additional branching optimization the performance is improved by *additional* 30-50% for the most relevant N (code in `BinarySearch2.cs` and results in the table 2 below)."
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Section Title: k-ary search on modern processors > Abstract and Figures",
			  "2.3Binary Search With SIMD Instructions",
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "The adaptive radix tree | Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013)",
			"url": "https://dl.acm.org/doi/10.1109/ICDE.2013.6544812",
			"excerpts": [
			  "Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck.",
			  "To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory.",
			  "Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well.",
			  "ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes.",
			  "Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.",
			  "Authors : [Viktor Leis]() , [Alfons Kemper]() , [Thomas Neumann]() [Authors Info & Claims]()",
			  "Pages 38 - 49",
			  "Contents",
			  "Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches."
			]
		  },
		  {
			"title": "How I implemented an ART (Adaptive Radix Trie) data structure in Go to increase the performance of my database 2x | by Farhan Ali Khan | techlog | Medium",
			"url": "https://medium.com/techlog/how-i-implemented-an-art-adaptive-radix-trie-data-structure-in-go-to-increase-the-performance-of-a8a2300b246a",
			"excerpts": [
			  "The paper introduces [ART](https://db.in.tum.de/~leis/papers/ART.pdf) , an adaptive radix tree (trie) for efficient indexing in main memory. It quotes the lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. It maintains the data in sorted order, which enables additional operations like range scan and prefix lookup."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf",
			"excerpts": [
			  "Tree puts all the child\nnodes for a given node contiguously in an array and\nstores only the pointer to the first child node.",
			  "a CSB + -Tree has fewer\npointers per node than a B + -Tree.",
			  "By having fewer\npointers per node, we have more room for keys and\nhence better cache performance.",
			  "We achieve this\ngoal by balancing the best features of the two index\nstructures.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "like a CSS-Tree, which requires batch updates, a\nCSB + -Tree is a general index structure that sup-\nports efficient incremental updates.",
			  "SB + -Trees need to maintain the property that\nsibling nodes are contiguous, even in the face of\n ... \nsystem). As observed in [RR99, CLH98], B + -Trees\nwith node size of a cache line have close to optimal\nperf",
			  "S-Trees were proposed in [RR99].\nThey improve on B + -Trees in terms of search per-\nformance because each node contains only keys,\nand no pointers.",
			  "Child nodes are identified by\nperforming arithmetical operations on array offsets.",
			  "Compared with B + -Trees, CSS-Trees utilize more\nkeys per cache line, and thus need fewer cache\naccesses and fewer cache misses.",
			  "The use of arithmetic to identify children requires\na rigid storage allocation policy.",
			  "This approach allows good\nutilization of a cache line.",
			  "We get away with fewer pointers by using a\nlimited amount of arithmetic on array offsets,\ntogether with the pointers, to identify child nodes.",
			  " describe\nvariants with more pointers per node. T",
			  "Cache Sensitive B + -Trees (CSB + -Trees)",
			  "Cache Sensitive B + -Trees (CSB + -Trees)"
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/314796211_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "Graefe and Larson [2001] summarized techniques of improving cache performance on B-tree indexes. Hankins and Patel [2003] explored the effect of node size on the performance of CSB+-trees and found that using node sizes larger than a cache line size (i.e., larger than 512 bytes) produces better search performance. While trees with nodes that are of the same size as a cache line have the minimum number of cache misses, they found 2 1B refers to 1 billion. ...",
			  "Both methods show that the best search performance is achieved by using a node size larger than 160 bytes. The best results are received when the node size is up to 3072 bytes [Han03] . For an insert operation the effect of node size is the opposite. ...",
			  " -tree has been further optimized using a variety of techniques, such as prefetching [4], storing only partial keys in nodes [5], and choosing the node size more carefully",
			  "Hankins and Patel [HP03] report that by tuning the node size of a CSB + -tree to be significantly bigger than cache line size they are able to reduce the number of instructions executed, TLB misses and branch mispredictions. ...",
			  "Hankins and Patel [37] studied the node size of CSB + -Trees on a Pentium III machine and concluded that it is desirable to use larger node size to reduce the number of tree levels because every level of the tree experiences a TLB miss and a fixed instruction overhead.",
			  "Chen et al. proposed pB+-tree [7] to use larger index nodes and rely on prefetching instructions to bring index nodes into cache before nodes are accessed.",
			  "Masstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary length.",
			  "In this paper, we present NitroGen, a framework for utilizing code generation for speeding up index traversal in main memory database systems."
			]
		  },
		  {
			"title": "A case study for Adaptive Radix Tree index",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S0306437921001228",
			"excerpts": [
			  "In the previous sections, we proposed a solution for cracking the Adaptive Radix Tree (ART), a popular IMDB index. ... Viktor Leis, Alfons Kemper, Thomas Neumann, ..."
			]
		  },
		  {
			"title": "A Case Study for Adaptive Radix Tree Index",
			"url": "https://arxiv.org/pdf/1911.11387",
			"excerpts": [
			  "Leis, A. Kemper, and T. Neumann. 2013. The adaptive radix tree: ARTful indexing for main-memory databases. In In Proceedings of the 29th ..."
			]
		  },
		  {
			"title": "FB -tree: A Memory-Optimized B -tree with Latch-Free Update",
			"url": "https://www.vldb.org/pvldb/vol18/p1579-li.pdf",
			"excerpts": [
			  "The Adaptive Radix Tree (ART) adaptively uses four different node layouts ... [43] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive ..."
			]
		  },
		  {
			"title": "Adaptive Hybrid Indexes",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/146253/3514221.3526121.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "[31] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix. Tree: ARTful Indexing for Main-Memory Databases. In ICDE, Vol. 13. 3849. [32] ..."
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.researchgate.net/publication/261087784_The_adaptive_radix_tree_ARTful_indexing_for_main-memory_databases",
			"excerpts": [
			  "Leis et al. [42] proposed a fast and space-efficient inmemory trie called ... Alfons Kemper  Thomas Neumann. The two areas of online transaction ..."
			]
		  },
		  {
			"title": "Fractal Prefetching B -Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/fpbtree.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effec- tively optimizes both CPU cache and disk performance on modern processors, ...Read more"
			]
		  },
		  {
			"title": "Amanieu/brie-tree: SIMD-optimized B+ ...",
			"url": "https://github.com/Amanieu/brie-tree",
			"excerpts": [
			  "A fast B+ Tree implementation that uses integer keys. The API is similar to the standard library's BTreeMap with some significant differences:.Read more"
			]
		  },
		  {
			"title": "ARTful indexing",
			"url": "http://daslab.seas.harvard.edu/classes/cs265/files/presentations/CS265_presentation_Sinyagin.pdf",
			"excerpts": [
			  "(P) The Adaptive Radix Tree: ARTful indexing for main-memory databases. Viktor Leis, Alfons Kemper, Thomas Neumann. International Conference on Data ..."
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "This is a follow up on a [previous article](https://algorithmica.org/en/eytzinger) about using Eytzinger memory layout to speed up binary search.",
			  "B-trees generalize the concept of binary search trees by allowing nodes to have more than two children.",
			  "Instead of single key, a B-tree node contains up to \\(B\\) sorted keys may have up to \\((B+1)\\) children, thus reducing the tree height in \\(\\frac{\\log_2 n}{\\log_B n} = \\frac{\\log B}{\\log 2} = \\log_2 B\\) times.",
			  "They were primarily developed for the purpose of managing on-disk databases, as their random access times are almost the same as reading 1MB of data sequentially, which makes the trade-off between number of comparisons and tree height beneficial.",
			  "In our implementation, we will make each the size of each block equal to the cache line size, which in case of `int` is 16 elements.",
			  "s.\nNormally, a B-tree node also stores \\((B+1)\\) pointers to its children, but we will only store keys and rely on pointer arithmetic, similar to the one used in Eytzinger array:\nThe root node is numbered \\(0\\) .\nNode \\(k\\) has \\((B+1)\\) child nodes numbered \\(\\{k \\cdot (B+1) + i\\}\\) for \\(i \\in [1, B]\\) .\nKeys are stored in a 2d array in non-decreasing order. If the length of the initial array is not a multiple of \\(B\\) , the last block is padded with the largest value if its data type.",
			  "Back in the 90s, computer engineers discovered that you can get more bang for a buck by adding circuits that do more useful work per cycle than just trying to increase CPU clock rate which [cant continue forever](https://en.wikipedia.org/wiki/Speed_of_light) .",
			  "This worked [particularly well](https://finance.yahoo.com/quote/NVDA/) for parallelizable workloads like video game graphics where just you need to perform the same operation over some array of data. This this is how the concept of *SIMD* became a thing, which stands for *single instruction, multiple data* .",
			  "ta* .\nModern hardware can do [lots of stuff](https://software.intel.com/sites/landingpage/IntrinsicsGuide) under this paradigm, leveraging *data-level parallelism* . For example, the simplest thing you can do on modern Intel CPUs is to:\nload 256-bit block of ints (which is \\(\\frac{256}{32} = 8\\) ints),\nload another 256-bit block of ints,\nadd them together,\nwrite the result somewhere else\nand this whole transaction costs the same as loading and adding just two intswhich means we can do 8 times more work. Magic!\n",
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD",
			  "data-intensive applications like sorting [2, 4],\nhash-based search [11], and relational query processing [5,",
			  "12] can also benefit from SIMD instructions.",
			  "This paper\ncomplements these techniques by providing efficient meth-\nods for sort-based search.\n",
			  "We classify the instructions into instructions\nfor data loading, element-wise instructions, and horizontal",
			  "e assume throughout\nthat the registers of the processor of interest are vectors of\n*k* ** 1 scalars",
			  "Table 1 lists the worst-case number of iterations performed\nby binary search (Bin), SIMDized binary search (Bin[ *k* ** 1])\nand *k* -ary search for various dataset sizes and values of *k",
			  "learly, *k* -ary search is the more attractive the larger the\ndataset and the larger the value of *k* .",
			  "uture gen-\nerations of processors will support much larger values of *k*\nand thus provide further efficiency gains",
			  "r example, Intel\nrecently announced that its upcoming processors will sup-\nport the AVX instruction set [6] with 256-bit vector registers\n( *k* = 9 for 32-bit keys) for the 2010 processor generation and\nup to 512-bit vector registers ( *k* = 17) for later generations.",
			  "Similarly, the upcoming Larrabee GPGPU [9]a hybrid be-\ntween a GPU and a multi-core CPUprovides 16-way vec-\ntor registers ( *k* = 17) for integer, single-precision float, and\ndouble-precision float instructi",
			  "**2.**",
			  "**PREREQUISITES**",
			  "**2.1**",
			  "**Binary Search**",
			  "Binary search is a dichotomic divide-and-conquer search",
			  "Binary search is a dichotomic divide-and-conquer search",
			  " restrict our attention to the case where the keys are of a\ntype natively supported by the underlying processor archi-\ntectures, i.e., integer and floating point types.",
			  "**3.2**",
			  "**On a Sorted Array**",
			  "p S1 consists of two parts: (a) calculate the indexes\nof the separators and (b) load them into *R* . Substep (a) is\nrequired because the *k* ** 1 separators are stored in non-\ncontiguous memory locations, see Figure 4",
			  "In this case, balanced search trees appear to be the method\nof choice. We conjecture that SIMD instructions will also\nbe valuable to speed up search on those trees, but this is\nbeyond the scope of our current work.",
			  "**4.2**",
			  "**IBM Cell BE: PPE**",
			  "The first experiment was conducted on a PPE core of the\nCell Broadband Engine in Sonys Playstation3.",
			  "gure 6a shows the results of our PPE experiments. For\nsmall datasetsup to 2 16 keysBin performs best. The rea-\nson is that the 128-bit AltiVec unit with its vector registers\noperates concurrently with the scalar integer/floating-point\nregisters and there is no way to directly move data between\nboth types of registers.",
			  "rs.\nTherefore, scalar replication and\nthe horizontal-sum instructions are expensive on the Pow-\nerPC platform. As a consequence, the benefit of fewer iter-\n",
			  "Performance Improvements and Energy Efficiency"
			]
		  },
		  {
			"title": "k-ary search on modern processors | Proceedings of the Fifth International Workshop on Data Management on New Hardware",
			"url": "https://dl.acm.org/doi/10.1145/1565694.1565705",
			"excerpts": [
			  "This paper presents novel tree-based search algorithms that exploit the SIMD instructions found in virtually all modern processors. The algorithms are a natural extension of binary search: While binary search performs one comparison at each iteration, thereby cutting the search space in two halves, our algorithms perform *k* comparisons at a time and thus cut the search space into *k* pieces. On traditional processors, this so-called *k* -ary search procedure is not beneficial because the cost increase per iteration offsets the cost reduction due to the reduced number of iterations. On modern processors, however, multiple scalar operations can be executed simultaneously, which makes *k* -ary search attractive. In this paper, we provide two different search algorithms that differ in terms of efficiency and memory access patterns. Both algorithms are first described in a platform independent way and then evaluated on various state-of-the-art processors. Our experiments suggest that *k* -ary search provides significant performance improvements (factor two and more) on most platforms."
			]
		  },
		  {
			"title": "DEX: Scalable Range Indexing on Disaggregated Memory",
			"url": "https://www.vldb.org/pvldb/vol17/p2603-lu.pdf%3C/ee%3E",
			"excerpts": [
			  "The adaptive radix tree:\nARTful indexing for main-memory databases."
			]
		  },
		  {
			"title": "[PDF] Improving index performance through prefetching | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/99d4b6749d93726e007da067c7b36cb06321d1a3",
			"excerpts": [
			  "### Prefetching J+-Tree: A Cache-Optimized Main Memory Database Index Structure",
			  "The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans and can provide better performance on both time (search, scan, update) and space aspects.",
			  "### Fractal prefetching B+-Trees: optimizing both cache and disk performance",
			  "Fractal prefetching B+-Trees are proposed, which embed \"cache-optimized\" trees within \"disk-optimization\" trees, in order to optimize both cache and I/O performance.",
			  "### Redesigning database systems in light of cpu cache prefetching",
			  "This thesis investigates a different approach: reducing the impact of cache misses through a technique called cache prefetching, and presents a novel algorithm, Inspector Joins, that exploits the free information obtained from one pass of the hash join algorithm to improve the performance of a later pass.",
			  "### BS-tree: A gapped data-parallel B-tree",
			  "BS-tree is proposed, an in-memory implementation of the B+-tree that adopts the structure of the disk-based index, setting the node size to a memory block that can be processed fast and in parallel using SIMD instructions.",
			  "### Making B+- trees cache conscious in main memory",
			  "CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node, and introduces two variants of CSB+, which can reduce the copying cost when there is a split and preallocate space for the full node group to reduce the split cost."
			]
		  },
		  {
			"title": "M.Sc. Thesis",
			"url": "https://skemman.is/bitstream/1946/7489/1/MSc_Arni-Mar-Jonsson.pdf",
			"excerpts": [
			  "Prefetching B + -tree (pB + -tree) [9] is another main-memory indexing method, which\nis almost identical to the B + -tree. It uses prefetching (see Section 2.3.3), which allows it\nto have nodes wider than a cache-line, without having to wait an entire cache-miss latency\nfor each cache-line accessed. Wider nodes result in shallower trees, and the benefits of\nhaving a shallow tree usually outweigh the prefetching overhead.",
			  "any modern CPUs have parallel memory systems. They can fetch multiple cache-lines\nfrom main-memory at the same time. Programs can instruct the CPU to fetch a given\ncache-line by issuing so-called prefetch instructions. Multiple prefetch instructions can\nbe executed at the same time. For example, the Alpha 21264 CPU has a 150 cycle cache-\nmiss latency. It can, however, fetch 15 cache-lines at the same time, meaning that a\ncache-line can be delivered to cache every 10 cycles. If a program knows what cache-line\nit will need 150 cycles later, it can prefetch it and 150 cycles later it is available in c",
			  "This way the perceived cache-miss latency is 0 cycles, even though the actual cache-miss\nlatency is unchanged",
			  "**Node Layout and Prefetching**",
			  "Pointer elimination and prefetching as described in this chapter are complementary tech-\nniques [9]. It is possible to increase the node width of the CSB + -tree and prefetching\nnodes like the pB + -tree does, resulting in the Prefetching CSB + -Tree (pCSB + -tree).\nThat way, you get better performance than the pB + -tree, since the branch factor is slightly\nhigher.",
			  "The cache-performance of the pCSB + -tree can be found using the same method as pB + -\ntree and doubling the branch factor.",
			  "Indicespredominantly B + -treesare a key performance component of DBMSs. Un-\nfortunately, however, B + -trees have been shown to utilize cache memory poorly [9], trig-\ngering the development of many cache-conscious indices. The CSS-tree [26] and CSB + -\ntree [25] improve cache performance by not storing pointers to all the children of a node,\neffectively compacting the index structure and improving locality.",
			  "In their seminal paper, Ailamaki et al. [2] showed that less than half of the CPU\ntime for commercial DBMSs is spent on computations.",
			  "he Prefetching CSB + -Tree",
			  "he Prefetching CSB + -Tree",
			  "**General Description**",
			  "**General Description**",
			  "Chapter 7",
			  "**Conclusions**",
			  "In this thesis we have studied the performance of the pB + -tree on the Itanium 2 processor."
			]
		  },
		  {
			"title": "(PDF) Improving Index Performance through Prefetching",
			"url": "https://www.researchgate.net/publication/2528098_Improving_Index_Performance_through_Prefetching",
			"excerpts": [
			  "This paper proposes and evaluates Prefetching B -Trees#, which use prefetching to accelerate two important operations on B -Tree indices: searches and range scans.",
			  "To accelerate searches, pB -Trees use prefetching to e create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B -Tree, thereby decreasing the number of expensive misses when going from parenttochild without signi increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.2.5 for main-memory B -Trees.",
			  "In addition, it outperforms and is complementary to #Cache-SensitiveB -Trees.",
			  "To accelerate range scans, pB -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys.",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "Improving index performance through prefetching | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/abs/10.1145/376284.375688",
			"excerpts": [
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://www.yumpu.com/en/document/view/21981799/the-adaptive-radix-tree-artful-indexing-for-main-memory-",
			"excerpts": [
			  "The Adaptive Radix Tree: ARTful Indexing for Main-Memory ..."
			]
		  },
		  {
			"title": "Adaptive Radix Tree (ART) Index - All things DataOS",
			"url": "https://dataos.info/resources/stacks/flash/art/",
			"excerpts": [
			  "Node Types in ART: Node4: Supports up to 4 child pointers. Node16: Supports up to 16 child pointers. Node48: Supports up to 48 child pointers. Node256 ...Read more"
			]
		  },
		  {
			"title": "Adaptive Radix Tree for Databases | PDF | Cpu Cache",
			"url": "https://www.scribd.com/document/189896702/Art",
			"excerpts": [
			  "In this work, we present the adaptive radix tree (ART) which is a fast and space-efcient in-memory indexing structure specically tuned for modern hardware.Read more"
			]
		  },
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "However, CSS-Trees are designed for decision support workloads with relatively static data.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Nevertheless, for applications that require incremental updates, traditional B + -Trees perform well.",
			  "Our goal is to make B + -Trees as cache conscious as CSS-Trees without increasing their update cost too much.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "The rest of the children can be found by adding an offset to that address.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "CSB + -Trees support incremental updates in a way similar to B + -Trees.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Content:",
			  "Content:",
			  "We also introduce two variants of CSB + -Trees. Segmented CSB + -Trees divide the child nodes into segments.",
			  "Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node.",
			  "Segmented CSB + -Trees can reduce the copying cost when there is a split since only one segment needs to be moved.",
			  "Full CSB + -Trees preallocate space for the full node group and thus reduce the split cost.",
			  "Our performance studies show that CSB + -Trees are useful for a wide range of applications."
			]
		  },
		  {
			"title": "Understanding the efficiency of ray traversal on GPUs | Proceedings of the Conference on High Performance Graphics 2009",
			"url": "https://dl.acm.org/doi/10.1145/1572769.1572792",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This works well for simple access patterns, like iterating over array in increasing or decreasing order, but for something complex like what we have here its not going to perform well.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree .",
			  "This blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.",
			  "The goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!",
			  "The source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Speeding up independent binary searches by interleaving them  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/",
			"excerpts": [
			  "Each data access is done using fewer than 10 instructions in my implementation, which is far below the number of cycles and small compared to the size of the instruction buffers, so finding ways to reduce the instruction count should not help.",
			  "redit: This work is the result of a collaboration with Travis Downs and Nathan Kurz, though all of the mistakes are mine.\nD",
			  "Daniel Lemire, \"Speeding up independent binary searches by interleaving them,\" in *Daniel Lemire's blog* , September 14, 2019, https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/ .",
			  "The condition move instructions are pretty much standard and old at this point.",
			  "On Cannon Lake, however, you should be able to do better.",
			  "9 on Cannon Lake, 7 on Skylake, 5 on Skylark"
			]
		  },
		  {
			"title": "cwisstable/DESIGN.md at main",
			"url": "https://github.com/google/cwisstable/blob/main/DESIGN.md",
			"excerpts": [
			  "The Abseil implementation implements SwissTable via the raw_hash_set type, which provides sufficient extension points that all of the various flavors of hash ...Read more"
			]
		  },
		  {
			"title": "Facebook open-sources F14 algorithm for faster and memory-efficient hash tables",
			"url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/facebook-open-sources-f14-algorithm-for-faster-and-memory-efficient-hash-tables?srsltid=AfmBOorcYqrbSM-yIqjNiyPD-2c3or2pVOwzIgzCg7M0yVcU3kq050zF",
			"excerpts": [
			  "F14 helps the hash tables provide a faster way for maintaining a set of keys or map keys to values, even if the keys are objects, like strings.Read more"
			]
		  },
		  {
			"title": "A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort | Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/abs/10.1145/2588555.2610522",
			"excerpts": [
			  "This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy.Read more"
			]
		  },
		  {
			"title": "abseil / Swiss Tables Design Notes",
			"url": "https://abseil.io/about/design/swisstables",
			"excerpts": [
			  "Within Swiss tables, the result of the hash function produces a 64-bit hash\nvalue. We split this value up into two parts:\nH1, a 57 bit hash value, used to identify the element index within the table\nitself, which is truncated and modulated as any normal hash value would be for\nlookup and insertion purposes.\nH2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "Swiss tables hold a densely packed array of metadata, containing presence\ninformation for entries in the table. This presence information allows us to\noptimize both lookup and insertion operations. This metadata adds one byte of\noverhead for every entry in the table.",
			  "The metadata of a Swiss table stores presence information (whether the element\nis empty, deleted, or full). Each metadata entry consists of one byte, which\nconsists of a single control bit and the 7 bit H2 hash. The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "When searching for items in the table we use [SSE instructions](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) to scan for\ncandidate matches. The process of finding an element can be roughly summarized\nas follows:\nUse the H1 hash to find the start of the bucket chain for that hash.\nUse the H2 hash to construct a mask.\nUse SSE instructions and the mask to produce a set of candidate matches.\nPerform an equality check on each candidate.\nIf no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates. Note that a deleted element does not cease\nprobing, though an empty element would.\nSteps 2+3 can be summarized visually as:\nEquivalent code for this lookup appears below:\nThis process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "Use the H2 hash to construct a mask.",
			  "Perform an equality check on each candidate.",
			  "For performance reasons, it is important that you use a hash function that\ndistributes entropy across the entire bit space well (producing an [avalanche effect]",
			  "This process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "This metadata adds one byte of\noverhead for every entry in the table.",
			  "H2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "Use the H1 hash to find the start of the bucket chain for that hash.",
			  "Use SSE instructions and the mask to produce a set of candidate matches.",
			  "If no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates.",
			  "The H2 hash bits are stored separately within the metadata section of\nthe table."
			]
		  },
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "For a keys hash, split into:\n`H1` : upper 57 bits to compute the group index\n`H2` : lower 7 bits as the fingerprint stored in the control byte",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "It uses an **open addressing** hash table structure.",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "Linear Probing ( `i + j` ): Move one slot ( `j` ) at a time from the original hash position ( `i` )",
			  "Quadratic Probing( `i + j` ): Move `j` slots from the original hash position ( `i` )",
			  "Swiss Table has the following key characteristics:",
			  "It combines **linear probing** with the **Robin Hood hashing algorithm** to resolve collisions.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split)",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split) > Lookup flow (high level):",
			  "Use `H1` to pick a starting group.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > Open Addressing Hash Table"
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Swiss Tables boast improvements to efficiency and provide C++11 codebases early\naccess to APIs from C++17 and C++20.",
			  "\nThese hash tables live within the [Abseil `container` library](",
			  "We are extremely pleased to announce the availability of the new Swiss Table family of hashtables in Abseil and the `absl::Hash` hashing framework that allows easy extensibility for user defined types.",
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "The magic of SwissTables** is in the implementation detail of how it determines if the group contains the key or not. Instead of iterating over all the slots in the group, **SwissTables** leverage the Control Word!",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Swiss Tables, popularised by Google's Abseil C++ library, takes a different approach, primarily using **open addressing** (instead of chaining) with a clever variation of **linear probing** leveraging a dedicated metadata array."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "The vector search is coded using SIMD intrinsics, SSE2 on x86_64 and\nNEON on aarch64. These instructions are a non-optional part of those\nplatforms (unlike later SIMD instruction sets like AVX2 or SVE), so no\nspecial compilation flags are required. The exact vector operations\nperformed differs between x86_64 and aarch64 because aarch64 lacks a\nmovemask instruction, but the F14 algorithm is the same.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate).",
			  "F14 performs collision resolution if a chunk overflows or if two keys both pass the filtering step. The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "The lower bits of the full hash code determine the chunk. The upper bits are used to filter which slots in a chunk might hold the search key.",
			  "Chunking is an effective strategy because the chance that 15 of the tables keys will map to a chunk with 14 slots is much lower than the chance that two keys will map to one slot. For instance, imagine you are in a room with 180 people. The chance that one other person has the same birthday as you is about 50 percent, but the chance that there are 14 people who were born in the same fortnight as you is much lower than 1 percent.",
			  "Below is a plot of the likelihood that an algorithm wont find a search key in the very first place it looks. The happiest place on the graph is the bottom right, where the high load factor saves memory and the lack of collisions means that keys are found quickly with predictable control flow. Youll notice that the plot includes lines for both F14 ideal and F14 with 7-bit tag. The former includes only chunk overflow, while the latter reflects the actual algorithm. Theres a 1/128 chance that two keys have the same 7-bit tag even with a high-quality hash function.",
			  "The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "Collisions are the bane of a hash table: Resolving them creates unpredictable control flow and requires extra memory accesses. Modern processors are fast largely because of pipelining  each core has many execution units that allow the actual work of instructions to overlap."
			]
		  },
		  {
			"title": "Database Processing-in-Memory: An Experimental Study",
			"url": "https://pages.cs.wisc.edu/~yxy/cs839-s20/papers/p334-kepe.pdf",
			"excerpts": [
			  "ns. The hash join and\naggregation require the *gather* and *scatter* SIMD memory\ninstructions to load and store multiple entries of hash tables.",
			  "n par-\nticular, we present a new SIMD sorting algorithm that re-\nquires fewer memory instructions compared to the state of\nthe art [21]. For each operator, we gauge the latency and en-\nergy spend to process TPC-H and Zipf distribution dataset",
			  "Finally, the sorting operation and sort-merge join require the\n*min/max* and *shuffle* SIMD instructions.",
			  "**SIMD units** Unified func. units (integer + floating-point) @1 GHz;",
			  "**4.**"
			]
		  },
		  {
			"title": "Lecture 22 Prefetching Recursive Data Structures",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s16/www/lectures/L22-Prefetching-Pointer-Structures.pdf",
			"excerpts": [
			  "Applicable because a list structure does not change over time",
			  "Improved accuracy outweighs increased overhead in this case",
			  "**H** = history-pointer prefetching",
			  "Health",
			  "Health",
			  "Performance of Data-Linearization Prefetching",
			  " hence data linearization is done without data restructuring",
			  "9% and 18% speedups over greedy prefetching through:",
			  " 94%->78% in perimeter, 87%->81% in treeadd\n",
			  " while maintaining good coverage factors:",
			  " 100%->80% in perimeter, 100%->93% in treeadd",
			  "**G** = greedy prefetching",
			  "**G** = greedy prefetching",
			  "**D** = data-linearization prefetching",
			  "15-745: Prefetching Pointer Structures",
			  "15-745: Prefetching Pointer Structures",
			  "load stall",
			  "load stall",
			  "load stall",
			  "store stall",
			  "store stall",
			  "store stall",
			  "inst. stall",
			  "inst. stall",
			  "inst. stall",
			  "busy",
			  "busy",
			  "busy",
			  "Three schemes to overcome the pointer-chasing problem:",
			  "Carnegie Mellon",
			  "\n**preorder(treeNode * t){**\n**if (t != NULL){**\n**pf(t->left);**\n**pf(t->right);**\n**process(t->data);**\n**preorder(t->left);**\n**preorder(t->right);**\n**}**\n*",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "**O** = original",
			  "Conclusions",
			  "Automated greedy prefetching in SUIF",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**"
			]
		  },
		  {
			"title": "Automatic compiler-inserted prefetching for pointer-based ...",
			"url": "https://ieeexplore.ieee.org/document/752654/",
			"excerpts": [
			  "As the disparity between processor and memory speeds continues to grow, memory latency is becoming an increasingly important performance bottleneck.",
			  "While software-controlled prefetching is an attractive technique for tolerating this latency, its success has been limited thus far to array-based numeric codes.",
			  "In this paper, we expand the scope of automatic compiler-inserted prefetching to also include the recursive data structures commonly found in pointer-based applications.",
			  "We propose three compiler-based prefetching schemes, and automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler.",
			  "Our experimental results demonstrate that compiler-inserted prefetching can offer significant performance gains on both uniprocessors and large-scale shared-memory multiprocessors."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  " We claim that we can achieve $O(\\log_B N)$ page accesses, but without having to know $B$ ahead of time.",
			  "The data structure were using is a good old balanced Binary Search Tree.",
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page.",
			  "One application of the recursive van Embde Boas layout",
			  "Suppose the page size is $B$. Every time step, the height of atoms halves.",
			  "Were interested in the height of the atoms at the first time step where an entire atom can fit in a page.",
			  "Since the number of vertices in a complete binary tree grows exponentially with height, that happens when atoms have height $\\Theta(\\log B)$. T",
			  ". Then, analysing the layout at this resolution, we can fit any atom (which all have the same height of $\\Theta(\\log B)$) into the cache with 1 page load*.",
			  "Then, now consider what happens in a search. A search basically consists of a path from the root of the BST to some leaf*. This path will spend some time in the first atom, until it reaches the leaf of the atom and goes into the next atom, and so forth.",
			  ". Since the path always start at the root vertex of an atom and ends on a leaf, it will spend $\\Theta(\\log B)$ steps in that atom.",
			  "Since the overall search path is $\\log N$ steps long, well need $O(\\frac{\\log N}{\\log B}) = O(\\log_B N)$ atoms, and thats the number of page accesses we need."
			]
		  },
		  {
			"title": "Lab note #044 Sailing by cache-oblivious data structures",
			"url": "https://interjectedfuture.com/lab-notes-044-sailing-by-cache-oblivious-data-structures/",
			"excerpts": [
			  "Cache-oblivious data structures base their Big-O on the number of cache line transfers between different levels of the memory hierarchy to minimize its growth as the data set gets bigger.",
			  "Through it, I learned about the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "If cache-oblivious works as advertised, why haven't I heard of any modern databases using it as an index?",
			  "the idea is 25 years old."
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "http://pdl.cmu.edu/PDL-FTP/Database/CMU-CS-00-177.pdf",
			"excerpts": [
			  "Luk and Mowry proposed three solutions to the pointer-chasing problem 13, 14 ... We then prefetch the next chunk ahead in the jump-pointer array.Read more"
			]
		  },
		  {
			"title": "[PDF] Automatic Compiler-Inserted Prefetching for Pointer-Based Applications | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Automatic-Compiler-Inserted-Prefetching-for-Luk-Mowry/be36f6c5b363116faf91e6eb62f5585b226656a5",
			"excerpts": [
			  "The scope of automatic compiler-inserted prefetching is expanded to also include the recursive data structures commonly found in pointer-based applications, ..."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Our goal: In this paper, we aim to provide an effective, bandwidth- efficient, and low-cost solution to prefetching linked data structures by 1) overcoming the ...Read more",
			  "is paper proposes a low-cost hardware/software cooperative*\n*technique that enables bandwidth-efficient prefetching of linked data*\n*structure",
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,",
			  "The prefetcher brings cache blocks into the L2 (last-\nlevel) cache, since we use an out-of-order execution machine that can\ntolerate short L1-miss latencies",
			  "far ahead of the demand miss\nstream the prefetcher can send requests is determined by the *Prefetch*\n*Distance* parameter.",
			  "ch relies on the observation that most virtual addresses share com-\n ... \ning and object metadata. In *PLDI* , 2004."
			]
		  },
		  {
			"title": "Lecture 21 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s11/public/lectures/L21-Data-Prefetching.pdf",
			"excerpts": [
			  "\nI.\nPrefetching for Arrays\nII. Prefetching for Recursive Data Structures\nReading: ALSU 11 11 4\n**Carnegie Mellon**\nReading: ALSU 11.11.4\nAdvanced readings (optional):\nT.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm for\nPrefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.\nC.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures. In\nProceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.\nTodd C. Mowry\n15-745: Data Prefetching\n1\nThe Memory Latency Problem\n**Carnegie Mellon**\n\n processor speed >>  memory speed\n\ncaches are not a panacea\nTodd C. Mowry\n15-745: Data Prefetching\n2\nUniprocessor Cache Performance on Scientific Code\n\nApplications from SPEC, SPLASH, and NAS Parallel.\nM\nb\nt\nt\ni\nl f MIPS R4000 (100 MH )\n**Carnegie Mellon**\n\nMemory subsystem typical of MIPS R4000 (100 MHz):\n 8K / 256K direct-mapped caches, 32 byte lines\n miss penalties: 12 / 75 cycles\n\n8 of 13 spend > 50% of time stalled for memory\nTodd C. Mowry\n15-745: Data Prefetching\n3\nPrefetching for Arrays: Overview\n\nTolerating Memory Latency\n\nPrefetching Compiler Algorithm and Results\n\nImplications of These Results\n**Carnegie Mellon**\nTodd C. Mowry\n15-745: Data",
			  "Performance of History-Pointer Prefetching\n**O** = original\n\nApplicable because a list structure does not change over time\n**O**\noriginal\n**G** = greedy prefetching\n**H** = history-pointer prefetching\nHealth\n**Carnegie Mellon**\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)\n\nImproved accuracy outweighs increased overhead in this case",
			  "Performance of Data-Linearization Prefetching\n**O** = original\n**G**\nd\nf t hi\n\nCreation order equals major traversal order in treeadd & perimeter\nhence data linearization is done without data restructuring\n**G** = greedy prefetching\n**D** = data-linearization prefetching\n**Carnegie Mellon**\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in treeadd",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "**Lecture 21**",
			  "**Compiler Algorithms for Prefetching Data**",
			  "**Compiler Algorithms for Prefetching Data**"
			]
		  },
		  {
			"title": "Lectures 26-27 Compiler Algorithms for Prefetching Data",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s15/public/lectures/L26-27-Data-Prefetching.pdf",
			"excerpts": [
			  "Propose 3 schemes to overcome the pointer-chasing problem:",
			  "**Compiler Algorithms for Prefetching Data**",
			  "II. Prefetching for Recursive Data Structures",
			  "f Data-Linearization Prefetching",
			  "59",
			  "Conclusions",
			  "Conclusions",
			  "Propose 3 schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetching",
			  "Automated greedy prefetching in SUIF",
			  "Automated greedy prefetching in SUIF",
			  "T.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm fo",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures.",
			  "fewer unnecessary prefetches:",
			  "94%->78% in perimeter, 87%->81% in treeadd",
			  "while maintaining good coverage factors:",
			  "100%->80% in perimeter, 100%->93% in treeadd",
			  "**Lectures 26-27**",
			  "Reading: ALSU 11.11.4",
			  "Advanced readings (optional):",
			  "Prefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.",
			  "In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "1",
			  "\ncaches are not a panacea",
			  "Uniprocessor Cache Performance on Scientific Code",
			  "Applications from SPEC, SPLASH, and NAS Parallel.\n",
			  "\nMemory subsystem typical of MIPS R4000 (100 MHz):",
			  " 8K / 256K direct-mapped caches, 32 byte lines\n",
			  " miss penalties: 12 / 75 cycles",
			  "8 of 13 spend > 50% of time stalled for memory",
			  "3",
			  "Prefetching for Arrays: Overview",
			  "Tolerating Memory Latency",
			  "Prefetching Compiler Algorithm and Results",
			  "Implications of These Results",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "4",
			  "2",
			  "2",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "Coping with Memory Latency",
			  "**Reduce Latency:**",
			  " Locality Optimizations\n",
			  " reorder iterations to improve cache reuse\n",
			  "**Tolerate Latency:**",
			  " move data close to the processor before it is needed\n",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry"
			]
		  },
		  {
			"title": "Lecture 27 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s12/public/lectures/L27-Data-Prefetching-1up.pdf",
			"excerpts": [
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n\n1\n\n",
			  "reedy Prefetching\n\nOur proposals:\n\n\nuse existing pointer(s) in ni to approximate &ni+d\n\na dd new pointer(s) to ni to a pproximate &ni+d\n\nni\nni+d\n\nn\n\na new p oin",
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n",
			  "History-Pointer Prefetching\n\n\nAdd new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n",
			  "Data-Linearization Prefetching\n\n\nNo pointer dereferences are required\n\nMap nodes close in the traversal to contiguous memory\n",
			  "Performance of Compiler-Inserted Greedy Prefetching\n\nload stall\nO = Original\n\nG = Compiler-Inserted Greedy Prefetching\n\nload stall\nstore stall\ninst. stall\nbusy\n\n\nEliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45",
			  "Performance of History-Pointer Prefetching\n\nO = original\n\nG = greedy prefetching\n\nH = history-pointer prefetching\n\n\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n\nHealth\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "Performance of Data-Linearization Prefetching\n\nO = original\nG = greedy prefetching\n\nD = data-linearization prefetching\n\n\nCreation order equals major traversal order in treeadd & perimeter\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n\n 94%->78% in perimeter, 87%->81% in treeadd\n\n while maintaining good coverage factors:\n\n 100% >80% in perimeter  100% >93% in treeadd\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance.",
			  "In the evaluation, the proposed solution achieves an average speedup of 1.37  over a set of memory-intensive benchmarks.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/635508.605427",
			"excerpts": [
			  "C.-K. Luk and T. Mowry. Compiler-based prefetching for recursive data structures. In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 222-233, Cambridge, Massachusetts, October 1996. ACM.",
			  "D. Joseph and D. Grunwald. Prefetching using markov predictors. In Proceedings of the 24th Annual International Symposium on Computer Architecture, pages 252-263, Denver, Colorado, June 1997. ACM."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[30] A. Roth, A. Moshovos, and G. S. Sohi. Dependence based prefetching\nfor linked data structures. In *ASPLOS-8* , 1998.",
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown",
			  "Dependence based\nprefetching for linked data structures, *",
			  "indirections through memory as seen in sparse tensor algebra\nand graphs [ 13  15 ], and more generally applications with\npointer chasing [ 10 , 11 ]. The target application influences the\ndata access pattern that the prefetcher tries to identify and\nprefetch for. For example, a common access pattern in sparse\ntensor algebra and graph computations is An [ *...* A1 [ A0 [ i ]] *...* ] .\nCorr",
			  "Yu et al. [ 13 ] (a.k.a. IMP) tries to detect\naccess patterns given by Y [ Z [ i ]] (2-level IMP) and X [ Y [ Z [ i ]]]\n(3-level IMP), for striding loop variable i , and prefetch\ndata by assuming that Y [ Z [ i + ** ]] and X [ Y [ Z [ i + ** ]]] will\nbe needed in the future. Ainsw"
			]
		  },
		  {
			"title": "Dependence Based Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1998/asplos-prefetch-lds.pdf",
			"excerpts": [
			  "k and Mowry [12] proposed and evaluated a greedy compiler\nalgorithm for scheduling software prefetches for linked data struc-\ntures. They showed this scheme to be effective for certain pro-\ngrams, citing instruction overhead and the generation of useless\nprefetches as performance degradation factors for other",
			  "ry [12] presented a case for history-pointer prefetch-\ning, which augments linked structure nodes with prefetching\npointer fields, and data-linearization, in which LDS are program-\nmatically laid out at runtime to allow sequential prefetch machin-\nery to capture their traversal.",
			  "eir\nalgorithm uses type information to identify recurrent pointer\naccesses, including those accessed via arrays, and may have advan-\ntages in tailoring a prefetch schedule to a particular traversal.",
			  "It collects these loads along with the\ndependence relationships that connect them and constructs a\ndescription of the steps the program has followed to traverse the\nstructure.",
			  "Predicting that the program will continue to follow these\nsame steps, a small prefetch engine takes this description and spec-\nulatively executes it in parallel with the original progra",
			  "Linked data structures (LDS) such as lists and trees are used in\nmany important applications."
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Later Luk and\nMowry proposed a compiler-based prefetching scheme for\nrecursive data structures [22].",
			  "This requires extra storage at\nruntime.",
			  "Jump Pointer [29], which has\nbeen used widely to break the serial LDS (linked data struc-\nture) traversal, stores pointers several iterations ahead in the\nnode currently visite",
			  "In a recent work, Gau-\ntam Doshi et al. [14] discussed the downside of software\nprefetching and exploited the use of rotating registers and\npredication to reduce the instruction overhead",
			  "Profile Guided Software Prefetching",
			  "Software prefetching is ineffective in pointer-based pro-\ngrams. To address this problem, Chi K. Luk et al. presented\na Profile Guided Post-Link Stride Prefetching [23] using a\nstride profile to obtain prefetching guidance for the com-\npil",
			  "[22] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching\nFor Recursive Data Structures. In *ASPLOS-7* , pages 222\n233. ACM Press, 1996",
			  "[23] C.-K. Luk, R. Muth, H. Patil, R. Weiss, P. G. Lowney, and\nR. Cohn. Profile-Guided Post-link Stride Prefetching. In\n*ICS-16* , pages 167178. ACM Press, 2002.",
			  "24] T. C. Mowry, M. S. Lam, and A. Gupta. Design and Evalua-\ntion of A Compiler Algorithm for Prefetching. In *ASPLOS-*\n*5* , pages 6273. ACM Press, ",
			  "[25] T. C. Mowry and C.-K. Luk. Predicting Data Cache Misses\nin Non-Numeric Applications Through Correlation Profil-\ning. In *Micro-30* , pages 314320. IEEE Computer Society\nPress, 199"
			]
		  },
		  {
			"title": "Dynamic Hot Data Stream Prefetching for General-Purpose ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s09/www/papers/prefetch_hds.pdf",
			"excerpts": [
			  "Jump pointers are a software technique for prefetching linked data\nstructures, overcoming the array-and-loop limitation.",
			  "Artificial\njump pointers are extra pointers stored into an object that point to\nan object some distance ahead in the traversal order.",
			  "d. Natural jump pointers are existing pointers in the\ndata\nstructure\nused\nfor\nprefetching.\n",
			  "ng.\nFor\nexample,\ngreedy\nprefetching makes the assumption that when a program uses an\nobject o, it will use the objects that o points to, in the near future,\nand hence prefetches the targets of all pointer fields.",
			  " These\ntechniques were introduced by Luk and Mowry in [22] and refined\nin [5, 18].",
			  "n\ndependence-based prefetching, producer-consumer pairs of loads\nare identified, and a prefetch engine speculatively traverses and\nprefetches them [26].",
			  "\nThe hardware technique that best corresponds to history-pointers is\ncorrelation-based prefetching. As originally proposed, it learns\ndigrams of a key and prefetch addresses: when the key is observed,\nthe prefetch is issued [6]. J"
			]
		  },
		  {
			"title": "2003 Workshop on Duplicating, Deconstructing and ... - PHARM",
			"url": "https://pharm.ece.wisc.edu/wddd/2003/wddd2003_proceedings.pdf",
			"excerpts": [
			  " ware prefetch when bandwidth is limited; with sufficient\nbandwidth software prefetch is the most successful strategy.\nHowever, their research also shows that the combination of\ncache-conscious allocation and software prefetch might not\nlead to further performance improvements, instead it coun-\nteracts changes in bandwidth or latency. Their results are\nsimilar to ours, although we have implemented a different\nsoftware prefetch that does not require any extra memory.\nSeveral researchers have studied hardware prefetch,\nor hybrid schemes, and successfully adapted hardware\nprefetch to pointer-based data structures with irregular ac-\ncess behavior. However, they generally require more hard-\nware than those evaluated in this study. Hardware support\nhas been investigated by the use of lock-up free prefetching,\n[13], and prefetch buffers, [10], and general prefetching in\nhardware is described in [20, 21] together with other cache\nmemory aspects. Karlsson et al., [11], propose a technique\nfor prefetching pointer-based data structures, either in soft-\nware combined with hardware or in software alone, by im-\nplementing prefetch arrays, making it possible to prefetch\nboth short data structures and longer data structures without\nknowing the traversal path. Roth et al. have investigated\nmore adaptable strategies for hybrid prefetch schemes, us-\ning dependence graphs, [18], and jump pointer prefetching,\n[19]. In [19], Roth et al. evaluate a framework for jump-\n ... \n[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999.\n[20] Alan J. Smith. Cache memories. *ACM Computing*\n*Surveys* , 14:3:473530, September 1982.\n[21] Steven P. VanderWiel and David Lilja. Data prefetch\nmechanisms.\n*ACM Computing Surveys* , 32:2:174\n199, June 2000.\n[22] Chengqiang Zhang and Sally A. McKee. Hardware-\nonly stream prefetching and dynamic access order-\ning. In *International Conference on Supercomputing* ,\npages 167175, 2000.\n[23] L. Zhang, S. McKee, W. Hsieh, and J. Carter. Pointer-\nbased prefetching within the impulse adaptable mem-\nory controller: Initial results. In *Proceedings of the*\n*Workshop on Solving the Memory Wall Problem* , June\n2000.\n[24] Craig B. Zilles. Benchmark health considered harm-\nful. *Computer Architecture News* , 29:3, 2001.\n13\n**Comparison of State-Preserving vs. Non-State-Preserving Leakage Control**\n**in Caches**\nDharmesh Parikh\n\n, Yan Zhang\n\n, Karthik Sankaranarayanan\n\n, Kevin Skadron\n\n, Mircea Stan\n\n Dept. ",
			  "[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999."
			]
		  },
		  {
			"title": "APT-GET | Proceedings of the Seventeenth European Conference on Computer Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3492321.3519583",
			"excerpts": [
			  "imin Chen, Anastassia Ailamaki, Phillip B Gibbons, and Todd C Mowry. 2004. Improving Hash Join Performance through Prefetching. In *Proceedings of the 20th International Conference on Data Engineering.* 116.",
			  "mir Roth, Andreas Moshovos, and Gurindar S Sohi. 1998. Dependence based prefetching for linked data structures. *ACM SIGOPS Operating Systems Review* 32, 5 (1998), 115--126",
			  "mison D Collins, Hong Wang, Dean M Tullsen, Christopher Hughes, Yong-Fong Lee, Dan Lavery, and John P Shen. 2001. Speculative pre-computation: Long-range prefetching of delinquent loads. In *Proceedings 28th Annual International Symposium on Computer Architecture.* IEEE, 14--25."
			]
		  },
		  {
			"title": "MetaSys-open-source-cross-layer-metadata- ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/MetaSys-open-source-cross-layer-metadata-management_taco22-arxiv22.pdf",
			"excerpts": [
			  "[30] Chi-Keung Luk and T. C. Mowry. Cooperative Prefetching: Compiler and Hardware Support for Effective Instruction Prefetching in Modern\nProcessors. In *MICRO* , 1998.",
			  "[31] Trishul M Chilimbi and Martin Hirzel. Dynamic Hot Data Stream Prefetching for General-Purpose Programs. In *PLDI* , 2002."
			]
		  },
		  {
			"title": "References - Engineering Information Technology",
			"url": "https://user.eng.umd.edu/~blj/memory/Book-References.pdf",
			"excerpts": [
			  "A. Roth, A. Moshovos, and G. Sohi. 1998. Dependence\nbased prefetching for linked data structures. In\nThe 8th Int. Conf. on Architectural Support for\nProgramming Languages and Operating Systems\n(ASPLOS), pp. 115126, October 1998.",
			  "A. Roth and G. S. Sohi. 1999. Effective jump-pointer\nprefetching for linked data structures. In Proc. 26th Int.\nSymp. on Computer Architecture (ISCA), Atlanta, GA,\nMay 1999.",
			  "E. Rotenberg, S. Bennett, and J. E. Smith. 1996.\nTrace cache: A low latency approach to high\nbandwidth instruction fetching. In Proc. 29th Ann.\nACM/IEEE Int. Symp. on Microarchitecture (MICRO-\n29), pp. 2435, Paris, France, December 1996"
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/248209.237190",
			"excerpts": [
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the ...Read more"
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/384265.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "Dependence based prefetching for linked data structures > Abstract",
			  "Content:\nWe introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different BVH ...",
			"url": "https://www.dominikwodniok.de/publications/Wodniok_EGPGV2013.pdf",
			"excerpts": [
			  "The best performing combination is the TDFS BVH layout with a threshold of 0.6 using the. AoS node layout. It is only marginally faster than the base- line ...Read more"
			]
		  },
		  {
			"title": "Why is SOA (Structures of Arrays) faster than AOS?",
			"url": "https://www.reddit.com/r/C_Programming/comments/9jg7hy/why_is_soa_structures_of_arrays_faster_than_aos/",
			"excerpts": [
			  "I have heard that SOA (Structures of Arrays) is faster due to CPU caching then AOS (Array of Structures). But that doesn't really make sense to me.Read more"
			]
		  },
		  {
			"title": "c - cache locality for a binary tree - Stack Overflow",
			"url": "https://stackoverflow.com/questions/31281905/cache-locality-for-a-binary-tree",
			"excerpts": [
			  "If you want data to be collocated to take advantage of cache locality a simple alternative is to allocate an array of the struct and then allocate from that ...Read more"
			]
		  },
		  {
			"title": "Help with BVH memory access optimizations",
			"url": "https://www.reddit.com/r/GraphicsProgramming/comments/1f90xbi/help_with_bvh_memory_access_optimizations/",
			"excerpts": [
			  "pointer chasing and frequent cache misses.",
			  "flatten the tree.",
			  "Note that `sizeof(BVHNode)` is exactly 32, so a node and one of its children should be pulled together to a cache line as far as I understand. This has made the program twice as much slower!",
			  "Surprisingly this did not improve performance. It is actually comparable to the pointer-based implementation.",
			  "My understanding is that this makes traversal slow due to pointer chasing and frequent cache misses.",
			  "So I quickly moved to flatten the tree."
			]
		  },
		  {
			"title": "Binary search is a pathological case for caches - Paul Khuong: some Lisp",
			"url": "https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/",
			"excerpts": [
			  "Binary search is a pathological case for caches",
			  "\nBinary search suffers from a related ailment when executed on medium\nor large vectors of almost power-of-two size (in bytes)",
			  "Two vectors of 64KB each, allocated at 0x10000 and 0x20000, have the unfortunate property that, for each index, the data at that index in both vectors will map to the same cache lines in a direct-mapped cache with 1024 lines of 64 bytes each.",
			  "the L1D cache has 512 lines of 64 bytes each, L2 4096\nlines, and L3 196608 lines (12 MB). Crucially, the L1 and L2 caches are\n8-way set-associative, and the L3 16-way.",
			  "There are workarounds, on both the hardware and software sides.",
			  "cache lines are power-of-two sizes, as are L1 and L2 set (buckets) counts."
			]
		  },
		  {
			"title": "Locality, B-trees, and splay trees",
			"url": "https://www.cs.cornell.edu/courses/cs312/2004sp/lectures/lec24.html",
			"excerpts": [
			  "Having caches only helps if when the processor needs to get some data, it is\nalready in the cache. Thus, the first time the processor access the memory, it\nmust wait for the data to arrive. On subsequent reads from the same location,\nthere is a good chance that the cache will be able to serve the memory request\nwithout involving main memory. Of course, since the cache is much smaller than\nthe main memory, it can't store all of main memory. The cache is constantly\nthrowing out information about memory locations in order to make space for new\ndata. The processor only gets speedup from the cache if the data fetched from\nmemory is still in the cache when it is needed again. When the cache has the\ndata that is needed by the processor,  it is called a **cache hit** . If\nnot, it is a **cache miss** . The ratio of the number of hits to misses is\ncalled the **cache hit ratio** .",
			  "Caches improve performance when memory accesses exhibit: reads from memory\ntends to request the same locations repeatedly, or at least memory locations\nnear previous requests. A tendency to revisit the same or nearby locations is\nknown as **locality** . Computations that exhibit locality will have a relatively high cache hit\nratio.",
			  "at caches actually store chunks of memory rather than individual words of\nmemory. So a series of memory reads to nearby memory locations are likely to\nmostly hit in the cache. When there is a cache miss, a whole sequence of memory\nwords is requested from main memory at once, because it is cheaper to read\nmemory that way. The cache records cached memory locations in units of **cache\nlines** whose size depends on the size of the cache (typically 4 - 32 words).",
			  "The same idea can be applied to trees. Binary trees are not good for locality\nbecause a given node of the binary tree probably occupies only a fraction of a\ncache line. **B-trees** are a way to get better locality. As in the hash\ntable trick above, we store several elements in a single node -- as many as will\nfit in a cache line."
			]
		  },
		  {
			"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
			"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
			"excerpts": [
			  "is seemingly simple decision can result in **10-100x performance differences** in modern computing system",
			  "The choice between AoS and SoA affects everything from CPU cache efficiency to SIMD vectorization capabilities to GPU memory coalescing patterns.",
			  "Memory Access Pattern Animation"
			]
		  },
		  {
			"title": "Array of Structs and Struct of Arrays | *^^*  HWisnu's blog   ^^",
			"url": "https://hwisnu.bearblog.dev/array-of-structs-and-struct-of-arrays/",
			"excerpts": [
			  "s (AoS) can be less cache-friendly because accessing multiple fields of a single element requires loading multiple cache lines. ",
			  "oA is almost 30% faster based on this simple example, the difference will get significantly magnified with more complex scenario, sometimes SoA (optimized) is 10x faster compared to AoS as shown in this [great article"
			]
		  },
		  {
			"title": "AoS and SoA - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
			"excerpts": [
			  "**Structure of arrays** ( **SoA** ) is a layout separating elements of a [record](/wiki/Record_(computer_science) \"Record (computer science)\") (or 'struct' in the [C programming language](/wiki/C_(programming_language) \"C (programming language)\") ) into one parallel array per [field](/wiki/Field_(computer_science) \"Field (computer science)\") . [[ 1 ]]() The motivation is easier manipulation with packed [SIMD instructions](/wiki/SIMD_instruction \"SIMD instruction\") in most [instruction set architectures](/wiki/Instruction_set_architecture \"Instruction set architecture\") , since a single [SIMD register](/wiki/SIMD_register \"SIMD register\") can load [homogeneous data](/w/index.php?title=Homogeneous_data&action=edit&redlink=1 \"Homogeneous data (page does not exist)\") , possibly transferred by a wide [internal datapath](/wiki/Internal_datapath \"Internal datapath\") (e.g. [128-bit](/wiki/128-bit \"128-bit\") ). If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line. The downside is requiring more [cache ways](/wiki/Cache_way \"Cache way\") when traversing data, and inefficient [indexed addressing](/wiki/Indexed_addressing \"Indexed addressing\") .",
			  "For example, to store *N* points in 3D space using a structure of arrays:",
			  "array of structures",
			  " ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved. This is often more intuitive, and supported directly by most [programming languages](/wiki/Programming_languages \"Programming languages\") .",
			  "AoS vs. SoA presents a choice when considering 3D or [4D vector](/wiki/4D_vector \"4D vector\") data on machines with four-lane SIMD hardware. SIMD ISAs are usually designed for homogeneous data, however some provide a [dot product](/wiki/Dot_product \"Dot product\") instruction [[ 5 ]]() and additional permutes, making the AoS case easier to handle.",
			  "Although most [GPU](/wiki/Graphics_processing_unit \"Graphics processing unit\") hardware has moved away from 4D instructions to scalar [SIMT](/wiki/Single_instruction,_multiple_threads \"Single instruction, multiple threads\") pipelines, [[ 6 ]]() modern [compute kernels](/wiki/Compute_kernel \"Compute kernel\") using SoA instead of AoS can still give better performance due to memory coalescing. [[ 7 ]]()",
			  "SoA is mostly found in languages, libraries, or [metaprogramming](/wiki/Metaprogramming \"Metaprogramming\") tools used to support a [data-oriented design](/wiki/Data-oriented_design \"Data-oriented design\") . Examples include:\n\"Data frames\", as implemented in [R](/wiki/R_(programming_language) \"R (programming language)\") , [Python](/wiki/Python_(programming_language) \"Python (programming language)\") 's Pandas package, and [Julia](/wiki/Julia_(programming_language) \"Julia (programming language)\") 's DataFrames.jl package, are interfaces to access SoA like AoS.\nThe Julia package StructArrays.jl allows for accessing SoA as AoS to combine the performance of SoA with the intuitiveness of AoS."
			]
		  },
		  {
			"title": "\n\tBuilding BVH4 or BVH8 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/td-p/1132356",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure ..."
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies(originally ...",
			"url": "https://www.embree.org/papers/2018-HPG-compressedleafbvh.pdf",
			"excerpts": [
			  "For each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "brees fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is\nstored as 2 ** 3 bytes, for the boxs *lower* and *upper* bounds,\nrequiring 48 bytes for all 8 children. Including the 8 child\npointers, this sums to a total of 136 bytes, slightly more than\nhalf an uncompressed BVH8 multi-node",
			  "sting 1: Illustration of the three BVH node types.**\n**Top: Embrees regular BVH8 nodes contain 8 point-**\n**ers and float boxes (256 bytes). Middle: Embrees**\n**quantized QBVH8 nodes contain 8 pointers, 8 quan-**\n**tized bounding boxes, and 6 floats to specify the**\n**dequantization domain (136 bytes). Bottom: At the**\n**leaf level, our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the no",
			  " resulting BVHwhich we call a Compressed-\nLeaf BVH ( CLBVH )has two multi-node types: regular BVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original BVH8 multi-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1).\nSince",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "Exactly\nwhat primitive data is stored in a leaf depends on the BVH\ntype: for triangles, it is either a list of triangle4 structures,\nfully pre-gathered vertices of four triangles in SoA layout;\nor a list of triangle4i with four triangles worth of vertex\nindices.",
			  "our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the nod",
			  " eliminating the primitive pointers for the leaf nodes our\ncompressed multi-leaf node requires only 72 bytes. Compared\nto an uncompressed BVH8 multi-node (256 bytes) this yields\na compression factor of over 3 ** .",
			  "We implement and evaluate the previously discussed strategy\nwithin a modified version of Embree 3.0.",
			  "mpressed-Leaf*\n*Bounding Volume Hierarchies* (CLBVH), which strike a bal-\nance between compressed and non-compressed BVH layouts",
			  "\nOur CLBVH layout introduces dedicated compressed multi-\nleaf nodes where most effective at reducing memory use, and\nuses regular BVH nodes for inner nodes and small, isolated\nleaves. We show that when implemented within the Embree\nray tracing framework, this approach achieves roughly the\nsame memory savings as Embrees current compressed BVH\nlayout, while maintaining almost the full performance of its\nfastest non-compressed BVH.",
			  "In Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "s fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is",
			  "cus on compressing just the leaf nodes, by introduc-\ning dedicated *compressed multi-leaf nodes* . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\ntraverse interior nodes.\n**",
			  "**2**\n**RELATED WORK**\nAcceleration structures for ray tracing have a long history;\na survey on the general concepts can be found in Havrans\nthesis [ 4 ]. Today, most ray tracers use some sort of BVH,\ntypically with a branching factor of 4 or 8, and in some\ncases 16 [ 2 , 3 , 7 , 9 , 10 ]. While in the past each ray tracer\nimplemented its own acceleration structures and traversal\nmethods, the last few years have seen the emergence of\ncommonly accepted ray tracing libraries such as Embree\nfor CPUs [ 10 ], and OptiX for GPUs [ 7 ], both of which use\nwide BVHs.",
			  "n**\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf."
			]
		  },
		  {
			"title": "Hardware-Accelerated Dual-Split Trees",
			"url": "https://hwrt.cs.utah.edu/papers/hardware_dual-split_trees.pdf",
			"excerpts": [
			  "A wider node (4 or 8 children) is attractive to current CPU\nor GPU architectures as computations on these wider nodes can benefit from SIMD computation.",
			  " BVH8 node has 200 bytes (192 bytes\nfor child bounding boxes, 4 bytes for offset, and 4 bytes for node types).",
			  "BVH4 and BVH8 nodes\nstore degenerate bounding boxes for empty children."
			]
		  },
		  {
			"title": "Building BVH4 or BVH8",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/m-p/1132356/highlight/true",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure and"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray ...",
			"url": "https://jo.dreggn.org/home/2008_qbvh.pdf",
			"excerpts": [
			  "The result is a large BVH node with a size of 128 bytes which\nperfectly matches the caches of modern computer hardware.",
			  "The 4 bounding boxes are stored in structure-of-arrays (SoA)\nlayout for direct processing in SIMD registers.",
			  "The leaf bounding box is already\nstored in the parent and the leaf data can be encoded directly\ninto the corresponding child integer.",
			  "We use the sign of the child index to encode whether a node\nis a leaf or an inner node.",
			  "The tree data is kept in a linear array of memory\nlocations, so the child pointer can be stored as integer indices\ninstead of using platform-dependent pointers.",
			  "Four\nchildren are created for each node.",
			  "Given a ray, the stack-based traversal algorithm starts by si-\nmultaneously intersecting the ray with the four bounding boxes\ncontained in the root SIMD BVH Node using SIMD instructions",
			  "The pointers of the children with a non-empty bounding box\nintersection are sorted and then pushed on the stack.",
			  "The ray is prepared prior to traversal by replicating the values\nfor the maximum ray distance ( tfar ), the origin, and reciprocal\nof the direction across a SIMD register."
			]
		  },
		  {
			"title": "Exploiting Local Orientation Similarity for Efficient Ray ...",
			"url": "https://www.embree.org/papers/2014-HPG-hair.pdf",
			"excerpts": [
			  "a BVH with a branching factor of four that will allow for al-\nways intersecting four child-bounds in parallel (Section 3.4 ).",
			  "This data-parallel intersection in particular requires that ev-\nery group of four sibling nodes have to be of the same type:\nIf only one prefers OBBs, all four nodes have to be OBB\nnodes.",
			  "**Node References.** A node stores 64-bit *node references* to\npoint to its children. These node references are decorated\npointers, where we use the lower 4 bits to encode the type of\ninner node we reference (AABB node or OBB node) or the\nnumber of hair segments pointed to by a leaf node. During\ntraversal we can use simple bit operations to separate the\nnode type information from the aligned pointer.",
			  "**AABB nodes.** For nodes with axis aligned bounds, we store\nfour bounding boxes in a SIMD friendly structure-of-array\nlayout (SOA). In addition to the single-precision floating\npoint coordinates for the four AABBs this node also stores\nthe four 64-bit node references, making a total of 128 bytes\n(exactly two 64-byte cache lines)."
			]
		  },
		  {
			"title": "Faster Incoherent Ray Traversal Using 8-Wide AVX ...",
			"url": "https://www.cs.ubbcluj.ro/~afra/publications/afra2013tr_mbvh8.pdf",
			"excerpts": [
			  ";\nThe node size for MBVH4 is 128 bytes, and for MBVH8\nit is 256 bytes.\nT",
			  "\nThe multi-triangles, similar to the bounding boxes in the\nnodes, have a fixed-size SoA layout to facilitate SIMD pro-\ncessing. The triangle data is pregathered to maximize inter-\nsection performance at the cost of higher memory usage.\n**"
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231581/06-1025-benthin.pdf",
			"excerpts": [
			  ".\n**4.1**\n**Node Compression and Decompression**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf.",
			  "**struct** BVH8MultiNode {\nbox3f childBounds [8]; //one float box per child\nuint64 childRef [8]; }; // child pointers",
			  "**struct** QBVH8MultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nuint64 childRef [8]; }; // child pointers",
			  "**struct** CLBVHMultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nLeafPrimData childPrims [0]; // implicit pointer\n}; // leaf data stored right behind this node",
			  "ion**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf. E",
			  "**4**\n**IMPLEMENTATION**\nWe implement and evaluate the previously discussed strategy within\na modified version of Embree 3.0.",
			  "The key to improving fast ray tracing is the use of acceleration data\nstructures. Though indispensable for performance, such structures\nrequire both time and memory to be built and stored. In particu-\nlar, the memory overhead of the acceleration structure can be a",
			  "roduces dedicated compressed multi-leaf nodes where most effec-\ntive at reducing memory use, and uses regular BVH nodes for inner\nnodes and small, isolated leaves. We show that when implemented\nwithin the Embree ray tracing framework, this approach achieves\nroughly the same memory savings as Embrees compressed BVH\nlayout, while maintaining almost the full performance of its fastest\nnon-compressed BVH.\n**CC"
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH ...Read more"
			]
		  },
		  {
			"title": "[PDF] Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://www.semanticscholar.org/paper/Efficient-incoherent-ray-traversal-on-GPUs-through-Ylitie-Karras/7d4816055b1a9aecb75f36e5e6ab5948b1354ed1",
			"excerpts": [
			  "Efficient incoherent ray traversal on GPUs through compressed wide BVHs  H. Ylitie, Tero Karras, S. Laine  Published in High Performance Graphics 28 July 2017 ..."
			]
		  },
		  {
			"title": "(PDF) Compressed-leaf bounding volume hierarchies",
			"url": "https://www.researchgate.net/publication/326762122_Compressed-leaf_bounding_volume_hierarchies",
			"excerpts": [
			  "Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH layouts. Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "Typically, wide BVHs use a data layout where all of an\nindividual nodes N children are stored together in a consec-\nutive block, typically in a SoA data layout. This allows for\naddressing all of a parents N children with a single pointer\nand aids vectorization, but slightly confuses the terminology\nof what a node in a wide BVH actually i",
			  "Throughout the\npaper we will refer to each group of N siblings as a N-wide\nmulti- node, with each sibling consisting of N individual nodes",
			  "s . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\n ... ",
			  " summary, our resulting BVHwhich we call a Compressed-\nLeaf BVH (\nCLBVH\n)has two multi-node types: regular\nBVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original\nBVH8\nmulti-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1)",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "CLBVH-fast\nuses our\nCLBVH\nnodes with regular,\nuncompressed leaf data, while\nCLBVH-compact\nperforms the\nleaf data compression described in Section 4.5.\nWhen looking at only the\nQBVH8\nand\nCLBVH-fast\nvariants,\nthe relative memory savings of both are higher, since they\nare no longer as dominated by the leaf data cost.",
			  "For an additional roughly 5% of performance,\nCLBVH-compact\ncan save even more memory, reaching, on average, a nearly\n3  reduction in total memory "
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call *Compressed-Leaf Bounding Volume Hierarchies* (CLBVH), which strike a balance between compressed and non-compressed BVH layouts.",
			  "Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "We show that when implemented within the Embree ray tracing framework, this approach achieves roughly the same memory savings as Embree's compressed BVH layout, while maintaining almost the full performance of its fastest non-compressed BVH."
			]
		  },
		  {
			"title": "Carsten Benthin, Ingo Wald, Sven Woop, Attila fra",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CompressedLeafBoundingVolumeHierarchies.pdf",
			"excerpts": [
			  "Tweak top-down BVH builder to primarily generate all-leaf multi-nodes",
			  "Store referenced primitive data directly behind all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "Leaf-Data 0",
			  "Leaf-Data 1",
			  "Leaf-Data 2",
			  "Leaf-Data 3",
			  "Leaf-Data 4",
			  "Leaf-Data 5",
			  "Leaf-Data 6",
			  "Leaf-Data 7",
			  "EVEN FURTHER COMPRESSION CLBVH\n(compact)",
			  "Extract shared features in geometry data in all-leaf multi-node",
			  "Object IDs, vertices, vertex indices, shader IDs, etc."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://pdfs.semanticscholar.org/f65c/b99458aaf6c9f609e7950711188ea97a025e.pdf",
			"excerpts": [
			  "8-wide BVH constructed with SAH-optimal widening",
			  "Compressed node storage format",
			  "Quantization grid position and scale\nstored in parent node",
			  "Quantize child node AABBs to a local grid",
			  "Quantize child node AABBs to a local grid"
			]
		  },
		  {
			"title": "[PDF] Accelerating Graph Analytics on CPU-FPGA Heterogeneous Platform | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Accelerating-Graph-Analytics-on-CPU-FPGA-Platform-Zhou-Prasanna/5cf2f8dd9491b2cf0c42b0d2a6ccdae5c764f906",
			"excerpts": [
			  "Graphicionado: A high-performance and energy-efficient accelerator for graph analytics  Computer Science, Engineering. 2016 49th Annual IEEE/ACM International ..."
			]
		  },
		  {
			"title": "Two-way skewed-associative caches | by Arpit Gupta | Medium",
			"url": "https://medium.com/@arpitguptarag/two-way-skewed-associative-caches-9bab2c36dcee",
			"excerpts": [
			  "Dr Andre Seznec describes a new design skewed-associative caches which helps in improving miss rate while not compromising with the frequency at the same time.Read more"
			]
		  },
		  {
			"title": "US20160188476A1 - Hardware prefetcher for indirect access patterns \n      - Google Patents",
			"url": "https://patents.google.com/patent/US20160188476A1/en",
			"excerpts": [
			  "Two techniques address bottlenecking in processors. The first is indirect prefetching. The technique can be especially useful for graph analytics and sparse ..."
			]
		  },
		  {
			"title": "ShadowLoad: Injecting State into Hardware Prefetchers",
			"url": "https://misc0110.net/web/files/shadowload_asplos25.pdf",
			"excerpts": [
			  "ShadowLoad exploits the missing isolation of hardware prefetcher state between applications or privilege levels on. Intel and AMD CPUs up to ( ...Read more"
			]
		  },
		  {
			"title": "Looking Under the Hood of CPU Cache Prefetching",
			"url": "https://dbis.cs.tu-dortmund.de/storages/dbis-cs/r/papers/2024/sw-prefetching-survey/sw-prefetching.pdf",
			"excerpts": [
			  "Both Intel and AMD note that these non-temporally prefetched cache lines are prioritized for quicker eviction [4, 13]. 2.3 Limitations. Modern ...Read more",
			  "Interplay with Hardware Prefetchers",
			  " saw that software prefetching has its limits when it is nec-\nessary to prefetch large data blocks at once. To circumvent this\nlimitation and associated costs, a collaboration between software\nand hardware prefetching emerges as a promising solution, form-\ning a symbiotic relationship: Software prefetching takes the initial\nstep by early hinting about unpredictable data accesses; hardware\nprefetchers can take over, incrementally fetching the remaining\ndata line by line. In this section, we delve into their cooperative\ndynamics, emphasizing how strategic software prefetching *stimu-*\n*lates* hardware prefetchers beyond the typical dependence on load\nmiss",
			  "ardware Prefetcher Patterns.* In our in-depth analysis, we con-\ncentrate on Intel processors, which offer valuable insights into\nprefetching dynamics by implementing sampling of data addresses\nand access latency through the Linux *perf* interface [ 1 ]. Intel uti-\nlizes multiple core-local prefetchers that serve the L 1 and L 2 caches\nacross both modern and legacy processor generations [ 13 , 32 ]. The\nL1d prefetchers monitor load streams within a single cache line,\nsuccessively requesting the next line. Meanwhile, L 2 prefetchers\nstudy the patterns of cache line requests traversing through the\nL 2 cache. Here, the *stream prefetcher* initiates prefetches based on\na sequence of consecutive cache line requests, while the *adjacent*\n*prefetcher* consistently loads a pair of neighboring cache lines at\nonce. This architectural design prompts two critical lines of inquiry:\nDo software and hardware prefetchers have any interaction? If yes,\nwhat are the impacts of different prefetch instructions?",
			  "Discussion.* The measured results allow us to conclude that hardware\nand software prefetchers interact with each other, even on different\nhardware platforms. Leveraging this interaction becomes essential\nwhen large amounts of data should be transferred from memory to\nthe CPU cachegiven the limitations of LFB s highlighted in Sec-\ntion 3.2. Consider the *range scan* of tree structures as an example:\nAlthough accessed in a logical order, leaf nodes are scattered in\nmemory and accessed in a seemingly random fashion from the\nmemory subsystems point of view. While software prefetching can\nsignal to the hardware what leaf nodes will be accessed shortly, thus\ninitiating the transfer into the cache, the hardware prefetcher can\neffectively take over once it recognizes the sequential access pat-\ntern. This principle also extends to morsel-driven database engines,\nsuch as HyPer [ 17 ] and Umbra [ 29 , 36 ], which process fine-grained\npackages of tuples"
			]
		  },
		  {
			"title": "Graphicionado: A High-Performance and Energy-Efficient ...",
			"url": "https://taejunham.github.io/data/graphicionado_micro16.pdf",
			"excerpts": [
			  "asily perform next-line prefetches and get the data into the\naccelerator before they are needed. We extend the *sequential*\n*vertex read* and *edge read* modules (stage P1 and P3 in Fig. 6)\nto prefetch and buffer up to *N* cachelines ( *N* = 4 is used for\nour evaluation) and configure them to continue fetching the\nnext cacheline from memory as long as the buffer is not ful",
			  "**exploits not only data structure-centric datapath specialization,**\n**but also memory subsystem specialization, all the while taking ad-**\n**vantage of the parallelism inherent in this domain. G",
			  "icionado\ncan process graph analytics workloads with reasonable effi-\nciency. However, thus far it is a single pipeline with theoretical\nmaximum throughput limited to one edge per cycle in the\n*Processing* phase and one vertex per cycle in the *Apply*\nphase",
			  "e on-chip storage allowed dramatic\nreduction of the communication latency and bandwidth by con-\nverting the frequent and inefficient random off-chip data com-\nmunication to on-chip, efficient, finer-granularity scratchpad\nmemory accesse",
			  "This paper makes the following contributions:",
			  " A specialized graph analytics processing hardware pipeline\nthat employs datatype and memory subsystem special-\nizations while offering workload-specific reconfigurable\nblocks called Graphicionado",
			  "Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "The Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "= 4 is used for\nour evaluation"
			]
		  },
		  {
			"title": "Graphicionado",
			"url": "https://mrmgroup.cs.princeton.edu/slides2/graphicionado_slide.pdf",
			"excerpts": [
			  "Graphicionado : A high-performance , energy-efficient graph analytics HW\naccelerator which overcomes the limitations of software frameworks while\nretaining the programmability benefit of SW frameworks",
			  "Graphicionado utilizes an on-chip storage to avoid wasting off-chip BW",
			  "Partition a graph before the execution; Then, process each subgraph\nat a time",
			  "Scaling On-Chip Memory Usage",
			  "Only half of the vertices to be stored in on-chip storage at a time"
			]
		  },
		  {
			"title": "adaptive prefetching on POWER7",
			"url": "https://people.ac.upc.edu/fcazorla/articles/vjimenez_pact_2012.pdf",
			"excerpts": [
			  "ardware data prefetch is a well-known technique to help\nalleviate the so-called *memory wall* problem [31]",
			  ", 7, 15] is programmable and al-\nlows the user to set different parameters (knobs) that control\nits behavior: i) *prefetch depth* , how many lines in advance\nto prefetch, ii) *prefetch on stores* , whether to prefetch store\noperations, and iii) *stride-N* , whether to prefetch streams\nwith a stride larger than one cache block.",
			  "The prefetcher is\ncontrolled via the data stream control register (DSCR).",
			  "The\nLinux kernel exposes the register to the user through the sys\nvirtual filesystem [22], allowing the user to set the prefetch\nsetting on a per-thread basis.",
			  "In this paper,\nwe present an adaptive prefetch scheme that dynamically\nadapts the prefetcher configuration to the running work-\nload, aiming to improve performan",
			  "e use the IBM\nPOWER7 [27] as the vehicle for this study, since: (i) this\nrepresents a state-of-the-art high-end processor, with a ma-\nture data prefetch engine that has evolved significantly since\nthe POWER3 time-frame; and (ii) this product provides\nfacilities for accurate measurement of performance metrics\nthrough a user-visible performance counter interface",
			  "POWER7 contains a programmable prefetch engine that\nis able to prefetch consecutive data blocks as well as those\nseparated by a non-unit stride [27].",
			  "The processor system is\nprovided to customers with a default prefetch setting that\nis targeted to improve performance for most applications."
			]
		  },
		  {
			"title": "Making Data Prefetch Smarter: Adaptive Prefetching on POWER7 | Conference Paper | PNNL",
			"url": "https://www.pnnl.gov/publications/making-data-prefetch-smarter-adaptive-prefetching-power7",
			"excerpts": [
			  "Hardware data prefetch engines are integral parts of many general purpose server-class microprocessors in the eld to- day. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default conguration during system bring-up and dynamic reconguration of the prefetch engine is not an autonomic feature of current machines.",
			  "In fact, they may actually de- grade performance due to useless bus bandwidth consump- tion and cache pollution.",
			  "In this paper, we present an adap- tive prefetch scheme that dynamically modies the prefetch settings in order to adapt to the workload requirements.",
			  "We implement and evaluate adaptive prefetching in the con- text of an existing, commercial processor, namely the IBM POWER7.",
			  "Our adaptive prefetch mechanism improves per- formance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively.",
			  "Categories",
			  "*Revised: July 23, 2014 | Published: September 19, 2012*"
			]
		  },
		  {
			"title": "Adaptive prefetching on power7: Improving performance and power consumption for ACM TOPC - IBM Research",
			"url": "https://research.ibm.com/publications/adaptive-prefetching-on-power7-improving-performance-and-power-consumption",
			"excerpts": [
			  "We present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to workloads' requirements.",
			  "Adaptive prefetching is also able to reduce power consumption in some cases.",
			  "showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively.",
			  "First we characterize\"in terms of performance and power consumption\"the prefetcher in that processor using microbenchmarks and SPEC CPU2006.We then present our adaptive prefetch mechanism showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Making data prefetch smarter: Adaptive prefetching on power 7 for PACT 2012 - IBM Research",
			"url": "https://research.ibm.com/publications/making-data-prefetch-smarter-adaptive-prefetching-on-power-7",
			"excerpts": [
			  "ware data prefetch engines are integral parts of many general purpose server-class microprocessors in the field today. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default configuration during system bring-up and dynamic reconfiguration of the prefetch engine is not an autonomic feature of current machines. Conceptually, however, it is easy to infer that commonly used prefetch algorithms, when applied in a fixed mode will not help performance in many cases. In fact, they may actually degrade performance due to useless bus bandwidth consumption and cache pollution. In this paper, we present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to the workload requirements. We implement and evaluate adaptive prefetc",
			  "Our adaptive prefetch mechanism improves performance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Skewed-Associative Caches: CS752 Final Project",
			"url": "https://pages.cs.wisc.edu/~chalpin/about/resume/cs752.pdf",
			"excerpts": [
			  "Skewed-associative caches were proposed as a way to decrease the miss rate, while\nnot further increasing the size or associativity.",
			  "In a single level cache system, skewing\nfunctions are useful to help decrease the miss rate, and cause fewer requests to main\nmemory.",
			  "They could be used in the L1 cache, but since they add a small amount of logic\nto the critical path of cache access, they do not meet the criteria of fast L1 cache access\nstated above.",
			  "We will explore the claim by Seznec that a two-way skewed-associative cache\nwill perform as well as a four-way set associative cache.",
			  "The basic requirement is that *A* 1 ** *A* 2 results in values\nappropriate to address the number of blocks in a bank. ",
			  "The shuffle function is simply a reordering of the wires."
			]
		  },
		  {
			"title": "Cache architecture research  ALF",
			"url": "https://team.inria.fr/alf/members/andre-seznec/cache-architecture-research/",
			"excerpts": [
			  "The skewed associative cache is a new organization for multi-bank caches.",
			  "Skewed-associative caches have been shown to have two major advantages over conventional set-associative caches.",
			  "First, at equal associativity degrees, a skewed-associative cache typically exhibits the same hardware complexity as a set-associative cache, but exhibits lower miss ratio.",
			  "This is particularly significant for BTBs and L2 caches for which a significant ratio of conflict misses occurs even on 2-way set-associative caches.",
			  "Second, the behavior of skewed-associative caches is quite insensitive to the precise data placement in memory.",
			  "We also  showed that the skewed associative structure offers a unique opportunity to build TLBs supporting multiple page sizes."
			]
		  },
		  {
			"title": "Minimally-skewed-associative caches",
			"url": "https://ieeexplore.ieee.org/document/1180765/",
			"excerpts": [
			  "Skewed-associativity is a technique that reduces the miss ratios of CPU caches by applying different indexing functions to each way of an associative cache.",
			  "Although several of the new architectures were welcomed by the industry, such as victim caches which are employed in AMD processors, skewed-associativity was not, presumably because implementation of the original scheme is complex and, most probably, involves access-time penal-ties among other costs",
			  ". The purpose of this work is to evaluate a simplified, easy to implement version that we call minimally-skewed-associativity (MSkA).",
			  "Miss ratios are not as good as those for full skewing, but they are still advantageous.",
			  "Minimal-skewing is thus proposed as a way to improve the hit/miss performance of caches, often without producing access-time delays or increases in power consumption as other techniques do (for example, using higher associativities).",
			  "**Published in:** [14th Symposium on Computer Architecture and High Performance Computing, 2002. Proceedings.](/xpl/conhome/8407/proceeding)",
			  "**Date of Conference:** 28-30 October 2002"
			]
		  },
		  {
			"title": "GraphP: Reducing Communication for PIM-based Graph ...",
			"url": "http://alchem.usc.edu/portal/static/download/graphp.pdf",
			"excerpts": [
			  "Martonosi,. Graphicionado: A high-performance and energy-efficient ac- celerator for graph analytics, in Microarchitecture (MICRO),. 2016 49th Annual IEEE ...Read more"
			]
		  },
		  {
			"title": "Seminar on Computer Architecture",
			"url": "https://pdfs.semanticscholar.org/efc1/28130e3b2170ca9102abbeb62f7984d1a81a.pdf",
			"excerpts": [
			  "DDR3-OoO HMC-OoO HMC-MC Tesseract Tesseract. LP. Tesseract. LP + MTP. +56%. +25%. 9.0x. 11.6x. 13.8x. Average Performance. Page 18. Evaluation Results. 18.Read more"
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://inria.hal.science/hal-02991204/document",
			"excerpts": [
			  "Our more recent tests have shown that the Emu hardware can achieve up to 1.6 GB/s per node and 12.8 GB/s on 8 nodes for the STREAM benchmark, ...Read more"
			]
		  },
		  {
			"title": "Improving Streaming Graph Processing Performance using ...",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3466752.3480096",
			"excerpts": [
			  "The protocol FSM takes appropriate actions on the MSHR status change: the task is forwarded to a FIFO buffer to the cache controller  and the ...Read more"
			]
		  },
		  {
			"title": "Energy characterization of graph workloads - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S221053792030189X",
			"excerpts": [
			  "For these reasons, there are several ongoing research efforts exploring custom architectures to enhance graph processing. Some solutions include custom processing elements that decouple computation from communication (e.g., Graphicionado [1], Lincoln Lab graph processor [2]), while other designs explore near memory processing (e.g., Tesseract [3], GraphPIM [4], GraphP [5]).",
			  "Some systems, like the Cray XMT [6], [7], exploit multithreading to tolerate, rather than reduce, latencies even at a large scale.",
			  "The EMU system [8] exploits the concept of migrating threads near to the data.",
			  "The DARPA Hierarchical Identify Verify and Exploit (HIVE) program [9] is looking to build a graph analytics processor that can process (streaming) graphs faster and at much lower power than current processing technology.",
			  "A particular focus of HIVE is to optimize both performance and power (i.e., the efficiency), trying to reach 1000 times the TEPS/W (traversed edges per second per Watt) of current designs (such as GPUs and conventional CPUs).",
			  "Ideally, a processor 1000faster in the same power envelope of a current design, or a processor as fast as a current one, but consuming 1/1000th of the power, would both reach the project objective."
			]
		  },
		  {
			"title": "A Scalable Processing-in-Memory Accelerator for Parallel ...",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/tesseract-pim-architecture-for-graph-processing_isca15.pdf",
			"excerpts": [
			  "ign a programmable PIM accelerator for large-scale*\n*graph processing called Tesseract. Tesseract is composed of*\n*(1) a new hardware architecture that fully utilizes the available*\n*memory bandwidth, (2) an efficient method of communication*\n*between different memory partitions, and (3) a programming*\n*interface that reflects and exploits the unique hardware de-*\n*sign. It also includes two hardware prefetchers specialized for*\n*memory access patterns of graph processing, which operate*\n*based on the hints provided by our programming model. Our*\n*comprehensive evaluations using five state-of-the-art graph*\n*processing workloads with large real-world graphs show that*\n*the proposed architecture improves average system perfor-*\n*mance by a factor of ten and achieves 87% average energy*\n*reduction over conventional s",
			  "* We provide case studies of how five graph processing work-\nloads can be mapped to our architecture and how they\ncan benefit from it. Our evaluations show that Tesseract\nachieves 10x average performance improvement and 87%\naverage reduction in energy consumption over a conven-\n ... ",
			  "\nOur prefetching mechanisms, when employed together, en-\nable Tesseract to achieve a 14x average performance improve-\nment over the DDR3-based conventional system, while min-\nimizing the storage overhead to less than 5 KB per core (see\nSection 4.1). Me",
			  "The reason why conventional systems fall behind Tesseract\nis that they are limited by the low off-chip link bandwidth\n( 102.4 GB/s in DDR3-OoO or 640 GB/s in HMC-OoO/-MC)\nwhereas our system utilizes the large internal memory band-\nwidth of HMCs ( 8 TB/s ). 10 "
			]
		  },
		  {
			"title": "Retrospective: A Scalable Processing-in-Memory Accelerator ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/Tesseract_50YearsOfISCA-Retrospective_isca23.pdf",
			"excerpts": [
			  "15 paper [1] provides a new pro-**\n**grammable processing-in-memory (PIM) architecture and system**\n**design that can accelerate key data-intensive applications, with**\n**a focus on graph processing wor",
			  "our accelerator system, Tesseract, using 3D-stacked memories**\n**with logic layers, where each logic layer contains general-purpose**\n**processing cores and cores communicate with each other using a**\n**message-passing programming mod",
			  " the first to completely design**\n**a near-memory accelerator system from scratch such that it is**\n**both generally programmable and specifically customizable to**\n**accelerate important applications, with a case study on major**\n**graph processing workloads. Ensuing work in academia and**\n**industry showed that similar approaches to system design can**\n**greatly benefit both graph processing workloads and other**\n**applications, such as machine learning, for which ideas from**\n**Tesseract seem to have been influential"
			]
		  },
		  {
			"title": "[2306.15577] Retrospective: A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing",
			"url": "https://arxiv.org/abs/2306.15577",
			"excerpts": [
			  ". We built our accelerator system, Tesseract, using 3D-stacked memories with logic layers, where each logic layer contains general-purpose processing cores and cores communicate with each other using a message-passing programming model.",
			  " Our ISCA 2015 paper provides a new programmable processing-in-memory (PIM) architecture and system design that can accelerate key data-intensive applications, with a focus on graph processing workloads. ",
			  "r major idea was to completely rethink the system, including the programming model, data partitioning mechanisms, system support, instruction set architecture, along with near-memory execution units and their communication architecture, su"
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://dl.acm.org/doi/10.1145/2749469.2750386",
			"excerpts": [
			  "The key modern enabler for PIM is the recent advancement of the 3D integration technology that facilitates stacking logic and memory dies in a single package, which was not available when the PIM concept was originally examined.",
			  "Tesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design.",
			  "In this work, we argue that the conventional concept of processing-in-memory (PIM) can be a viable solution to achieve such an objective.",
			  "In order to take advantage of such a new technology to enable memory-capacity-proportional performance, we design a programmable PIM accelerator for large-scale graph processing called Tesseract.",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model.",
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems."
			]
		  },
		  {
			"title": "An Initial Characterization of the Emu Chick",
			"url": "https://fruitfly1026.github.io/static/files/ipdpsw18-hein.pdf",
			"excerpts": [
			  "The Emu Chick prototype is still in active development. The\ncurrent hardware iteration uses an Arria 10 FPGA on each\nnode card to implement the Gossamer cores, the migration\nengine, and the stationary cores.",
			  ".\nThe Emu is a cache-less system built around nodelets that\neach execute lightweight threads and migrate threads to data\nrather than moving data through a traditional cache hierarchy.\nThi",
			  "e current prototype hardware uses**\n**FPGAs to implement cache-less Gossamer cores for doing**\n**computational work and a stationary core to run basic operating**\n**system functions and migrate threads betw",
			  "Pointer chasing on the Xeon architecture performs poorly\nfor several reasons. For small block sizes, the memory system\nbandwidth is used inefficiently. An entire 64-byte cache line\nmust be transferred from memory, but only 16 bytes will\nbe used. The best performance is achieved with a block size\nbetween 256 and 4096 elements.",
			  "Performance on Emu remains mostly flat regardless of block\nsize. Emus memory access granularity is 8 bytes, so it never\ntransfers unused data in this benchmark. As long as a block\nfits within a single nodelets local memory channel, there is no\npenalty for random access within the block.",
			  "The Emu Chick is a prototype system designed**\n**around the concept of migratory memory-side processing. Rather**\n**than transferring large amounts of data across power-hungry,**\n**high-latency interconnects, the Emu Chick moves lightweight**\n**thread contexts to near-memory cores before the beginning**\n*",
			  "The\nEmu architecture is designed from the ground up to support\nhigh bandwidth utilization and efficiency for demanding data\nanalysis workloads.",
			  "he Emu Chick provides stable,**\n**predictable performance with 80% bandwidth utilization on a**\n**random-access pointer chasing benchmark with weak lo"
			]
		  },
		  {
			"title": "A Microbenchmark Characterization of the Emu Chick",
			"url": "https://arxiv.org/pdf/1809.07696",
			"excerpts": [
			  "The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring\nlarge amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to\nnear-memory cores before the beginning of each memory read.",
			  "Emu bandwidth is currently limited by CPU speed and thread\ncount rather than DDR bus transfer rates. However even with\nthis prototype system we can observe improvements in other\nbenchmarks where the memory access pattern is not as linear\nand predictable as it is with STREAM.",
			  "Pointer chasing on the Xeon architecture performs poorly\nfor several reasons. For small block sizes, the memory system\nbandwidth is used ine ffi ciently. An entire 64-byte cache line\nmust be transferred from memory, but only 16 bytes will be used.",
			  "e pointer chasing benchmark in Section 4.2 achieves a\nstable 60-65% bandwidth utilization across a wide range of lo-\ncality parameters. These pointer chasing results and data layout\nstudies show how random accesses with SpMV can be improved\nand while performance of SpMV does not quite match a well-\noptimized x86 implementation, these optimizations can provide\na template for future benchmarking and application development\nand show how application memory layouts and smart thread\nmigration can be used to maximize performance on the Emu\nsystem",
			  "results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more e ffi ciently than a more\ntraditional, cache-based architecture although bandwidth usage su ff ers for computationally intensive workloads like SpMV.",
			  "Moreover,\nthe Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access\npointer chasing benchmark with weak locality.",
			  "The main high-level finding is that an Emu-style architecture\ncan more e ffi ciently utilize available memory bandwidth while\nreducing the variability of that bandwidth to the memory access\npatter"
			]
		  },
		  {
			"title": "Programming the EMU Architecture: Algorithm Design ...",
			"url": "https://sc18.supercomputing.org/proceedings/tech_poster/poster_files/post213s2-file2.pdf",
			"excerpts": [
			  "**EMU-Chick System:** 8 nodes, 64GB memory per node.",
			  "**Node:** Each node has 8x nodelets, an array of DRAMs, a migration engine, PCI-\nExpress interfaces, and a stationary core (SC), accompanied with an SSD.",
			  "**Nodelet:** A nodelet contains 2x Gosamer cores (GC), each of which supports 64\nconcurrent in-order, single-issue hardware threads",
			  "**Memory:** Each node has a 64-byte channel DRAM, divided into eight 8-byte\nnarrow-channel-DRAMs (NC-DRAM).",
			  "**EMU Architecture**",
			  "Using higher number of memory blocks in mw_malloc2d will force threads to\nmigrate more frequently across nodelets.",
			  "Allocating fewer blocks will distribute memory into fewer nodelets and will\nrestrict parallelism.",
			  "\n Flat spawn: The parent thread sequentially iterates over the for loop and spawns\nchildren. Thread creation complexity is O(n) , where n is the number of threads.\n",
			  " Tree spawn: The process of creating children in EMU can be parallelized by using\na recursive, hierarchal spawn. Thread creation complexity is O(log n).\nT",
			  "Three factors play a major role in performance on EMU systems:",
			  "Each nodelet in EMU-chick can support up to 256 live threads and also can hold\ncontext information of up to 500 threads.",
			  "Threads that exceed this level will fail to spawn and execute as a regular function.",
			  "The optimal number of threads per nodelet varies by the load.",
			  "**Conclusion**",
			  "EMU presents an unorthodox approach to solve the increasingly worsening\nmemory bottleneck problem in HPC.",
			  "There are unique architectural considerations that need to be addressed while\ndeveloping for the EMU platform.",
			  "**Level-Synchronous BFS for EMU**",
			  "We use compressed sparse row representation (CSR) to represent graphs in\nmemory.",
			  "BFS-EMU follows a hierarchical launch strategy",
			  " First, visit_node_block is spawned for every node block, so that further\nnode related operations become local to the spawned children.",
			  " Then, visit_node is spawned for each node in the nodelet. Each thread of\nthis function further migrates to the nodelet where the portion of the edges\narray holding current nodes neighbor indicies resides.",
			  " Finally, for each neighbor, visit_neighbor is spawned. Each thread of\nthis function migrates to the nodelet where the corresponding indicies for the\ncosts and masks arrays are located. They are updated with proper values.",
			  "2\n1\n3\n4\n1\n2\n3\n4",
			  "**Want to collaborate with us on our EMU system?**",
			  "**Contact us:** **https://excl.ornl.gov/contact/**",
			  "[1] Kogge Dysart et.al Highly scalable near memory processing with migrating\nthreads on the emu system architecture. In Proceedings of the Sixth Workshop on\nIrregular Applications: Architectures and Algorithms (IA3), ACM, 2016",
			  "[2] Mehmet E. Belviranli, Seyong Lee and Jeffrey S. Vetter. \"Designing Algorithms\nfor the EMU Migrating-threads-based Architecture.\" In High Performance Extreme\nComputing Conference (HPEC), IEEE, 2018",
			  "**References**"
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3418077",
			"excerpts": [
			  "Nodelets combine narrowly banked memory with highly multi-threaded, cacheless Gossamer cores to provide a memory-centric environment for migrating threads.Read more"
			]
		  },
		  {
			"title": "\n        Graphicionado: A high-performance and energy-efficient accelerator for graph analytics\n      -  Princeton University",
			"url": "https://collaborate.princeton.edu/en/publications/graphicionado-a-high-performance-and-energy-efficient-accelerator/",
			"excerpts": [
			  "Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76-6.54x speedup while consuming 50-100x less energy compared to a state-of-The-Art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.",
			  "Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs."
			]
		  },
		  {
			"title": "(Open Access) Graphicionado: a high-performance and energy-efficient accelerator for graph analytics (2016) | Tae Jun Ham | 295 Citations",
			"url": "https://scispace.com/papers/graphicionado-a-high-performance-and-energy-efficient-3kyusmd4up?citations_page=45",
			"excerpts": [
			  "Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks, for high-performance, energy-efficient processing of graph analytics workload",
			  "s Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs.",
			  "Our results show that Graphicionado achieves a 1.76  6.54x speedup while consuming 50  100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.",
			  "Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks, for high-performance, energy-efficient processing of graph analytics workloads."
			]
		  },
		  {
			"title": "Papers on Graph Analytics - Julian Shun - MIT",
			"url": "https://jshun.csail.mit.edu/graph.shtml",
			"excerpts": [
			  "[Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics](https"
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://ieeexplore.ieee.org/document/7284059/",
			"excerpts": [
			  ":\nTesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design. I",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model.",
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems.",
			  "**Published in:** [2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)](/xpl/conhome/7270878/proceeding)",
			  "**Date of Conference:** 13-17 June 2015"
			]
		  },
		  {
			"title": "Tesseract Pim Architecture For Graph Processing - Isca15 | PDF | Cpu Cache | Computer Data Storage",
			"url": "https://www.scribd.com/document/663293050/tesseract-pim-architecture-for-graph-processing-isca15",
			"excerpts": [
			  "Tesseract is composed of\n(1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efci ent method of communic ation\nbetween different memory partitions, and (3) a programming\ninterface that reects and exploits the unique hardware de-\nsign.",
			  "It also includes two hardware prefetchers specialized for\nmemory access patterns of graph processing, which operate\nbased on the hints provided by our programming model.",
			  "Our\ncomprehensive evaluations using ve state-of-the-art graph\nprocessing workloads with large real-world graphs show that\nthe proposed architecture improves average system perfor- mance by a factor of ten and achieves 87% average energy\nreduction over conventional systems.\n"
			]
		  },
		  {
			"title": "[1809.07696] A Microbenchmark Characterization of the Emu Chick",
			"url": "https://arxiv.org/abs/1809.07696",
			"excerpts": [
			  "The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring large amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each memory read.",
			  "The current prototype hardware uses FPGAs to implement cache-less \"Gossamer cores for doing computational work and a stationary core to run basic operating system functions and migrate threads between nodes.",
			  "n this multi-node characterization of the Emu Chick, we extend an earlier single-node investigation (Hein, et al. AsHES 2018) of the the memory bandwidth characteristics of the system through benchmarks like STREAM, pointer chasing, and sparse matrix-vector multiplicati",
			  "We compare the Emu Chick hardware to architectural simulation and an Intel Xeon-based platform.",
			  "Our results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more efficiently than a more traditional, cache-based architecture although bandwidth usage suffers for computationally intensive workloads like SpMV.",
			  "Moreover, the Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access pointer chasing benchmark with weak locality."
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://arxiv.org/pdf/1901.02775",
			"excerpts": [
			  "The Emu Chick prototype implements migratory memory-side processing in a novel hardware\nsystem.",
			  "Rather than transferring large amounts of data across the system interconnect, the Emu\nChick moves lightweight thread contexts to near-memory cores before the beginning of each\nremote memory read.",
			  "the Emu architecture is designed to scale applications with poor data locality to supercomputing\nscale by more effectively utilizing available memory bandwidth and by dedicating limited power\nresources to networks and data movement rather than caches.",
			  "The\nkey differentiators for the Emu architecture are the use of cache-less processing cores, a high-radix\nnetwork connecting distributed memory, and PGAS-based data placement and accesses.",
			  "In short,\nthe Emu architecture is designed to scale applications with poor data locality to supercomputing\nscale by more effectively utilizing available memory bandwidth and by dedicating limited power\nresources to networks and data movement rather than caches."
			]
		  },
		  {
			"title": "Runtime Systems and Scheduling Support for High-End ...",
			"url": "https://etd.ohiolink.edu/acprod/odb_etd/ws/send_file/send?accession=osu1338324367&disposition=inline",
			"excerpts": [
			  "Next, we described a general dynamic scheduling framework for data parallel loops geared towards CPU-GPU heterogeneous machines. Our approach involves ..."
			]
		  },
		  {
			"title": "Data Prefetch Mechanisms",
			"url": "https://www.ece.lsu.edu/tca/papers/vanderwiel-00.pdf",
			"excerpts": [
			  "Data prefetching has been proposed as a technique for hiding the access latency of data referencing patterns that defeat caching strategies."
			]
		  },
		  {
			"title": "Frontiers | Runtime support for CPU-GPU high-performance computing on distributed memory platforms",
			"url": "https://www.frontiersin.org/journals/high-performance-computing/articles/10.3389/fhpcp.2024.1417040/full",
			"excerpts": [
			  "This paper presents an effort to address the challenges of efficiently utilizing distributed heterogeneous computing nodes in a portable and performant way. It introduces a heterogeneous tasking framework as an extensible layer of portability.",
			  "gn and implementation of a heterogeneous tasking framework for the development of performance portable applications that can scale from single-core to multi-core, multi-device ( *CPUs, GPUs* ) platforms efficiently, *without any code refac",
			  "A novel integration of a distributed runtime with the heterogeneous tasking framework to provide an end-to-end solution that scales over distributed heterogeneous computing nodes while exposing a high-level and abstract programming model.",
			  "A series of memory, scheduling, and threading performance optimizations that achieve significant improvements, up to 300% on a single GPU and linear scalability on a multi-GPU platform, that are directly applicable to similar systems and applications.",
			  "The memory captured by a hetero_object should mainly be accessed and modified through hetero_tasks for optimal performance.",
			  "the application can also explicitly request access to the underlying data on the host after specifying the type of access requested to maintain coherence. This method will trigger (if needed) an asynchronous transfer from the device with the most recent version of the data and immediately return a future.",
			  "Heterogeneous tasks (hetero_tasks) are opaque structures that consolidate the parameters characterizing a computational task.",
			  "Heterogeneous tasks are executed asynchronously by the tasking framework. A task submitted for execution is appended to a list of task execution requests. The control is then immediately returned to the application, which can continue to issue more tasks or execute other work.",
			  "However, once tasks reach the runnable list, they are ensured that all of their dependencies have been resolved and, thus, the scheduler can run them in parallel and in any order.",
			  "With the introduction of more heterogeneous computing devices and workloads, it is expected that scheduling and load balancing will only become more complicated. To provide flexibility for different use cases, the actual implementation of the scheduler is designed to be modular and separate from the rest of the heterogeneous tasking framework.",
			  "The abstract scheduler class allows the development of as simple or complex custom data structures and policies as the user might need.",
			  "The heterogeneous tasking framework is implemented in the C++ programming language leveraging its performance and object-oriented design. It is developed in three software layers to allow easy integration with new device types, programming APIs, and scheduling policies (see [Figure 4]",
			  "gure 4** . A high-level representation of the heterogeneous tasking framework software stack and operations. The operations performed by the Core Runtime include (a) Memory monitoring to keep track of the available device memory and deallocate unused objects when running low on resources, (b) Memory transfer and task execution request handling that dispatches such requests when it is safe, (c) Memory coherence among different copies of the same hetero_object in multiple devices, and (d) Task scheduling to optimize for a reduction in memory transfers and optimize overall execution time.",
			  "The device API maps high-level abstractions like hetero_objects and hetero_tasks to actual device-specific constructs.",
			  "To fully utilize the bandwidth capabilities of the respective hardware, NVIDIA GPUs require that host data to be transferred to the GPU should reside in a page-locked memory region.",
			  "ptimization gives a significant boost that ranges between 30% and 145% for different matrix sizes ( [Figure 8]() ; TF-PageLocked)",
			  "The most significant improvement of this optimization is manifested for the smallest matrix case (64  64) with another 60%.",
			  "The results of this optimization are substantial in most matrix sizes when the overlap between memory transfers and kernel executions can be large enough.",
			  "In PREMA, messages are one-sided and asynchronous and are received implicitly to invoke a designated task on their target. Thus, the receiver cannot specify a memory region where the message buffer shall be stored (like MPI).",
			  "The cache allowed us to attain performance within 10% overhead of that achieved by the MPI ( [Figure 10]() ; PREMA+CUDA). "
			]
		  },
		  {
			"title": "CoreTuner: Predicting and Scheduling Framework for Optimizing the Joint Allocation of CPU and GPU in Training Cluster | Proceedings of the 54th International Conference on Parallel Processing",
			"url": "https://dl.acm.org/doi/10.1145/3754598.3754634",
			"excerpts": [
			  "Periodic scheduling, through the CPU influence model and the OCCC metric, for the different resource sensitivities of the tasks, allocates resources to the tasks that make the best use of them and maximizes the shortening of the overall task execution duration of the cluster with limited resources.",
			  "In addition, in the face of the huge solution space brought about by the CPU-GPU combination, a caching mechanism is designed based on the similarity between the allocation schemes to increase the number of search rounds in a limited time to improve the quality of the solution.",
			  "This performance degradation is mainly due to the increased parallel computing and scheduling overhead.",
			  "The OCCC shows distinct heterogeneous characteristics.",
			  "Figure [2]() illustrates the OCCC for various models and batch sizes in an environment with a single P40 GPU, as well as the OCCC for the AlexNet model across different GPU allocation schemes."
			]
		  },
		  {
			"title": "A Scheduling and Runtime Framework for a Cluster ...",
			"url": "https://ieeexplore.ieee.org/document/7161504/",
			"excerpts": [
			  "We present a runtime system for simple and efficient programming of CPU+GPU clusters.",
			  "The programmer focuses on core logic, while the system undertakes task allocation, load balancing, scheduling, data transfer, etc.",
			  "Our programming model is based on a shared global address space, made efficient by transaction style bulk-synchronous semantics.",
			  "This model broadly targets coarse-grained data parallel computation particularly suited to multi-GPU heterogeneous clusters.",
			  "Our runtime system achieves a performance of 5.61 TFlop/s while multiplying two square matrices of 1.56 billion elements each over a 10-nodecluster with 20 GPUs.",
			  "This performance is possible due toa number of critical optimizations working in concert. These include perfecting, pipelining, maximizing overlap between computation and communication, and scheduling efficiently across heterogeneous devices of vastly different capacities."
			]
		  },
		  {
			"title": "Prefetch Instruction - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/prefetch-instruction",
			"excerpts": [
			  "Hardware-initiated prefetching is performed by hardware stream prefetchers located in the last-level (L2) caches, which dynamically detect access patterns and begin prefetching future addresses in those patterns.",
			  "Hardware prefetchers can throttle themselves in response to software prefetching and remain mostly inactive if not effective for a given application.",
			  "Automatic compiler prefetching is generally recommended, with manual prefetching reserved for cases where compiler-generated prefetches are insufficient, such as complex [data processing](../phys",
			  "Tuning parameters for manual prefetching include prefetch distance and cache level hints; systematic variation of these parameters, such as varying prefetch distances and hint values, is described as a methodology for refining prefetch behavior.",
			  "In multi-core and many-core processors, prefetch requests from one core can delay demand requests from another due to memory bandwidth contention, and this interference becomes more significant as the number of cores increases.",
			  "software prefetching allows application writers to provide hints to the memory system.",
			  "prefetching can significantly increase instruction throughput and reduce pipeline stalls, its effectiveness depends on careful tuning of parameters such as prefetch distance and cache level hints to avoid cache pollution and excessive memory bandwidth usage. Com",
			  "The hardware prefetchers can throttle themselves in response to software prefetching, so even if hardware prefetching is not effective for a certain application, it does not need to be disabled because it will remain mostly inactive.",
			  "Note that the hardware/software boundary is not firm."
			]
		  },
		  {
			"title": "Optimizing Memory Access: A Practical Guide to Scheduling Algorithms | by Mohit Mishra | ILLUMINATION | Medium",
			"url": "https://medium.com/illumination/optimizing-memory-access-a-practical-guide-to-scheduling-algorithms-01a8de0c50da",
			"excerpts": [
			  "Section Title: **Optimizing Memory Access: A Practical Guide to Scheduling Algorithms** > **FR-FCFS vs. PAR-BS vs. Adaptive Thread-Aware Scheduling**\nContent:\nWhile First Ready-First Come First Serve (FR-FCFS) offers a foundational approach to memory scheduling, pursuing enhanced performance and fairness demands exploring more advanced techniques. This article goes into two contenders: **Parallelism-Aware Batch Scheduling (PAR-BS) and Adaptive Thread-Aware Scheduling** , unveiling their innovative mechanisms and highlighting their potential to revolutionize memory management.",
			  "FR-FCFS is a memory scheduling algorithm that balances fairness and performance optimization. It prioritizes requests based on row buffer hits and request age.",
			  "The algorithm maintains separate queues for row buffer hits and misses, with each request tagged with a timestamp upon arrival. This approach minimizes the overhead of row activations and pre-charges while ensuring older requests are not starved."
			]
		  },
		  {
			"title": "Lecture 5: September 18 5.1 Scheduling",
			"url": "https://lass.cs.umass.edu/~shenoy/courses/fall13/lectures/Lec05_notes.pdf",
			"excerpts": [
			  "First-Out (FIFO), algorithm is the simplest scheduling algorithm.\nProcesses are dispatched in order according to their arrival time on the ready queue. Being a non-preemptive\ndiscipline, once a process has a CPU, it runs to completion. The FCFS scheduling is fair in the formal sense\nor human sense of fairness but it is unfair in the sense that long jobs make short jobs wait and unimportant\njobs make important jobs wait. FCFS is more predictable than most of other schemes since the scheduling\norder can be easily understood and the code for FCFS scheduling is simple to write and understand. However,\nthe FCFS scheme is not useful in scheduling interactive processes because it cannot guarantee good response\ntime. One of the major drawbacks of this scheme is that the average time is often quite long. Short jobs\nor jobs that frequently perform I/O can have very high waiting times since long jobs can monopolize the\nCPU. FCFS originally didnt have a job relinquish the CPU when it was doing I/O. We assume that a FCFS\nscheduler will run when a process is doing I/O, but it is still non-preemptive. The First-Come-First-Served\nalgorithm is rarely used as a master scheme in modern operating systems but it is often embedded within\nother schemes.\n**Advantage:** simple\n**Disadvantage:** poor performance for short jobs or tasks performing frequent I/O operations\n**5.3**\n**Round Robin Scheduling (RR)**\nRound-robin (RR) is one of the simplest preemptive scheduling algorithms for processes in an operating\nsystem. RR assigns short time slices to each process in equal portions and in order, cycling through the\nprocesses. Round-robin scheduling is both simple and easy to implement, and starvation-free (all processes\nwill receive some portion of the CPU). Round-robin scheduling can also be applied to other scheduling\nproblems, such as data packet scheduling in computer networks. However, round robin does not support\nprioritizing jobs, meaning that less important tasks will receive the same CPU time as more important ones.\nMost time sharing systems actually use variations of round robin for scheduling.\nExample: The time slot could be 100 milliseconds. If job1 takes a total time of 250ms to complete, the\n*Lecture 5: September 18*\n5-3\nround-robin scheduler will suspend the job after 100ms and give other jobs their time on the CPU. Once the\nother jobs have had their equal share (100ms each), job1 will get another allocation of CPU time and the\ncycle will repeat. This process continues until the job finishes and needs no more time on the CPU.\nA challenge with the RR scheduler is determining how to determine the time slice to allocate tasks. If the\ntime slice is too large, then RR performs similarly to FCFS. However, if the time slice is too small, then the\noverhead of performing context switches so frequently can reduce overall performance. The OS must pick\na time slice which balances these tradeoffs. A general rule is that the time for a context switch should be\nroughly 1% of the time slice.\n**Advantage:** Fairness (each job gets an equal amount of the CPU)\n**Disadvantage:** Average waiting time can be bad (especially when the number of processes is large)\n**5.4**\n**Shortest Job First (SJF)**\nShortest Job First (SJF) is a scheduling policy that selects the waiting process with the smallest (expected)\namount of work (CPU time) to execute next. Once a job is selected it is run non-preemptively. Shortest\njob first is advantageous because it provably minimizes the average wait time. However, it is generally not\npossible to know exactly how much CPU time a job has remaining so it must be estimated. SJF scheduling\nis rarely used except in specialized environments where accurate estimations of the runtime of all processes\nare possible. This scheduler also has the potential for process starvation for processes which will require\na long time to complete if short processes are continually added. A variant of this scheduler is Shortest\nRemaining Time First (SRTF) which is for jobs which may perform I/O. The SRTF scheduler brings task\nback to the run queue based on the estimated time remaining to run after an I/O activity. SRTF also picks\nthe shortest job (like SJF), but it runs it for a quantum (time slice), then does a context switch to the next\nshortest remaining time job. This means that SRTF is unlike SJF in that it is a preemptive scheduler.\n**Advantage:** Minimizes average waiting time. Works for preemptive and non-preemptive schedulers. Gives\npriority to I/O bound jobs over CPU bound jobs.\n**Disadvantage:** Cannot know how much time a job has remaining. Long running jobs can be starved for\nCPU.\n**5.5**\n**Multilevel Feedback Queues (MLFQ)**\nThe Multilevel feedback queue scheduling algorithm is what is used in Linux and Unix systems. The MLFQ\nalgorithm tries to pick jobs to run based on their observed behavior. It does this by analysing whether a\njob runs for its full time slice or if it relinquishes the CPU early to do I/O. This allows the scheduler to\ndetermine if a job is I/O bound or CPU bound so that they can be treated differently. This allows the\nscheduler to approximate the behavior of a SJF scheduler without requiring perfect information about job\ncompletion times. While over long periods of time a job may change from I/O to CPU bound and back,\nover a short period it will tend to behave in one manner or the other.\nThe MLFQ scheduler uses several different ready queues and associates a different priority with each queue.\nEach queue also has a time slice associated with it; the highest priority queue has the smallest time slice.\nWhen a new job arrives, it is assigned to the highest priority queue by default. At each scheduling decision,\nthe Algorithm attempts to choose a process from the highest priority queue that is not empty. Within the\nhighest priority non-empty queue, jobs are scheduled with a round robin scheduler.\nWhen a job relinquishes the CPU it is because either it has used its full time quantum, or because it has\n5-4\n*Lecture 5: September 18*\npaused to perform an I/O activity. The scheduler uses this information to change the priority of the job;\nCPU bound jobs using their full time quantum are moved down one priority level, while I/O bound jobs that\ngive up their time slice are moved to a one level higher priority queue. This approach gives I/O bound tasks\nbetter response times since they are given higher priority when they need the CPU. The MLFQ scheduler has\na starvation problem just like SJF if there is a continual stream of small jobs. There are potential schemes to\nprevent starvation, for example: low priority tasks can periodically be bumped up to higher priority queues\nif they have not been run in a long time.\n**Advantage:** Adaptive behavior can distinguish between CPU and I/O bound tasks and schedule them\naccordingly.\n**Disadvantage:** More complicated. Uses past behavior to predict future.\n**5.6**\n**Lottery Scheduling**\nLottery Scheduling is a probabilistic scheduling algorithm for processes in an operating system. Processes\nare each assigned some number of lottery tickets, and the scheduler draws a random ticket to select the\nnext process. The distribution of tickets need not be uniform; granting a process more tickets provides it a\nrelative higher chance of selection. This technique can be used to approximate other scheduling algorithms,\nsuch as SJF and Fair-share scheduling. Allocating more tickets to a process gives it a higher priority since\nit has a greater chance of being scheduled at each scheduling decision.\nLottery scheduling solves the problem of starvation. Giving each process at least one lottery ticket guarantees\nthat it has a non-zero probability of being selected at each scheduling operation. On average, CPU time is\nproportional to the number of tickets given to each job. For approximating SJF, most tickets are assigned to\nshort running jobs and fewer to longer running jobs. To avoid starvation, every job gets at least one ticket.\nImplementations of lottery scheduling should take into consideration that there could be a large number of\ntickets distributed among a large pool of threads",
			  "evel Feedback Queues (MLFQ)**\nThe Multilevel feedback queue scheduling algorithm is what is used in Linux and Unix systems. The MLFQ\nalgorithm tries to pick jobs to run based on their observed behavior. It does this by analysing whether a\njob runs for its full time slice or if it relinquishes the CPU early to do I/O. This allows the scheduler to\ndetermine if a job is I/O bound or CPU bound so that they can be treated differently. This allows the\nscheduler to approximate the behavior of a SJF scheduler without requiring perfect information about job\ncompletion times. While over long periods of time a job may change from I/O to CPU bound and back,\nover a short period it will tend to behave in one manner or the other.\nThe MLFQ scheduler uses several different ready queues and associates a different priority with each queue.\nEach queue also has a time slice associated with it; the highest priority queue has the smallest time slice.\nWhen a new job arrives, it is assigned to the highest priority queue by default. At each scheduling decision,\nthe Algorithm attempts to choose a process from the highest priority queue that is not empty. Within the\nhighest priority non-empty queue, jobs are scheduled with a round robin scheduler.\nWhen a job relinquishes the CPU it is because either it has used its full time quantum, or because it has\n5-4\n*Lecture 5: September 18*\npaused to perform an I/O activity. The scheduler uses this information to change the priority of the job;\nCPU bound jobs using their full time quantum are moved down one priority level, while I/O bound jobs that\ngive up their time slice are moved to a one level higher priority queue. This approach gives I/O bound tasks\nbetter response times since they are given higher priority when they need the CPU. The MLFQ scheduler has\na starvation problem just like SJF if there is a continual stream of small jobs. There are potential schemes to\nprevent starvation, for example: low priority tasks can periodically be bumped up to higher priority queues\nif they have not been run in a long time.\n**Advantage:** Adaptive behavior can distinguish between CPU and I/O bound tasks and schedule them\naccordingly.\n**Disadvantage:** More complicated. Uses past b"
			]
		  },
		  {
			"title": "05. Scheduling Algorithms",
			"url": "https://www.cl.cam.ac.uk/teaching/2223/OpSystems/materials/05-SchedulingAlgorithms.pdf",
			"excerpts": [
			  "7\nPriority scheduling\n Associate integer priority with process, and schedule the highest\npriority ( ~ lowest number) process, e.g.,",
			  "\n Average waiting time now\n1 + 5 + 0 + 1 + 5 + 10 + 1 + 5 + 10 + 2 + 1\n5\n= 41\n5 ",
			  "Dynamic priority scheduling",
			  "\nRound Robin\n A pre-emptive scheduling scheme for time-sharing systems\n",
			  "Multilevel Queues\n Partition Ready queue into many queues\nfor different types of process, e.g.,\n Foreground/interactive processes\n Background/batch processes\n Eac",
			  "ms\n21\nRound Robin\n A pre-emptive scheduling scheme for time-sharing systems\n Give each process a **quantum** (or time-slice) of CPU time e.g., 10100 milliseconds\n Once quantum elapsed, process is pre-empted and appended to the ready queue\n",
			  "4\nFirst-Come First-Served (FCFS)\n Schedule depends purely on the order in which processes arrive\n Simplest possible scheduling algorithm"
			]
		  },
		  {
			"title": "Fair and Efficient Scheduling in Data Ferrying Networks",
			"url": "http://conferences.sigcomm.org/co-next/2007/papers/papers/paper13.pdf",
			"excerpts": [
			  "To be able to\nprovide bandwidth guarantees at all times, the sum of\nrates allocated to all kiosks is not allowed to exceed\nthe capacity of the system.",
			  "Bundles leaving the regulator are deemed *eligible*\nfor subsequent scheduling and are buffered in per-kiosk\nqueues.",
			  "If the token bucket is empty, bundles are tem-\nporarily buffered in a pre-bucket queue with a fixed ca-\n3 T",
			  "If the pre-bucket queue is full, newly arriving\nbundles are dropped.",
			  "The use of TB regulators *decouples* fairness and delay\nminimization. The scheduler only considers the set of\neligible bundles and focuses solely on delay minimiza-\ntion, without any regard to fairnes",
			  "The use of TB regulators *decouples* fairness and delay\nminimization. The scheduler only considers the set of\neligible bundles and focuses solely on delay minimiza-\ntion, without any regard to fairnes",
			  "Moreover, every time new bundles leave\nthe regulator, a utility-maximizing scheduler is invoked\nto compute a schedule for all unserved bundles.",
			  "n\nother words, we associate with each bundle some util-\nity, which captures the value gained from delivering\nit as a function of its delay. The scheduler computes\na schedule that maximizes the total utility",
			  " show\nthat if we define the utility function in a certain way,\nthe optimal scheduling problem can be formulated as\na minimum cost network flow problem, for which effi-\ncient algorithms exist.",
			  "r sharing of bandwidth in packet-switching net-\nworks has been well studied in the context of tradi-\ntional networks. The well known max-min fairness cri-\nterion, is closely approximated by a number of packe-\ntized scheduling algorithms ",
			  "r work is different in two main\naspects.\nFirst, our notion of fairness is defined on a\nlonger time scale. While classical scheduling disciplines\ntry to achieve max-min allocation of bandwidth at time\nintervals as short as possible, we focus on long-term fair-\nness which is exactly what token bucket regulator can\nprovide.",
			  "We are willing to allocate a disproportionately\nlarge portion of bandwidth to some users at one time\nand compensate for other users at another, provided\nthe bandwidth is allocated fairly in the long run.",
			  "Second, besides\nensuring fair allocation of bandwidth, our scheme also\ntries to minimize end-to-end delay."
			]
		  },
		  {
			"title": "Two Optimization Methods for Raytracing",
			"url": "https://vvise.iat.sfu.ca/user/data/papers/rtopt.pdf",
			"excerpts": [
			  "Previously bounding volume hierarchies have been used to speed raytracing. A new method to speed the traversal of a bounding volume hierarchy is presented.Read more"
			]
		  },
		  {
			"title": "Exploiting Caches to Maximize Locality in Graph Processing",
			"url": "https://people.csail.mit.edu/sanchez/papers/2017.cgs.agp.pdf",
			"excerpts": [
			  "The key idea of this paper is to exploit the cache hierarchy to find a graph traversal schedule that results in high locality. The cache has plentiful ..."
			]
		  },
		  {
			"title": "Batch Query Processing and Optimization for Agentic Workflows",
			"url": "https://arxiv.org/html/2509.02121v2",
			"excerpts": [
			  "Tool coalescing:* Many workflows issue identical or overlapping tool calls (e.g., SQL retrieval queries) across agents and sessions. Coalescing these requests to a single physical execution amortizes overhead and enables shared reuse of results (Facebook, [2019](https://arxiv.org/html/2509.02121v2.bib43 \"DataLoader: data loading utility for javascript\") ; Giannikis et al. , [2012](https://arxiv.org/html/2509.02121v2.bib54 \"SharedDB: killing one thousand queries with one stone\") )",
			  "rk reduction via request coalescing . Beyond executing tasks, the Processor reduces redundant work across concurrent workflows. For tool operators, it canonicalizes an operator *signature* (operator type plus normalized arguments) and\nmerges pending tasks with identical signatures into a single physical execution. The resulting output is then fanned out to all dependent logical nodes. This coalescing is particularly effective for high-fanout agents that issue repeated SQL templates or identical retrieval calls.",
			  "Coordinator prioritizes CPU tasks that unlock the immediate GPU frontier. Concretely, ready tool nodes are ordered by increasing DAG depth to the next unsatisfied LLM node (shallower, more critical prerequisites first).",
			  "The Processor enforces bounded concurrency per tool backend (e.g., per-DB connection pool or HTTP client) and applies backpressure when queues exceed configured thresholds, stabilizing latency under bursty workloads.",
			  " batching, cache sharing/prefetching, and overlap of compute and communication, within a unified DAG scheduler that respects inter-operator dependencies and avoids pathological stalls (e.g., GPU underutilization while CPU tools are backlogged, o",
			  "Disabling this module forces the backend to process the raw, unreduced stream of requests, exposing the system to the full complexity of the workload and causing severe I/O congestion.",
			  "Halo models each request as a DAG spanning GPU LLM operators and CPU tool operators, and applies cost-based scheduling and placement to overlap execution, preserve locality, and exploit cross-request batching and sharing."
			]
		  },
		  {
			"title": "Making efficient Gremlin upserts with fold()/coalesce()/unfold() - Amazon Neptune",
			"url": "https://docs.aws.amazon.com/neptune/latest/userguide/gremlin-efficient-upserts-pre-3.6.html",
			"excerpts": [
			  "Section Title: Making efficient Gremlin upserts with `fold()/coalesce()/unfold()`",
			  "For example, the following query upserts a vertex by first looking for the specified\nvertex in the dataset, and then folding the results into a list.",
			  "In the first traversal\nsupplied to the `coalesce()` step, the query then unfolds this list.",
			  "If the\nunfolded list is not empty, the results are emitted from the `coalesce()` .",
			  "If, however, the `unfold()` returns an empty collection because the vertex\ndoes not currently exist, `coalesce()` moves on to evaluate the second\ntraversal with which it has been supplied, and in this second traversal the query creates\nthe missing vertex."
			]
		  },
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads."
			]
		  },
		  {
			"title": "NVIDIA A100 Tensor Core GPU Architecture",
			"url": "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf",
			"excerpts": [
			  "the NVIDIA A100 GPU has 40 GB of high-speed\nHBM2 memory with a class-leading 1555 GB/sec of memory bandwidth",
			  "The A100 L2 cache is a shared resource for the GPCs and SMs and lies outside of the GPCs.",
			  "The global and local memory areas accessed by CUDA programs reside in HBM2 memory\nspace, and is referred to as device memory in CUDA parlance. Constant memory space resides\nin device memory and is cached in the constant cache.",
			  "A100 L2 cache residency controls help applications reduce DRAM bandwidth. This example shows\ndifferent data buffers highlighted with colors to indicate data that has been marked for persistent caching\nin L2.",
			  "A100 Compute Data Compression improves DRAM bandwidth, L2 bandwidth, and L2 capacity.",
			  "latency benefits of A100's new L2 cache. Each L2 cache partition is divided into 40 L2 cache slices. Eight 512 KB L2 slices are associated with each memory ...Read more",
			  "New asynchronous copy instruction loads data directly from global memory into shared\nmemory, optionally bypassing L1 cache, and eliminating the need for intermediate\nregister file (RF) usage"
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Optimization Reference ...",
			"url": "https://cdrdv2-public.intel.com/779559/355308-Software-Optimization-Manual-048-Changes-Doc-2.pdf",
			"excerpts": [
			  "This document is an update to the optimization recommendations contained in the Intel 64 and IA-32. Architectures Optimization Reference Manual, also known as ...Read more"
			]
		  },
		  {
			"title": "Iterating over linked list in C++ is slower than in Go - Stack Overflow",
			"url": "https://stackoverflow.com/questions/50274433/iterating-over-linked-list-in-c-is-slower-than-in-go",
			"excerpts": [
			  "Section Title: [Iterating over linked list in C++ is slower than in Go](/questions/50274433/iterating-over-linked-list-in-c-is-slower-than-in-go) > 3 Comments",
			  "Hey, thanks for the reply! Unfortunately I'm not able to replicate your results; gcc runs as slow for me as clang++. Are you running something like `gcc -std=c++11 -stdlib=libc++ -lstdc++ minimal.cpp -O3; ./a.out` ?",
			  "I have compiled with: g++ --std=c++11 -O3 -lstdc++"
			]
		  },
		  {
			"title": "Understanding CPU port contention. | Easyperf ",
			"url": "https://easyperf.net/blog/2018/03/21/port-contention",
			"excerpts": [
			  "In my IvyBridge CPU I have 2 ports for executing loads, meaning that we can schedule 2 loads at the same time.",
			  "We can see that our 16 loads were scheduled equally between PORT2 and PORT3, each port takes 8 uops.",
			  "On modern x86 processors load instruction takes at least 4 cycles to execute even the data is in the L1-cache.",
			  "So, we will have 16 reads of 4 bytes.",
			  "the throughput is what matters (as Travis mentioned in his comment).",
			  "There will be still 2 retired load instructions per cycle.",
			  "There will be still 2 retired load instructions per cycle.",
			  "We can see that all `bswap` instructions nicely fit into the pipeline causing no hazards."
			]
		  },
		  {
			"title": "Skylake: Intel's Longest Serving Architecture",
			"url": "https://chipsandcheese.com/p/skylake-intels-longest-serving-architecture",
			"excerpts": [
			  "Client Skylakes port layout is similar to Haswells. There are four integer execution ports, two general purpose AGU ports, and a store-only AGU port. Three of the integer execution ports, numbered 0, 1, and 5, handle floating point and vector execution as well.",
			  "Even though port layout wasnt changed, Intel did beef up floating point and vector execution capabilities.",
			  "On the floating point side, latency for floating point adds, multiplies, and fused multiply adds are all standardized at four cycles and two per cycle throughput, suggesting the fused multiply add (FMA) units are handling all of those operations.",
			  "On both architectures, stores crossing a 64 byte cacheline take two cycles to complete.",
			  "Loads that cross a cacheline boundary can often complete in a single cycle, likely because the L1 data cache has two load ports."
			]
		  },
		  {
			"title": "\n\tHaswell L2 cache bandwidth to L1 (64 bytes/cycle)? - Intel Community\n",
			"url": "https://community.intel.com/t5/Software-Tuning-Performance/Haswell-L2-cache-bandwidth-to-L1-64-bytes-cycle/m-p/1014866",
			"excerpts": [
			  "The Haswell core can only issue two loads per cycle, so they have to be 256-bit AVX loads to have any chance of achieving a rate of 64 Bytes/cycle.",
			  "There are very clearly only 2 load units (ports 2 and 3 shown in Figure 2-1 of the Intel Architectures Optimization Manual, document 248966-030, September 2014), so only two load uops can be executed per cycle.",
			  "The biggest load payload is 256 bits == 32 Bytes, and 64 Bytes / 2 loads = 32 Bytes per load.",
			  "There is absolutely no doubt that you can only move 64 Bytes per cycle into registers by using AVX instructions.  There are very clearly only 2 load units (ports 2 and 3 shown in Figure 2-1 of the Intel Architectures Optimization Manual, document 248966-030, September 2014), so only two load uops can be executed per cycle.   The biggest load payload is 256 bits == 32 Bytes, and 64 Bytes / 2 loads = 32 Bytes per load.",
			  "Then the next interrupt occurs and the cycle starts over again. The new version of the Optimization Reference manual (248966-037, Table 2-1) says that the Skylake Server processor has twice the maximum L2 bandwidth of the Broadwell processor -- 64 Bytes/cycle \"max\" instead of 32, with 52 Bytes/cycle \"sustained\" instead of 25.",
			  "Haswell L2 cache bandwidth to L1 (64 bytes/cycle)?",
			  "If you add a second read to the above loop, to the same cache line, even to the same location, I can never get better than 2 cycles per iteration (I get close, about 2.05 cycles). So it seems the L1D can satisfy a load from L2, and a load from the core, but not a *second* load from the core in the same cycle."
			]
		  },
		  {
			"title": "Basics on NVIDIA GPU Hardware Architecture - HECC Knowledge Base",
			"url": "https://www.nas.nasa.gov/hecc/support/kb/basics-on-nvidia-gpu-hardware-architecture_704.html",
			"excerpts": [
			  "\nThese differences speak to the fact that GPUs are designed to maximize throughput instead of minimizing latency. The high throughput is realized via a large number of registers and the use of high bandwidth memory. Instead of minimizing latency, GPUs rely on hiding latency through asychronous SIMT operations.",
			  "There tend to be more layers of caches in CPUs than GPUs. For example, the Intel Skylake, Intel Cascade Lake, AMD Rome, AMD Milan, and NVIDIA Grace CPU processors all have three levels of caches: L1, L2, and L3.On the other hand, the NVIDIA V100, A100, and H100 only have L1 and L2 caches.",
			  "The latency of L2 cache in GPU (shared by SMs) is approximately 200 cycles as reported by independent benchmarking works for V100 and A100 listed in the Reference section.",
			  "A warp is a group of 32 threads within a thread block that executes the same instruction simultaneously. These threads are scheduled in batches by a warp scheduler to execute on an SM.",
			  "Latency of device memory is between about 200 to 1000 cycles, as reported by independent benchmarking work for the V100 and A100, listed in the Reference section.",
			  "The size of a thread block is set by an application. However, it should be a multiple of 32 due to the warp scheduling. A common choice of a thread block size is 256.",
			  "Note: In concept, executing a warp (32 threads) on multiple cores of an SM is similar to executing vector instructions (using xmm/ymm/zmm registers on 4/8/16 data elements in a vector) on a CPU core.",
			  "The GPU memory hierarchy does not look like a pyramid. Specifically, as shown below, the L1 cache size can be smaller than the register size.",
			  "The latency of L1/shared memory is approximately 20 - 30 cycles as reported by independent benchmarking works listed in the Reference section. In comparison, L1 latency in CPUs is roughly five cycles.",
			  "L2 cache keeps data read/written from/to device memory to service requests from SMs in the GPU."
			]
		  },
		  {
			"title": "Nvidia's H100: Funny L2, and Tons of Bandwidth",
			"url": "https://chipsandcheese.com/p/nvidias-h100-funny-l2-and-tons-of-bandwidth",
			"excerpts": [
			  "H100s L2 cache feels like a two-level setup rather than a single level of cache. A thread running on H100 can access the far L2 cache a bit faster than on A100, so Nvidia has improved compared to the prior generation",
			  "With five stacks of HBM2e, the H100 can pull just short of 2 TB/s from VRAM.",
			  "H100s VRAM bandwidth will be very useful for massive working sets without cache-friendly access patterns.",
			  "Consumer GPUs in recent years have moved towards maintaining good performance when faced with small workloads."
			]
		  },
		  {
			"title": "Comparing NVIDIA H100 vs A100 GPUs for AI Workloads | OpenMetal IaaS",
			"url": "https://openmetal.io/resources/blog/nvidia-h100-vs-a100-gpu-comparison/",
			"excerpts": [
			  "The H100s combination of FP8 support and HBM3 memory allows it to handle more concurrent inference requests with reduced latency. This is particularly important for real-time applications like chat assistants, code generation tools, [fraud detection](/resources/blog/big-data-for-fraud-detection-a-guide-for-financial-services-and-e-commerce/) systems, and other latency-sensitive inference pipelines.",
			  "A100** : Equipped with 40 GB or 80 GB of HBM2e memory, it offers up to 2 TB/s memory bandwidth.",
			  "H100** : Comes with 80 GB of HBM3 memory, delivering up to 3.35 TB/s memory bandwidth.",
			  "This memory improvement in the H100 supports higher batch sizes, larger model inference, and more concurrent user sessions.",
			  "| Feature | A100 (Ampere) | H100 (Hopper) |\n| Release Year | 2020 | 2022 |\n| Memory | 40 GB or 80 GB HBM2e | 80 GB HBM3 |\n| Memory Bandwidth | Up to 2 TB/s | Up to 3.35 TB/s |\n| MIG Support | Yes (up to 7 instances) | Yes (2nd Gen MIG, 7 instances) |\n| Transformer Engine | No | Yes (FP8 support) |\n| Training Performance | Baseline | Up to 2.4x faster |\n| Inference Throughput | ~130 tokens/second | 250300 tokens/second |\n| Latency | Moderate | Lower |\n| Power Efficiency | Good | Higher |"
			]
		  },
		  {
			"title": "NVIDIA Ampere Architecture In-Depth | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/",
			"excerpts": [
			  "The combined capacity of the L1 data cache and shared memory is 192 KB/SM in A100 vs. 128 KB/SM in V100. Simultaneous execution of FP32 and ...Read more"
			]
		  },
		  {
			"title": "assembly - How to interpret uops.info? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/71418230/how-to-interpret-uops-info",
			"excerpts": [
			  "So 0.5 means it can run once per 0.5 cycles, i.e. 2/clock, as expected for CPUs that we know have 2 load ports.Read more"
			]
		  },
		  {
			"title": "Pipeline of Intel Core CPUs",
			"url": "https://www.uops.info/background.html",
			"excerpts": [
			  "Each port can accept at most one op in every cycle. However, as most functional units are fully pipelined, a port can typically accept a new op in every cycle ...Read more"
			]
		  },
		  {
			"title": "Question about Skylake Execution Unit Ports : r/simd",
			"url": "https://www.reddit.com/r/simd/comments/9ztifk/question_about_skylake_execution_unit_ports/",
			"excerpts": [
			  "My understanding is that Port 7 is an AGU like Port 2/3 has (Port 2/3 also performs loads themselves), but can only be used for for store operations.Read more"
			]
		  },
		  {
			"title": "The penalty is quite small, 1-3 cycles each direction. RAM latency is 1-2 orders... | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24062677",
			"excerpts": [
			  "L1D is not many cycles away: it is 4 or 5 for scalar loads, 6 or 7 for xmm or ymm loads. If the load misses, it doesn't much matter if it's a ..."
			]
		  },
		  {
			"title": "Going Armchair Quarterback on Golden Cove's Caches",
			"url": "https://chipsandcheese.com/p/going-armchair-quarterback-on-golden-coves-caches",
			"excerpts": [
			  "On top of that, M1's L1D has 3 cycle latency. It's like a better version of Phenom's L1D. We saw earlier how much potential that had ..."
			]
		  },
		  {
			"title": "performance - Load/stores per cycle for recent CPU architecture generations - Stack Overflow",
			"url": "https://stackoverflow.com/questions/45106951/load-stores-per-cycle-for-recent-cpu-architecture-generations",
			"excerpts": [
			  "**Sandy/Ivy** : per cycle, 2 loads, or 1 load and 1 store. 256bit loads and stores count double, but only with respect to the load or store itself - it still only has one address so the AGU becomes available again the next cycle. By mixing in some 256b operations you can still get 2x 128b loads and 1x 128b store per cycle.",
			  "**Haswell/Broadwell** : 2 loads *and* a store, and 256bit loads/stores *don't* count double. Port 7 (store AGU) can only handle *simple* address calculations (base+const, no index), complex cases will go to p2/p3 and compete with loads, simple cases may compete anyway but at least don't *have* to.",
			  "**Sky/Kaby** : the same as Broadwell",
			  "**Ice/Tiger Lake** : 2 loads and 2 stores per clock, with fully separate execution units for each (store-address uops don't run on load ports.) 2/clock stores can only be sustained if stores are to the same cache line. i.e. 1/clock write to L1d cache, but a write can commit two store-buffer entries if they're to the same line. For memory-ordering reasons, the two store-buffer entries have to be the two oldest, so alternating stores to two separate arrays couldn't benefit from this unless you can unroll.",
			  "**Alder Lake / Sapphire Rapids** : 3 loads and 1 store, or 2 loads and 2 stores. Agner Fog reports those throughputs for sizes up to 128-bit, but \"somewhat less\" for 256 and 512-bit loads/stores. Commit to L1d may be limited like Ice Lake for more than 1 store per clock.",
			  "**Bulldozer** : 2 loads, or 1 load and 1 store. 256bit loads and stores count double.",
			  "e.\n**Jaguar** : 1 load or 1 store, and 256bit loads and stores count double. By far the worst one in this list, because it's the only low-power arch in the list.",
			  "**Zen 1** (first-gen Ryzen): 2 loads, or 1 load and 1 store. 256bit loads and stores count double.",
			  "[**Zen 2**](https://en.wikichip.org/wiki/amd/microarchitectures/zen_2) (Most Ryzen 3xxx and 4xxx, but there are some 3xxx models that are only Zen+ not Zen 2).",
			  "3 AGUs (2 load/store, 1 store-only). Up to two 256-bit load operations and one 256-bit store per cycle.",
			  "[**Zen 3**](https://en.wikichip.org/wiki/amd/microarchitectures/zen_3) : Load throughput increased from 2 to 3 for scalar integer GPRs.",
			  "Store throughput increased from 1 to 2 for scalar integer GPRs. (Wikichip incorrectly states this as \"if not 256-bit\", but https://uops.info/ testing [confirms only 1/clock](https://uops.info/html-tp/ZEN3/VMOVAPS_M128_XMM-Measurements.html) vector stores even with 128-bit `vmovaps [mem], xmm` .",
			  "zen_4)** : no change from Zen 3. AVX-512 512-bit ops are single-uop, but occupy load and store-data units for 2 cycles each, like how Sandy/Ivy Bridge handled 256-bit load/store. (Same for 512-bit ALU uops, single uop unlike how Zen 1 handled 256-bit.)\n"
			]
		  },
		  {
			"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
			"url": "https://www.agner.org/optimize/microarchitecture.pdf",
			"excerpts": [
			  "The width of the memory ports has been doubled relative to previous processors. The\nmaximum throughput to the level-1 cache is now two 32-byte reads and one 32-byte write\nper clock. This makes it possible to copy a block of memory at a speed of 32 bytes per clock\ncycle.",
			  "**Cache**\n**Skylake**\nop cache\n1536 ops, 8 way, 6 op line size, per core\nLevel 1 code\n32 kB, 8 way, 64 sets, 64 B line size,\nlatency 4, per core\nLevel 1 data\n32 kB, 8 way, 64 sets, 64 B line size,\nlatency 4, per core\nLevel 2\n256 kB - 1MB, 4 - 16 ways, 1024 sets, 64 B line\nsize, latency 14, per core.\nLevel 3\n3-24 MB, 64 B line size, latency 34-85, shared\n**Table 11.2. Cache sizes on Skylake**",
			  "The 256-bit or 512-bit read and write bandwidth (see p. 144) makes it advantageous to use\nYMM or ZMM registers for copying or zeroing large blocks of memory. The REP MOVS\n ..."
			]
		  },
		  {
			"title": "NVIDIA ADA GPU ARCHITECTURE",
			"url": "https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf",
			"excerpts": [
			  "The Ada SM contains 128 KB of Level 1 cache. This cache features a unified architecture that can\nbe configured to function as an L1 data cache or shared memory depending on the workload.",
			  "AD102 has been\noutfitted with 98304 KB of L2 cache, an improvement of 16x over the 6144 KB that shipped in\nGA102. All applications will benefit from having such a large pool of fast cache memory available,\nand complex operations such as ray tracing (particularly path tracing) will yield the greatest\nbenefit."
			]
		  },
		  {
			"title": "1. NVIDIA Ada GPU Architecture Tuning Guide  Ada Tuning Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html",
			"excerpts": [
			  " the portion of the L1 cache dedicated to shared memory (known as the *carveout* ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using `cudaFuncSetAttribute()` with the attribute `cudaFuncAttributePreferredSharedMemoryCarveout` . T",
			  ".\nCUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. T",
			  "nt:\nLike the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp."
			]
		  },
		  {
			"title": "Decomposing BVH to accelerate traversal - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/decomposing-bvh-to-accelerate-traversal/283911",
			"excerpts": [
			  "If I have a large BVH, can I speed up the BVH traversal through decomposing this BVH into multiple smaller BVHs and accessing these GAS handles at the same ...Read more"
			]
		  },
		  {
			"title": "algorithm - Optimizing BVH Traversal with GPU - Stack Overflow",
			"url": "https://stackoverflow.com/questions/25703174/optimizing-bvh-traversal-with-gpu",
			"excerpts": [
			  "Traversal is the single most expensive computation for my program right now and it prevents large scenes (>2k triangles) from running at acceptable frame rates.",
			  "I am at a loss as to how it can be performed faster.",
			  "I created a bounding volume hierarchy that is generated every frame. Due to it's use, each node must have two children, no more, no less.",
			  "To traverse it I used the concepts provided by the paper at this [link](https://graphics.cg.uni-saarland.de/fileadmin/cguds/papers/2011/hapala_sccg2011/hapala_sccg2011.pdf) , where each thread traverses the following code.",
			  "I tried to minimize the memory accesses within the context of the algorithm, however the divergence, which I assume to be the problem, seems an impossible hurdle to overcome."
			]
		  },
		  {
			"title": "c++ - Optimizing BVH traversal in ray tracer - Stack Overflow",
			"url": "https://stackoverflow.com/questions/61222620/optimizing-bvh-traversal-in-ray-tracer",
			"excerpts": [
			  "Sticking all the nodes in a contiguous array gave around a 2% speedup on average over a bunch of runs, thanks!",
			  "You should definitively keep your node in an array or use some kind of memory pool, this will indeed improve your performance w.r.t. the cache lines.",
			  "You should definitively keep your node in an array or use some kind of memory pool, this will indeed improve your performance w.r.t. the cache lines. Regarding the speed of float versus double, it's really a tricky question and really depends on what you are doing. If you are missing a lot of L1 cache then double will cost you more than float as the hardware has to load more data into the register. If you decide to use SSE then float is a real advantage because the CPU can crunch 4 operations for float whereas you only have two for double.",
			  "Also this makes me think: are you using any mathematical function from the std into your intersection code? If so, you should really consider changing them to less robust but faster version of them (I'm mostly thinking of exp and sqrt).",
			  "By far the most common intersection is the BBox intersection which only uses really uses *,-,+,<,>. The other intersection functions (triangle and sphere) do use more complicated functions (division, sqrt) but only make up 7% of execution time combined, so I haven't bothered to optimize them yet.",
			  "VX! With a bit of refactoring in your intersection code, you could intersect four bounding boxes at once and therefore have nice boost in your application. You could have a look at what [embree](https://github.com/embree/embree) does (the intel tracer) especially in the math "
			]
		  },
		  {
			"title": "How to speed up the traversal speed of BVH? - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/how-to-speed-up-the-traversal-speed-of-bvh/256481",
			"excerpts": [
			  "Are there any factors that can affect the traversal speed of BVH? For example, the size of the triangles, the distance between triangles, the density of triangles or the length of rays. Thank you!",
			  "Yes, please read this thread and take note of the AS compaction and different AS build flags. Follow the links to them inside the OptiX Programming Guide.",
			  "Then read this thread again because it touches on the acceleration structures and geometry flags as well:",
			  "Try to avoid nesting AABBs of things inside each other.",
			  "If you just have independent triangles in a single GAS, that might not affect you. If the geometric primitives penetrate each other or if there are huge differences in their AABB extents so that AABB overlap a lot, that would affect traversal performance,",
			  "You would need to benchmark that yourself for your specific use case.",
			  "The ray [tmin, tmax] interval length could affect the traversal performance greatly.",
			  "Obviously the shorter the rays are, compared to your scene size, the fewer AABB can be intersected.",
			  "This might also be interesting when considering how to implement the individual device programs.",
			  "https://forums.developer.nvidia.com/t/relationship-of-nvidia-optix-7-programs/251690"
			]
		  },
		  {
			"title": "How is the triangle vertex data of BVH arranged in memory? - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/how-is-the-triangle-vertex-data-of-bvh-arranged-in-memory/289903",
			"excerpts": [
			  "OptiX has some flags which influence how the acceleration structure is built.",
			  "Specifically [OPTIX_BUILD_FLAG_PREFER_FAST_TRACE](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html) and [OPTIX_BUILD_FLAG_PREFER_FAST_BUILD](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html)",
			  "and the [OPTIX_GEOMETRY_FLAG_REQUIRE_SINGLE_ANYHIT_CALL](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html)",
			  "Described here: [https://raytracing-docs.nvidia.com/optix8/guide/index.html_structures](https://raytracing-docs.nvidia.com/optix8/guide/index.html)\nThen there",
			  "Described here: [https://raytracing-docs.nvidia.com/optix8/guide/index.html_structures](https://raytracing-docs.nvidia.com/optix8/guide/index.html)\nThen there",
			  "Then there is acceleration structure compaction which will improve memory locality of the final AS data.",
			  "Also there is the [OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html) flag which allows to read vertex positions from the AS which can affect performance negatively.",
			  "Is it arranged according to spatial location or just according to the BVH structure?",
			  "The BVH structure depends on the spatial location of the primitives and their sizes."
			]
		  },
		  {
			"title": "How to build a BVH  Part 1: Basics - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/04/13/how-to-build-a-bvh-part-1-basics/",
			"excerpts": [
			  "In this first article we discuss the bare basics: what the BVH is used for, and how a basic BVH is constructed and traversed with a ray.Read more"
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/2601097.2601222",
			"excerpts": [
			  "an efficient algorithm for traversing large packets of rays against a bounding volume hierarchy in a way that groups coherent rays during traversal.",
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing.",
			  "our new technique increases traversal performance by 36--53%, and is applicable to most ray tracers."
			]
		  },
		  {
			"title": "A State-of-the-Art Performance Withstanding the Test-of-Time - ACM SIGGRAPH Blog",
			"url": "https://blog.siggraph.org/2025/12/a-state-of-the-art-performance-withstanding-the-test-of-time.html/",
			"excerpts": [
			  "y Tracing](https://dl.acm.org/doi/10.1145/2601097.2601199) was awarded a SIGGRAPH 2025 Test-of-Time Award, given to Technical Papers that have had a significant and lasting impact on the computer graphics and interactive techniques over the last decade. This paper introduces the Embree system and describes in detail what it takes to build a professional-grade modular ray tracing framework. It achieved state-of-the-art performance with a simple and elegant architecture and came with an open-source implementation that has been adopted by many since then."
			]
		  },
		  {
			"title": "DUAL STREAMING FOR HARDWARE-ACCELERATED RAY ...",
			"url": "https://radiantflux.net/data/PHDCG_19/KShkurko_PhdThesis.pdf",
			"excerpts": [
			  "The scene stream consists of multiple *scene segments* that collectively represent the entire\nscene geometry data and that can be processed independently.",
			  "Each scene segment is\norganized in memory so that all of its nodes and geometry can be accessed sequentially to\nimprove the spatial locality of the data.",
			  "rior nodes use\n64 bytes, while the leaf nodes use 8 bytes.",
			  "The ray stream is the collection of all rays that are in flight, split into multiple queues,\none per scene segment.",
			  "The ray bucket header stores the number of rays contained within and a\npointer (a memory address) to the next ray bucket in the list.",
			  "Because each ray uses the\nsame number of words for storage, we can treat each ray bucket as an array rather than a",
			  "The interior nodes use\n64 bytes, while the leaf nodes use 8 bytes. The interior nodes are larger because they use\n48 bytes extra to store the AABBs for their child",
			  "\nStoring scene segments changes more significantly, Figure 5.18d. The compressed rep-\nresentation adds a 7 word header in the beginning, which stores the uncompressed AABB\nfor the scene segment (6 words) and an address for the first triangle in the scene segment (1\n ",
			  ". The compressed rep-\nresentation adds a 7 word header in the beginning, which stores the uncompressed AABB\nfor the scene segment (6 words) and an address for the first triangle in the scene segment (1\n ."
			]
		  },
		  {
			"title": "GitHub - jbikker/tinybvh: Single-header dependency-free BVH construction and traversal library.",
			"url": "https://github.com/jbikker/tinybvh",
			"excerpts": [
			  "`BVH::Build` : Efficient plain-C/C+ binned SAH BVH builder which should run on any platform.",
			  "`BVH_GPU` : This format uses 64 bytes per node and stores the AABBs of the two child nodes. This is the format presented in the [2009 Aila & Laine paper](https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf) . It can be traversed with a simple GPU kernel.",
			  "`BVH4_GPU` : A compact version of the `BVH4` format, which will be faster for GPU ray tracing.",
			  "`BVH4_CPU` : SSE-optimzied wide BVH traversal. The fastest option for CPUs that do not support AVX.",
			  "GPU-friendly layouts, including 'Compressed Wide BVH' (CWBVH) for state-of-the-art GPU performance",
			  "Wide BVHs (any width) using collapsing",
			  "A constructed BVH can be used to quickly intersect a ray with the geometry, using `BVH::Intersect` or `BVH::IsOccluded` , for shadow rays.",
			  "The 32-byte size allows for cache-line alignment."
			]
		  },
		  {
			"title": "tinybvh  Manual  Advanced - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2025/01/25/tinybvh-manual-advanced/",
			"excerpts": [
			  "`BVH4_GPU` : This layout uses a wide (and thus shallow) BVH to speedup traversal of divergent rays. The reduced number of traversal steps improves memory coherency at the expense of more calculations per step, which is a good trade-off in most cases. Conversion is however more expensive than for the `BVH_GPU` layout.",
			  "`CWBVH` : This is the final layout proposed by NVIDIA before introducing their RTX hardware. CWBVH uses an 8-wide BVH with heavily compressed nodes. Additionally, triangle data is stored without any indirection interleaved with the nodes for optimal access. This is a fast layout, but obtaining it is relatively costly. This is mostly useful for static geometry. Also note that the traversal kernel is complex; it requires specialized bit manipulation instructions to run at full speed. Porting this code will be challenging.",
			  "`BVH_GPU` : This layout follows the recommendations in a 2009 paper by Aila & Laine. This BVH layout is quickly obtained from the default BVH layout and offers reasonable ray tracing performance. The traversal kernel is short and simple and can easily be implemented in any shader or GPGPU language.",
			  "`BVH_GPU` : This layout follows the recommendations in a 2009 paper by Aila & Laine. This BVH layout is quickly obtained from the default BVH layout and offers reasonable ray tracing performance. The traversal kernel is short and simple and can easily be implemented in any shader or GPGPU language.",
			  "The manual is split up in multiple compact documents:\n[Basic Use. Building a BVH, tracing a ray, alternative layouts, TLAS/BLAS](https://jacco.ompf2.com/2025/01/24/tinybvh-manual-basic-use/) .\n**Advanced Topics (this document).** GPU ray tracing, custom math libraries, tinybvh settings."
			]
		  },
		  {
			"title": "How to build a BVH  part 2: Faster Rays - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/04/18/how-to-build-a-bvh-part-2-faster-rays/",
			"excerpts": [
			  "To make better use of the caches, we should ensure that data that we use is similar to data we have recently seen. This is known as *temporal data locality* . In a ray tracer this can be achieved by rendering the image in tiles.",
			  "The only thing that we can do is shorten the ray: Any hit closer than ray.t reduces ray.t, and if that means that the entry point to another AABB is now out of reach, we can cull that AABB."
			]
		  },
		  {
			"title": "Ray Tracing Acceleration with Near-memory Computing",
			"url": "https://dl.acm.org/doi/full/10.1145/3725843.3756067",
			"excerpts": [
			  "Ray tracing generates realistic images, but even with specialized hardware support for memory traversal ... Ray Tracing Gems II. Apress ...Read more"
			]
		  },
		  {
			"title": "SIGGRAPH 2019: Ray Tracing Gems 1.1 - Cinematic ...",
			"url": "https://developer.nvidia.com/siggraph/2019/video/sig935-vid",
			"excerpts": [
			  "The new book \"Ray Tracing Gems\" (http://raytracinggems.com, free electronically) is a collection of 32 articles by experts in the field. Authors of selected ..."
			]
		  },
		  {
			"title": "HLBVH: Hierarchical LBVH Construction for Real-Time ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2010-06_HLBVH-Hierarchical-LBVH/HLBVH-final.pdf",
			"excerpts": [
			  "As future work, the authors suggest the possibility of exploiting this scheme in the context of BVH construction by sorting the primitives along a Morton curve,.Read more"
			]
		  },
		  {
			"title": "Ray Tracing Gems - II (Preview) | NVIDIA Developer",
			"url": "https://developer.nvidia.com/ray-tracing-gems-ii/preview",
			"excerpts": [
			  "Section Title: Ray Tracing Gems II - Chapter Previews\nContent:\nRendering experts come together to unearth true gems for developers of games, architectural applications, visualization, and more in this exciting era of real-time rendering.\nEvery Wednesday in July, NVIDIA Developer's can access pre-print PDFs of full chapters from the upcoming book Ray Tracing Gems II available free for download."
			]
		  },
		  {
			"title": "3rd Installment of Ray Tracing Gems Now Available For Free",
			"url": "https://developer.nvidia.com/blog/3rd-installment-of-ray-tracing-gems-now-available-for-free/",
			"excerpts": [
			  "Ray tracing provides developers with an organic, photoreal solution to crafting reflections, refractions, and shadows."
			]
		  },
		  {
			"title": "Geometry Query Optimizations in CAD-based Tessellations ...",
			"url": "https://asset.library.wisc.edu/1711.dl/NNODVI4XHMSOV8F/R/file-4a60b.pdf",
			"excerpts": [
			  "Nodes in Embrees BVH are stored com-\npactly using an encoding scheme which contains the memory address of\nbox information as an integer value. Because nodes are byte-aligned, mem-\nory addresses will be offset by the same amount as the node alignment. For\nexample, if nodes are memory aligned to 16 bytes, the four least significan",
			  "Embree applies Pointer-Based prefetching while traversing its BVH.\nAs nodes intersected by the ray are added to the stack, the bounding box\ninformation for the node at the top of the stack is prefetched for intersection\nin the next step of the BVH traversal."
			]
		  },
		  {
			"title": "Thinking Parallel, Part II: Tree Traversal on the GPU | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/thinking-parallel-part-ii-tree-traversal-gpu/",
			"excerpts": [
			  "Section Title: Thinking Parallel, Part II: Tree Traversal on the GPU > Bounding Volume Hierarchy []()",
			  "We will build our approach around a [bounding volume hierarchy](http://en.wikipedia.org/wiki/Bounding_volume_hierarchy) (BVH), which is a commonly used acceleration structure in [ray tracing](https://developer.nvidia.com/discover/ray-tracing) (for example).",
			  "A bounding volume hierarchy is essentially a [hierarchical grouping](https://developer.nvidia.com/discover/cluster-analysis) of 3D objects, where each group is associated with a conservative bounding box.",
			  "Suppose we have eight objects, O 1 -O 8 , the green triangles in the figure above. In a BVH, individual objects are represented by leaf nodes (green spheres in the figure), groups of objects by internal nodes (N 1 -N 7 , orange spheres), and the entire scene by the root node (N 1 ).",
			  "Each internal node (e.g. N 2 ) has two children (N 4 and N 5 ), and is associated with a bounding volume (orange rectangle) that fully contains all the underlying objects (O 1 -O 4 ).",
			  "The bounding volumes can basically be any 3D shapes, but we will use axis-aligned bounding boxes (AABBs) for simplicity.",
			  "Our overall approach is to first construct a BVH over the given set of 3D objects, and then use it to accelerate the search for potentially colliding pairs. We will postpone the discussion of efficient hierarchy construction to the third part of this series. For now, lets just assume that we already have the BVH in place"
			]
		  },
		  {
			"title": "Minimizing Ray Tracing Memory Traffic through Quantized ...",
			"url": "https://jo.dreggn.org/home/2025_quant.pdf",
			"excerpts": [
			  "Traversing a BVH8 with ray streams entails additional algorithmic complexity, with the promise to amortize quantization data over more child ...Read more"
			]
		  },
		  {
			"title": "Classifying Memory Access Patterns for Prefetching",
			"url": "https://hlitz.github.io/papers/asplos2020.pdf",
			"excerpts": [
			  "Linearizing irregular memory accesses for improved correlated prefetching. ... Prefetching with helper threads for loosely coupled multiprocessor systems.Read more"
			]
		  },
		  {
			"title": "DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations",
			"url": "https://arxiv.org/html/2506.22849v1",
			"excerpts": [
			  "Our BVH traversal is based on a SIMD stack-based traversal loop executed on the GPU.",
			  "DOBB, a BVH that leverages discrete rotations and shared OBB transforms at the hierarchy node level to improve traversal performance in ray tracing.",
			  "The ray is then tested for intersection against all child nodes.",
			  "The input ray is transformed into the rotated space prior to intersection with all internal node children.",
			  "Additionally, we leverage hardware-accelerated ray-matrix transformations to enable ray-OBB intersection."
			]
		  },
		  {
			"title": "A Study of Persistent Threads Style GPU Programming for ...",
			"url": "https://www.classes.cs.uchicago.edu/archive/2016/winter/32001-1/papers/AStudyofPersistentThreadsStyleGPUProgrammingforGPGPUWorkloads.pdf",
			"excerpts": [
			  "Aila\net al. present a thorough study of improving the e ffi ciency\nof processing the irregular the *trace()* phase in ray tracing\non vector processors through PT [1].",
			  "warp-wide blocks for avoiding a single ray holding several\nwarps within a block hostage 2 ,and bypassing the hardware\nscheduler by the use of PT."
			]
		  },
		  {
			"title": "Software Prefetches - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/software-prefetches",
			"excerpts": [
			  "Software prefetching faces challenges with irregular pointer-based structures due to the pointer-chasing problem, which limits early prefetch initiation because the address of future nodes cannot be determined without traversing intermediate nodes.",
			  "Greedy prefetching is a technique for [linked data](../computer-science/linked-data) structures, inserting prefetch instructions for successor nodes, but its effectiveness is limited by the inability to overlap prefetch latency with more than a [single iteration](../computer-science/single-iteration) .",
			  "Jump pointer prefetching and prefetch arrays are used to address early node prefetching in linked lists.",
			  "Natural Pointer Techniques The simplest software prefetching technique for LDS traversal is greedy prefetching, proposed by Luk and Mowry .",
			  "Greedy prefetching is attractive due to its simplicity. However, its ability to properly time prefetch initiation is limited. For the linked-list traversal in Figure 3.25 (a) , each prefetch overlaps with a single-loop iteration only."
			]
		  },
		  {
			"title": "The pros and cons of explicit software prefetching - Johnny's Software Lab",
			"url": "https://johnnysswlab.com/the-pros-and-cons-of-explicit-software-prefetching/",
			"excerpts": [
			  "Pointer chasing codes (linked lists or binary trees) need to be rewritten in order to benefit from prefetching. Here is the example binary tree lookup:",
			  "This code has a instruction dependency on memory loads. In order to prefetch, we need to know a few memory addresses in advance. However, with these type of codes this is not possible: we dont know the address of the next node until we have loaded the current node and performed the comparison.",
			  "Issuing a prefetch request immediately before accessing a variable doesnt work.",
			  "Luckily, there is a solution. We already covered the techniques which are aimed at braking dependency chains or interleaving several dependency chains in our post about [instruction level parallelism](https://johnnysswlab.com/instruction-level-parallelism-in-practice-speeding-up-memory-bound-programs-with-low-ilp/) . Techniques presented there combine nicely with explicit software prefetching. Lets take our binary tree example and perform 16 parallel searches:",
			  "This code performs 16 parallel searches. We can easily add two prefetches like this:",
			  "This will cause the CPU to prefetch the correct child. If we were running one search in parallel, the access to this child would be immediately after the prefetch and prefetching wouldnt make sense.",
			  "With 16 parallel searches, there is enough time for the hardware to prefetch the child before it is being accessed."
			]
		  },
		  {
			"title": "Wide BVH Traversal with a Short Stack",
			"url": "https://www.embree.org/papers/2019-HPG-ShortStack.pdf",
			"excerpts": [
			  "Applying our algorithm to wide BVHs, we demonstrate that the number of traversal steps with just five stack entries is close to that of a full traversal stack."
			]
		  },
		  {
			"title": "Bf-Tree: A Modern Read-Write-Optimized Concurrent Range ...",
			"url": "https://vldb.org/pvldb/vol17/p3442-hao.pdf",
			"excerpts": [
			  "**Figure 2: High level architecture of** **Bf-Tree** **.** Like conventional\nB-Tree, but pages in the buffer pool are variable lengths.",
			  "The key insight of this paper is that we can *separate cache pages*\n*from disk pages* , i.e., the cache pages are no longer a mirror of their\ndisk content. Instead, they contain a judiciously chosen subset of\nthe disk page that is worth caching. Called *mini-pages* , these cached\npages are a native part of the trees memory component and can\n ... ",
			  "Caching hot records.** Before reading the leaf page from disk, a\nread operation first (binary) searches the mini-page for the desired\nrecord and terminates early if the record is found. Searching the\nmini-page is efficient as the records are sorted and in-memory. If\nthe record is not found in the mini-page, we load the corresponding\nleaf page from disk.",
			  "**Figure 3: Mini-page (var len) / leaf node (4096 bytes) layout**\nthe search early. Note that the mini-page will cache the individual\nrecords, not the entire page, avoiding the inefficient page-level\ncaching of conventional B-Trees. To avoid flooding the mini-page\nwith cold records, we only cache the records from the leaf page at a\nlow probability, e.g., 1%. Caching hot records is implemented as an\nin-memory insert operation to the mini-page, which may trigger\nthe mini-page to grow as needed.",
			  "**Caching range gaps.** Record caching does not help with range\nscans because the range query has to look at the leaf page for the\nfull set of records; in other words, record caching breaks the spatial\nlocality needed for range scans. On the other hand, a page-level\ncache is ideal for range scans, as it preserves the spatial locality of\nrecords. Mini-pages support caching range gaps: when a mini-page\ngrows to the full size, we merge (if necessary) and convert it into\na full leaf page mirroring disk.",
			  "**3.2**\n**Mini and leaf page layout**\nMini-pages and leaf pages share the same layout, storing key-value\npairs in sorted order and allowing efficient lookups. In Bf-Tree , they\nshare the same implementation, except that mini-pages can have\nvarying lengths.",
			  "Tree still has the best caching ratio because its mini-page design can\nbetter handle the internal fragmentation of the leaf pages. Practical\nB-Tree leaf pages only have about 70% of the space used due to the\nnature of page splitting. When caching the entire page in memory,\nthis leads to a 30% waste of memory. Bf-Tree s mini-pages will\ndynamically grow and shrink to fit the actual number of records in\nthe page, leading to a better caching ratio.",
			  "*Figure 11: Impact of read and write workloads.**\nFigure 11 shows the impact of read and write workloads on Bf-\nTree and baseline systems. For systems that implement buffered\nwrite, e.g., Bf-Tree , B ** -Tree, and RocksDB, the throughput is higher\nwith more write operations. This is because most write operations\nare buffered in memory and only flushed to disk when the buffer\nis full. But for read operations, each cache miss incurs a random\ndisk IO to read the data from the disk.",
			  "Figure 2: High level architecture of** **Bf-Tree** **.** Like conventional\nB-Tree, but pages in the buffer pool are variable lengt"
			]
		  },
		  {
			"title": "Dual Streaming for Hardware-Accelerated Ray Tracing",
			"url": "https://hwrt.cs.utah.edu/papers/hwrt_hpg17.pdf",
			"excerpts": [
			  "The streaming processor uses many Thread Multiprocessors\n(TMs) for computation, which share chip-wide stream units.",
			  "The streaming processor uses many Thread Multiprocessors\n(TMs) for computation, which share chip-wide stream units.",
			  "e stream scheduler marshals the data required\nfor ray traversal to prevent TPs from accessing main memory di-\nrectly for both scene and ray stream data.",
			  "he stream scheduler\nalso tracks the current state of traversal, including the *working set*\nof active scene segments, the mapping of TMs to scene segments,\nand the status of the scene and ray streams",
			  "al streaming provides a new ray traversal order that resolves\nsome of the decades-old problems of high-performance ray tracing:\n *Random access to main memory during traversal is avoided",
			  "This is particularly important for large scenes and incoherent\nrays (such as secondary rays).",
			  "Memory latency is hidden by perfect prefetching.* A traditional\nsolution hides memory latency by adding more threads, which\n .",
			  "e *ray stream* consists of all rays in flight\ncollected as a queue per scene segment they intersect.",
			  "Rays at the same depth\nare traced as a wavefront, so each additional bounce requires an\nadditional pass.",
			  "he complete streaming processor is built from many TMs which\nshare access to several global units: the *stream scheduler* , the *scene*\n*buffer* , and the *hit record ",
			  "he complete streaming processor is built from many TMs which\nshare access to several global units: the *stream scheduler* , the *scene*\n*buffer* , and the *hit record "
			]
		  },
		  {
			"title": "Divergence-Aware Warp Scheduling",
			"url": "https://engineering.purdue.edu/tgrogers/publication/rogers-micro-2013/rogers-micro-2013.pdf",
			"excerpts": [
			  "Divergence-Aware Warp Scheduling (DAWS), which\nintroduces a divergence-based cache footprint predictor to estimate\nhow much L1 data cache capacity is needed to capture intra-warp\nlocality in loops.",
			  "WS uses these predictions to schedule warps\nsuch that data reused by active scalar threads is unlikely to ex-\nceed the capacity of the L1 data cache.",
			  "memory divergence detec-\ntor is used to classify static load instructions as convergent or diver-\ngent.",
			  "oalescing reduces the num-\nber of memory requests by merging accesses from multiple lanes\ninto cache line sized chunks when there is spatial locality across\nthe warp [1]",
			  "Memory divergence oc-\ncurs when coalescing fails to reduce the number of memory re-\nquests generated by an instruction to two or l",
			  "Divergence-Aware Warp Scheduling\nis a novel technique that proactively uses predictions to prevent\ncache thrashing before it occurs and aggressively increases cache\nsharing between warps as their thread activity decreases.",
			  "ergence-Aware\nWarp Scheduling uses this predicted code behaviour in combina-\ntion with live thread activity information to make more locality-\naware scheduling decisions.",
			  "ergence-Aware\nWarp Scheduling uses this predicted code behaviour in combina-\ntion with live thread activity information to make more locality-\naware scheduling decisions."
			]
		  },
		  {
			"title": "Enabling advanced GPU features in PyTorch  Warp Specialization  PyTorch",
			"url": "https://pytorch.org/blog/warp-specialization/",
			"excerpts": [
			  "Warp specialization (WS) is a GPU programming technique where warps (a group of 32 threads on NVIDIA GPUs) within a threadblock are assigned distinct roles or tasks. This approach optimizes performance by enabling efficient execution of workloads that require task differentiation or cooperative processing. It enhances kernel performance by leveraging an asynchronous execution model, where different parts of the kernel are managed by separate hardware units. Data communication between these units, facilitated via shared memory on the NVIDIA H100, is highly efficient. Compared to a uniform warp approach, warp specialization allows the hardware multitasking warp scheduler to operate more effectively, maximizing resource utilization and overall performance.",
			  "Using GEMM as an example, a typical uniform warp approach on the H100 GPU involves 8 warps per thread block collectively computing a tile of the output tensor. These 8 warps are divided into two warp groups (WG), with each group cooperatively computing half of the tile using efficient warp-group-level MMA (WGMMA) instructions, as illustrated in Figure 1.",
			  "ask Partitioning** : The entire kernel is automatically divided into asynchronous tasks based on predefined heuristics. The compiler determines how to utilize one producer warp group and a user-specified number of consumer warp groups to execute the kernel. It assigns task IDs to specific anchor operations, which then influence the task assignments for remaining operations through asynchronous task ID propagation and dependency analysi",
			  "ta Partitioning for Multiple Consumer Groups** : Efficiently partitioning data among multiple consumer groups is key to optimizing workload distribution. On the H100 GPU, the compiler, by default, attempts to partition the input tensor `A` along the `M` dimension, allowing each consumer group to compute half of the output tensor independently. This strategy, known as [cooperative partitioning](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md) , maximizes efficiency under most conditions. However, if this split leads to inefficienciessuch as producing a workload smaller than the native WGMMA instruction sizethe compiler dynamically adjusts and partitions along the `N` dimension instead"
			]
		  },
		  {
			"title": "Thread Cluster Memory Scheduling: Exploiting Differences ...",
			"url": "https://www.cs.cmu.edu/~harchol/Papers/micro2010.pdf",
			"excerpts": [
			  "This paper presents a new memory scheduling algorithm that addresses system throughput and fairness separately with the goal of achieving the best of both. The ...Read more"
			]
		  },
		  {
			"title": "Coordinated Control of Multiple Prefetchers in Multi-Core ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/coordinated-prefetching_micro09.pdf",
			"excerpts": [
			  "Our goal in this paper is to develop a hardware framework that enables large performance improvements from prefetching in CMPs by significantly reducing ...Read more"
			]
		  },
		  {
			"title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
			"url": "https://arxiv.org/html/2504.19365v3",
			"excerpts": [
			  "To avoid redundant requests, AGILE coalesces identical data requests issued by different threads, which is essential because user threads may independently request the same data chunk from SSDs.",
			  "To avoid redundant requests, AGILE coalesces identical data requests issued by different threads, which is essential because user threads may independently request the same data chunk from SSDs.",
			  "Then, AGILE selects one thread to forward the request to the second-level coalescing stage.",
			  "Then, AGILE selects one thread to forward the request to the second-level coalescing stage.",
			  "he second level is handled by the AGILE software cache (Section [3.4](https://arxiv.org/html/2504.19365v3.SS4 \"3.4. AGILE Software Cache  3. AGILE Design & Implementation  AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration\") ), which filters remaining redundant requests that are not eliminated in the first warp-level coalescing stag",
			  "he second level is handled by the AGILE software cache (Section [3.4](https://arxiv.org/html/2504.19365v3.SS4 \"3.4. AGILE Software Cache  3. AGILE Design & Implementation  AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration\") ), which filters remaining redundant requests that are not eliminated in the first warp-level coalescing stag",
			  "AGILE prioritizes the warp-level coalescing since accessing the shared software cache requires atomic operations to maintain consistency, which creates critical sections and serializes execution. This serialization can cause stalls and different execution paths for threads in a warp, which introduces warp divergence and degrades overall GPU performance.",
			  "AGILE prioritizes the warp-level coalescing since accessing the shared software cache requires atomic operations to maintain consistency, which creates critical sections and serializes execution. This serialization can cause stalls and different execution paths for threads in a warp, which introduces warp divergence and degrades overall GPU performance.",
			  "For async_issue(src,dst) , which mimics cp.async (Nvidia, [2025d](https://arxiv.org/html/2504.19365v3.bib37) ) or cuda::memcpy_async (Nvidia, [2025g](https://arxiv.org/html/2504.19365v3.bib40) ) in CUDA and no warp-level coalescing is performed.",
			  "Even if threads in a warp request the same data, each thread will still obtain its own copy of the requested data.",
			  "Even if threads in a warp request the same data, each thread will still obtain its own copy of the requested data.",
			  "t:\nFlash memory cannot be accessed randomly, and data is managed at a coarse-grained page level, typically 4KB per page (Gal and Toledo, [2005](",
			  "Therefore, the software cache line should align with the SSDs granularity. This alignment can avoid redundant I/Os when multiple threads access different parts of the same SSD page concurrently.",
			  "To ensure correctness during accessing the same cache line simultaneously, atomic operations are required to avoid conflicts and data hazards.",
			  "AGILE promises to offer flexibility in the software cache policy, and therefore, eliminating the potential for deadlock caused by the software cache is necessary.",
			  "A common scenario resulting in a deadlock is simultaneous threads accessing multiple cache lines.",
			  "To prevent redundant SSD accesses, once a thread checks the software cache and the requested data is foundi.e., a cache hit occursaccess to the corresponding cache lines must be atomic to avoid eviction before accesses in process are completed.",
			  "When multiple threads\nblock cache line eviction while requesting new cache lines, a deadlock could occur."
			]
		  },
		  {
			"title": "Optimizing Storage Performance with Calibrated Interrupts",
			"url": "https://www.usenix.org/system/files/osdi21-tai.pdf",
			"excerpts": [
			  " Typically, interrupt coalescing addresses interrupt storms\nby batching requests into a single interrupt. Batching, how-\n** Denotes co-first authors with equal contribution.\never, creates a trade-off between request latency and the inter-\nrupt rate. For the workloads we inspected, CPU utilization in-\ncreases by as much as 55% without coalescing (Figure 12 (d)),\nwhile under even the minimum amount of coalescing, request\nlatency increases by as much as 10 ** for small requests, due\nto large timeouts. Interrupt coalescing is disabled by default\nin Linux, and real deployments use alternatives ( 2 ).\nThis paper addresses the challenge of dealing with expo-\nnentially increasing interrupt rates without sacrificing latency.\nWe initially implemented adaptive coalescing for NVMe, a dy-\nnamic, device-side-only approach that tries to adjust batching\nbased on the workload, but find that it still adds unneces-\nsary latency to requests ( 3.2 ). This led to our core insight\nthat device-side heuristics, such as our adaptive coalescing\nscheme, cannot achieve optimal latency because the device\nlacks the semantic context to infer the requesters intent: is the\nrequest latency-sensitive or part of a series of asynchronous\nrequests that the requester completes in parallel? Sending this\nvital information to the device bridges the semantic gap and\nenables the device to interrupt the requester when appropriate.\nWe call this technique *calibrating* 1 interrupts (or simply,\n ... \nsemantic information is easily accessible in the storage stack\nand available at submission time.",
			  "We initially implemented adaptive coalescing for NVMe, a dy-\nnamic, device-side-only approach that tries to adjust batching\nbased on the workload, but find that it still adds unneces-\nsary latency to requests ( 3.2 ). This led to our core insight\nthat device-side heuristics, such as our adaptive coalescing\nscheme, cannot achieve optimal latency because the device\nlacks the semantic context to infer the requesters intent: is the\nrequest latency-sensitive or part of a series of asynchronous\nrequests that the requester completes in parallel? Sending this\nvital information to the device bridges the semantic gap and\nenables the device to interrupt the requester when appropriate.\n",
			  "ll this technique *calibrating* 1 interrupts (or simply,\n ... \nsemantic information is easily accessible in the storage stack\nand available at submission t",
			  ".\n**3.2**\n**Adaptive Coalescing**\nIdeally, an interrupt coalescing scheme adapts dynamically to\nthe workload. Figure 4 shows that even if the timeout granu-\nlarity in the NVMe specification were smaller, it is still *fixed* ,\nwhich means that interrupts will be generated when the work-\nload does not need interrupts ( *c* 1 ** *c* 8 ), while completions\nmust wait for the timeout to expire ( *c* 9 ) when the workload\ndoes need interrupts.\nInstead, as shown in the bottom row of Figure 4 , the adap-\ntive coalescing strategy in cinterrupts observes that a device\nshould generate a single interrupt for a burst, or a sequence\n ... "
			]
		  },
		  {
			"title": "Optimizing Storage Performance with Calibrated Interrupts",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3505139",
			"excerpts": [
			  ":\n*Comparison to NVMe Coalescing.* The adaptive strategy outperforms various NVMe coalescing configurations, even those with smaller timeouts, across different workloads. We compare the adaptive strategy, configured with $thr=32$ , $\\Delta =6$ , to no coalescing (default), nvme100, which uses a timeout of 100 $\\mu$ s, the smallest possible in standard NVMe, nvme20, which uses a theoretical timeout of 20 $\\mu$ s, and nvme6, which uses a theoretical timeout of 6 $\\mu$ s. All NVMe coalescing configurations have threshold set to 32.\nWe",
			  "The adaptive strategy outperforms various NVMe coalescing configurations, even those with smaller timeouts, across different workloads.",
			  "*YCSB-E.* Scans are interesting because their latency is determined by the completion of requests that can span multiple submission boundaries. Table [5]() shows throughput results for YCSB-E with different scan lengths, and Figure [24]() shows latency CDFs for scans of length 16 and 256.",
			  "Similar to the other YCSB workloads, the adaptive strategy again can almost match the throughput of cinterrupts, because it is designed for batching. At higher scan lengths, factors such as application-level queueing begin affecting scan throughput, reducing the benefit of cinterrupts.",
			  "Excessive interrupt generation limits default throughput to 86%-89% of cinterrupts."
			]
		  },
		  {
			"title": "Multi-stage coordinated prefetching for present-day processors | Proceedings of the 28th ACM international conference on Supercomputing",
			"url": "https://dl.acm.org/doi/10.1145/2597652.2597660",
			"excerpts": [
			  "Data prefetching is an important technique for hiding memory latency. Latest microarchitectures provide support for both hardware and software prefetching.",
			  "Based on our study of the interaction between the host architecture and prefetching, we find that coordinated multi-stage prefetching that brings data closer to the core in stages, yields best performance.",
			  "On SandyBridge, the mid-level cache hardware prefetcher and L1 software prefetching coordinate to achieve this end, whereas on Xeon Phi, pure software prefetching proves adequate.",
			  "We implement our algorithm in the ROSE source-to-source compiler framework."
			]
		  },
		  {
			"title": "Maximizing Hardware Prefetch Effectiveness with Machine ...",
			"url": "https://userweb.cs.txstate.edu/~burtscher/papers/hpcc15.pdf",
			"excerpts": [
			  " can help the user gain up to 96% of the achievable speedup\nprovided by the hardware prefetchers.",
			  "First, it\nenables us to use existing hardware more effectively.",
			  "Second, it\ndoes not require any source-code changes of the program being\noptimized.",
			  "stly,\nthe framework relies on open-source technologies, making it\nis easy to extend and port to other architectures",
			  "Prefetching is a widely explored area, and the benefits of\nprefetching are broadly documented and studied."
			]
		  },
		  {
			"title": "BLISS: Balancing Performance, Fairness and Complexity ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bliss-memory-scheduler_ieee-tpds16.pdf",
			"excerpts": [
			  "utlu and Moscibroda propose PARBS [4], an applica-\ntion-aware memory scheduler that batches the oldest\nrequests from applications and prioritizes the batched\nrequests, with the goals of preventing starvation and\nimproving fairness",
			  "ithin each batch, PARBS ranks indi-\nvidual applications based on the number of outstanding\nrequests of each application and, using this total rank order,\nprioritizes requests of applications that have low-memory-\nintensity to improve system throughpu",
			  "im et al. [6] observe that applications that receive low\nmemory service tend to experience interference from applica-\ntions that receive high memory service. Based on this observa-\ntion, they propose ATLAS, an application-aware memory\nscheduling policy that ranks individual applications based on\nthe amount of long-term memory service each receives and\nprioritizes applications that receive low memory service, with\nthe goal of improving overall system throughp",
			  "Thread cluster memory scheduling (TCM) [7] ranks\nindividual applications by memory intensity such that\nlow-memory-intensity applications are prioritized over\nhigh-memory-intensity applications (to improve system\nthroughput). Kim et al. [7] also observed that ranking all\napplications based on memory intensity and prioritizing\n ...",
			  "RFCFS has the lowest average request latency among\nall the schedulers. This is expected since FRFCFS maximizes\nDRAM throughput by prioritizing row-buffer hits. Hence,\nthe number of requests served is maximized overall (across\nall applications). However, maximizing throughput (i.e.,\nminimizing overall average request latency) degrades the\nperformance of low-memory-intensity applications, since\nthese applications requests are often delayed behind row-\nbuffer hits and older requests. This results in degradation in\nsystem performance and fairness, as shown in Fig. 4.",
			  "nd, ATLAS and TCM, memory schedulers that priori-\ntize\nrequests\nof\nlow-memory-intensity\napplications\nby\nemploying a full ordered ranking achieve relatively low aver-\nage latency. This is because these schedulers reduce the\nlatency of serving requests from latency-critical, low-mem-\nory-intensity applications significantly. Furthermore, priori-\ntizing low-memory-intensity applications requests does not\nincrease the latency of high-memory-intensity applications\nsignifican",
			  "ons, we conclude that BLISS achieves the best\nperformance and a good trade-off between fairness and per-\nformance for most of the workloads we examine."
			]
		  },
		  {
			"title": "DESIGNING EFFICIENT MEMORY SCHEDULERS FOR ...",
			"url": "https://users.cs.utah.edu/~rajeev/pubs/nil-thesis.pdf",
			"excerpts": [
			  "2.3.1\nFirst Read First-Come-First-Served (FR-FCFS)\nProposed by Rixner et al. [12], this is the most popular memory scheduling\nalgorithm that has been explored in detail in academia and also implemented in\nalmost all commercial memory schedulers today. The basic idea of this scheduler is\nto allow requests that require less time to be serviced, by virtue of being a row-hit, to\npreempt older row-miss requests. Evidently, this has higher performance and better\nenergy characteristics than a first-come first-served (FCFS) policy.",
			  "2.3.3\nParallelism-Aware Batch Scheduling (PARBS)\nProposed by Moscibroda et al. [34], this scheme tries to maintain the fairness and\nquality-of-service notions introduced in STFM and, in addition, aims at improving\nthe system throughput. The scheduler first forms batches of requests by grouping\nconsecutive outstanding requests in the memory request buffers and services all re-\nquests in a batch before moving over to the next batch. By grouping requests into\n17\nbatches, the scheme avoids starvation of threads at a very fine granularity and ensures\nsteady and fair progress across all threads. Within a batch, row-hits are prioritized\nover row-misses and threads with few requests or those that display high-bank-level\nparallelism are prioritized over others to minimize the service time of a bat",
			  "2.3.4\nAdaptive per-Thread Least-Attained-Service\nMemory Scheduling (ATLAS)\nProposed by Kim et al. [13], ATLAS is a scheme that allows multiple memory-\ncontrollers to coordinate their scheduling decisions to improve throughput. Execution\ntime is split into long epochs. During each epoch, the memory-controllers keep track of\nthe level of service received by each thread from the memory system. At the beginning\nof the next epoch, this information is accumulated at a central coordinator, which\nincreases the priorities of the threads that received the least service in the previous\nepoch. This information is propagated to the memory-controllers and thereafter, the\nselected threads are prioritized.",
			  "2.3.5\nThread-Cluster Memory Scheduling (TCM)\nProposed by Kim et al. [14], TCM argues that techniques such as STFM, PAR-BS,\nand ATLAS are unable to provide adequate fairness and high throughput because they\nuse the same policy for all threads. In contrast, TCM uses the memory behavior of the\nthread to decide its priority. First, it prioritizes requests from non-memory-intensive\nthreads over memory-intensive ones during memory scheduling. After making the\nobservation that unfairness in memory scheduling techniques stems from interference\namong memory-intensive threads, TCM periodically shuffles the priority order among\nsuch threads to increase fairness. However, not all threads get to enjoy all priority lev-\nels; instead, threads with higher bank-level parallelism are prioritized over streaming\nthreads that have high row-buffer locality.",
			  "4.6.3\nPAR-BS:\nThe PAR-BS scheme [34] forms batches of requests in a CPUs memory controller\nand issues requests from a batch to the memory system. The express motivation\nbehind the batch-formation is fairness and as a result, a batch in PAR-BS will include\nrequests from many threads and have different batches for different banks.\nOur\nbatching scheme does exactly the opposite and groups requests from a warp together\nto reduce the latency divergence of a warp. In addition, we arbitrate between batches\nbased on a bank-aware shortest job first policy to reduce wait time for warps, which\nis different from PAR-BS, which uses a MLP-based SJF policy for thread priorities."
			]
		  },
		  {
			"title": "The Blacklisting Memory Scheduler",
			"url": "https://users.ece.cmu.edu/~lsubrama/pubs/tr-2015-004.pdf",
			"excerpts": [
			  "rank order incurs high hardware complexity, as we demon-\nstrate in Section 7.2, slowing down the memory scheduler\nsignificantly (by 8x for TCM compared to FRFCFS), while\nalso increasing its area (by 1.8x).",
			  "Second, a total-order ranking is unfair to\napplications at the bottom of the ranking stack.",
			  "e implement them in Register Transfer Level\n(RTL), using Verilog. We synthesize the RTL implementations\nwith a commercial 32 nm standard cell library, using the\nDesign Compiler tool from Synopsys.\n",
			  "BLISS\nachieves 5% better weighted speedup, 25% lower maximum\nslowdown and 19% better harmonic speedup than the best",
			  "First, FRFCFS has the lowest average request latency\namong all the schedulers. This is expected since FRFCFS\nmaximizes DRAM throughput by prioritizing row-buffer hits.",
			  "Second, ATLAS and TCM, memory schedulers that prior-\nitize requests of low-memory-intensity applications by em-\nploying a full ordered ranking achieve relatively low av-\nerage latency.",
			  "we conclude that BLISS achieves the best trade-off between\nperformance, fairness and simplicity."
			]
		  },
		  {
			"title": "ATLAS: A scalable and high-performance scheduling ...",
			"url": "https://ieeexplore.ieee.org/document/5416658/",
			"excerpts": [
			  "ATLAS (Adaptive per-Thread Least-Attained-Service memory scheduling), a fundamentally new memory scheduling technique that improves system throughput without requiring significant coordination among memory controllers. The key idea is to periodically order threads based on the service they have attained from the memory controllers so far, and prioritize those threads that have attained the least service over others in each period."
			]
		  },
		  {
			"title": "GraphIt: a high-performance graph DSL",
			"url": "https://commit.csail.mit.edu/papers/2018/graphit.pdf",
			"excerpts": [
			  "This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics ..."
			]
		  },
		  {
			"title": "Inter-core Prefetching for Multicore Processors Using ...",
			"url": "https://cseweb.ucsd.edu/~swanson/papers/ASPLOS2011Prefetching.pdf",
			"excerpts": [
			  "This paper describes and evaluates helper threads that run on separate cores of a multicore and/or multi-socket computer system. Multiple threads running on ...Read more"
			]
		  },
		  {
			"title": "Exploring Memory Access Patterns for Graph Processing ...",
			"url": "https://arxiv.org/abs/2010.13619",
			"excerpts": [
			  "In this work, we propose a simulation environment for the analysis of graph processing accelerators based on simulating their memory access patterns."
			]
		  },
		  {
			"title": "Irregular accesses reorder unit: improving GPGPU memory ...",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/f1ddd05c-f631-449d-bf34-8de6a3105a44/content",
			"excerpts": [
			  "The IRU reorders data processed by the threads on irregular accesses to\nimprove memory coalescing, i.e., it tries to assign data elements to threads as to\nproduce coalesced accesses in SIMT groups.",
			  "The IRU is a compact and efficient hardware\nunit integrated into the Memory Partition (MP) of the GPU architecture as shown\nin Fig. 4 a, which incurs in very small energy and area overheads.",
			  "The GPU architecture with our IRU improves memory coalescing by a factor of\n1.32x and reduces NoC traffic by 46%, which result in 1.33x speedup and 13%\nenergy savings on average for a diverse set of graph-based applications.",
			  "raffic between SM and MP is reduced to as low as 23% for the *human* benchmark\non PR, overall reducing NoC traffic to 54% of the original interconnection traffic",
			  "IRU filtering further\nreduces accesses by removing/merging duplicated elements, that avoids additional\nmemory accesses.",
			  "IRU coalescing and\nfiltering improvement for these operations reduces L2 accesses but not L1 accesses,\nexplaining the larger reduction in L2 accesses compared to L1 for SSSP and PR.",
			  "ig. 14** Improvement in memory coalescing achieved with the IRU over the Baseline GPU system (GTX\n980 GPU with parameters shown in Table 2 )",
			  "\n**Fig. 13** Normalized interconnection traffic between SM (Streaming Multiprocessors) and MP (Memory\nPartitions) for the IRU enabled GPU system over the Baseline GPU system (GTX 98"
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "the\ndependence relationships between loads and stores, and prefetch\nthe subsequent nodes on their basis [ 4 ].",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "line design consists of a simple five stage\npipelined processor designed in ECE4750 interfaced with a blocking\ncache, which in turn is connected to memor",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system.",
			  "oosely-coupled processors have an advantage*\n*in that fine-grain resources, such as processor and L1 cache re",
			  "sources, are not contended by the application and helper threads,",
			  "*present techniques to alleviate this. O",
			  "r approach exploits large*\n*loop-based code regions and is based on a new synchronization",
			  "mechanism between the application and helper threads.",
			  "This*\n*mechanism precisely controls how far ahead the execution of t"
			]
		  },
		  {
			"title": "Security and Performance Aspects of HugePages Configuration",
			"url": "https://linuxgd.medium.com/security-and-performance-aspects-of-hugepages-configuration-72f138dc7d09",
			"excerpts": [
			  "Since HugePages are larger than standard pages, they might contain more sensitive data, increasing the risk of unauthorized access if security ...Read more"
			]
		  },
		  {
			"title": "caching - What is TLB shootdown? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/3748384/what-is-tlb-shootdown",
			"excerpts": [
			  "The actions of one processor causing the TLBs to be flushed on other processors is what is called a TLB shootdown. Copy link CC BY-SA 3.0. ...Read more"
			]
		  },
		  {
			"title": "\n\tThe term \"TLB shootdown\" was - Intel Community\n",
			"url": "https://community.intel.com/t5/Software-Tuning-Performance/Do-the-terms-tlb-shootdown-and-tlb-flush-refer-to-the-same-thing/m-p/1155420",
			"excerpts": [
			  "The \"shootdown\" refers to the software coordination of TLB invalidations, so it can be counted by the OS -- eg, \"cat /proc/interrupts | grep TLB\" in Linux.Read more"
			]
		  },
		  {
			"title": "Mitigating the Performance Impact of TLB Shootdowns ...",
			"url": "https://www.researchgate.net/publication/220884710_DiDi_Mitigating_the_Performance_Impact_of_TLB_Shootdowns_Using_a_Shared_TLB_Directory",
			"excerpts": [
			  "In VMs, TLB shootdown cost is even greater due to the ICR MSR write VM exits associated to sending IPIs and TLB shootdown preemption. ... Despite ...Read more"
			]
		  },
		  {
			"title": "A.12.numactl | Performance Tuning Guide | Red Hat Enterprise Linux | 7 | Red Hat Documentation",
			"url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-tool_reference-numactl",
			"excerpts": [
			  "Numactl lets administrators run a process with a specified scheduling or memory placement policy. Numactl can also set a persistent policy for shared memory ...Read more"
			]
		  },
		  {
			"title": "Understanding Huge Pages | Netdata\n",
			"url": "https://www.netdata.cloud/blog/understanding-huge-pages/",
			"excerpts": [
			  "Section Title: Understanding Huge Pages > Huge Pages",
			  "Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size. They help reduce the pressure on the Translation Lookaside Buffer (TLB) and lower the overhead of managing memory in systems with large amounts of RAM.",
			  "Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size. They help reduce the pressure on the Translation Lookaside Buffer (TLB) and lower the overhead of managing memory in systems with large amounts of RAM."
			]
		  },
		  {
			"title": "Cache-Aware Virtual Page Management - UWSpace",
			"url": "https://uwspace.uwaterloo.ca/bitstreams/71c06c89-7b69-42af-8e59-7f7134aa151c/download",
			"excerpts": [
			  "Page colouring assigns every bin a colour (a unique ID) and attempts to allocate memory\nof as many different colours as possible. In this way, the memory allocator spreads the\nvirtual pages across the CPUs cache.",
			  "Linuxs Hugepages serve as an experimental verification of this hypothesis. By using\nextremely large page sizes, the amount of bins is reduced to a single one. The experiment\nshows decreased variability as well as improved CPI measurements due to a reduction",
			  "The buffer strategy utilizes Linuxs Hugepages, which results in a perfect page-colouring as\nwell as a reduction in TLB misses.",
			  "Page colouring\nallows whole pages of data to be carefully positioned within the cache, providing the\nmechanism with which pages are placed into various partitions.",
			  "The Power 7 architecture particularly improves in-\nstruction throughput performance due to page colouring. With a best case performance\nimprovement of 66 %, the Power 7 architecture demonstrates the significance of cache-\naware memory allocation.",
			  "The initial colouring is done at boot-time, allowing\nthe operating system to allocate its own data-structures in a cache-aware manner.",
			  "The experiment\nshows decreased variability as well as improved CPI measurements due to a reduction\n55\nin conflict page mappin",
			  "Linuxs Hugepages serve as an experimental verification of this hypothesis. By using\nextremely large page sizes, the amount of bins is reduced to a single one.",
			  "A shared CPU cache has been shown to suffer excessive conflict misses when\nsubjected to certain combinations of workloads [26]."
			]
		  },
		  {
			"title": "Huge Pages are a Good Idea (evanjones.ca)",
			"url": "https://www.evanjones.ca/hugepages-are-a-good-idea.html",
			"excerpts": [
			  "On my Intel 11th generation Core i5-1135G7 (Tiger Lake) from 2020, using 2 MiB huge pages is 2.9 faster. I also tried 1 GiB pages, which is 3.1 faster than 4 kiB pages, but only 8% faster than 2 MiB pages.",
			  "In 2021, Google published a [paper about making their malloc implementation (TCMalloc) huge page aware (called Temeraire)](https://www.usenix.org/conference/osdi21/presentation/hunter) . They report this improved average requests-per-second throughput across their fleet by 7%, by increasing the amount of memory that is backed by huge pages.",
			  "Notably, when the Linux kernel's transparent huge page implementation was first introduced, it was enabled by default, which caused many performance problems.",
			  "Today's default to use huge pages only for applications that opt-in (aka madvise) should improve this.",
			  "The Linux kernel's implementation of transparent huge pages has been the source of performance problems.",
			  "Unfortunately, using larger pages is not without its disadvantages. Notably, when the Linux kernel's transparent huge page implementation was first introduced, it was enabled by default, which caused many performance problems. See the section below for more details. Today's default to use huge pages only for applications that opt-in (aka madvise) should improve this. The kernel's policies for managing huge pages have also changed since then, and are hopefully better now. At the very least, the fact that Google uses transparent huge pages for all their applications is some evidence that this can work for a wide variety of workloads."
			]
		  },
		  {
			"title": "CPU performance  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/cpu-benchmarks/",
			"excerpts": [
			  "Section Title: Latency & pointer chasing [Link to heading]() > Pointer chasing [Link to heading]()",
			  "Content:\nWe start with a simple *pointer chasing* experiment: we create a large array in\nwhich each position contains the index of another position and then follow the chain.\nIn particular, we ensure that the array is a random *derangement* , or rather, a permutation\nthat is just one long cycle, so that memory\ncannot be prefetched by the hardware prefetcher."
			]
		  },
		  {
			"title": "Don't shoot down TLB shootdowns!",
			"url": "https://nadav.amit.zone/publications/pdfs/amit2020tlb.pdf",
			"excerpts": [
			  "TLB shootdowns are costly, as they invoke a com-\nplex protocol which burdens all processors in the system\nand must wait for the acknowledgement of remote cores,\nrequiring several thousand cycles to complet",
			  "We introduce four general techniques to improve\nshootdown performance: (1) concurrently flush initiator and\nremote TLBs, (2) early acknowledgement from remote cores,\n(3) cacheline consolidation of kernel data structures to reduce\ncacheline contention, and (4) in-context flushing of userspace\nentries to address the overheads introduced by Spectre and\nMeltdown mitigations.",
			  "e apply our optimizations to Linux 5.2.8 and show that\nwe obtain significant performance improvements in both mi-\ncrobenchmarks and real-world workloads such as Sysbench\nand Apache",
			  "TLB flushes and shootdowns are mechanisms used by operat-\ning systems to synchronize page table entries (PTEs) cached\nin TLBs with the underlying page table",
			  "order to perform the shootdown, the initiator sends an IPI\nwith *work* , a data structure which indicates which entries\nneed to be flushed, to remote cores, which perform flushes\nlocally and send an acknowledgement back to the initiator\nby clearing a bit that the initiator spin-waits on.",
			  "Translation Lookaside Buffers (TLBs) are critical for build-\ning performant virtual memory systems.",
			  "The x86 architecture provides several mechanisms to control\nthe contents of the TLB."
			]
		  },
		  {
			"title": "Optimizing the TLB Shootdown Algorithm with Page ...",
			"url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
			"excerpts": [
			  "OS Solutions and Shortcomings**\nTo reduce TLB related overheads, OSes employ\nseveraltechniques to avoidunnecessaryshootdowns,\nreduce their time, and avoid TLB misses.\n",
			  "A common method to reduce shootdown time is\nto batch TLB invalidations if they can be deferred [ 21 ,\n47 ]. Batching, however, cannot be used in many\ncases, for example when a multithreaded application\nchanges the access permissions of a single page.",
			  "Linux tries to balance between the overheads of\nTLB flushes and TLB misses when a core becomes\nidle, using a lazy TLB invalidation scheme.",
			  "Per-Core Page Tables**\nCurrently, the state-of-the-art software solution for\nTLB shootdowns is setting per-core page tables, and\naccording to the experienced page-faults track which\ncores usedeachPTE [ 11 , 19 ]. When a PTE invalidation\nis needed, a shootdown is sent only to cores whose\npage tables hold the invalidated PT"
			]
		  },
		  {
			"title": "AMD Optimizes EPYC Memory with NUMA",
			"url": "https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/AMD-Optimizes-EPYC-Memory-With-NUMA.pdf",
			"excerpts": [
			  "NUMA reduces\nmemory latencies and reduces cross-die data traffic,\nbecause Intels Manhattan Mesh (Figure 4) has no\ndiagonal connections.\n",
			  "With dual socket configurations, latency for memory\naccess between sockets will have a significant latency\npenalty when memory accesses cross a socket-to-socket interconnect, whether that interconnect\nis AMD Infinity Fabric or Intel QPI.",
			  "EPYC can support 2 TB."
			]
		  },
		  {
			"title": "AMD's EPYC 9355P: Inside a 32 Core Zen 5 Server Chip",
			"url": "https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core",
			"excerpts": [
			  "NPS1 mode stripes memory accesses across all 12 of the chips memory controllers, presenting software with a monolithic view of memory at the cost of latency.",
			  "Different NUMA configurations can subdivide EPYC 9355P, associating cores with the closest memory controllers to improve latency.",
			  "An individual NPS4 node achieves 117.33 GB/s to its local memory pool, and just over 107 GB/s to the memory on the other three nodes."
			]
		  },
		  {
			"title": "NUMA, numactl, and taskset: A Practical Guide for AI/ML Engineers | by Tony Seah | Nov, 2025 | Medium",
			"url": "https://tonyseah.medium.com/numa-numactl-and-taskset-a-practical-guide-for-ai-ml-engineers-0fbacb450f22?source=rss------ia-5",
			"excerpts": [
			  "**What it does** : Bind a process (or PID) to specific CPU cores.",
			  "**Limitation** : Only controls CPU affinity, **not** memory placement.",
			  "Remember: this only controls **where the threads run** , not **where the memory is allocated** .",
			  "`taskset` is the lightweight hammer for CPU affinity.",
			  "Run a process on specific CPU cores:",
			  "This is great for:\nEnsuring dataloader or preprocessing threads dont migrate across all cores\nKeeping noisy neighbors away from your main service\nPinning a latency-sensitive inference server to a subset of cores\n"
			]
		  },
		  {
			"title": "Memory Performance of AMD EPYC Rome and Intel ...",
			"url": "https://research.spec.org/icpe_proceedings/2022/proceedings/p165.pdf",
			"excerpts": [
			  "The main memory bandwidth of a single NUMA node of a CLX\nprocessor is higher than on Rome, as CLX has three memory chan-\nnels per NUMA node, Rome tw",
			  "transferring data to/from a remote\nsocket comes at a high additional latency penalty."
			]
		  },
		  {
			"title": "Check L1/L2 prefetching settings in XPS 8700 BIOS",
			"url": "https://www.dell.com/community/en/conversations/locked-topics-desktops-general/help-needed-check-l1l2-prefetching-settings-in-xps-8700-bios/647f498df4ccf8a8de1f3cc3",
			"excerpts": [
			  "In Linux you can enable or disable the hardware prefetchers using msr-tools http://www.kernel.org/pub/linux/utils/cpu/msr-tools/. The ...Read more"
			]
		  },
		  {
			"title": "A Closer Look at Intel Resource Director Technology (RDT)",
			"url": "https://dl.acm.org/doi/10.1145/3534879.3534882",
			"excerpts": [
			  "The RDT framework comprises five components: Cache Allocation Technology (CAT) and Code and Data Prioritization (CDP) for managing shared cache, Memory ...Read more"
			]
		  },
		  {
			"title": "Performance tuning at the edge using Cache Allocation Technology",
			"url": "https://www.redhat.com/en/blog/performance-tuning-at-the-edge",
			"excerpts": [
			  "Resource Control ( resctrl ) is a kernel interface for CPU resource allocation using Intel Resource Director Technology. The resctrl interface ...Read more"
			]
		  },
		  {
			"title": "A Closer Look at Intel Resource Director Technology (RDT)",
			"url": "https://cs-people.bu.edu/rmancuso/files/papers/CloserLookRDT_RTNS22.pdf",
			"excerpts": [
			  "For cache management, the partitioning is way-based, whereas the main memory bandwidth controls limit the amount of band- width extracted on a ...Read more"
			]
		  },
		  {
			"title": "Enabling memory access isolation in real-time cloud ...",
			"url": "https://www.sciencedirect.com/science/article/pii/S1383762123000279",
			"excerpts": [
			  "Once the partitioning of the LLC is implemented, memory bandwidth regulation is still necessary to limit memory access contention.Read more"
			]
		  },
		  {
			"title": "Assessing Intel's memory bandwidth allocation for resource ...",
			"url": "https://www.researchgate.net/publication/361053277_Assessing_Intel's_memory_bandwidth_allocation_for_resource_limitation_in_real-time_systems",
			"excerpts": [
			  "MBA Memory Bandwidth Allocation. MPAM Memory System Resource Partitioning and Monitoring.Read more"
			]
		  },
		  {
			"title": "What is Cache Coloring and How Does it Work?",
			"url": "https://www.lynx.com/blog/what-is-cache-coloring",
			"excerpts": [
			  "Cache coloring suffers from a number of difficulties that, while not insurmountable, make it difficult and risky for RTOS vendors to implement.",
			  "Linus Torvalds is strongly against cache coloring for Linux.",
			  "Hey, there have been at least four different major cache coloring trials for the kernel over the years. This discussion has been going on since the early nineties. And _none_ of them have worked well in practice.",
			  "The real degradation comes in just the fact that cache coloring itself is often expensive to implement and causes nasty side effects like bad memory allocation patterns, and nasty special cases that you have to worry about (ie special fallback code on non-colored pages when required)."
			]
		  },
		  {
			"title": "caching - cache coloring on slab memory management in Linux kernel - Stack Overflow",
			"url": "https://stackoverflow.com/questions/15016359/cache-coloring-on-slab-memory-management-in-linux-kernel",
			"excerpts": [
			  "The final task of the slab allocator is optimal hardware cache use. If there is space left over after objects are packed into a slab, the remaining space is used to color the slab. Slab coloring is a scheme that attempts to have objects in different slabs use different lines in the cache. By placing objects at a different starting offset within the slab, objects will likely use different lines in the CPU cache, which helps ensure that objects from the same slab cache will be unlikely to flush each other."
			]
		  },
		  {
			"title": "Disclosure of H/W prefetcher control on some Intel processors",
			"url": "https://radiable56.rssing.com/chan-25518398/article18.html",
			"excerpts": [
			  "The above mentioned processors support 4 types of h/w prefetchers for prefetching data. There are 2 prefetchers associated with L1-data cache (also known as DCU) and 2 prefetchers associated with L2 cache. There is a Model Specific Register (MSR) on **every core** with address of 0x1A4 that can be used to control these 4 prefetchers. Bits 0-3 in this register can be used to either enable or disable these prefetchers. Other bits of this MSR are reserved.",
			  "If any of the above bits are set to 1 on a core, then that particular prefetcher on that core is disabled. Clearing that bit (setting it to 0) will enable the corresponding prefetcher. Please note that this MSR is present in every core and changes made to the MSR of a core will impact the prefetchers only in that core. If hyper-threading is enabled, both the threads share the same MSR.",
			  "Most BIOS implementations are likely to leave all the prefetchers enabled (i.e MSR 0x1A4 value at 0) as prefetchers are either neutral or positively impact the performance for a large number of applications. However, how these prefetchers may impact your application is going to be highly dependent on the data access patterns in your application."
			]
		  },
		  {
			"title": "uarch-configure/intel-prefetch/intel-prefetch-disable.c at master  deater/uarch-configure  GitHub",
			"url": "https://github.com/deater/uarch-configure/blob/master/intel-prefetch/intel-prefetch-disable.c",
			"excerpts": [
			  "/* The key is MSR 0x1a4 */",
			  "/* bit 0: L2 HW prefetcher */",
			  "/* bit 1: L2 adjacent line prefetcher */",
			  "/* bit 2: DCU (L1 Data Cache) next line prefetcher */",
			  "/* bit 3: DCU IP prefetcher (L1 Data Cache prefetch based on insn address) */",
			  "/* This code uses the /dev/msr interface, and you'll need to be root. */"
			]
		  },
		  {
			"title": "Thinking about A New Mechanism for Huge Page ...",
			"url": "https://www.cse.unsw.edu.au/~cs9242/19/exam/paper2.pdf",
			"excerpts": [
			  "ABSTRACT. The Huge page mechanism is proposed to reduce the TLB misses and benefit the overall system performance. On the system with large memory capacity, ..."
			]
		  },
		  {
			"title": "Hmm, with AMD Threadripper, you're already looking at TLB issues at these L3 siz... | Hacker News",
			"url": "https://news.ycombinator.com/item?id=26468499",
			"excerpts": [
			  "Case in point: AMD Zen2 has 2048 TLB entries (L2), under a default (in Linux and Windows) of 4kB per TLB entry. That's 8MBs of TLB before your ...Read more"
			]
		  },
		  {
			"title": "Reduce NUMA balance caused TLB-shootdowns in a VM [LWN.net]",
			"url": "https://lwn.net/Articles/940764/",
			"excerpts": [
			  "Reduce NUMA balance caused TLB-shootdowns in a VM. From: Yan Zhao <yan.y.zhao-AT-intel.com>. To: linux-mm-AT-kvack.org, linux-kernel-AT-vger ...Read more"
			]
		  },
		  {
			"title": "Understanding TLB from CPUID results on Intel",
			"url": "https://stackoverflow.com/questions/58128776/understanding-tlb-from-cpuid-results-on-intel",
			"excerpts": [
			  "The TLB information for Ice Lake and Goldmont Plus processors is present in leaf 0x18. This leaf provides more flexibility in encoding TLB ..."
			]
		  },
		  {
			"title": "More than 20% UPS gain on Linux with huge pages (AMD Zen 2) : r/factorio",
			"url": "https://www.reddit.com/r/factorio/comments/j68o2w/more_than_20_ups_gain_on_linux_with_huge_pages/",
			"excerpts": [
			  "A TLB entry stores the mapping between virtual and physical address. TLB misses are quite expensive and require walking the page table. Zen 2 ...Read more"
			]
		  },
		  {
			"title": "arm64: support batched/deferred tlb shootdown during page reclamation/migration [LWN.net]",
			"url": "https://lwn.net/Articles/938347/",
			"excerpts": [
			  "arm64: support batched/deferred tlb shootdown during page reclamation/migration ... Comments and public postings are copyrighted by their creators ...Read more"
			]
		  },
		  {
			"title": "LUF(Lazy Unmap Flush) reducing tlb numbers over 90%",
			"url": "https://lwn.net/Articles/973209/",
			"excerpts": [
			  "I'm suggesting a new mechanism, LUF(Lazy Unmap Flush), defers tlb flush until folios that have been unmapped and freed, eventually get allocated again.Read more"
			]
		  },
		  {
			"title": "Zen 2 - Microarchitectures - AMD - WikiChip",
			"url": "https://en.wikichip.org/wiki/amd/microarchitectures/zen_2",
			"excerpts": [
			  "he fully-associative L1 instruction TLB contains 64 entries and holds 4-Kbyte, 2-Mbyte, or 1-Gbyte page table entries. The 512 entries of the 8-way set-associative L2 instruction TLB can hold 4-Kbyte and 2-Mbyte page table entries. 1-Gbyte pages are *smashed* into 2-Mbyte entries in the L2 ITLB. A hardware page table walker handles L2 ITLB misses",
			  "TLB**\n64 entry L1 TLB, fully associative, all page sizes\n512 entry L2 TLB, 8-way set associative\n4-Kbyte and 2-Mbyte pages\nParity protecte",
			  "A two-level translation lookaside buffer (TLB) assists load and store address translation. The fully-associative L1 data TLB contains 64 entries and holds 4-Kbyte, 2-Mbyte, and 1-Gbyte page table entries. The L2 data TLB is a unified 12-way set-associative cache with 2048 entries, up from 1536 entries in Zen, holding 4-Kbyte and 2-Mbyte page table entries, as well as page directory entries (PDEs) to speed up DTLB and ITLB table walks. 1-Gbyte pages are *smashed* into 2-Mbyte entries but installed as 1-Gbyte entries when reloaded into the L1 TLB."
			]
		  },
		  {
			"title": "Skylake (server) - Microarchitectures - Intel - WikiChip",
			"url": "https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)",
			"excerpts": [
			  "TLBs:\nITLB\n4 KiB page translations:\n128 entries; 8-way set associative\ndynamic partitioning\n2 MiB / 4 MiB page translations:\n8 entries per thread; fully associative\nDuplicated for each thread",
			  "DTLB\n4 KiB page translations:\n64 entries; 4-way set associative\nfixed partition\n2 MiB / 4 MiB page translations:\n32 entries; 4-way set associative\nfixed partition\n1G page translations:\n4 entries; 4-way set associative\nfixed partition",
			  "STLB\n4 KiB + 2 MiB page translations:\n1536 entries; 12-way set associative. (Note: STLB is incorrectly reported as \"6-way\" by CPUID leaf 2 (EAX=02H). Skylake erratum SKL148 recommends software to simply ignore that value.)\nfixed partition\n1 GiB page translations:\n16 entries; 4-way set associative\nfixed partition"
			]
		  },
		  {
			"title": "Huge Pages and PostgreSQL | CYBERTEC PostgreSQL | Services & Support",
			"url": "https://www.cybertec-postgresql.com/en/huge-pages-postgresql/",
			"excerpts": [
			  " Linux this problem can be mitigated by using Huge Pages. Since page tables are organized hierarchically, they enable us to summarize allocations in much larger pages than the default. The size of huge pages are architecture-dependent, on x86 systems we can usually expect 2MB or 1GB sizes, IBM POWER allows 64kB, 16MB and 16 GB.\n*",
			  "**Lookups for specific virtual addresses of memory allocations are then much faster** and hopefully more independent of the entries found in the TLB.",
			  "On x86, when configuring huge pages, the 2MB page size is the default. You can easily get your system current settings via `/proc/meminfo` :",
			  "Hugepagesize: 2048 kB",
			  "For people not familiar with the meaning of huge pages, here is a short overview of what they are and which problems they try to solve.",
			  "On any modern system, applications don't use physical memory directly. Instead, they use a virtual memory address model to make it easier to handle memory allocation and avoid the complexity of computing and mapping physical addresses into the application memory space."
			]
		  },
		  {
			"title": "PostgreSQL and Huge Pages: Boosting Database Performance the Right Way | by Tomasz Gintowt | Medium",
			"url": "https://tomasz-gintowt.medium.com/postgresql-and-huge-pages-boosting-database-performance-the-right-way-32a27b25a819",
			"excerpts": [
			  "**Huge Pages** solve this problem by using **larger memory pages** , typically **2 MB** (or even 1 GB on some systems).",
			  "This reduces overhead and improves performance for memory-intensive applications like PostgreSQL.",
			  "Section Title: PostgreSQL and Huge Pages: Boosting Database Performance the Right Way > Why PostgreSQL benefits from Huge Pages",
			  "In Linux, memory is divided into small chunks called **pages** , usually **4 KB** each. When PostgreSQL allocates shared memory, it might use **thousands or millions of pages** . Managing so many small pages increases overhead  both in memory management and TLB (Translation Lookaside Buffer) lookups inside the CPU.",
			  "PostgreSQL uses a **shared memory segment** ( `shared_buffers` ) for caching data.",
			  "If you have a large shared_buffers setting  for example, several gigabytes  enabling Huge Pages helps by:",
			  "Reducing CPU overhead in managing small pages.",
			  "Preventing memory fragmentation.",
			  "Providing a small but consistent **performance boost** (15% on most systems).",
			  "Making memory usage more predictable."
			]
		  },
		  {
			"title": "PostgreSQL Performance Tuning: Ultimate Guide to Optimize Your Database Server",
			"url": "https://www.enterprisedb.com/postgres-tutorials/introduction-postgresql-performance-tuning-and-optimization",
			"excerpts": [
			  "By default, the page size on Linux is 4KB. A typical PostgreSQL instance may allocate many GBs of memory, leading to potential performance problems due to the small page size.",
			  "Enabling huge pages on Linux will boost PostgreSQL performance as it will allocate large blocks (huge pages) of memory altogether.",
			  "By default, huge pages are not enabled on Linux, which is also suitable for PostgreSQLs default huge_pages setting try, which means use huge pages if available on the OS, otherwise no.",
			  "There are two aspects to setting up huge pages for PostgreSQL: Configuring the OS and configuring PostgreSQL."
			]
		  },
		  {
			"title": "High Performance and Scalable GPU Graph Traversal",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2011-08_High-Performance-and/BFS%20TR.pdf",
			"excerpts": [
			  "Most parallel BFS algorithms are level-synchronous: each level may be processed in parallel as long as the sequential ordering of levels is preserved. An.Read more"
			]
		  },
		  {
			"title": "Mach-RT: A Many Chip Architecture for Ray Tracing",
			"url": "https://users.cs.utah.edu/~elvasiou/papers/vasiou_HPG19_machrt.pdf",
			"excerpts": [
			  "Other shared large-area units consist of L1 data cache, instruction cache, Ray. Staging Buffer and large execution units such as floating point di- vision.Read more"
			]
		  },
		  {
			"title": "A Case Study for Ray Tracing Cores: Performance Insights ...",
			"url": "https://xiaodongzhang1911.github.io/Zhang-papers/TR-25-2.pdf",
			"excerpts": [
			  "Following the similar idea, we develop our RT-based BFS, which is shown in Algorithm 1 . The\nunvisited vertices are explored in iterations, and inside an iteration, neighbors of the vertices\nvisited in the last iteration are traversed. A CPU-side global synchronization is executed to remove\nredundant neighbors between iterations. The difference from a CUDA-based BFS method is that\nwe need to convert the graph into a BVH (Line 1) and then expand the queue of unvisited vertices\nusing RT cores (Line 8). As a result, the two keys of RT-based BFS are how to construct a BVH\nrepresenting the graph, and how to issue rays realizing the neighbor visiting."
			]
		  },
		  {
			"title": "Efficient Ray Tracing Kernels for Modern CPU Architectures",
			"url": "https://jcgt.org/published/0004/04/05/paper-lowres.pdf",
			"excerpts": [
			  "he efficiency of packet traversal has motivated algorithms that attempt to ex-\ntract coherent subsets out of incoherent ray batches, such as stream filtering and\nrelated methods [ Overbeck et al. 2008 ], where ray batches are intersected with the\ncurrent node in a breadth-first manner, and coherency is extracted implicitly during\ntree traversal. While this algorithm potentially allows high SIMD utilization, it re-\nquires expensive gather-and-scatter operatio",
			  "stead of testing n rays against one node, a single\nray is intersected with n nodes of a n-ary BVH simultaneously, where branching fac-\ntors of 416 have been investigated. Aside from efficient SIMD utilization, higher\nbranching factors reduce the depth and memory footprint of a BVH. On the negative\nside, early culling opportunities are reduced because some nodes are intersected that\nwould not have been visited in a tree with a lower branching factor. This leads to\nwasted operations and bandwidth.",
			  ", 2015\nhttp://jcgt.org\nrithms, a major drawback is a common traversal order that is enforced for every ray\nin the batch. This restriction was relaxed by the introduction of dynamic ray stream\ntraversal (DRST) [ Barringer and Akenine-Moller 2014 ], where, in the case of BVH4,\neach ray can follow almost the same traversal order that would result from single ray\ntraversal. Despite its high performance, two major drawbacks are apparent:\n ... \nA",
			  "Ray Stream Traversal**\nRay streams have emerged as an efficient approach to extract hidden coherence from\nincoherent ray batches, reducing memory bandwidth and increasing SIMD utilization\ncompared to single ray traversal. The dynamic ray stream traversal (DRST) [ Bar-\nringer and Akenine-Moller 2014 ] appears to be the fastest algorithm so far, deriving\nits performance from the ability to grant each ray its individual traversal order to in-\ncrease culling efficiency. In the BVH4 case, however, the possible permutations of\nthe traversal order are limited to eight out of 24 (compare balanced type, Section 2.2 ).\nIn addition, during every traversal step, rays are mapped to nine different bins, pro-\nducing considerable maintenance overhead and leading to batch fragmentation. In the\nfollowing, we propose a ray stream implementation, which like the DRST allows each\nray in the batch to traverse the tree in its preferred order. Unlike the DRST however,\na ray may follow any of the 24 possible permutations. In addition, no fragmentation\nof the ray batch happens during traversal (only once initially) and the number of bin",
			  "r contribution is twofold: For coherent ray sets, we introduce\na large packet traversal tailored to the BVH4 that is faster than the original BVH2 variant, and\nfor incoherent ray batches we propose a novel implementation of ray streams which reduces\nthe bookkeeping cost while strictly maintaining the preferred traversal order of individual\nrays. B",
			  "An entirely different approach has\nbeen proposed by several papers concurrently [ Wald et al. 2008 ; Ernst and Greiner\n2008 ; Dammertz et al. 2008 ]. Instead of testing n rays against one node, a single\nray is intersected with n nodes of a n-ary BVH simultaneously, where branching fac-\ntors of 416 have been investigated.",
			  "e solve challenge (1) with our efficient look-up mechanism introduced in Sec-\ntion 2.2 and challenge (2) with a combination of a deferred packet test of last re-\n ... ",
			  "\nFor coherent rays, SIMD packet traversal was first introduced for the BSP acceler-\nation structure [ Wald et al. 2001 ] and later extended to bounding volume hierarchies\n(BVH). Packet traversal traces n rays in parallel, where n is the number of SIMD\nelements. As long as all rays follow the same path through the acceleration structure,\nfull SIMD utilization is achieved. Otherwise, divergent rays need to be masked and\nthe calculations on the corresponding vector elements are wasted. For tightly bun-\ndled coherent rays, identical paths are the common case, and a significant speed-up is\nobserved for packet tracing.",
			  "\nFor coherent rays, SIMD packet traversal was first introduced for the BSP acceler-\nation structure [ Wald et al. 2001 ] and later extended to bounding volume hierarchies\n(BVH). Packet traversal traces n rays in parallel, where n is the number of SIMD\nelements. As long as all rays follow the same path through the acceleration structure,\nfull SIMD utilization is achieved. Otherwise, divergent rays need to be masked and\nthe calculations on the corresponding vector elements are wasted. For tightly bun-\ndled coherent rays, identical paths are the common case, and a significant speed-up is\nobserved for packet tracing.",
			  "The coherence of a ray set can be defined as the tendency of the individual rays to\nfollow the same traversal path through an acceleration structure and to intersect the\nsame primitives."
			]
		  },
		  {
			"title": "15.1 Mapping Path Tracing to the GPU",
			"url": "https://www.pbr-book.org/4ed/Wavefront_Rendering_on_GPUs/Mapping_Path_Tracing_to_the_GPU",
			"excerpts": [
			  "Another advantage of the wavefront approach is that different numbers of registers can be allocated to different kernels. Thus, simple kernels can use fewer ...Read more"
			]
		  },
		  {
			"title": "Algorithms and Data Structures for Interactive Ray Tracing ... - SciDok",
			"url": "https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/26447/1/Stefan_Popov_PhD_Thesis.pdf",
			"excerpts": [
			  "The key observation that enables our algorithm to work is that BVH traversal does\nnot need to store the entry or exit distances or points in the traversal stack. They\ncan be computed on the fly, since the AABB is always stored with the node. Thus,\nthe per-ray traversal stack only holds pointers to the far nodes that still need to be\ntraversed. Applied to packet traversal, this means again that the stack needs to hold\nonly the far nodes that have to be visited by the packet in the future.",
			  "Our algorithm (see Algorithm 9.1) maps one ray to one thread and thus one packet\nto one SIMD width warp. It traverses the tree synchronously with the whole packet,\nworking on one node at a time and processing the whole packet against it. If the node\nis a leaf, it intersects the rays in the packet with the contained geometry. Each thread\nkeeps the distance to the closest intersected primitive found so far in a variable. If a\ncloser primitive is found in the leaf, this variable is updated.",
			  "If the currently visited node is not a leaf, the algorithm intersects all rays of the\npacket with both children to determine the entry/exit distances. Each ray determines\nfor itself which of the two children it intersects and in case it intersects both, in which\norder it wants to visit them (line 18).",
			  "Applied to packet traversal, this means again that the stack needs to hold\nonly the far nodes that have to be visited by the packet in the future.",
			  "Thus, if we use\n32 bit pointers to the nodes (which is the case, because of the limited GPU memory),\nthe amortized storage requirement of a ray becomes exactly 1 bit per node.",
			  "lgorithm 9.1) maps one ray to one thread and thus one packet\nto one SIMD width warp."
			]
		  },
		  {
			"title": "Branch divergence - OptiX",
			"url": "https://forums.developer.nvidia.com/t/branch-divergence/176258",
			"excerpts": [
			  "There are different sources of divergence, so it depends on the scene & renderer whether divergence will be a problem. One source of divergence is ray traversal, intersection, and any-hit shaders. When rays in a warp are traversing different depths of an acceleration structure, divergence happens. If the rays are going through parts of the scene with very different geometric density, divergence appears.",
			  "Yes there are studies on this topic, and yes there are things you can do to improve situations that have significant divergence. For traversal, using the RTX hardware traversal and intersection for all of the scene geometry is a major way to both improve performance and cut down divergence. Avoiding any-hit programs and custom intersection programs when possible is another way to improve performance and potentially improve divergence. (Its not always possible to avoid any-hit programs or custom intersectors, and I dont want to stigmatize the use of valid necessary features, but any-hit and intersection programs do interrupt hardware traversal to execute your code on the SMs, and so theres significant overhead.)",
			  "A wavefront architecture also allows you consolidate ray tracing work at every step of path depth, so the kernels get smaller as you go, and warps stay compacted with active work on all threads."
			]
		  },
		  {
			"title": "Wavefront Path Tracing - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2019/07/18/wavefront-path-tracing/",
			"excerpts": [
			  "There is a problem with the algorithm. A primary ray may find a light right away, or after a single random bounce, or after fifty bounces. A CPU programmer may see a potential stack overflow; a GPU programmer should see *low hardware utilization* . The problem is caused by the (conditional) tail recursion: a path may get terminated at a light source, or it may continue if it hit something else. Translated to many threads: a portion of the threads will get terminated, and a portion continues.",
			  "The hardware utilization problem is amplified by the SIMT execution model of GPUs. Threads are organized in groups, e.g. 32 threads go together in a *warp* on a Pascal GPU (10xx class NVidia hardware). The threads in a warp share a single program counter: they execute in lock-step, so every program instruction is executed by the 32 threads simultaneously. SIMT stands for: *single instruction multiple thread* , which describes this concept well.",
			  "The streaming path tracing algorithm is designed to combat the root of the occupancy problem. Streaming path tracing splits the path tracing algorithm in four phases:",
			  "**Generate**",
			  "**Extend**",
			  "**Shade**",
			  "**Connect**",
			  "Each phase is implemented as a separate program. So instead of running the full path tracer as a single GPU program (kernel), we now have *four* kernels. And on top of that, they execute in a loop, as we will see shortly.",
			  "The problem is caused by the (conditional) tail recursion: a path may get terminated at a light source, or it may continue if it hit something else. Translated to many threads: a portion of the threads will get terminated, and a portion continues. After a few bounces, we have a few threads that have work left to do, while most threads are waiting for the final threads to finish."
			]
		  },
		  {
			"title": "Raytracing on AMDs RDNA 2/3, and Nvidias Turing and Pascal",
			"url": "https://chipsandcheese.com/p/raytracing-on-amds-rdna-2-3-and-nvidias-turing-and-pascal",
			"excerpts": [
			  "It defines BVH-es in two structures  a top level acceleration structures (TLAS), and a bottom level acceleration structure (BLAS).Read more"
			]
		  },
		  {
			"title": "How to build a BVH  part 5: TLAS & BLAS - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/05/07/how-to-build-a-bvh-part-5-tlas-blas/",
			"excerpts": [
			  "The nodes that we use to combine a set of BVHs into a single BVH are referred to as the top level acceleration structure, or TLAS. It ...Read more"
			]
		  },
		  {
			"title": "an-adaptive-heterogeneous-runtime-for-irregular- ...",
			"url": "https://scispace.com/pdf/an-adaptive-heterogeneous-runtime-for-irregular-applications-4ybnel0b2m.pdf",
			"excerpts": [
			  "The process\nstarts with a 16-wide packet traversal which performs 16-wide box tests. At any\npoint in time, the bit in an active mask will be counted to indicate how many\nof the packets rays are still active for a subtree. If this number falls below a\ngiven threshold, which is set to 7, the process leaves the packet traversal mode\nand sequentially traces all active rays in the single-ray mode[12].",
			  "The drawback\nof this method is that the threshold may need to change and the program will\nrequire recompilation if the system is moved to a machine with a shorter SIMD\nlane.",
			  "the hybrid packet/single-ray tracing algorithm is\nimplemented by utilizing a specific type of BVH tree.",
			  "his method does not prevent any possible divergent execution that\nlowers the effectiveness of the SIMD engine."
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "Hybrid traversal**\nAs mentioned above this method is a combination of the\npacket and if-if traversal ones.\nThe idea is that packet\ntraversal performs best near the tree root where rays are\ncoherent whereas if-if traversal is better suited for travers-\ning nodes near the leaves. It is, however, unclear when and\nhow to switch between the two of the method",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Intel Resource Director Technology (Intel RDT)",
			"url": "https://eci.intel.com/docs/3.0/development/intel-pqos.html",
			"excerpts": [
			  "Intel Cache Allocation Technology (CAT) provides a method to partition processor caches and assign these partitions to a Class-of-Service (COS)."
			]
		  },
		  {
			"title": "Adaptive Ray Packet Reordering",
			"url": "https://graphics.stanford.edu/~boulos/papers/reorder_rt08.pdf",
			"excerpts": [
			  "Empirically, our threshold of 50% works rather well (for more detailed comparisons see Section 5), but a more complicated heuristic might reorder less often for ...Read more"
			]
		  },
		  {
			"title": "Combining Single and Packet-Ray Tracing for Arbitrary Ray Distributions on the Intel MIC Architecture - PubMed",
			"url": "https://pubmed.ncbi.nlm.nih.gov/22084142/",
			"excerpts": [
			  "In this paper, we introduce a single-ray tracing scheme for incoherent rays that uses just one traversal stack on 16-wide SIMD hardware. It uses a bounding ...Read more"
			]
		  },
		  {
			"title": "Introduction to Cache Allocation Technology in the Intel Xeon...",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-to-cache-allocation-technology.html",
			"excerpts": [
			  "Intels Cache Allocation Technology (CAT) helps address shared resource concerns by providing software control of where data is allocated into the last-level cache (LLC), enabling isolation and prioritization of key applications.",
			  "CAT) provides software-programmable control over the amount of cache space that can be consumed by a given thread, app, VM, or container",
			  "cation Technology (CAT) enables privileged software such as an OS or VMM to control data placement in the last-level cache (LLC), enabling isolation and prioritization of important threads, apps, containers, or VMs, given [software support](/content/www/us/en/develop/articles/software-enabling-for-cache-allocation-technology.html) . While an initial version of CAT was introduced on a limited set of [communications processors in the Intel Xeon processor E5-2600 v3 family](http://www.intel.com/content/www/us/en/communications/cache-monitoring-cache-allocation-technologies.html) , the CAT feature is significantly enhanced and now available on all SKUs starting with the Intel Xeon processor E5 v4 famil"
			]
		  },
		  {
			"title": "An Evaluation of Intel Cache Allocation Technology for Data",
			"url": "https://www.diva-portal.org/smash/get/diva2:1622362/FULLTEXT01.pdf",
			"excerpts": [
			  "Intel CAT is a tool from Intel that can be used\nto control cache allocation for some of their Xeon CPU models.",
			  "In particular,\nIntel CAT can allocate shared cache resources to specific CPU cores and in turn\nto specific applications running on those cores.",
			  "Applications over-utilizing the\nL3 cache are commonly called L3 cache noisy neighbors [ 3 ] [ 4 ] and in Figure\n2.5 an example of a noisy neighbor application and a prioritized application\nis illustrated.",
			  "The L3 cache in the Intel Xeon Scalable CPUs is N-way set-associative and\nthe particular model used in this thesis has 11 cache ways (N=11). [ 2 , p. 482]",
			  "Intel CAT uses the notion of Classes of Service (CLOS) to group CPU\ncores into classes which can then be allocated a certain number of cache ways\navailable to its disposal. More concretely, CAT defines Capacity Bitmasks\n(CBMs) for each CLOS to specify how much cache is allocated to a CLOS.",
			  "The bits set to\n1 in the CBM must be a continuous string of bits. The cache allocations defined\nby the CBMs can overlap for multiple CLOSs, resulting in that they share that\nportion of the cache. An example of this is shown in Figure 2.7 . Intel CAT\nis supported by hardware in the CPU and configured through Model-Specific\nRegisters (MSRs) . [ 1 ]",
			  " was successfully used to increase the performance of the GET\n ... \n--disable - libunwind - exceptions\n--\nenable -gnu -unique - object\n--enable -linker -build -\nid\n--with -gcc -major - version - only\n--with -linker -\nhash - style =gnu\n--enable - plugin\n--enable - initfini"
			]
		  },
		  {
			"title": "Cache Allocation Technology - Real-time Ubuntu documentation",
			"url": "https://documentation.ubuntu.com/real-time/latest/tutorial/intel-tcc/intel-cat/",
			"excerpts": [
			  "\nLets take a look at how Intel Cache Allocation Technology (CAT) can help mitigate these sources of contention.\nCA",
			  "CAT provides the ability to partition caches at various levels in the caching hierarchy.",
			  "CAT provides the ability to partition caches at various levels in the caching hierarchy.",
			  "Initially, the default cache configuration is used, where all cache ways are shared.",
			  "In the second step, the statistics are compared with the Last Level Cache (LLC) partitioned to provide an exclusive portion of the cache to the real-time test application."
			]
		  },
		  {
			"title": "Why Intel added cache partitioning",
			"url": "https://danluu.com/intel-cat/",
			"excerpts": [
			  "needs.\nIntel's  [Cache Allocation Technology](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.pdf)  (CAT) allows the LLC to limit which cores can can access different parts of the cache. Since we often want to pin performance sensitive tasks to cores anyway, this allows us to divide up the cache on a per-task basis.\nIn",
			  "Intel's [April 2015 whitepaper on what they call Cache Allocation Technology (CAT)](http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/cache-allocation-technology-white-paper.pdf) has some simple benchmarks comparing CAT vs. no-CAT. In this example, they measure the latency to respond to PCIe interrupts while another application has heavy CPU-to-memory traffic, with CAT on and off.",
			  "order to address this problem, Intel introduced what they call Code and Data Prioritization Technology (CDP). This is an extension of CAT that allows cores to separately limit which subsets of the LLC instructions and data can occupy. Since it's targeted at the last-level cache, it doesn't directly address the graph above, which shows L2 cache miss rates. However, the cost of an L2 cache miss that hits in the LLC is something like [26ns on Broadwell vs. 86ns for an L2 miss that also misses the LLC and has to go to main memory](http://users.atw.hu/instlatx64/GenuineIntel00306D4_Broadwell2_NewMemLat.txt) , which is a substantial difference"
			]
		  },
		  {
			"title": "A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree.pdf",
			"excerpts": [
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "Yet\nthe memory storage order for a BVH optimized for packets may be\nsuboptimal for single-ray methods (the converse may also be true).",
			  "This\nmode begins traversal using packets, and dynamically switches to\nsingle-ray traversal when the number of active rays in a packet falls\nbelow a threshold [Benthin et al. 2012].",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "**5.1.4**",
			  "**Hybrid Traversal and Intersection**"
			]
		  },
		  {
			"title": "benchmarking - How to write a pointer-chasing benchmark using 64-bit pointers in CUDA? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/36416843/how-to-write-a-pointer-chasing-benchmark-using-64-bit-pointers-in-cuda",
			"excerpts": [
			  "This research paper runs a series of several CUDA microbenchmarks on a GPU to obtain statistics like global memory latency, instruction throughput, etc.Read more"
			]
		  },
		  {
			"title": "Solved: Thank you both for your",
			"url": "https://community.intel.com/t5/Intel-Moderncode-for-Parallel/Topology-Bandwidth-and-Latency-of-the-Cache-Coherence-Network/m-p/1116922",
			"excerpts": [
			  "Given these lists of addresses mapping to a single L3 slice, one can then perform standard pointer-chasing latency tests with full control over ...Read more"
			]
		  },
		  {
			"title": "Accelerating Pointer Chasing in 3D-Stacked Memory",
			"url": "http://pdl.cmu.edu/PDL-FTP/associated/16iccd_impica.pdf",
			"excerpts": [
			  "\nPointer chasing is currently performed by the CPU cores, as part\nof an application thread. While this approach eases the integration of\npointer chasing into larger programs, pointer chasing can be inefficient\nwithin the CPU, as it introduces several sources of performance degra-\ndation: (1) dependencies exist between memory requests to the linked\nnodes, resulting in serialized memory accesses and limiting the avail-\nable instruction-level and memory-level parallelism [33,61,62,67,75];\n(2",
			  "we profile two popular applications that heavily de-\npend on linked data structures, using a state-of-art Intel Xeon system: 1\n(1) *Memcached* [25], using a real Twitter dataset [23] as its input; and\n(2) *DBx1000* [94], an in-memory database system, using the TPC-\nC benchmark [87] as its input.",
			  "We propose to improve the performance of pointer chasing by lever-\naging processing-in-memory (PIM) to alleviate the memory bottle-\nneck. Instead of sequentially fetching *each node* from memory and\nsending it to the CPU when an application is looking for a particular\nnode, PIM-based pointer chasing consists of (1) traversing the linked\ndata structures *in memory* , and (2) returning only the final node found\nto the CPU.",
			  "Linked list** . We use the linked list traversal microbenchmark [98]\nderived from the *health* workload in the Olden benchmark suite [73].\nThe parameters are configured to approximate the performance of\nthe *health* workload. We measure the performance of the linked list\ntraversal after 30,000 iterations.",
			  "**Hash table** . We create a microbenchmark from the hash table\nimplementation of *Memcached* [25]. The hash table in Memcached\nresolves hash collisions using chaining via linked lists. When there are\nmore than 1.5 *n* items in a table of *n* buckets, it doubles the number of\n2 We sweep the size of the IMPICA cache from 32KB to 128KB, and find that\nit has negligible effect on our results [35].\n5\nbuckets. We f",
			  "building blocks in a wide range of workloads, to evaluate the native\nperformance of performance chasing operations: linked lists, hash\ntables, and B-trees. We also evaluate the performance improvement\nin a real data-intensive workload, measuring the transaction latency\nand throughout of DBx1000 [94], an in-memory OLTP database. We\nmodify all four workloads to offload each pointer chasing request to\nIMPICA. To minimize communication overhead, we map the IMPICA\nregisters to user mode address space, thereby avoiding the need for\ncostly kernel code intervention.",
			  "IMPICA effectively utilizes the in-\nternal memory bandwidth in 3D-stacked memory, which is cheap\nand abundant."
			]
		  },
		  {
			"title": "c - what is ChaseNS in this pointer-chasing benchmark - Stack Overflow",
			"url": "https://stackoverflow.com/questions/72620752/what-is-chasens-in-this-pointer-chasing-benchmark",
			"excerpts": [
			  "212 ns is a *long* time to wait for a cache-miss load, but with contention from multiple cores it's maybe plausible?",
			  "This is a [pointer-chasing](https://en.wikichip.org/wiki/pointer_chasing#:%7E:text=Pointer%20chasing%20refers%20to%20a,serially%2Ddependent%20chain%20of%20loads.) microbenchmark, like `p = p->next` , so you're measuring load latency by making each load-address dependent on the previous load's result.",
			  "This is a [pointer-chasing](https://en.wikichip.org/wiki/pointer_chasing#:%7E:text=Pointer%20chasing%20refers%20to%20a,serially%2Ddependent%20chain%20of%20loads.) microbenchmark, like `p = p->next` , so you're measuring load latency by making each load-address dependent on the previous load's result.",
			  "So hopefully the access pattern is *not* regular, otherwise hardware prefetching would defeat it, by having the next thing to load already in local L1d cache before the load-address is known.",
			  "e.g. make an array of pointers to pointers (like `struct foo { struct foo *next; };` ) with each one pointing to the next, then shuffle it, so iterating over that linked list touches cache lines in a random order within that 512 MiB working set.",
			  "e.g. make an array of pointers to pointers (like `struct foo { struct foo *next; };` ) with each one pointing to the next, then shuffle it, so iterating over that linked list touches cache lines in a random order within that 512 MiB working set."
			]
		  },
		  {
			"title": "[PDF] A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/A-Study-of-Pointer-Chasing-Performance-on-Systems-Weisz-Melber/e43bf3e494a945744c84a7eb64f2bb08e9862802",
			"excerpts": [
			  "Section Title: A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems > Topics",
			  "Content:\nAI-Generated\n[Field Programmable Gate Array (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/502114721?corpusId=17857032) [Traversal Performance (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/7293980890?corpusId=17857032) [In-memory (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/16217196227?corpusId=17857032) [Pointer Chasing (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/42373953410?corpusId=17857032)"
			]
		  },
		  {
			"title": "356477-Optimization-Reference-Manual-V2-002.pdf",
			"url": "https://cdrdv2-public.intel.com/814199/356477-Optimization-Reference-Manual-V2-002.pdf",
			"excerpts": [
			  "MOV rax, [rax]\n4\nMOV rax, disp32[rax] , disp32 < 2048\n4\nMOV rax, [rcx+rax]\n5\nMOV rax, disp32[rcx+rax] , disp32 < 2048\n5"
			]
		  },
		  {
			"title": "Characterizing and Modeling Non-Volatile Memory Systems",
			"url": "https://www.microarch.org/micro53/papers/738300a496.pdf",
			"excerpts": [
			  "to bypass CPU caches, while still generating\ncacheline-sized memory accesses, all pointer chasing tests are\nimplemented using non-temporal AVX512 load/store instruc-\ntions. ",
			  "Buffer Prober** detects the on-DIMM buffer capacity, entry\n ..",
			  "*Pointer chasing* is a random memory access benchmark:\nit divides a contiguous memory region  referred to as a\n498\npointer chasing region (PC-Region)  into equal-sized blocks\n(PC-Blocks); it reads/writes all PC-Blocks in a PC-Region in\nrandom order, and sequentially accesses data within each PC-\nBlock.",
			  "*Pointer chasing* is a random memory access benchmark:\nit divides a contiguous memory region  referred to as a\n498\npointer chasing region (PC-Region)  into equal-sized blocks\n(PC-Blocks); it reads/writes all PC-Blocks in a PC-Region in\nrandom order, and sequentially accesses data within each PC-\nBlock.",
			  "Pointer chasing has three variants to detect various buffer\narchitecture characteristics: (1) collecting average latency per\ncache line with a fixed PC-Block size across various PC-\nRegion sizes, (2) quantifying read and write amplification by\nusing a fixed PC-Region size, while varying the size of PC-\nBlock, (3) issuing read-after-write requests, which issue writes\nin a pointer chasing order, followed by reads in the same order."
			]
		  },
		  {
			"title": "Characterizing and Modeling Non-Volatile Memory Systems",
			"url": "https://swanson.ucsd.edu/data/bib/pdfs/MICRO20-LensVans.pdf",
			"excerpts": [
			  "cacheline-sized memory accesses, all pointer chasing tests are\nimplemented using non-temporal AVX512 load/store instruc-\ntions.",
			  "Pointer chasing has three variants to detect various buffer\narchitecture characteristics: (1) collecting average latency per\ncache line with a fixed PC-Block size across various PC-\nRegion sizes, (2) quantifying read and write amplification by\nusing a fixed PC-Region size, while varying the size of PC-\nBlock, (3) issuing read-after-write requests, which issue writes\nin a pointer chasing order, followed by reads in the same order.",
			  "*Overwrite* repeatedly generates sequential writes to a fixed\nmemory region, and then measures the execution time of each\niteration.",
			  "Figure 9a shows the load and store latency of the pointer\nchasing microbenchmark across different access region sizes\nwith single-DIMM configuration.",
			  "The store latency curves of\nVANS match the curves of real-system profiling well with\na difference lower than 10% across all the access region\nsizes.",
			  "Figure 9b shows the pointer chasing latency of six DIMMs",
			  "Overall, our evaluation shows that\nVANS achieves an average 86.5% accuracy across four metrics\n(Figure 9e)."
			]
		  },
		  {
			"title": "assembly - Not sure about the AMD Zen 3 Architecture (functional units, issue time / latency of instructions) - Stack Overflow",
			"url": "https://stackoverflow.com/questions/75543818/not-sure-about-the-amd-zen-3-architecture-functional-units-issue-time-latenc",
			"excerpts": [
			  "The minimum time (in cycles) between execution of dependent operations is called latency. uops.info measures that for every instruction, or ..."
			]
		  },
		  {
			"title": "A Study of Pointer-Chasing Performance on Shared- ...",
			"url": "https://users.ece.cmu.edu/~jhoe/distribution/2016/fpga16.pdf",
			"excerpts": [
			  "Pointer-Chasing.** While the class of irregular parallel ap-\nplications is broad and varied, a fundamental behavior is\npointer-chasing. In pointer chasing, the computation is re-\nquired to dereference a pointer to retrieve each node from\nmemory, which contains both a data payload to be processed\nand a pointer (or pointers) to subsequent nodes. The exact\ncomputation on the payload and the determination of the\nnext pointer to follow depend on the specific data structure\nand algorithm in use. In this paper, we ignore these differ-\nences and focus on only the basic effects of memory access\nlatency and bandwidth on pointer chasing. It is our con-\ntention that the optimization of basic pointer-chasing per-\nformance ultimately determines the opportunities for FPGA\nacceleration of irregular parallel application",
			  "For this purpose, we fixed a simple reference behavior,\nnamely a linked-list traversal. This reference behavior is pa-\nrameterized by (1) node layout in memory (best- vs. worst-\ncase in our experiments); (2) per node data payload size;\n(3) payload dependence (an artificial constraint that payload\nmust be retrieved before following the next pointer); and (4)\nconcurrent traversals (availability of multiple independent\ntraversals).",
			  "Taken together, these parameters abstractly\ncapture the execution differences of different pointer-based\nalgorithms and data-structures. The two execution require-"
			]
		  },
		  {
			"title": "uops.info",
			"url": "https://www.uops.info/cache.html",
			"excerpts": [
			  "L1 data cache\nSize: 48 kB\nAssociativity: 12\nNumber of sets: 64\nWay size: 4 kB\nLatency: 5 cycles [Link](cache/lat_ICL.html)",
			  "L2 cache\nSize: 512 kB\nAssociativity: 8\nNumber of sets: 1024\nWay size: 64 kB\nLatency: 13 cycles [Link](cache/lat_ICL.html)",
			  "L3 cache\nSize: 6 MB\nAssociativity: 12\nNumber of CBoxes: 4\nNumber of slices: 8\nNumber of sets (per slice): 1024\nWay size (per slice): 64 kB\nLatency: 41 cycles [Link](cache/lat_ICL.html)",
			  "Core i3-8121U (Cannon Lake)**\nL1 data cache\nSize: 32 kB\nAssociativity: 8\nNumber of sets: 64\nWay size: 4 kB\nLatency: 4 cycles [Link](cache/lat_CNL.htm",
			  "Section Title: Caches\nContent:\n4\nNumber of sets: 1024\nWay size: 64 kB\nLatency: 12 cycles [Link](cache/lat_CFL.html)",
			  "L3 cache\nSize: 12 MB\nAssociativity: 16\nNumber of CBoxes: 6\nNumber of slices: 12\nNumber of sets (per slice): 1024\nWay size (per slice): 64 kB\nLatency: 41 cycles [Link](cache/lat_CFL.html)"
			]
		  },
		  {
			"title": "Popping the Hood on Golden Cove - by Chester Lam",
			"url": "https://chipsandcheese.com/p/popping-the-hood-on-golden-cove",
			"excerpts": [
			  "For address generation, Golden Cove can handle three loads and two stores per cycle. The load and store AGUs are separate on Intels diagram, unlike AMD where three AGUs handle both loads and stores.",
			  "Ideal (core width limited) instruction bandwidth for GLC/Zen 3 is 48 bytes/cycle. For Ice Lake, its 40 bytes/cycle, and for Skylake, its 32 bytes/cycle.",
			  "Golden Cove puts its floating point units behind three ports, an improvement over previous Intel architectures that only had two ports for floating point loads.",
			  "AMD uses split scheduling queues, which might be easier to implement in silicon, but requires careful tuning to make instructions for most loads are well distributed among the queues.",
			  "Golden Cove can do floating point additions with two cycle latency. Weve seen that before in low frequency designs like VIAs Nano. But Golden Cove is doing this at over 5 GHz. Thats incredible, and Intels engineers should be proud.",
			  "AMD and Intel can both do vector integer addition with 1 cycle latency. However, Golden Coves vector integer multiplier has 10 cycle latency (with packed 32-bit integers), just like older Intel CPUs. Thats far worse than Zen 3s 3 cycle latency, or Zen 2s 4 cycle latency.",
			  "Past L1, all CPUs here can read 16 bytes/cycle from L2. The average x86 instruction is 3-4 bytes long in integer code (our test with 8 byte NOPs is more applicable to very AVX-heavy code). 16 bytes/cycle is therefore enough instruction bandwidth to feed the core with 4 to 5 instructions per cycle.",
			  "For integer loads, Golden Coves extra ALU and tweaked renamer should still boost performance. But I feel Intel left some integer performance on table to stretch Golden Coves design across Alder Lake and Sapphire Rapids."
			]
		  },
		  {
			"title": "Memory-level parallelism",
			"url": "https://grokipedia.com/page/memory_level_parallelism",
			"excerpts": [
			  "The Cimple runtime system extends this by implementing an IMLP task model using coroutines that yield at long-latency memory points, multiplexing up to 50 independent requests per core to saturate hardware prefetch buffers and achieve up to 6.4 single-thread speedup in pointer-chasing tasks like binary tree lookups.",
			  "The Cimple runtime system extends this by implementing an IMLP task model using coroutines that yield at long-latency memory points, multiplexing up to 50 independent requests per core to saturate hardware prefetch buffers and achieve up to 6.4 single-thread speedup in pointer-chasing tasks like binary tree lookups.",
			  "In out-of-order (OoO) execution, memory-level parallelism (MLP) is exploited by allowing the processor to issue and track multiple independent memory operations concurrently, thereby tolerating latency from cache misses without stalling the entire pipeline. The reorder buffer (ROB) plays a central role in this mechanism, maintaining the speculative execution state of instructions while load/store queues handle the tracking of outstanding memory accesses; this setup enables non-dependent instructions to proceed even as memory requests are pending, effectively overlapping their latencies to uncover and utilize MLP. A key enabler of this parallelism is the miss status holding register (MSHR), which manages multiple unresolved cache misses by queuing details such as the requested address and associated instructions, preventing the pipeline from blocking on a single miss and allowing subsequent independent loads to be dispatched in parallel. In modern OoO cores, this capability typically sustains 8-16 outstanding misses, which can reduce stall cycles by up to 50% in memory-intensive workloads by exploiting the inherent independence among memory references.",
			  "To realize high MLP, processors require robust miss status holding registers (MSHRs) to track multiple outstanding memory requests. Conventional MSHRs support 816 entries, but workloads with sustained high MLP demand 3264 entries to avoid stalls. Banked or set-associative MSHR organizations with subentries for secondary misses (dependent requests to the same cache line) are essential to sustain bandwidth without excessive area overhead. [2] Additional techniques, such as out-of-order execution with large reorder buffers and runahead execution, further enhance MLP by allowing continued progress during memory stalls, potentially doubling performance in latency-bound scenarios.",
			  " per queue entry) and yields performance gains of up to 24% in pointer-chasing benchmarks like mst, with average IPC improvements of 11% across memory-bound workloads by increasing effective MLP from 2 to over 6 overlapping misses.",
			  "For instance, in a simulated MIPS R10000-like processor, this setup adds minimal hardware (e.g., two flag bits per queue entry) and yields performance gains of up to 24% in pointer-chasing benchmarks like mst, with average IPC improvements of 11% across memory-bound workloads by increasing effective MLP from 2 to over 6 overlapping misses.",
			  "In modern CPU architectures such as Intel and AMD x86 processors, exploiting high memory-level parallelism (MLP) significantly enhances performance by overlapping memory accesses, leading to instructions per cycle (IPC) uplifts of 20-60% in memory-bound server workloads like databases.",
			  "each core supports up to 10 outstanding memory requests, enabling effective tolerance of long latencies in server environments.",
			  "Memory-level parallelism > Applications in Modern Computing > Impact on CPU and GPU Performance",
			  "Intel and AMD x86 processors, exploiting high memory-level parallelism (MLP) significantly enhances performance by overlapping memory accesses, leading to instructions per cycle (IPC) uplifts of 20-60% in memory-bound server workloads like databases"
			]
		  },
		  {
			"title": "Measuring the memory-level parallelism of a system using a small C++ program?  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2018/11/05/measuring-the-memory-level-parallelism-of-a-system-using-a-small-c-program/",
			"excerpts": [
			  "The idea is simple: we visit N random locations in a big array.",
			  "We make sure that the processor cannot tell which location we will visit next before the previous location has been visited.",
			  "There is a data dependency between memory accesses.",
			  "Section Title: Measuring the memory-level parallelism of a system using a small C++ program?",
			  "| Intel Haswell | 7 |",
			  "| Intel Skylake | 9 |",
			  "| ARM Cortex A57 | 5 |",
			  "The pointer chasing test has the downside that it starts to produce ugly code once you run out of registers.",
			  "Our processors can issue several memory requests at the same time. In a multicore processor, each core has an upper limit on the number of outstanding memory requests, which is reported to be 10 on recent Intel processors.",
			  "Our processors can issue several memory requests at the same time. In a multicore processor, each core has an upper limit on the number of outstanding memory requests, which is reported to be 10 on recent Intel processors."
			]
		  },
		  {
			"title": "A Framework for Managing Heterogeneous Memory for ...",
			"url": "https://escholarship.org/content/qt7k32s3tv/qt7k32s3tv_noSplash_c93e1141dd2e856492f34784e85a0f7e.pdf",
			"excerpts": [
			  "In 2LM, all benchmarks were run on two NUMA nodes and assigned all 96 threads. ... extra level of pointer chasing causes a slight slowdown when embedding tables ...Read more"
			]
		  },
		  {
			"title": "What are the differences in HBM memory latency between ...",
			"url": "https://massedcompute.com/faq-answers/?question=What%20are%20the%20differences%20in%20HBM%20memory%20latency%20between%20NVIDIA%20A100%20and%20H100%20GPUs%20in%20large%20language%20model%20training?",
			"excerpts": [
			  "HBM3 vs. HBM2e: The H100 (when equipped with HBM3) offers lower latency compared to the A100's HBM2e, thanks to higher memory clock speeds and improved ...Read more"
			]
		  },
		  {
			"title": "[2506.22849] DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations",
			"url": "https://arxiv.org/abs/2506.22849",
			"excerpts": [
			  "Oriented bounding box (OBB) bounding volume hierarchies offer a more precise fit than axis-aligned bounding box hierarchies in scenarios with thin elongated and arbitrarily rotated geometry, enhancing intersection test performance in ray tracing.",
			  "we introduce a novel OBB construction technique where all internal node children share a consistent OBB transform, chosen from a fixed set of discrete quantized rotations.",
			  "This allows for efficient encoding and reduces the computational complexity of OBB transformations.",
			  "Despite a 12.6% increase in build time, our experimental results demonstrate an average improvement of 18.5% in primary, 32.4% in secondary rays, and maximum gain of 65% in ray intersection performance, highlighting its potential for advancing real-time applications."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://www.researchgate.net/publication/281895915_Array_Layouts_for_Comparison-Based_Searching",
			"excerpts": [
			  "In addition to the obvious sorted order/binary search combination we consider the Eytzinger breadth-first-search (BFS) layout normally used ...Read more"
			]
		  },
		  {
			"title": "performance - Using SIMD/AVX/SSE for tree traversal - Stack Overflow",
			"url": "https://stackoverflow.com/questions/20616605/using-simd-avx-sse-for-tree-traversal",
			"excerpts": [
			  "I've used SSE2/AVX2 to help perform a B+tree search. Here's code to perform a \"binary search\" on a full cache line of 16 DWORDs in AVX2: Copy.Read more"
			]
		  },
		  {
			"title": "What role do branch mispredictions play in hash table ...",
			"url": "https://stackoverflow.com/questions/62997105/what-role-do-branch-mispredictions-play-in-hash-table-lookup-performance",
			"excerpts": [
			  "High performance hash tables would optimise with tricks like checking batches of four keys at once between branches to reduce mispredictions.Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing improves SIMD hash tables",
			"url": "https://reiner.org/cuckoo-hashing",
			"excerpts": [
			  "Baseline SIMD hash tables such as Swiss Tables use SIMD quadratic probing. This starts searching from a position determined by the hash function, searches 4 ...Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Cuckoo_hashing",
			"excerpts": [
			  "Cuckoo hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table, with worst-case constant lookup time.Read more"
			]
		  },
		  {
			"title": "SIMD Vectorized Hashing for Grouped Aggregation",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Gurumurthy:ADBIS18.pdf",
			"excerpts": [
			  "Cuckoo hashing resolves collision by using multiple hash tables [9]. These tables resolve collision by swapping collided keys. The collided key, during ...Read more"
			]
		  },
		  {
			"title": "SIMD Acceleration for Main-Memory Index Structures",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Wallewein-EisingBS:BDAS18.pdf",
			"excerpts": [
			  "processor architectures [1]. Several index structures have already shown that the\nbottleneck from RAM to CPU can be overcome using Single Instruction Multiple\nData (SIMD) [24] operations. These index structures include: the k-ary Search\nTree (Seg-Tree) [5], Adapted Radix Tree (ART) [6], Fast Architecture Sensitive\nTree (FAST) [7], and Vector-Advanced and Compressed Structure Tree (VAST)\n[8]. All approaches use SIMD only for key comparison within tree traversal and\ntry to decrease the key size to fit more keys into one SIMD register. There-\nfore FAST and Seg-Tree only provide implementations for search algorithms",
			  "In this work we make the following contributions:\n**** We compare different optimizations of index structures to fulfill requirements\nof modern database systems\n2\nSIMD Acceleration for Index Structures\n**** We highlight the usage of SIMD and the cache-line adaptations in all ap-\nproaches\n**** We state the performance impact of optimizations on the index structures\n**** We discuss the usefulness of optimizations for our own index structure Elf [9].",
			  "We organized the rest of the paper as follows. In Section 2, we give the prelim-"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "the scatter instruction is only\navailable in limited types of processors hence the vertically-vectorized insertions can only be\nimplemented on specific hardware.",
			  "In each\nvectorized lookup, the vertical approach will pass a vector (of register size) of inputs through\nthe lookup process and by using mask registers and advanced SIMD features (like SIMD\npermutations) probe those keys at the same time.",
			  "rizontal vectorization, each cell of the hash-table entries array is bucketed into *N*\ninner cells. Then, while doing a lookup on the hash table and after computing the hash\nvalue, using the SIMD logical operations, the lookup algorithm can concurrently check the\nvalue of *N* bucket key",
			  "vertical vectorization yields\nhigher performance than the horizontal approach."
			]
		  }
		],
		"reasoning": "To optimize parallel tree traversal for 256 walkers with a strict memory load constraint, a hybrid approach is essential. This involves transforming the data structure, employing a hybrid traversal strategy, and aggressively hiding memory latency. \n\nFirst, the tree structure itself needs optimization. A simple pointer-based binary tree is inefficient due to scattered memory accesses. Instead, a cache-conscious layout like the van Emde Boas (vEB) layout is recommended. This layout improves memory locality across different cache levels. Furthermore, adopting a wide N-ary tree structure (e.g., 4-ary or 8-ary) and compressing nodes (e.g., using quantization) can reduce memory footprint and bandwidth consumption, making each load operation more efficient. This addresses the scattered memory access issue by improving data locality and reducing the number of loads required. \n\nSecond, a hybrid traversal algorithm is crucial. Given that all walkers start at the root, an initial phase should leverage this coherence. A level-synchronous, breadth-first 'packet' traversal is suitable for the early stages, batching all walkers to maximize cache reuse and enable coordinated prefetching for subsequent levels. As walker paths diverge, the algorithm should transition to an independent, asynchronous model. For GPUs, this can be implemented using persistent threads with work queues, ensuring compute units remain busy despite irregular execution times. On CPUs, a thread pool with task-stealing is a viable alternative. \n\nThird, to hide memory latency and maximize parallelism, software-directed prefetching is vital. In the batched phase, prefetching all nodes for the next level is beneficial. In the divergent phase, helper-thread prefetching or a Decoupled Access/Execute (DAE) model can be employed, where dedicated threads or warps issue memory requests ahead of computation threads. This strategy transforms the memory-latency-bound problem into a throughput-oriented one by maximizing memory-level and instruction-level parallelism. SIMD instructions (e.g., AVX-512, SVE) should be used for vertical vectorization to process batches of walkers in parallel, especially for the hash computation and node reads, utilizing `gather` instructions for scattered accesses.",
		"confidence": "high"
	  },
	  {
		"field": "quantitative_performance_model",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "This is\nachieved by sharing the traversal stack among the rays in a warp-\nsized packet, which implies rays will visit (potentially many) nodes\nthey do not intersect but also that memory accesses are very co-\nherent because each node is fetched only once per pack",
			  "e observe that previously known methods are a factor of\n1.52.5X off from theoretical optimum, and most of the gap is not\nexplained by memory bandwidth, but rather by previously unidenti-\nfied inefficiencies in hardware work distribution",
			  "We observe that previously known methods are a factor of\n1.52.5X off from theoretical optimum, and most of the gap is not\nexplained by memory bandwidth, but rather by previously unidenti-\nfied inefficiencies in hardware work distribution"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs -- Project page",
			"url": "https://users.aalto.fi/~ailat1/HPG2009/",
			"excerpts": [
			  "We observe that previously known methods are a factor of 1.5-2.5X off from theoretical optimum, and most of the gap is not explained by memory bandwidth, but rather by previously unidentified inefficiencies in hardware work distribution."
			]
		  },
		  {
			"title": "Understanding the efficiency of ray traversal on GPUs | Proceedings of the Conference on High Performance Graphics 2009",
			"url": "https://dl.acm.org/doi/10.1145/1572769.1572792",
			"excerpts": [
			  "We observe that previously known methods are a factor of 1.5--2.5X off from theoretical optimum, and most of the gap is not explained by memory bandwidth, but rather by previously unidentified inefficiencies in hardware work distribution.",
			  "Content:"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/items/35c0b361-759c-4075-b59c-0868d50b4780",
			"excerpts": [
			  "We observe that previously known methods are a factor of 1.5 2.5X off from theoretical optimum, and most of the gap is not explained by memory bandwidth, but rather by previously unidentified inefficiencies in hardware work distribution."
			]
		  },
		  {
			"title": "General Transformations for GPU Execution of Tree ...",
			"url": "https://engineering.purdue.edu/~milind/docs/sc13.pdf",
			"excerpts": [
			  "However, because the tree structures\nare irregular, and the points traversals are input-dependent, sim-\nply running multiple traversals simultaneously on the GPU can-\nnot take advantage of efficient memory accesses, seriously hinder-\ning performance (Section 2.2 discusses GPU architectures and the\nGPU performance model in more detail)."
			]
		  },
		  {
			"title": "Parallel Tree Traversal for Nearest Neighbor Query on the ...",
			"url": "http://dicl.skku.edu/publications/icpp2016.pdf",
			"excerpts": [
			  "Traversing hierarchical tree structures in an irregular manner makes it difficult to exploit parallelism since GPUs are tailored for deterministic memory ...Read more"
			]
		  },
		  {
			"title": "Generalizing Ray Tracing Accelerators for Tree Traversals ...",
			"url": "https://intra.engr.ucr.edu/~htseng/files/2024MICRO-TTA.pdf",
			"excerpts": [
			  "Figure 1 shows the average SIMT efficiency (percent of\nactive threads per warp due to control flow divergence) and\nDRAM bandwidth utilization of several tree traversal applica-\ntions on GPUs, profiled on an NVIDIA RTX 2080 Ti GPU and\nalso measured using Vulkan-Sim [ 83 ] with configurations list",
			  "ire ray traversal as a single instruction (** **traceRay** **)**\n**on the GPU.** When threads diverge in the tree traversal,\nRTAs handle the control flow divergence directly, avoiding\ninactivating computing lanes in the SIMD pipeline [ 83 ]. Thus,\nthe warps only need to synchronize the rays at the end of\nthe traversal when RTAs complete the instruction",
			  ") RTAs use a dedicated hardware memory scheduler**\n**to improve memory bandwidth utilization.** This scheduler\ncoalesces node requests between threads when possible and\narbitrates for one memory request into the GPU memory system\nper cycle. The dedicated memory scheduler better handles\nirregular memory access patterns by only focusing on node\nrequests, allowing the scheduler to track more concurrent\ntraversals and increase DRAM utilization by nearly ",
			  "Barringer et al. [ 11 ]\nintroduced a traversal algorithm that leverages ray coherence\nfor high SIMD efficiency, but packet tracing is ineffective for\nincoherent rays. Shkurko et al. [ 85 ] proposed a dual streaming\napproach that organizes ray-tracing memory accesses into\ntwo predictable data streams, and Vasiou et al. [ 90 ] analyzed\nthe energy and time cost of data movement for ray tracing.\nThese optimizations can be applied orthogonally to our work."
			]
		  },
		  {
			"title": "REGTT: Accelerating Tree Traversals on GPUs by ...",
			"url": "https://cgi.cse.unsw.edu.au/~jingling/papers/icpp16.pdf",
			"excerpts": [
			  "There are three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The thre",
			  "re three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access overhead.\n** **Load Imbalance** Different threads in the same warp may\nhave different workloads due to query-dependent",
			  "e three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access over"
			]
		  },
		  {
			"title": "SIMD Parallelization of Applications that Traverse Irregular ...",
			"url": "https://www.cs.wm.edu/~bren/files/papers/CGO13.pdf",
			"excerpts": [
			  "In order to make our approach fast, we demonstrate\nseveral optimizations including a stream compaction method\nthat aids with control flow in SIMD, a set of layouts that\nreduce memory latency, and a tiling approach that enables\nmore effective prefetching.",
			  "ine-grained data parallelism is increasingly common in\nmainstream processors in the form of longer vectors and on-\nchip GPUs.",
			  "This paper develops support for exploiting such\ndata parallelism for a class of non-numeric, non-graphic\napplications, which perform computations while travers-\ning many independent, irregular data structure",
			  "Our work has considered specific challenges arising\nfor pointer-based traversals, which have not been considered\nin the past."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.researchgate.net/publication/221249082_Understanding_the_Efficiency_of_Ray_Traversal_on_GPUs",
			"excerpts": [
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal. Aila and Laine [2009] proposed to increase SIMD efficiency by replacing already finished rays with new ones from a global queue.",
			  "Aila and Laine [2009] proposed to increase SIMD efficiency by replacing already finished rays with new ones from a global queue. Techniques such as speculative ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "If a warp is going to execute node traversal\nanyway, why not let all rays participate?\nAlternative: be idle\nCan perform redundant node fetches\nShould help when not bound by memory speed\n5-10% higher performance in primary and AO\nNo improvement in diffuse\nDisagrees with simulation (10-20% expected)\nFirst evidence of memory bandwidth issues?\nNot latency, not computation, not load balancing",
			  "\nWhat limits the performance of fastest traversal\nmethods on GPUs?\n",
			  "?\nMemory speed (bandwidth, latency)?",
			  "Resource conflicts (serialization, scoreboard, )?\nL",
			  "?\nHow far from theoretical optimum?",
			  "How much performance on table?\nS",
			  "\nSIMD with execution divergence handling built into\nhardware\n",
			  "npredictable sequence of *acceleration structure*\n*traversal* and *primitive intersecti",
			  "Understanding the Efficiency of Ray Traversal on GPUs. Timo Aila Samuli Laine. NVIDIA Research. Page 2. Agenda. What limits the performance of fastest traversal.Read more",
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed",
			  "**2. Local work queues**\nAssign 64 rays to a 32-wide warp\nKeep the other 32 rays in shared mem/registers\n32+ rays will always require either node traversal or\nprimitive intersection\nAlmost perfect SIMD efficiency (% threads active)\nShuffling takes time\nToo slow on GTX285\nWith ENUM + POPC, in Fairy scene\nAmbient occlusion +40%\nDiffuse +80%\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization."
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays",
			"url": "https://users.aalto.fi/~ailat1/publications/aila2010hpg_paper.pdf",
			"excerpts": [
			  "his paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for*\n*global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration*\n*structure and repeated queueing/postponing of rays to reduce cache pressure",
			  "heduling algorithms can have an important*\n*effect on results, and that using fixed-size queues is not an appealing design choice. Increased auxiliary traffic,*\n*including traversal stacks, is identified as the foremost remaining challenge of this architectur",
			  "This paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for global illumination.Read more"
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays | Research",
			"url": "https://research.nvidia.com/publication/2010-06_architecture-considerations-tracing-incoherent-rays",
			"excerpts": [
			  "This paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration structure and repeated queueing/postponing of rays to reduce cache pressure.",
			  "Increased auxiliary traffic, including traversal stacks, is identified as the foremost remaining challenge of this architecture."
			]
		  },
		  {
			"title": "Architecture Considerations for Tracing Incoherent Rays",
			"url": "https://dl.acm.org/doi/pdf/10.5555/1921479.1921497",
			"excerpts": [
			  "is paper proposes a massively parallel hardware architecture for efficient tracing of incoherent rays, e.g. for*\n*global illumination. The general approach is centered around hierarchical treelet subdivision of the acceleration*\n*structure and repeated queueing/postponing of rays to reduce cache pressure."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  ". Aila et al. [ 2 ] proposed group-\ning rays into ray packets and traversing rays together, improving\nmemory coherence. ",
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  },
		  {
			"title": "Batching of divergent rays on GPU architectures",
			"url": "https://studenttheses.uu.nl/bitstream/handle/20.500.12932/41250/thesis_final.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "A number of advanced traversal algorithms have been devised in recent years that increase ray coherence by bundling active rays together dynamically as they ...Read more",
			  "Benthin et al.[ Ben+12 ] devised a hybrid traversal model that combines ray packets and single ray\ntraversal.",
			  "**2.2. Ray Batching**\nPharr et al sought to address the problem as a scheduling challenge [ Pha+97 ]. They developed a system\ndesigned to handle scenes far too large for system memory, based on voxels. When tracing through a\ngiven voxel, required data would need to be explicitly fetched from disc before tracing could continue\ninto that segment. They also ran into issues with the sheer volume of rays that are spawned by the\ntree structure of by Whitted-style ray tracing [ Whi79 ], especially when they scheduled a large number\nof active concurrent traces rather than the traditional depth-first implementation. Their schedul"
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level."
			]
		  },
		  {
			"title": "Treelet Accelerated Ray Tracing on GPUs",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/chou.asplos2025.pdf",
			"excerpts": [
			  "This work explores how to efficiently implement treelet\nqueues on modern ray tracing capable GPUs. We propose\nVirtualized Treelet Queues, an architecture that increases\nthe number of concurrent rays in flight and treelet queues\nthat dynamically switch between treelet and ray stationary\ntraversal modes to increase efficiency.",
			  "ay virtualization greatly increases the\nnumber of concurrent rays in flight which would normally\nbe limited by the amount of warp slots in the RT unit since\neach ray is executed by a thread. We achieve this by ter-\nminating raygen shaders after a thread issues its trace ray\ninstruction to the RT unit, allowing the GPU to reclaim the\nCUDA cores and launch new threads / raygen shaders from\nalready queued up Cooperative Thread Arrays (CTAs)",
			  "o\ntake advantage of the increase in concurrent rays, we imple-\nment dynamic treelet queues in the GPUs RT unit to group\nup rays that access the same treelet together, achieving better\ncache locality and reduced miss rates",
			  "During later phases of\nray traversal when rays diverge more, treelet queues end up\nunderpopulated and it becomes inefficient to process rays\nin treelets. We propose to group up these underpopulated\ntreelet queues and traverse them regularly instead.",
			  "How-\never, this leads to a sharp decrease in SIMT efficiency due\nto varying BVH node access counts from different ray",
			  "We\napply warp repacking to regroup active rays together into a\nnew warp, boosting SIMT efficiency and performance.",
			  "a et al. [ 5 ]\nproposed to subdivide the BVH tree into smaller subtrees, or\ntreelets, that fit in the processors cache to reduce memory\ntraff",
			  "Treelet queues essentially achieve a similar goal by grouping\nup rays based on their accessed treelet, but without the high\noverhead."
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://meistdan.github.io/publications/bvh_star/paper.pdf",
			"excerpts": [
			  "nd Kar-\nras [ AK10 ] propose a SIMD architecture for ray tracing, which is\ndesigned to reduce arithmetic and memory divergence for incoher-\nent ray distributions. The design consists of a set of *processors* ,\nwhich execute *warps* consisting of multiple threads, similar to con-\nventional GPUs. Each thread manages one ray. The authors note\nthat the design could accommodate fixed-function traversal units,\nbut choose to focus on the contribution of reducing memory traf",
			  " BVH is divided into *treelets* (small subtrees within the to-\ntal BVH) which are set to the size of either the L1 or L2 cache. The\narchitecture maintains a set of ray *queues* , with queues assigned\nto treelets at runtime. Rays begin tracing at the root treelet, and\nas rays cross treelet boundaries, they are placed in the ray queues\nto be processed later when their required treelet is present in the\ncache. The architecture thus attempts to maximize the number of\nrays which are processed each time a treelet is loaded on-chip, re-\nducing memory bandwidth",
			  "To reduce stack traffic, the architecture\nmaintains a stack-top cache on chip, while keeping the remainder\nin DRAM. A second key innovation of this work is the inclusion\nof *work compaction* logic which detects when the SIMD utilization\nof a given warp has fallen below an efficient level, at which point\nthe unit terminates the warp and diverts the remaining active rays\nto the *launcher* which is responsible for warp creation.",
			  "odniok et al. [ WSWG13 ] proposed new layouts: *swapped*\n*subtrees* (SWST) and *treelet-based depth-first-search/breadth-first-*\n*search* (TDFS/TBFS). These layouts are determined based on the\nnode access statistics obtained by casting a small number of sample\nrays in a preprocessing step. SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged. The latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order. The authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.\nHowever, none of these layouts is always better",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH.",
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Tree traversal is an intensive pointer-chasing operation, requiring traversing to a node in the tree and finding the child pointers, before being able to find the child node addresses and issue loads.",
			  "With treelet prefetching, as rays traverse the BVH tree and visit the root node of treelets, corresponding treelets can be prefetched to load deeper levels of the tree before they are needed.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulations show treelet based traversal reduces performance slightly by 3.7% over a DFS baseline. However, when combined with treelet prefetching, the overall speedup reaches 32.1% while maintaining the same power consumption.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping rays that traverse similar parts of the AS. Pharr et al. [ [39]() ] reordered ray computation to improve ray coherency and cache utilization. Garanzha and Loop [ [16]() ] sorted rays based on ray origin and direction before processing in packets. Moon et al. [ [32]() ] sorted rays with their final hit points. Meister et al. [ [30]() ] improved sorting heuristics to minimize ray divergence.",
			  "**Acceleration Structure Optimizations.** Ylitie et al. [ [48]() ] explored wide BVH trees to increase SIMD utilization. Lin et al. [ [26]() ] restructured BVH nodes with node splitting, reducing memory footprint. Benthin et al. [ [10]() ] and Liktor et al. [ [25]() ] perform BVH compression for memory bandwidth reduction. BVH optimizations benefit our work as more nodes fit into the same memory footprint, making prefetching more effective.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache. Ray tracing is a pointer-chasing application and memory accesses are divergent and hard to predict. With the treelet based traversal algorithm introduced previously, memory accesses are now clustered as individual treelets, making it possible to prefetch easily.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "... BVH tree statistics for each scene are outlined in Table 2. The scene ... We use the concept of treelets which are connected subpartitions of a BVH tree.Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "We form treelets by grouping connected BVH nodes to maximize the size of each treelet. It is a greedy algorithm that starts from the BVH root node and greedily adds nodes to the current treelet until the maximum treelet size is reached.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "We propose to add a treelet prefetcher that prefetches treelets into the L1 cache of the GPU based on the rays in the warp buffer.",
			  "The threshold comparator generates the prefetch enable signal if the treelet popularity is greater than a manually set threshold which ranges from 0 to the maximum number of rays in the warp buffer ().",
			  "The prefetch enable is ANDed with the upper bits of the treelet root node address to generate the treelet prefetch address and sent to the prefetch queue to be processed ().",
			  "We only require the upper bits of the treelet root node address because the treelets have a fixed maximum size and nodes within a treelet are organized to be packed together in memory.",
			  "The treelet prefetcher also records the address of the last treelet it prefetches to avoid pushing duplicate treelet addresses to the prefetch queue and prefetching the same treelet multiple times in a row.",
			  "We combine treelet prefetching with a treelet based traversal algorithm in the ray tracing accelerator to further reduce ray traversal latenc",
			  "\nRay traversal is typically done by traversing the BVH tree in a depth-first or breadth-first manner",
			  "This section describes our proposed treelet prefetching technique for ray tracing.",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "As a ray visits a treelet root node, its subsequent memory accesses will also be to the nodes in the treelet since accesses to nodes from different treelets are deferred to the *otherTreeletStack",
			  "Thus, we can prefetch the entire treelet to the GPU's cache and reduce the latency of accessing nodes in the current treelet.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "[15] Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First\nPacket Traversal for GPU Ray Tracing. *Computer Graphics Forum* (2010).",
			  "[30] Daniel Meister, Jakub Boksansky, Michael Guthe, and Jiri Bittner. 2020. On Ray\nReordering Techniques for Faster GPU Ray Tracing. In *Proc. ACM SIGGRAPH*\n*Symp. on Interactive 3D Graphics and Games (I3D)* . 19.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "d\nTreelet Prefetching For Ray Tracing\nMICRO 23, October 28November 01, 2023, Toronto, ON, Canada\nmore frequently than the lower levels. In the next sections, we pro-\npose a treelet based ray traversal algorithm with an accompanying\nprefetcher.",
			  "s a ray visits a treelet root node, its subse-\nquent memory accesses will also be to the nodes in the treelet since\naccesses to nodes from different treelets are deferred to the *oth-*\n*erTreelet",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "refetching techniques can\nbe used to improve memory latency tolerance in GPGPU applica-\ntions [ 22 , 24 , 42 ]",
			  "a treelet prefetcher to the RT unit to speed\nup ray traversal along with a prefetch queue to hold the issued\nprefetch addresses, both of which are highlighted in red in Figur",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "In Vulkan-Sim [ 41 ], the simulator we use\nfor evaluation, the ray tracing accelerator is referred to as the RT\nunit. When a warp issues a trace ray instruction, it enters the RT\nunit during the pipelines execute stage and is queued in the warp\nbuffer, which holds ray metadata for all 32 threads of the warp.",
			  "This work presents a treelet prefetching scheme to improve ray\ntraversal performance.",
			  "**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS. Pharr et al. [ 39 ] reordered\nray computation to improve ray coherency and cache utilization.",
			  "**7.3**\n**Data Structure Partitioning**\nFeng et al. [ 13 ] take advantage of data structure partitioning to ex-\nploit parallelism. Within parallel regions during traversal identified\nby the programmer with compiler pragmas, they subdivide graphs\nand trees and distribute each partition to be processed by different\ncores to improve parallelism for CPUs. Locality improves by hav-\ning the same cores process the same partitions repeatedly.",
			  "**7.4**\n**Treelet Based Ray Tracing Techniques**\nNavratil et al. [ 33 ] tackled incoherent rays by collecting rays into\n ... \nstaging buffer which might require non-trivial shader modifications\nto realize on a GPU and are not discussed in their paper.",
			  "**7.5**\n**Ray Traversal Acceleration Techniques**\n**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "We propose to add\na treelet prefetcher that prefetches treelets into the L1 cache of\nthe GPU based on the rays in the warp buffer.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict.",
			  "With 512B treelets, the treelet\nbased memory layout performs best with a 31.9% speedup over\nthe baseline.",
			  "Loose and Strict Wait represent a rough upper and\nlower bound performance when using an unmodified BVH tree that\nrequires mapping table loads, falling behind the repacked BVH with\nonly a 29.7% speedup and 2.5% slowdown respectively.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The original BVH node\nlayout is allocated to be 64 bytes and stores the bounding box,\npointers, and other metadata for the child nodes of a 6-wide BVH\ntree.",
			  "It is a greedy algorithm that starts from the BVH\nroot node and greedily adds nodes to the current treelet until the\nmaximum treelet size is reached. T",
			  "Figure 3 shows\nan example of a two-wide BVH tree partitioned into treelets with a\nmaximum size of 4 nodes each.",
			  "reelet formation initializes the *remainingBytes* to the maximum\ntreelet size and adds the BVH root address to the *pendingTreelets*\nqueue and traversal sta"
			]
		  },
		  {
			"title": "Efficient SIMD Single-Ray Traversal using Multi-branching ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/wald08_widebvh.pdf",
			"excerpts": [
			  "In this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.\nNote that this forces every multi-node to contain 16 node slots even\nif the parent node has less than 16 children; any unused nodes are\nflagged as invalid. ",
			  "\n**M** **ETHOD** **O** **VERVIEW**\nIn this paper, we investigate the use of BVHs with branching fac-\ntors of up to 16 (the SIMD width of our target architecture). For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16. Due to this SIMD processing, we use\na SIMD-friendly data layout in which all 16 children of the same\nnode are stored in a structure-of-array (SoA) multi-node layout.",
			  "The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  "the optimal branching factor is somewhere between 4 and 8.",
			  "Concurrently to this paper, the idea of BVHs with branching fac-\ntors higher than two has also been investigated by Dammertz et\nal. [4]",
			  "The only difference to the standard SAH is that the cost function\nfor leaves and inner nodes is slightly different. Since we always\nperform 16 triangle tests respectively 16 axis-aligned box tests in\nparallel for the same ray, intersecting 16 triangles in a leaf now\ncosts roughly as much as intersecting a single triangle; the same\nargument holds true for box tests in inner nodes.",
			  "For\ninner nodes, we test 16 nodes in SIMD; for leaf nodes, we intersect\ntriangles in batches of 16.",
			  "**B** **UILDING** **G** **OOD** **M** **ULTI** **-** **BRANCHING** **BVH** **S**\nThough an efficient traversal routine is key to performance, the ac-\ntual way the data structure is built often can have a similar per-\nformance impact.",
			  "onsequently, if there were a method that would use the 16-wide\nSIMD to intersect less than 16 triangles (say, four), and that would\ndo that *faster* than we currently perform 16 triangle intersections,\nthen a BVH with a less extreme branching factor might be even\nfaster",
			  "We also introduce an efficient SIMD traversal technique for these\nBVHs that produces a strict front-to-back traversal with a minimum\nof scalar code to determine the traversal order.",
			  "ber of inner nodes drops even further, by consistently\naround 50 *x* . Though every node now is 16 *x* as large as a traditional\nnode, the net savings is still significant (we had, in fact, expected a\n*higher* memory consumption",
			  " unused nodes are\nflagged as invalid. The actual true hierarchy is governed by a sur-\nface area heuristic that takes the modified traversal and intersection\ncost into account. The resulting trees have an average branching\nfactor and leaf size of 1012 each (the most often traversed nodes\nhigh up in the tree usually are completely filled), and produce high\nSIMD utilization even for single-ray traversal.",
			  " traditional assumptions for the surface area heuris-\ntic [6, 7], the probability of any node *n* being traversed by a random\nray is proportional to the nodes surface area *SA* ( *n* ) . Thus, the ag-\ngregate cost in SIMD triangle intersections and SIMD box tests for\na given multi-BVH can be estimated a",
			  "We demonstrate that with a properly built bounding volume hierarchy (BVH) and a front-to- back traversal algorithm, this approach is somewhat slower than."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://users.aalto.fi/~laines9/publications/ylitie2017hpg_paper.pdf",
			"excerpts": [
			  "Specifically, we start\nby building a binary BVH using an existing high-quality builder,\nand then convert it into an 8-wide BVH in a SAH-optimal fashion.\nWe also employ octant-aware fixed-order traversal [Garanzha and\nLoop 2010], and present an improved method for ordering the child\nnodes at build time to obtain a better traversal order.",
			  "We present a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "Wide BVHs.* Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instead. Shallower hierar-\nchies reduce the number of visited nodes and memory accesses\nduring traversal, but they typically increase the number of ray-box\nand ray-triangle intersection tests [Afra 2013]. Wider BVHs also",
			  "The high branching factor amortizes the memory fetch latencies\nover 8 bounding box intersection tests and increases instruction\nlevel parallelism during internal node tests.",
			  "he memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code.",
			  "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, U",
			  "In addition, we present an algorithmically efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "e also encode the quantiza-\ntion grid and child node indexing information in a compact for",
			  "We compare the performance of our method ( **Ours** ) to four pre-\nviously published GPU-based methods: the traversal kernels by\nAila et al. [2012] ( **Baseline** ), latency-optimized four-wide traver-\nsal by Guthe [2014] ( **4-wide** ), stackless traversal by Binder and\nKeller [2016] ( **Stackless** ), and irregular grids by Prard-Gayot et\nal. [2017] ( **IrrGrid** ). ",
			  " ). We use the authors original implementations\nfor all comparison methods, with no changes to the traversal code.\nFor *",
			  "**5.1**",
			  "**Memory usage**",
			  "Table 6 shows the amount of memory consumed by each method.",
			  "We only report the memory usage of the acceleration structure\nitself, excluding the triangle data to make the comparison as fair as\npossible.",
			  "icient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs\nHPG 17, July 28-30, 2017, Los Angeles, CA, USA\nRay-node tests\nRay-triangle tests\nBounce\n0\n1\n4\n0\n1\n4\nFull precision\n14.30\n14.54\n14.62\n6.45\n7.64\n8.61\n8 bits\n14.69\n14.95\n15.02\n6.59\n7.98\n9.03\n7 bits\n15.04\n15.32\n15.38\n6.73\n8.23\n9.31\n6 bits\n15.69\n16.03\n16.06\n7.01\n8.70\n9.81\n5 bits\n17.18\n17.53\n17.50\n7.62\n9.56\n10.76\n4 bits\n20.56\n20.86\n20.67\n8.94\n11.36\n12.71\n**Table 1: Effect of AABB quantization on intersection test**\n**counts. The numbers represent arithmetic means over our**\n**test scenes, excluding Pow",
			  "resent a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed B",
			  "Similarly to previous methods [Keely 2014; Mahovsky and Wyvill\n2006; Segovia and Ernst 2010; Vaidyanathan et al . 2016], we quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "an 8-wide BVH, and we\n ... \ninternal nodes require only ten bytes per child, obtaining 1 : 3 . 2\ncompression ratio compared to the standard uncompressed BVH\nnode format.",
			  "y efficient method for\nconverting a binary BVH into a wide BVH in a SAH-optimal fashion,\nand an improved method for ordering the child nodes at build time\nfor the purposes of octant-aware fixed-order traversal.",
			  "Child Bounding Box Compression",
			  "quantize\nchild node AABBs to a local grid and store locations of the AABB\nplanes with a small number of bits.",
			  "ficient Incoherent Ray Traversal on GPUs Through**\n**Compressed Wide BV",
			  "HPG 17, July 28-30, 2017, Los Angeles, CA, USA",
			  "Bounding volume hierarchies with higher branch-\ning factor [Dammertz et al . 2008; Ernst and Greiner 2008; Wald\net al . 2008] have many appealing properties compared to binary\nBVHs. They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code. Even though our method uses a\nwide BVH, it does not attempt such distributed computation but\noperates on a single SIMD lane per ray instea",
			  "Our acceleration structure is a compressed 8-wide BVH that uses\naxis-aligned bounding boxes (AABBs) as the bounding volumes.",
			  "Wide BVHs.",
			  "Managing a full traversal stack is costly in GPU ray tracers,\nbecause the caches in GPUs are too small to capture the stack traffic.\nThis tends to lead to high DRAM traffic from the traversal stacks\nonly.",
			  "We compress both bounding boxes and child pointers so that our"
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs | Research",
			"url": "https://research.nvidia.com/publication/2017-07_efficient-incoherent-ray-traversal-gpus-through-compressed-wide-bvhs",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal.",
			  "the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion",
			  "Our method reduces the amount of memory traffic significantly, which translates to 1.9-2.1x improvement in incoherent ray traversal performance compared to the current state of the art.",
			  "Furthermore, the memory consumption of our hierarchy is 35-60% of a typical uncompressed BVH.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CPUStyleSIMDRayTraversal.pdf",
			"excerpts": [
			  "**Same Old, Same Old**",
			  "**Same Old, Same Old**",
			  "Binary BVH\n",
			  "Binary BVH\n",
			  "Stack based\n",
			  " Root node on stack",
			  "Test for intersections\n",
			  "* Check distanc",
			  "* Push onto stac",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**CPU-Style SIMD Ray Traversal on GPUs**",
			  "**Usual Suspects**",
			  "SIMT / GPU"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://jcgt.org/published/0011/04/01/paper.pdf",
			"excerpts": [
			  "odern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes",
			  "ald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *,*\n(",
			  "**3.**\n**Implementation**\nIn this section, we provide implementations details of the evaluated methods. Most\nof the tested methods have publicly available implementations. However, to provide\na fair comparison, we have to use the same settings for all methods, which might be\ndifficult in different frameworks. Thus, we created a single unified framework with\npublicly available implementations of the algorithm integrated into it. Our framework\nis based on Ailas ray tracing framework [ Aila and Laine 2009 ].\nAilas framework contains high-performance traversal kernels for binary BVHs.\nTo support wide BVHs, we adopted the publicly available implementations of the\nmethod proposed by Lier et al. [ 2018 ]. In the original version of these kernels, the",
			  " exploit.\nModern hardware architectures are equipped with SIMD units that allow the pro-\ncessing of multiple intersections tests at once, for example, in wide BVHs, where a\nsingle ray is tested against multiple bounding boxes. Triangles in leaves are processed\nin a similar fashion, and thus it is desirable to have the number of triangles aligned\nwith the SIMD width as the number of executed intersection tests is equal to the SIMD\nwidth. Motivated by this fact, Wald et al. [ 2008 ] proposed a modified version of the\ncost model:\n*c* ( *N* ) =\n1\n*SA* ( *N* )\n\n *c* *T*\n\n*N* *i*\n*SA* ( *N* *i* ) + *c* *I*\n\n*N* *l*\n*SA* ( *N* *l* ) *k*\n *|* *N* *l* *|*\n*k*\n \n *",
			  "Traversal on the GPU might be challenging due to warp divergence and incoherent\nmemory accesses.",
			  "Aila and Laine [ 2009 ] proposed a stack-based traversal algorithm\nwith persistent warps and dynamic fetch.",
			  "To prevent warp divergence, the traversal\nis divided into two independent loops processing interior and leaf nodes separately\n(i.e., the *while-while* traversal).",
			  "Dynamic fetch allows to fetch new rays if a certain number of threads are\ninactive to keep parallel resources occupied enough."
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through compressed wide BVHs",
			"url": "https://www.researchgate.net/publication/318730238_Efficient_incoherent_ray_traversal_on_GPUs_through_compressed_wide_BVHs",
			"excerpts": [
			  "de BVHs on the GPU On GPUs, Guthe [Gut14] found that tracing a binary hierarchy was latency-bound on their hardware and that using a 4-wide BVH increases tracing performance",
			  ". In practice, we build a binary BVH (all the aforementioned methods produce binary BVHs), and then we convert it to a wide BVH by pulling child nodes to the parent nodes in a top-down manner [Wald et al. 2014]. Pinto [2010] and Ylitie et al. [2017] proposed an algorithm based on dynamic programming that performs this conversion optimally with regard to the BVH cost.",
			  "Ylitie et al. showed how to do the conversion in a SAH-optimal fashion and presented a compressed wide layout [YKL17] ",
			  "The direct construction of wide BVHs is considered difficult, remaining as an open problem.",
			  "wide BVHs are the state-of-the-art acceleration structure for GPU raytracing, therefore we want our construction algo-rithm to output the nodes in a compressed format. We use a representation similar to the one described by Ylitie et al. [YKL17] : ",
			  "By quantizing the coordinates of child bounding boxes to a single byte they reduce the size of an 8-wide node to 80 bytes.",
			  "The presented 8-wide BVH ray stream implementation reduces memory traffic to only 18% of traditional approaches by using 8-bit quantization for box and triangle coordinates and directly ray tracing these quantized structures.",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH."
			]
		  },
		  {
			"title": "Wide BVH Traversal with a Short Stack",
			"url": "https://www.intel.com/content/dam/develop/external/us/en/documents/wide-bvh-traversal-with-a-short-stack-837099.pdf",
			"excerpts": [
			  "In this paper we introduce an algorithm for wide bounding volume hierarchy (BVH) traversal that uses a short stack of just a few entries. This stack can be ...Read more"
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231583/07-1038-lier.pdf",
			"excerpts": [
			  "In this paper we describe and evaluate an implementation of CPU-\nstyle SIMD ray traversal on the GPU. We show how spreading\nmoderately wide BVHs (up to a branching factor of eight) across\nmultiple threads in a warp can improve performance while not\nrequiring expensive pre-processing. Te presented ray-traversal\nmethod exhibits improved traversal performance especially for\nincreasingly incoherent rays.",
			  "Te foundation of our approach is teaming multiple lanes of a warp\nand leting them traverse the BVH together for one single ray. Tis\nconcept mimics a regular SIMD-based BVH traversal known from\nmethods utilizing SSE and AVX extension on the CPU [ Dammertz\net al . 2008 ; Ernst and Greiner 2008 ; Wald et al . 2014 ]. But in con-\ntrast to CPUs, switching between vector instruction (e.g. parallel\nintersection tests) and scalar instruction (e.g. stack management) is\nnot easily (or even efciently) possible on GPUs. In our case, some\noperations (e.g. loading, storing, and stack management) have to\nbe handled individually and at times redundantly on each lane.\nTerefore, we supply each lane with its own copy of the ray data,\nnearest hit information, and stack pointer. However, the stack itself\nresides in shared memory and thus is not redundant.\n",
			  "We start the construction of our acceleration\nstructure with a regular binary BVH. Tere are no limits to the\nconstruction of this BVH, except that we strive for the number of\nleaf primitives to match the width of our target BVH, i.e. 2, 4, or\n8 elements, similarly to CPU implementations [ Ernst and Greiner\n2008 ].",
			  "BVH Construction.* Starting with an efcient binary BVH [ Stich\net al . 2009 ], a similarly efcient wide BVH can be constructed ef-\nciently by pulling up individual nodes to create a BVH of a specifc\nwidth [ Wald et al . 2008 ]. Similarly, fast construction methods for\nbinary BVHs [ Karras 2012 ; Lauterbach et al . 2009 ; Selgrad et al .\n2015 ] can be turned into efcient methods for wide BVHs in the\nsame way",
			  "all our BVHs, we slightly adjust the node layout\n(from Aila et al. [ 2012 ]) in order to improve coalesced memory\naccesses. Te original implementation loads two bounding boxes\nand two child indices at once. Each thread individually fetches four\n*Vec4* elements and both child indices are stored in the last *Vec4*\nelement:\n**int** i = node_index * 4;\nnodes[i+0] = **vec4** (box1.min.x , box1.max.x , box1.min.y , box1.max.y );\nnodes[i+1] = **vec4** (box2.min.x , box2.max.x , box2.min.y , box2.max.y );\nnodes[i+2] = **vec4** (box1.min.z , box1.max.z , box2.min.z , box2.max.z );\nnodes[i+3] = **vec4** (child1.idx , child2.idx , 0\n, 0\n);\nIn our version, nodes are individually loaded in parts by multiple\nadjacent threads. Terefore, it is benefcial to separate individual\nbounding volumes and child indices. In case of a binary BVH, the\nfollowing layout allows coalesced memory access to particular *Vec4*\n ... \nand distributing them among threads was generally slower than\nduplica",
			  "versal.* Te traversal can be classifed as *if-if* -based [ Aila and\nLaine 2009 ]. In each iteration, a node index is fetched from the\nstack. Based on that index, either a box-intersection or a triangle-\nhit test is performed. We do not incorporate a *while-while* setup,\nsince it resulted in slower traversal speed than the straight-forward\napproac",
			  "Triangle intersection is handled similarly by each thread apply-\ning an ofset and loading one single triangle from global memory.\nConsequently, each thread tests only one bounding box or one\nsingle triangle for intersections in each iteration"
			]
		  },
		  {
			"title": "CPU-style SIMD ray traversal on GPUs",
			"url": "https://www.researchgate.net/publication/326762001_CPU-style_SIMD_ray_traversal_on_GPUs",
			"excerpts": [
			  "Lier et al. [2018] proposed a stack-based traversal algorithm for wide BVHs. The idea is to process a single node by multiple threads in a similar manner as ...Read more"
			]
		  },
		  {
			"title": "(PDF) Embree ray tracing kernels: overview and new features",
			"url": "https://www.researchgate.net/publication/305455456_Embree_ray_tracing_kernels_overview_and_new_features",
			"excerpts": [
			  "Embree is an open source ray tracing library consisting of high-performance kernels optimized for modern CPUs with increasingly wide SIMD units.Read more"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree-tutorial.pdf",
			"excerpts": [
			  " Adopt algorithms from Embree into your code.  However Embree internals change frequently!  As a library through the Embree API (recommended).Read more"
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://dl.acm.org/doi/10.1145/3105762.3105773",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH.",
			  "In addition, we present an algorithmically efficient method for converting a binary BVH into a wide BVH in a SAH-optimal fashion, and an improved method for ordering the child nodes at build time for the purposes of octant-aware fixed-order traversal."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://code.google.com/archive/p/understanding-the-efficiency-of-ray-traversal-on-gpus",
			"excerpts": [
			  "Timo Aila and Samuli Laine, Proc. High-Performance Graphics 2009 http://www.tml.tkk.fi/~timo/publications/aila2009hpg_paper.pdf. In addition to the original ...Read more"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://github.com/matt77hias/GPURayTraversal",
			"excerpts": [
			  "The results for these GPUs have been published in the following technical report: \"Understanding the Efficiency of Ray Traversal on GPUs - ...Read more"
			]
		  },
		  {
			"title": "Fused Collapsing for Wide BVH Construction",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/cgf.70213?af=R",
			"excerpts": [
			  "We propose a novel approach for constructing wide bounding volume hierarchies on the GPU by integrating a simple bottom-up collapsing ..."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://scispace.com/papers/understanding-the-efficiency-of-ray-traversal-on-gpus-kepler-15z9c6llb5",
			"excerpts": [
			  "This technical report is an addendum to the HPG2009 paper \"Understanding the Efficiency of Ray Traversal on GPUs\", and provides citable performance results ..."
			]
		  },
		  {
			"title": "Compressed Bounding Volume Hierarchies for Efficient ...",
			"url": "https://diglib.eg.org/bitstream/handle/10.2312/vmv20181258/097-102.pdf",
			"excerpts": [
			  "We then build a BVH for these segments with axis-aligned or oriented bounding boxes, avoiding the memory-heavier oriented bounding boxes in the deep levels of ...Read more"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2017/Papers-Session2/HPG2017_AcceleratedSingleRayTracing.pdf",
			"excerpts": [
			  "Encode fixed traversal orders into nodes",
			  "One order per ray signs combination (8)",
			  "Order defined by permute vector",
			  "Map node ordering to a single permute vector operation",
			  "Map stack operation to a single vector compress operation",
			  "Initially nodes are in memory order",
			  "Permute nodes (A)",
			  "Intersection test (B)",
			  "Compress (C)",
			  "Store to stack (D)",
			  "WiVe Node Order",
			  "Binary treelet with split axis labels (a)",
			  "Leaves form BVH8 node cluster",
			  "Split hierarchy determines permute vectors",
			  "BVH8 node cluster in spatial domain (b)",
			  "Ray with +x and -y signs (octant is 10b = 2)",
			  "If signs[split axis] negative: Swap default order",
			  "A)",
			  "x",
			  "x",
			  "x",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "y",
			  "B)",
			  "Permute_vector[2] =",
			  "WiVe Configurations (BVH8-half)",
			  "BVH branching-factor equals half the vector width",
			  "E.g BVH8 with 16-wide vector instructions",
			  "t min and t max values interleaved in register",
			  "Child offset and t min interleaved on stack",
			  "WiVe Configurations (BVH8-full)",
			  "BVH branching-factor equals full vector width",
			  "E.g. BVH8 with 8-wide vector instructions",
			  "t min and t max values in separate registers",
			  "Child offset and t min on separate stacks",
			  "4b",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "6",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "7",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "5",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "0",
			  "a",
			  "b",
			  "c",
			  "d",
			  "\n4b\nn\nn\nn\nn\nt $%&\nt $%&\nt $%&\nt $%&\nt $()\n*\nt $()\n'\nt $()\n+\nt $%&\n*\nt $%&\n'\nt $%&\n+\n**EVALUATION**\nWiVe\n",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels",
			"url": "https://www.embree.org/papers/2015-Siggraph-Embree-talk.pdf",
			"excerpts": [
			  "Initial AVX512 support\n 16 wide AVX512 traversal kernels\n Full AVX512 optimizations will come when\nhardware available!",
			  "\nEmbree API (C++ and ISPC)\nRay Tracing Kernel Selection\nAccel. structure\nbvh4.triangle4,\nbvh8.triangle8,\nbvh4aos.triangle1,\nbvh4.grid\n\n"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://web.cs.ucdavis.edu/~hamann/FuetterlingLojewskiPfreundtHamannEbertHPG2017PaperFinal06222017.pdf",
			"excerpts": [
			  "The sweet spot for\nthe branching factor appears to be between two and eight, depend-\ning on the hardware architecture and the implementation used. The\ntwo approaches have also been combined for hybrid traversal meth-\nods. For real world applications the most relevant approach by far is\nsingle ray traversal of multi-branching BVHs due to its straightfor-\nward integration into complex shading pipelines.",
			  ".\nOur contribution is a novel single ray traversal algorithm which\nmaps all relevant traversal operations to vector instructions, includ-\ning front-to-back ordering for multi-branching BVHs with branch-\ning factors of 8 (BVH8) or higher. The bene  t is signi  cantly reduced\nalgorithm complexity and constant-time execution, which is ideal\nfor current and future wide vector micro architectures. In ",
			  "We have evaluated our ray traversal algorithm by generating per-\nformance data based on our AVX2 and AVX-512 implementations\non the dual-socket Intel  Xeon  E5 2680v3 @ 2.5GHz (HW) and\nthe Intel  Xeon Phi  7250 @ 1.4GHz (KNL), respectively. We com-\npare our results with those obtained with Embree 2.15.0 [Wald\net al . 2014], the leading high-performance ray tracing library for\nCPUs. In order to ensure comparability of performance data, we\nintegrated our code into the open source Embree benchmark suite\nProtoray [Intel Corporation 2017a], which by default o  ers Embree\nand Nvidia  OptiX  [Parker et al . 2010] kernels. A compariso",
			  "Embree constructs a native\nBVH8 using SAH-based centroid binning[Wald 2007], which we\ndirectly convert to our own data layout retaining the exact same\ntopology. W",
			  "We have disabled spatial splits [Stich et al . 2009] to en-\n ...",
			  ". The result is a simple and highly e  cient technique for\nmulti-branching BVH coherent traversal.",
			  "WiVe promises to accelerate\nsingle ray traversal for multi-branching bounding volume hierar-"
			]
		  },
		  {
			"title": "Embree Ray Tracing Kernels: Overview and New Features",
			"url": "https://pdfs.semanticscholar.org/4f68/0f47d5b8a052fb6fa698508a42320a7f1cc4.pdf",
			"excerpts": [
			  "Embree features",
			  "Embree API (C++ and ISPC)",
			  "Ray Tracing Kernel Selection",
			  "Acceleration",
			  "structures",
			  "bvh4.triangle4",
			  "bvh8.triangle4",
			  "bvh4.quad4v",
			  "",
			  "builders",
			  "SAH Builder",
			  "Spatial Split Builder",
			  "Morton Builder",
			  "BVH Refitter",
			  "intersection",
			  "Mller-Trumbore",
			  "Single rays, ray packets (4, 8, 16), ray streams (N)",
			  "traversal",
			  "traversal",
			  "Single ray",
			  "Single ray",
			  "Packet/Hybrid",
			  "Packet/Hybrid",
			  "ray stream",
			  "ray stream",
			  "Common Vector and SIMD Library",
			  "Common Vector and SIMD Library",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)",
			  "(Vec3f, Vec3fa, vfloat4, vfloat8, vfloat16, , SSE2, SSE4.1, AVX, AVX2, AVX -512)"
			]
		  },
		  {
			"title": "8-Wide BVH Ray Stream Implementation",
			"url": "https://www.emergentmind.com/topics/8-wide-bvh-ray-stream-implementation",
			"excerpts": [
			  "An 8-wide BVH (Bounding Volume Hierarchy) ray stream implementation is a memory-efficient, SIMD-friendly approach to accelerating ray tracing ...Read more"
			]
		  },
		  {
			"title": "The Minimal Bounding Volume Hierarchy",
			"url": "https://graphics.tu-bs.de/upload/publications/minimal-bounding-volume-hierarchy.pdf",
			"excerpts": [
			  "In this paper we present the Minimal Bounding Volume Hierarchy (MVH) as a k-ary object partitioning scheme. Similar to BVHs a ray traverses this tree in a top- ...Read more",
			  "... two-level BVH which uses both un- compressed BVH nodes for the top levels and compressed nodes for the rest of the hierarchy. This idea of a two-level.Read more"
			]
		  },
		  {
			"title": "Ray Classification for Accelerated BVH Traversal",
			"url": "https://meistdan.github.io/publications/shafts/paper.pdf",
			"excerpts": [
			  "The green nodes are entry points of our traversal algorithm. The yellow nodes are visited by neither traversal method and only denote the path to candidate list.Read more"
			]
		  },
		  {
			"title": "Using Embree generated BVH trees for GPU raytracing  Interplay of Light",
			"url": "https://interplayoflight.wordpress.com/2020/07/21/using-embree-generated-bvh-trees-for-gpu-raytracing/",
			"excerpts": [
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for SIMD acceleration. Such trees are harder to use during a GPU-based traversal.",
			  "It appears that Embree is tuned for CPU-side traversal and by default it produces wide BVH trees, such as BVHs with 4 or 8 children (BVH4 or BVH8) suitable for ...Read more"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray Tracing of Incoherent Rays",
			"url": "https://www.researchgate.net/publication/220506625_Shallow_Bounding_Volume_Hierarchies_for_Fast_SIMD_Ray_Tracing_of_Incoherent_Rays",
			"excerpts": [
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006; Dammertz et al. 2008; Ernst and Greiner 2008]. Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "A 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008]. Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008 ] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ..",
			  "MBVH has a more compact memory layout than BVH, as noted by Dammertz et al. [Dammertz et al. 2008 ] and illustrated in Figure 1. Consequently, it improves the hit rate of caches and reduces external memory traffic. ...",
			  "Storing nearby nodes grouped into small subtrees closer in memory increases the L1 or L2 cache hit rates, thus reducing bandwidth required to render the scene [Aila and Karras 2010]. ...",
			  "In this paper we describe and evaluate an implementation of CPU-style SIMD ray traversal on the GPU. We show how spreading moderately wide BVHs (up to a branching factor of eight) across multiple threads in a warp can improve performance while not requiring expensive pre-processing. The presented ray-traversal method exhibits improved traversal performance especially for increasingly incoherent rays."
			]
		  },
		  {
			"title": "Multi Bounding Volume Hierarchies",
			"url": "https://www.researchgate.net/publication/4375554_Multi_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "In this work, we investigate using Multi-Bounding Volume Hierarchies (MBVH) [Ernst and Greiner 2008] [Wald et al. 2008] [Dammertz et al. 2008] in a ray tracing accelerator: this is a variant of BVH with a higher branching factor, typically 4. MBVH was originally intended to take advantage of SIMD instruction sets such as SSE in CPUs, but the technique also has general benefits: ...",
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006;Dammertz et al. 2008; Ernst and Greiner 2008] . Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ...",
			  "In case of a leaf, the active child mask is set to zero, the five-byte index points to a list of triangles, and the number of triangles is saved in the look-up byte. This somewhat flexible structure allows a saving of about 5% of memory compared to the standard 128-byte layout [Dammertz et al. 2008; Ernst and Greiner 2008] and also performs marginally faster during traversal. ...",
			  " 4-wide BVH in combination with a ray-direction ordered traversal utilizing SIMD was proposed by Dammertz et al. [2008] as well as by Ernst and Greiner [2008] . Ray-direction ordered processing is a heuristic that does not necessarily process the scene's bounding volumes in a strict front-to-back order, to save on sorting time. ...",
			  "Quad-BVH structures in which child nodes are stored together in memory, enable efficient single-ray traversal on architectures with a vector width of 4 [Ernst and Greiner 2008; Dammertz et al. 2008]. At each traversal step, the ray is tested for intersection against the 4 child nodes in parallel. ..."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de- compression. Our breadth-first BVH traversal ...Read more",
			  "om-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs.",
			  " stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections",
			  " utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests ",
			  "ur breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  " novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive intersections. We utilize the well known parallel primitives scan and segmented scan in order to*\n*process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and de-*\n*compress",
			  "A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack",
			  "Ray sorting is used to store spatially coherent rays in\nconsecutive memory locations. Compared to unsorted rays,\nthe tracing routine for sorted rays has less divergence on a\nwide SIMD machine such as GPU.",
			  "We explicitly maintain ray coherence in our pipeline\nby using this procedure.",
			  "his algorithm amortizes the cost of node access\npattern among the rays. A stack is used to defer intersection\ntests for neighboring nodes within a BVH. Our breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack.",
			  "The binary BVH is\nconstructed on the CPU and 2/3 rds of tree levels are\neliminated and an Octo-BVH is created (all the nodes are\nstored in a breadth-first storage layout).",
			  "he traversal pro-\ncedure we perform intersection tests for each frustum cor-\nresponding to *Qin.FrustumIDs* *i* with all the children of the\nBVH-node specified by *Qin.NodeIDs* *i* (usi",
			  "he new ray tracing pipeline provides the possibility to\ntrace relatively big packets of rays and perform efficient\nview-independent queries using a breadth-first frustum\ntraversal. Memory access patterns for breadth-first traversal\nare coherent as we perform operations in parallel for each\nBVH level (and the BVH is stored in a breadth-first lay-\nout)",
			  "Since we store all the leaf references for all the frustums\nthe memory consumption may be considerable (and we\nalso store the rays). But this consumption may be reduced\nthrough using a screen-space tiling (send reasonably big\ntiles to render on the GPU) or even frustum depth tiling.",
			  "Abstract"
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/10.1145/2601097.2601222",
			"excerpts": [
			  "While each new generation of processors gets larger caches and more compute power, external memory bandwidth capabilities increase at a much lower pace. Additionally, processors are equipped with wide vector units that require low instruction level divergence to be efficiently utilized.",
			  "Our main contribution is an efficient algorithm for traversing large packets of rays against a bounding volume hierarchy in a way that groups coherent rays during traversal.",
			  "Ray tracing algorithms is a mature research field in computer graphics, and despite this, our new technique increases traversal performance by 36--53%, and is applicable to most ray tracers.",
			  "In order to exploit these trends for ray tracing, we present an alternative to traditional depth-first ray traversal that takes advantage of the available cache hierarchy, and provides high SIMD efficiency, while keeping memory bus traffic low.",
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing."
			]
		  },
		  {
			"title": "Dynamic Ray Stream Traversal",
			"url": "http://cseweb.ucsd.edu/~ravir/274/15/papers/drst.pdf",
			"excerpts": [
			  "Divergence leads to underutilization since SIMD lanes will\nneed to be masked out. This is not a trait generally attributed to\ndepth-first traversal. Even when packets of rays are traced in a\nSIMD fashion, rays usually diverge quickly when incoherent, such\nas for diffuse interreflections, for example.",
			  "r algorithm is designed to\nhave a predictable memory access pattern with high data coherence,\nwhich substantially reduces the amount of memory bandwidth us-\nage in our tests.",
			  "Despite this, our results show that total ray tracing performance can\nbe improved by 2237%, while traversal alone is increased by 36\n53%, which is rather remarkable.",
			  "ck-less ray traver-\nsal [Hughes and Lim 2009; Laine 2010; Hapala et al. 2011; Bar-\nringer and Akenine-Moller 2013] is a more recent endeavor that\nwas, at least initially, motivated by the high overhead of main-\ntaining a traversal stack on previous generations of G",
			  "ually, a stack is maintained that contains the next node to be pro-\ncessed during ray traversal. The technique has been combined with\nvarious forms of ray sorting and tracing of whole packets [Wald\net al. 2001] of rays to improve performance.",
			  " rays in the ray stream take the same path in the tree, which decreases\ndivergence and increases SIMD utilization.",
			  ".\nEmbree also includes a set of packet traversal kernels [Wald et al.\n2001], as well as hybrid kernels that starts with packets and\nswitches to single-ray traversal as utilization becomes low [Ben-\nthin et al. 2012]. ",
			  "To use these kernels, it is the responsibility of the\nrenderer to manage ray packets.",
			  "s. The example renderer that supports\nthese kernels constitutes a total rewrite of the single-ray renderer us-\ning *ISPC* [Pharr and Mark 2012], which makes the entire renderer\nvectorized.",
			  "which makes the entire renderer\nvectorized. This makes it a bit difficult to compare performance di-\nrectly with our algorithm and single-ray traversal.",
			  "However, we have chosen to keep the scalar shaders"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving | Research",
			"url": "https://research.nvidia.com/publication/2022-01_gpu-subwarp-interleaving",
			"excerpts": [
			  "In this paper, we present an architectural enhancement called Subwarp Interleaving that exploits thread divergence to hide pipeline stalls in divergent ...Read more"
			]
		  },
		  {
			"title": "GPU Subwarp Interleaving - Cloudfront.net",
			"url": "https://d1qx31qr3h6wln.cloudfront.net/publications/Damani_Subwarp_Interleaving_HPCA_IT_2022.pdf",
			"excerpts": [
			  "Subwarp Interleaving allows for fine-grained interleaved execution of diverged paths within a warp with the goal of increasing hardware utilization and reducing ...Read more"
			]
		  },
		  {
			"title": "Dynamic Stackless Binary Tree Traversal",
			"url": "https://jcgt.org/published/0002/01/03/paper.pdf",
			"excerpts": [
			  "We evaluate our algorithm using a ray tracer with a bounding volume hierarchy for which source code is supplied. 1. Introduction. Traversing a ..."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Section Title: Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware > Abstract\nContent:\nRecent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  " key insight is that typical control flow inten-\nsive CUDA applications exhibit sufficient control flow lo-\ncality within the group of scalar threads used for bulk-\nsynchronous parallel execution that full DWF and/or MIMD\nflexibility is not necessary to regain most of the perfor-\nmance loss due to branch diverge",
			  "he compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  " The\nprevious best (on average) policy known, majority, incurs\npoor performance when a small number of threads falls\nbehind the majority of threads",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o",
			  "Dynamic warp formation,\n",
			  " dynamic warp\nformation and scheduling mechanism.",
			  "rn *graphics processing unit* (GPU) can be\nviewed as an example of the latter approach [18, 28, 6].\nEarlier generations of GPUs consisted of fi xed function 3D\nrendering pipelines.\nThis required new hardware to en-\nable new real-time rendering techniques, which impeded\nthe adoption of new graphics algorithms and thus motivated\nthe introduction of programmability, long available in tradi-\ntional of fl ine computer animation [35], into GPU hardwar",
			  "One approach is to add a stack*\n*to allow different SIMD processing elements to execute dis-*\n*tinct program paths after a branch instruc",
			  "Dynamic warp formation improves performance by cre-\nating new thread warps out of diverged warps as the shader\nprogram execute",
			  "Every cycle, the thread scheduler tries to\nform new warps from a pool of ready threads by combining\nscalar threads whose next PC values are the same.",
			  "We\nexplore the design space of this scheduling policy in detail\nin Section 4.3.",
			  "e thread scheduler policy is\ncritical to the performance impact of dynamic warp forma-\ntion in Section 6."
			]
		  },
		  {
			"title": "Thread block compaction for efficient SIMT control flow",
			"url": "https://ieeexplore.ieee.org/document/5749714/",
			"excerpts": [
			  "Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data cores to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22% over a baseline per-warp, stack-based reconvergence mechanism, and 17% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "A survey of architectural approaches for improving GPGPU ...",
			"url": "https://mkhairy.github.io/Docs/jpdc-survey.pdf",
			"excerpts": [
			  "Fung and Aamodt [46] proposed thread block compaction (TBC) that allows a group of warps, that belong to the same thread block, to share the same PDOM stack.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://www.researchgate.net/publication/392717030_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing | Request PDF",
			"url": "https://www.researchgate.net/publication/341162869_On_Ray_Reordering_Techniques_for_Faster_GPU_Ray_Tracing",
			"excerpts": [
			  "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully ...Read more"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[8] S. Damani, M. Stephenson, R. Rangan, D. Johnson, R. Kulkami, and S. W. Keckler, Gpu subwarp interleaving, in *International Symposium on High-Performance Computer Architecture (HPCA)* , 2022.",
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011",
			  " [14] W. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, Dynamic warp formation and scheduling for efficient gpu control flow, in *International Symposium on Microarchitecture (MICRO)* , 2007. "
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Evaluated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path",
			  "SIMT execution.",
			  "**Acknowledgments**",
			  "The authors would like to thank Wilson Fung, Tayler",
			  "Hetherington, Ali Bakhoda, Timothy G. Rogers, Ayub",
			  "Gubran, Hadi Jooybar and the reviewers for their insightful",
			  "feedback. This research was funded in part by a Four Year",
			  "Doctoral Fellowship from University of British Columbia,",
			  "the Natural Sciences and Engineering Research Council of",
			  "Canada and a grant from Advanced Micro Devices Inc.",
			  "[9] W. Fung, I. Sham, G. Yuan, and T. Aamodt. Dynamic Warp",
			  "Formation and Scheduling for Efficient GPU Control Flow.",
			  "[10] W. W. L. Fung and T. M. Aamodt.",
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages",
			  "2536, 2011.",
			  "[6] S. Collange. Stack-less SIMT Reconvergence at Low Cost.",
			  "hnical Report hal-00622654, ARENAIRE - Inria Greno-\nble Rhne-Alpes / LIP Laboratoire de lInformatique du Par-\nalllisme, 2011",
			  ". Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "W. Fung, I. Sham, G. Yuan, and T. Aamodt.",
			  "Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "In *Proc. IEEE/ACM Symp. on Microarch. (MICRO)* , pages",
			  "407420, 2007.",
			  "407420, 2007.",
			  " Dynamic Warp\nFormation and Scheduling for Efficient GPU Control Flow.\nI",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "valuated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path\nSIMT execution",
			  "**References**"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://arxiv.org/html/2506.11273v1",
			"excerpts": [
			  "Garanzha and Loop (2010) Kirill Garanzha and Charles Loop. 2010. Fast Ray Sorting and Breadth-First Packet Traversal for GPU Ray Tracing. *Computer Graphics Forum* 29, 2 (2010), 28929",
			  "/2506.11273v1.bib12) ) used breadth-first packet traversal after a ray sorting step. They proposed the idea of sorting rays to reduce divergence in computation using a hash-based method for sorting the rays into coherent packets. T",
			  "In addition, the rays are grouped into frusta, which are further tested against the scene as proposed by Reshetov\net al . ( [200",
			  "This way, the total number of intersection tests is reduced.",
			  "While they report impressive speedups for primary rays and deterministic ray tracing, this does not translate to path tracing because the frusta become too large and intersect most of the scene.",
			  "For production rendering, not only the trace kernel but also shading might be limited by memory bandwidth. Therefore, Eisenacher et al . ( [2013](https://arxiv.org/html/2506.11273v1.bib11) ) proposed to sort termination points to improve shading performance. While this approach is designed for out-of-core path tracing, grouping shading calculations by a material also improves in-core performance for complex shaders. For highly detailed scenes, Hanika\net al . ( [2010](https://arxiv.org/html/2506.11273v1.bib15) ) proposed to use a two-level hierarchy combined with ray sorting to facilitate efficient on the fly micro-polygon tessellation. The rays are traversed through the top-level hierarchy, and they are repeatedly sorted to determine sets of rays traversing the same leaf nodes of the top-level hierarchy.",
			  "When coherence among rays exists, the packet traversal (Gunther\net al . , [2007](https://arxiv.org/html/2506.11273v1.bib14) ) exploits it by forcing a SIMD processing of a group of rays. This, on the other hand, increases inter-thread communication and synchronization. Furthermore, it assumes high ray coherence and is significantly slower than depth-first traversal for incoherent rays. Bikker ( [2012](https://arxiv.org/html/2506.11273v1.bib7) ) proposed a packet traversal algorithm that uses batching to improve data locality.",
			  " \nSection Title: On Ray Reordering Techniques for Faster GPU Ray Tracing > References\nContent:\nDynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In *Proceedings of IEEE Symposium on Interactive Ray Tracing* . 95104. Nimier-David et al . (2019) Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2: A Retargetable Forward and Inverse Renderer. *ACM Transactions on Graphics* 38, 6 (2019), 203. Pharr et al . (1997) Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. 1997. Rendering Complex Scenes with Memory-coherent Ray Tracing. In *Proceedings of International Conference on Computer Graphics and Interactive Techniques* . 101108. Reis et al . (2017) Nuno T. Reis, Vasco S. Costa, and Joo M. Pereira. 2017. Coherent Ray-Space Hierarchy via Ray Hashing and Sorting. In *Proceedings of International Joint Conference on Computer Vision, Imaging, and Computer Graphics Theory and Applications* . Resh",
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal."
			]
		  },
		  {
			"title": "[PDF] Two-level ray tracing with reordering for highly complex scenes | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Two-level-ray-tracing-with-reordering-for-highly-Hanika-Keller/ee5673f3141a61924798d7642f06971dd41d871c",
			"excerpts": [
			  "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision Surfaces",
			  "Nikolaus Binder](/author/Nikolaus-Binder/2231395) [A. Keller](/author/A.-Keller/145661463)",
			  "* 2018",
			  "Besides introducing an optimized method to determine axis aligned bounding boxes of Gregory patches restricted in the parametric domain, several techniques are introduced that accelerate the recursive subdivision process including stackless operation, efficient work distribution, and control flow optimizations.",
			  "TLDR"
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Proceedings of High Performance Graphics",
			"url": "https://dl.acm.org/doi/10.5555/2977336.2977343",
			"excerpts": [
			  "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time ... Stackless multi-BVH traversal for CPU, MIC and GPU ray tracing."
			]
		  },
		  {
			"title": "Extending GPU Ray-Tracing Units for Hierarchical Search ...",
			"url": "https://engineering.purdue.edu/tgrogers/publication/barnes-micro-2024/barnes-micro-2024.pdf",
			"excerpts": [
			  "Binder and A. Keller, Efficient Stackless Hierarchy Traversal on. GPUs with Backtracking in Constant Time, in Proceedings of High. Performance Graphics (HPG).Read more"
			]
		  },
		  {
			"title": "A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees",
			"url": "https://jcgt.org/published/0014/01/03/paper.pdf",
			"excerpts": [
			  "BINDER, N. AND KELLER, A. Efficient stackless hierarchy traversal on GPUs with back- tracking in constant time. In Proceedings of High ...Read more"
			]
		  },
		  {
			"title": "[PDF] Thread block compaction for efficient SIMT control flow",
			"url": "https://www.semanticscholar.org/paper/Thread-block-compaction-for-efficient-SIMT-control-Fung-Aamodt/8bd6f67ef03b3c138c52f3e9b1716aebe937d244",
			"excerpts": [
			  "This paper proposes and evaluates the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, ..."
			]
		  },
		  {
			"title": "HIPRT: A Ray Tracing Framework in HIP",
			"url": "https://gpuopen.com/download/HIPRT-paper.pdf",
			"excerpts": [
			  "Binder and Keller [2016] proposed backtracking in constant time based on a path to the node in a bitset and perfect hashing. 3 HIPRT API OVERVIEW. Similarly to ...Read more",
			  "Later, Ylitie et al. [2017] showed that a compressed. 8-wide BVH can reduce memory traffic even further, improving the overall ray tracing performance.Read more"
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "http://www.danielwong.org/classes/_media/ee260_w17/threadblockcompaction.pdf",
			"excerpts": [
			  ", we propose and*\n*evaluate the benefits of extending the sharing of resources*\n*in a block of warps, already used for scratchpad mem-*\n*ory, to exploit control flow locality among threads (where*\n*such sharing may at first seem detrimental). In our pro-*\n*posal, warps within a thread block share a common block-*\n*wide stack for divergence handling. At a divergent branch,*\n*threads are compacted into new warps in har",
			  "he modifications consist of three major parts: a modi-\nfied branch unit ( 1 ), a new hardware unit called the thread\ncompactor ( 2 ), and a modified instruction buffer called the\nwarp buffer ( 3 ). The branch unit ( 1 ) has a block-wide re-\nconvergence stack for each block. Each entry in the stack\nconsists of the starting PC (PC) of the basic block that cor-\nresponds to the entry, the reconvergence PC (RPC) that in-\ndicates when this entry will be popped from the stack, a\nwarp counter (WCnt) that stores the number of compacted\nwarps this entry contains, and a block-wide activemask that\nrecords which thread is executing the curre",
			  " comparison to DWF [9], thread block compaction ac-\ncomplishes the lookup-and-merge operation of the warp\nLUT and the warp pool [9] with simpler hardware. In\nDWF, an incoming warp is broken down every cycle and\n .."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow | Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2007.12",
			"excerpts": [
			  "We show that a realistic hardware im- plementation that dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes improves performance by an average of 20.7% for an estimated area increase of 4.7%."
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Research",
			"url": "https://research.nvidia.com/publication/2016-06_efficient-stackless-hierarchy-traversal-gpus-backtracking-constant-time",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling and use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or state memory. We show that the next node in such a traversal actually can be determined in constant time and state memory. In fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and outperforms the fastest stack-based algorithms on GPUs. In addition, it reduces memory access during traversal, making it a very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal on GPUs with ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/1c026ceb-0c54-4e20-9fd2-2ff77222894d/content",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling",
			  "nd use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or*\n*state memory",
			  "We show that the next node in such a traversal actually can be determined in constant time and state memory.",
			  "*fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and*\n*outperforms the fastest stack-based algorithms on GPUs.",
			  " addition, it reduces memory access during traversal, making it a*\n*very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal with Backtracking in ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2016/2016-HPG-Binder-StacklessTraversalPerfectHash.pdf",
			"excerpts": [
			  "Efficient Stackless Hierarchy Traversal with Backtracking in Constant Time",
			  "Results: Performance in M rays/s, NVIDIA Titan X, for Primary/Shadow/Diffuse Rays"
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Software Developer's Manual ...",
			"url": "https://kib.kiev.ua/x86docs/Intel/SDMs/326018-062.pdf",
			"excerpts": [
			  "Page 1. Intel 64 and IA-32 Architectures. Software Developer's Manual ... _mm512_i64gather_epi64( __m512i vdx, void * base ... Intel C/C++ Compiler Intrinsic ..."
			]
		  },
		  {
			"title": "\n\tGather of byte/word with avx2 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Gather-of-byte-word-with-avx2/td-p/921687",
			"excerpts": [
			  "1) Use _mm256_i32gather_epi32. We would fetch an extra 16-bits that we do not want for each voxel, and then either mask off the extra bits or ...Read more"
			]
		  },
		  {
			"title": "_mm_i32gather_epi32, _mm256_i32gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-1F275401-A760-49B1-944A-B02C075514D8.htm",
			"excerpts": [
			  "Gathers 2/4 doubleword values from memory referenced by the given base address, dword indices, and scale. The corresponding Intel AVX2 instruction is"
			]
		  },
		  {
			"title": "_mm_i64gather_epi32, _mm256_i64gather_epi32",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/compiler_c/main_cls/intref_cls/common/intref_avx2_mm256_i64gather_epi32.htm",
			"excerpts": [
			  "Gather 2/4 doubleword values from memory referenced by the given base address, qword indices and scale. The corresponding Intel AVX2 instruction is VPGATHERQD."
			]
		  },
		  {
			"title": "An introduction to Arm Scalable Vector Extensions",
			"url": "https://epicure-hpc.eu/wp-content/uploads/2025/03/SVE_Vectorization_Ricardo_Fonseca.pdf",
			"excerpts": [
			  "SVE gather / scatter operations.  Gather operations.  Gather scalar values ...  https://developer.arm.com/documentation/101458/2404/Optimize.  Arm ...Read more"
			]
		  },
		  {
			"title": "Mirror of Intel Intrinsics Guide",
			"url": "https://www.laruence.com/sse/",
			"excerpts": [
			  "Intel. Intrinsics Guide. Technologies. MMX. SSE. SSE2. SSE3. SSSE3. SSE4.1. SSE4.2 ... This intrinsic is provided for conversion between little and big endian ...Read more"
			]
		  },
		  {
			"title": "Intrinsics for Integer Gather and Scatter Operations",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-35D298CC-B89B-4B38-856B-FCD0EBB3AA23.htm",
			"excerpts": [
			  "**_mm512_i32gather_epi32**\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i32gather_epi32**\nSection Title: Intrinsics for Integer Gather and Scatter Operations\nContent:\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i32scatter_epi32**\nScatters int32 from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ).",
			  "**_mm512_mask_i32scatter_epi64**\nScatters int64 from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ) subject to mask k (elements are not stored when the corresponding mask bit is not set).",
			  "**_mm512_i64scatter_epi64**\nScatters int64 from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale )."
			]
		  },
		  {
			"title": "Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel2021.7/HPC/cpp_compiler/cpp_compiler_classic_dev_guide/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.html",
			"excerpts": [
			  "\nThe prototypes for Intel Advanced Vector Extensions 512 (Intel AVX-512) intrinsics are located in the zmmintrin.h header file.\nTo u",
			  "To use these intrinsics, include the immintrin.h file as follows:",
			  "ement.\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Data Types for Intel AVX-512 Intrinsics\nC",
			  "s:\nIntel AVX-512 intrinsics have vector variants that use __m128 , __m128i , __m128d , __m256 , __m256i , __m256d , __m512 , __m512i , and __m512d data types.",
			  ".\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Naming and Usage Syntax\nCon"
			]
		  },
		  {
			"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
			"excerpts": [
			  "**AVX2 has gathers (not scatters), but they're only fast on Skylake and newer** . They're ok on Broadwell, slowish on Haswell, and slow on AMD. (Like one per 12 clocks for Ryzen's `vpgatherqq` ). See http://agner.org/optimize/ and other performance links in [the x86 tag wiki](https://stackoverflow.com/tags/x86/info) .\nIntel's optimization manual has a small section on manual gather / scatter (using insert/extract or `movhps` ) vs. hardware instructions, possibly worth reading. In this case where the indices are runtime variables (not a constant stride or something), I think Skylake can benefit from AVX2 gather instructions here",
			  "**extract indices, manually gather into a vector with `vmovq` / `vmovhps` for a SIMD `vpor` , then scatter back with `vmovq` / `vmovhps`** .Just like a HW gather/scatter, **correctness requires that all indices are unique** , so you'll want to use one of the above options until you get to that point in your algo. (vector conflict detection + fallback would not be worth the cost vs. just always extracting to scalar: [Fallback implementation for conflict detection in AVX2](https://stackoverflow.com/questions/44843518/fallback-implementation-for-conflict-detection-in-avx2) ).See [selectively xor-ing elements of a list with AVX2 instructions](https://stackoverflow.com/questions/50583718/selectively-xor-ing-elements-of-a-list-with-avx2-instructions) for an intrinsics version.",
			  "**AVX2 `vpgatherqq` for the gather ( `_mm256_i64gather_epi64(sieveX, srli_result, 8)` ), then extract indices and manually scatter.** So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather. (Two 128-bit gathers cost more than one 256-bit gather, so you would want to take the instruction-level parallelism hit and gather into a single 256-bit register).",
			  "Possibly a win on Skylake (where `vpgatherqq ymm` is 4 uops / 4c throughput, plus 1 uop of setup), but not even Broadwell (9 uops, one per 6c throughput) and definitely not Haswell (22 uops / 9c throughput). You do need the indices in scalar registers anyway, so you're *only* saving the manual-gather part of the work. That's pretty cheap.",
			  "**manual gather/scatter: 20 uops, 5 cycles of front-end throughput** (Haswell / BDW / Skylake). Also good on Ryzen.",
			  "**Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.** (Worse on any earlier uarch or on AMD).",
			  "\nvextracti128 indices (1 uop for port 5)",
			  "2x vmovq extract (2p0)",
			  "2x vpextrq (4 = 2p0 2p5)",
			  "`vpcmpeqd ymm0,ymm0,ymm0` create an all-ones mask for `vpgatherqq` (p015)",
			  "`vpgatherqq ymm1, [rdi + ymm2*8], ymm0` 4 uops for some ports.",
			  "`vpor ymm` (p015)",
			  "vextracti128 on the OR result (p5)\n",
			  "2x vmovq store (2x 1 micro-fused uop, 2p23 + 2p4). Note no port7, we're using indexed stores.",
			  "2x vmovhps store (2x 1 micro-fused uop, 2p23 + 2p4)."
			]
		  },
		  {
			"title": "c++ - AVX2 Gather Instruction Usage Details - Stack Overflow",
			"url": "https://stackoverflow.com/questions/58832024/avx2-gather-instruction-usage-details",
			"excerpts": [
			  "The offsets in `vindex` are in bytes. Therefore, you gather 32-bit integer values from addresses `{arr, arr+2, arr+4, ...}` .",
			  "Either change these indexes from `{0,2,4...}` to `{0,8,16,...}` , or update the scale factor as:\nThis prints out the expected values."
			]
		  },
		  {
			"title": "Arm C Language Extensions",
			"url": "https://arm-software.github.io/acle/main/acle.html",
			"excerpts": [
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B)",
			  "\nThe SVE ACLE intrinsics have the form:\nwhere the individual parts are as follows:\n**base**\nFor most intrinsics this is the lower-case name of an SVE\ninstruction, but with some adjustments:The most common change is to drop `F` , `S` and `U` if they\nstand for floating-point, signed and unsigned respectively,\nin cases where this would duplicate information in the type\nsuffixes below.Simple non-extending loads and non-truncating stores drop the\nsize suffix ( `B` , `H` , `W` or `D` ), which would always duplicate\ninformation in the suffixes.Conversely, extending loads always specify an explicit extension\ntype, since this information is not available in the suffixes.\nA sign-extending load has the same base as the architectural\ninstruction (for instance, `ld1sb` ) while a zero-extending load replaces\nthe `s` with a `u` (for instance, `ld1ub` for a zero-extending `LD1B` ).\nThus [`svld1ub_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1ub_u32) zero-extends 8-bit data to a vector of `uint32_t` s while [`svld1sb_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1sb_u32) sign-extends 8-bit data to a vector of `uint32_t` s.",
			  "| LD1D (vector plus immediate) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather%5B) |",
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B) ",
			  "| ST1D (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) |",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |"
			]
		  },
		  {
			"title": "Arm Scalable Vector Extension and application to Machine ...",
			"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Arm-scalable-vector-extensions-and-application-to-machine-learning.pdf",
			"excerpts": [
			  "the function svld1_gather_u32base_offset_s32 , with signature\nsvint32_t svld1_gather_u32base_offset_s32(svbool_t pg, svuint32_t bases, int64_t\noffset)\nis a *gather load* ( ld1_gather ) of *signed 32-bit integer* ( _s32 ) from a vector of *unsigned 32-bit integer* base\naddresses ( _u32base ) plus an *offset in bytes* ( _offset ).",
			  "The SVE ACLE are compatible with C++ overloading and C _Generic association, so that the names\ncan be contracted removing those parts that can be derived from the arguments types.",
			  "The SVE ACLE (or ACLE hereafter) is a set of functions and types that exposes the vectorization capabilities of\nSVE at C/C++ level.",
			  "They introduce a set of *size-less* types and *intrinsic functions* that a C/C++ compiler can directly convert into\nSVE assembly.",
			  "An additional svbool_t type is defined to represent predicates for masking operations.",
			  "SVE intrinsic functions",
			  "The naming convention of the intrinsic functions in the SVE ACLE is described in detail in section 4 of the SVE\nACLE document (ARM limited 2017b).",
			  "Most of them are in the form: svbase[_disambiguator][_type0][_type1]...[_predication] .",
			  "For example, the name of the intrinsic svadd_n_u16_m , with signature svuint16_t svadd_n_u16_m(svbool_t\npg, svuint16_t op1, uint16_t op1) , describes a vector *addition* ( add ) of *unsigned 16-bit integer* ( u16 ),\nwhere one of the arguments is a scalar ( _n ) and the predication mode is *merging* ( _m ).",
			  "Some of the functions, like loads and stores, have a different form for the names, with additional tokens that\nspecify the addressing mode.",
			  "All the examples of this document use the short form. For simplicity, we also assume no aliasing, meaning that"
			]
		  },
		  {
			"title": "Arm C Language Extensions for SVE - Version 00bet6",
			"url": "https://rci.stonybrook.edu/sites/default/files/documents/acle_sve_100987_0000_06_en.pdf",
			"excerpts": [
			  "COMPACT: Compact vector and fill with zero",
			  "These functions concatenate the active elements of the input vector, filling any remaining elements with\nzero.",
			  "6.23. Predicate creation",
			  "6.23.1. PTRUE: Return an all-true predicate for a given pattern",
			  "These functions return an all-true predicate for a particular vector pattern and element size. When an\nelement has more than one predicate bit associated with it, only the lowest of those bits is ever true.",
			  "There are two forms: one with a _pat suffix that takes an explicit vector pattern and one without a _pat\nsuffix in which the pattern is implicitly SV_ALL .",
			  "svbool_t **svptrue_b8** ()",
			  "svbool_t **svptrue_b16** ()",
			  "svbool_t **svptrue_b32** ()",
			  "svbool_t **svptrue_b64** ()"
			]
		  },
		  {
			"title": "ARM's Scalable Vector Extensions: A Critical Look at SVE2 ...",
			"url": "https://gist.github.com/zingaburga/805669eb891c820bd220418ee3f0d6bd",
			"excerpts": [
			  "Under ACLE, NEON  SVE value transfer must go through memory. Interestingly ... SVE adds support for gather/scatter operations, which helps vectorize ..."
			]
		  },
		  {
			"title": "_mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-8B147603-6A6A-4F23-8529-1609A13AB784.htm",
			"excerpts": [
			  "extern __m512i __cdecl _mm512_i32gather_epi32(_m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "tent:\n| extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "The non-masked variant of the intrinsic is equivalent to the masked variant with full mask ( k1 =0xffff).",
			  ":\nGather int32 vector with int32 indices. Corresponding instruction is VPGATHERDD . This intrinsic only applies to Intel Many\nIntegrated Core Architecture (Intel MIC Architecture).\n",
			  "Up-converts a set of 16 memory locations pointed by base address mv and int32 index vector index with scale scale , and gathers them into a int32 vector.",
			  "The resulting vector for the masked variant is populated by elements for which the corresponding bit in the writemask vector k1 is set. The remaining elements of the resulting vector for the masked variant is populated by corresponding elements from v1_old .",
			  "Returns the result of the up-convert load operation."
			]
		  },
		  {
			"title": "x86 Intrinsics Cheat Sheet",
			"url": "https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf",
			"excerpts": [
			  "AVX2\n**mi** **i64gather_epi32**\n( **i*** ptr, **mi** a, **i** s)\n**FOR** j := 0 to 1;\ni := j*32\nm := j*64\n**dst** [i+31:i] :=\n*(ptr + a[m+63:m]*s])\n**dst** [MAX:64] := 0\nMask Gather\nmask_ *i32* / *i64* gather\nepi32-64,ps/d\n**NOTE:** Same as gather but takes\nan additional mask and src\nregister. Each element is only\ngathered if the highest\ncorresponding bit in the mask is\nset. Otherwise it is copied from\nsrc. Memory does not need to\nbe aligned.",
			  "AVX2\n**mi mask_i64gather_epi32** ( **mi** src,\n**i*** ptr, **mi** a, **mi** mask, **i32** s)\n**FOR** j := 0 to 1; i:=j*32;m:=j*64\n**IF** mask[i+31]\n**dst** [i+31:i]:=*(ptr+a[i+63:i]*s)\nmask[i+31] := 0\n**ELSE**\n**dst** [i+31:i] := src[i+31:i]\nmask[MAX:64] := 0\n**dst** [MAX:64] := 0\n256bit Insert\ninsertf128\nsi256,ps/d\n**m** **insertf128_ps** ( **m** a, **m** b, **ii** i)\n**dst** [255:0] := a[255:0]\nsel := i*128\n**dst** [sel+15:sel]:=b[127:0]"
			]
		  },
		  {
			"title": "Filtering a Vector with SIMD Instructions (AVX-2 and AVX-512) | Quickwit",
			"url": "https://quickwit.io/blog/simd-range",
			"excerpts": [
			  "Let's start with `compact` .\nAVX2 does not exactly have an instruction for this. In the 128-bits world, [`PSHUFB`](https://www.felixcloutier.com/x86/pshufb.html#:~:text=PSHUFB%20performs%20in%2Dplace%20shuffles,leaving%20the%20shuffle%20mask%20unaffected.) is a powerful instruction that\nlets you apply a permutation over the bytes of your register.",
			  "The equivalent instruction exists and is called `vpshufd` , but there is a catch: it only\napplies two disjoint permutations within the two 128-bits lanes, which is perfectly useless to us.",
			  "This is a very common pattern in AVX2 instructions. Instructions crossing that dreaded 128bit-lane are seldom.",
			  "\nFortunately, applying a permutation over `u32s` (which is what we need) is actually possible,\nvia the `VPERMPS` instruction [__mm256_permutevar8x32_epi32](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-vector-extensions-2/intrinsics-for-permute-operations/mm256-permutevar8x32-epi32.html) .",
			  "Then I did what every sane engineer should do. I asked [Twitter](https://twitter.com/fulmicoton/status/1539534316405161984) (Ok I am lying a bit, at the time of the tweet I was playing with SSE2).",
			  "a single byte. The instruction is called `VMOVMSKPS` . I could not find it because it is presented as a floating point instruction to extract the sign of a bunch of 32-bits floats.\n"
			]
		  },
		  {
			"title": "On the usage of the Arm C Language Extensions for a High ...",
			"url": "https://hal.science/hal-03029933v1/document",
			"excerpts": [
			  "In our case, we\nuse the following code:\nauto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "the gath-\nering step of our kernel, we first need to gather indices of\nthe first point of each element considered.",
			  "At order 4, each\nelement is composed of 125 points.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "In our case, we\nuse the following code:",
			  "auto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "At iteration i , the first element of the vector is at position\ni*svcntw()*125 and we need to duplicate it and add the\nvstrides values to obtain the indices of the first point of\neach element in the vector:"
			]
		  },
		  {
			"title": "Introduction to SVE",
			"url": "https://documentation-service.arm.com/static/67ab35a4091bfc3e0a9478b5?token=",
			"excerpts": [
			  "The ACLE (Arm C Language Extension) for SVE defines which SVE instruction functions\nare available, their parameters and what they do.",
			  " To use the ACLE intrinsics,\nyou must include the header file arm_sve.h, which contains a list of vector types and instruction\nfunctions (for SVE) that can be used in C/C++.",
			  "The following example C code has been manually optimized with SVE intrinsics:\n//intrinsic_example.c\n\\ <arm_sve.h>\nsvuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)\n{\n// widening add of even elements\nsvuint64_t result = svaddlb(Zs1, Zs2);\nreturn result;\n}"
			]
		  },
		  {
			"title": "\n\tAgner's tables show the - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Throughput-MUL-FMA-Broadwell/m-p/1151730/highlight/true",
			"excerpts": [
			  "Agner's tables show the throughput for sequences of independent instructions, and the latency for sequences of dependent instructions."
			]
		  },
		  {
			"title": "Release Notes for Intel Intrinsics Guide",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/release-notes/intrinsics-guide-release-notes.html",
			"excerpts": [
			  "Removed extended gather/scatter intrinsics."
			]
		  },
		  {
			"title": "Surprising new feature in AMD Ryzen 3000 | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24302057",
			"excerpts": [
			  " But the scatter/gather instructions do random access memory operations. You have one SIMD register with a 8 (or whatever the width is) indexes to be applied to a base address in a scalar register, and the hardware then goes and does 8 separate memory operations on your behalf, packing the results into a SIMD register at the end.",
			  "That has to hit the cache 8 times in the general case. It's extremely expensive as a single instruction, though faster than running scalar code to do the same thing.",
			  "I looked Agner's tables, and was curious how Intel fared with it. All numbers are reciprocal throughput. So how many cycles per instruction in throughput. zen 2 has it's gather variants mostly 9 and 6 cycles and one variant with 16. Broadwell has only 6,7 and 5 cycles.",
			  "Skylake has mostly 4 and 2 and one variant with 5.",
			  "Now I was surprised by Agners figures for zen2 LOOP and CALL which both have reciprocal throughput of 2. Being equal to doing with just normal jump instructions."
			]
		  },
		  {
			"title": "4. Instruction tables",
			"url": "https://www.agner.org/optimize/instruction_tables.pdf",
			"excerpts": [
			  "VPGATHERDD\nx,[r+s*x],x\n24\n5\nP0123\nAVX2",
			  "VPGATHERDD\ny,[r+s*y],y\n42\n8",
			  "VPGATHERQQ\nx,[r+s*x],x\n18\n4",
			  "VPGATHERQQ\ny,[r+s*y],y\n24\n5",
			  "VPSCATTERDD\n[r+s*x]{k},x\n27\n6",
			  "VPSCATTERQQ\n[r+s*z]{k},z\n48\n12",
			  "VPCOMPRESSD/Q\nv{k},v\n2\n2",
			  "VPEXPANDB/W/D/Q\nx{k},x\n2\n2"
			]
		  },
		  {
			"title": "(PDF) Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://www.academia.edu/145129609/Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Section Title: Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads\nContent:\n[Karthik Sankaranarayanan](https://independent.academia.edu/KarthikSankaranarayanan)\n2020, ArXiv\nvisibility\n\ndescription See full PDF download Download PDF\nbookmark Save to Library share Share\nclose"
			]
		  },
		  {
			"title": "Prefetching for complex memory access patterns",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-923.pdf",
			"excerpts": [
			  "This thesis makes three contributions. I first contribute an automated software prefetch- ing compiler technique to insert high-performance prefetches into ...Read more"
			]
		  },
		  {
			"title": "On Reusing the Results of Pre-Executed Instructions in a ...",
			"url": "https://dl.acm.org/doi/abs/10.1109/L-CA.2005.1",
			"excerpts": [
			  "Previous research on runahead execution took it for granted as a prefetch-only technique. Even though the results of instructions independent of an L2 miss ..."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/pdf/2505.21669",
			"excerpts": [
			  "Hardware prefetcher designs have been previously created with pointer-chasing access patterns in mind. Some techniques compute and store jump ...",
			  "oth et al. describe a prefetching technique in [ **50** ] that utilizes dependency chains to\ndetermine the shape of a linked data structure node in hardware, and issue requests ac-\ncordingly. While this approach requires no modification of the executable, it cannot learn\nthe full structure of an object without observing accesses to all children. Additionally, the\nauthors limit their prefetcher to only a single node ahead, meaning prefetches are only\ntriggered when the core issues new loads, *and* blocks must be returned from the memory\nsystem before new prefetches can be issuedleading to significantly worse performance\nas effective memory access latencies incr",
			  " [ **37** ] build on dependence based prefetching (DBP) as described in [ **50** ] to en-\nable *timely* prefetches of children; this is needed as increased memory access times require\nprefetches to be issued earlier to avoid stalls."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Observing the backward slice shown in Figure 3a, we see that\nthe one cycle in the graph is comprised of a single instruction\n0x6cf , *i.e.* , the stride address increment, and that it is the only\nloop-carried dependence in this backward slice.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "We duplicate the backward slice code\nand assign new registers to it. By analogy, this code is the carrot\nand the main computation is the horse.",
			  "Prior to the entry into\nthe loop, the carrot is first extended *k* iterations ahead of the horse.\nWe call this phase in the dynamic execution the *head start* phase.",
			  "After the entry into the loop, the carrot locks steps with the horse\nand stays a constant *k* iterations ahead. We call this phase in the\ndynamic execution the *stay ahead* phase.",
			  "During the last *k* iterations\nof the loop, the carrot ceases to stay ahead and merges with the\nhorse. We call this phase of dynamic execution the *join* phase.",
			  "**4**",
			  "**M** **ETHOD**",
			  "In the previous section, we explained the problem of memory-\nbound DILs through a hash table example and outlined the\nchallenges in implementing a prefetcher with helper threads",
			  "Here,\nwe will outline our approach to a solution, with a reminder that we\nwant to create a prefetcher implementation without threads.",
			  "We exclude\nsuch scenarios by design for two reasons: first, such situations\nare rare and second, prefetcher complexity increases tremendously\nin such cases.",
			  "To see why, let us consider the example of the\nbinary tree where both the paths are equally likely. If we want to\nprefetch *k* iterations ahead, then there are 2 *k* possible addresses\nto prefetch.",
			  "We have the option of either prefetching all of those\naddresses or implementing a software-based branch predictor to\nselect one of the addresses to prefetch."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "Content:\nAugust 2020\nDOI: [10.48550/arXiv.2009.00202](https://doi.org/10.48550/arXiv.2009.00202)",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/html/2505.21669v1",
			"excerpts": [
			  "[52] Sankaranarayanan, K., Lin, C.-K., and Chinya, G. N. Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads. ArXiv (Sept. 2020).",
			  "These two benchmarks ( bintree_dfs and bintree_bfs ) perform common traversals of binary tree data structures to compute a sum.",
			  "The DFS benchmark is implemented recursively, while the BFS benchmark uses a std::queue from the C++ STL.",
			  "This tree is staticonly lookups are performed.",
			  "In [ [61](https://arxiv.org/html/2505.21669v1.bib61) ] , Wang et al. describe a prefetch engine that utilizes hints from a compiler analysis to inform issued requests.",
			  "Notably, the analysis detects common cases of recursive pointers as used in linked-list traversals, which hardware then exploits to issue prefetches of the LDS up to six levels deep.",
			  "However, layout information is not provided to the hardware prefetch engineit instead speculates on values in a cache block being pointers, falling victim to the same cache pollution flaws as other CDPs.",
			  "Nodes are also assumed to be smaller than two cache blocks, which may not hold across real-world applications.",
			  "The researchers also state the technique does not perform well on trees.",
			  "Most importantly, prefetch requests for an LDS are issued sequentially, and thus performance degrades with increased effective memory access times."
			]
		  },
		  {
			"title": "(PDF) On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor",
			"url": "https://www.academia.edu/126501330/On_Reusing_the_Results_of_Pre_Executed_Instructions_in_a_Runahead_Execution_Processor",
			"excerpts": [
			  "Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs.",
			  "runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient.",
			  "In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance."
			]
		  },
		  {
			"title": "Pointer Cache Assisted Prefetching - Computer Science",
			"url": "https://cseweb.ucsd.edu/~calder/papers/MICRO-02-PCache.pdf",
			"excerpts": [
			  ".\nSpeculative precomputation [6] works by identifying the\nsmall number of static loads, known as delinquent loads, that\nare responsible for the vast majority of memory stall cycles.",
			  "\nPrecomputation slices (p-slices), sequences of dependent in-\nstructions which, when executed, produce the address of a\nfuture delinquent load, are extracted from the program be-\ning accelerated. ",
			  " When an instruction in the non-speculative\nthread that has been identified as a trigger instruction reaches\nsome point in the pipeline (typically commit or rename),\nthe corresponding p-slice is spawned into an available SMT\nthread context.\n",
			  "Speculative slices [28] focus largely on the use of precom-\nputation to predict future branch outcomes and to correlate\npredictions to future branch instances in the non-speculative\nthread, but they also support load prefetching.",
			  "\nSoftware controlled pre-execution [13] focuses on the use\nof specialized, compiler inserted code that is executed in\n",
			  "e trace to extract data reference sequences that fre-\nquently repeat in the same order. At this point, the system\ninserts prefetch instructions to detect and prefetch these fre-\nquent data references.",
			  "The sampling and optimization are\ndone dynamically at runtime with very low overhead.\n*",
			  "The Pointer Cache holds mappings between heap point-\ners and the address of the heap object they point to.",
			  "\nThe primary function of the pointer cache is to break the\nserial dependence chains in pointer chasing code. ",
			  ". When one\nload depends on the data loaded by another, a cache miss by\nthe first load forces the second load to stall until the first load\ncompletes.",
			  "When executing a long sequence of such depen-\ndent pointer-chasing loads, instructions can only be executed\nat the speed of the serial accesses to memory.",
			  "*\nOnly pointer loads are candidates to be inserted into the\npointer cache.",
			  "The\nprograms static instructions are analyzed in the reverse order\nof execution from a delinquent load, building up a slice of in-\nstructions the load is directly and indirectly dependent upon.",
			  "Slice construction terminates when an-\nalyzing an instruction far enough from the delinquent load\nthat a spawned thread can provide a timely prefetch, or when\nfurther analysis will add additional instructions to the slice\nwithout providing further performance benefits.",
			  ". In this form,\na slice consists of a sequence of instructions in the order they\nwere analyzed.",
			  "The single path slices constructed in this work are trig-",
			  "Jump pointers are a software technique for prefetching\nlinked data structures.",
			  "Artificial jump pointers are extra\npointers stored into an object that point to other objects some\ndistance ahead in the traversal order.",
			  "Natural jump pointers are existing pointers in the\ndata structure used for prefetching.",
			  "These techniques were introduced by\nLuk and Mowry [12] and refined in [11] and [17].",
			  "Chilimbi and Hirzel [4] proposed an automated\nsoftware approach based on correlation. Their scheme first\ngathers a data reference profile via sampling. Next, they pro-",
			  "Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer ...Read more",
			  "However, traditional prefetching techniques have diffi- culty with sequences of irregular accesses. A common ex- ample of this type of access is pointer chains, ..."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Speculative precomputation: long-range prefetching of delinquentloads",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Deep-Learning-Driven Prefetching for Far Memory",
			"url": "https://arxiv.org/html/2506.00384v1",
			"excerpts": [
			  "Section Title: Deep-Learning-Driven Prefetching for Far Memory > 1. Introduction",
			  "Many data-center workloads including graph processing ( [PageRank ,](https://arxiv.org/html/2506.00384v1.bib25) ; [han2024graph ,](https://arxiv.org/html/2506.00384v1.bib20) ) , tree and index structures ( [guttman1984r ,](https://arxiv.org/html/2506.00384v1.bib19) ; [gusfield1997algorithms ,](https://arxiv.org/html/2506.00384v1.bib18) ) , pointer chasing ( [hsieh2017implementing ,](https://arxiv.org/html/2506.00384v1.bib24) ) , and recursive data structures ( [harold2004xml ,](https://arxiv.org/html/2506.00384v1.bib21) ) exhibit memory access patterns that defy rule-based prefetching.",
			  "If these access patterns could be learned and predicted accurately, far-memory systems could proactively fetch data and mitigate the performance penalties associated with remote access, even in the absence of new hardware.",
			  "Figure 1. FarSight Achieving Three Key Goals Together. FarSight, FastSwap ( [fastswap ,](https://arxiv.org/html/2506.00384v1.bib2) ) , and Hermit ( [hermit ,](https://arxiv.org/html/2506.00384v1.bib35) ) are far-memory systems that run in the Linux kernel. Voyager ( [voyager ,](https://arxiv.org/html/2506.00384v1.bib39) ) , Hashemi etal. ( [pmlr-v80-hashemi18a ,](https://arxiv.org/html/2506.00384v1.bib22) ) , and Twilight ( [duong2024twilight ,](https://arxiv.org/html/2506.00384v1.bib15) ) are micro-architecture CPU cache prefetchers implemented in simulation or with offline traces.",
			  "Content:"
			]
		  },
		  {
			"title": "An Event-Triggered Programmable Prefetcher for Irregular ...",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/programmableprefetcher.pdf",
			"excerpts": [
			  "Ainsworth and T. M. Jones. Graph prefetching using data structure knowledge. In ICS, 2016. [2] S. Ainsworth and T. M. Jones. Software prefetching for indirect ...Read more"
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads ...",
			"url": "https://www.bohrium.com/paper-details/helper-without-threads-customized-prefetching-for-delinquent-irregular-loads/867745821129441724-108609",
			"excerpts": [
			  "Download the full PDF of Helper Without Threads: Customized Prefetching for Delinquent. Includes comprehensive summary, implementation ..."
			]
		  },
		  {
			"title": "[PDF] Speculative precomputation: long-range prefetching ...",
			"url": "https://www.semanticscholar.org/paper/Speculative-precomputation%3A-long-range-prefetching-Collins-Wang/cd42d31aa8f4d07a41556ee4640cb47d3401b9ef",
			"excerpts": [
			  "This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of ..."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~tmj32/papers/docs/ainsworth19-tocs.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automatically generate software prefetches for indirect\nmemory accesses, a special class of irregular memory accesses often seen in high-performance workloads.",
			  "Across a set of memory-bound benchmarks, our automated pass achieves average\nspeedups of 1.3  for an Intel Haswell processor, 1.1  for both an ARM Cortex-A57 and Qualcomm Kryo,\n1.2  for a Cortex-72 and an Intel Kaby Lake, and 1.35  for an Intel Xeon Phi Knights Landing, each of which\nis an out-of-order core, and performance improvements of 2.1  and 2.7  for the in-order ARM Cortex-A53\nand first generation Intel Xeon Phi.",
			  "Hardware prefetchers in real systems focus on stride patterns [ 10 , 12 , 18 ,\n37 , 40 ]. These pick up and predict regular access patterns, such as those in dense-matrix and array\niteration, based on observation of previous addresses being accessed."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses | Department of Computer Science and Technology",
			"url": "https://www.cst.cam.ac.uk/blog/tmj32/software-prefetching-indirect-memory-accesses",
			"excerpts": [
			  "In the paper we create a compiler pass that will automatically identify opportunities to insert prefetches where we find these memory-indirect accesses.",
			  "One of these is that there must be an induction variable within the transitive closure of the source operand (which, for a load, is the operand that calculates the address to load from).",
			  "This means we search backwards through the data dependence graph, starting at the load, until we find this induction variable.",
			  "When we have identified loads that need to be prefetched then we duplicate all of the necessary computation to calculate the address and insert the software prefetch instructions.",
			  "We also have to add code around any loads that are part of this address computation to prevent them from causing errors at runtime if they calculate an invalid address, for example running beyond the end of an array.",
			  "There are more details in the paper including information about how we schedule the prefetches so that data is available immediately before being used."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automat-\nically generate software prefetches for indirect memory\naccesses, a special class of irregular memory accesses of-\nten seen in high-performance workloads. We evaluate this\nacross a wide set of systems, all of which gain benefit from\nthe technique. We then evaluate the extent to which good\nprefetch instructions are architecture dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Categ",
			  " dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Cat"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective: ACM Transactions on Computer Systems: Vol 36, No 3",
			"url": "https://dl.acm.org/doi/10.1145/3319393",
			"excerpts": [
			  "CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being required.",
			  ")Indirect memory accesses have irregular access patterns that limit the performance\nof conventional software and hardware-based prefetchers. To address this problem,\nwe propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect\nmemory ...",
			  "Software prefetching for indirect memory accesses\")CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being require"
			]
		  },
		  {
			"title": "Filtered runahead execution with a runahead buffer | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830812",
			"excerpts": [
			  "Section Title: Filtered runahead execution with a runahead buffer",
			  "Section Title: Filtered runahead execution with a runahead buffer > References",
			  "Authors : [Milad Hashemi](# \"Milad Hashemi\") Milad Hashemi",
			  "Milad Hashemi",
			  "The University of Texas at Austin",
			  "Pages 358 - 369",
			  "https://doi.org/10.1145/2830772.2830812"
			]
		  },
		  {
			"title": "Copyright by Milad Olia Hashemi 2016",
			"url": "https://repositories.lib.utexas.edu/bitstreams/4d988cbc-f809-418f-972d-8202b4c72bf4/download",
			"excerpts": [
			  "Jeffery A. Brown, Hong Wang, George Chrysos, Perry H. Wang, and John P.\nShen.\nSpeculative precomputation on chip multiprocessors.\nIn *Workshop on*\n*Multithreaded Execution, Architecture, and Compilation* , 2001.",
			  "Luis Ceze, James Tuck, Josep Torrellas, and Calin Cascaval. Bulk disambiguation\nof speculative threads in multiprocessors. In *ISCA* , 2006.",
			  "Murali Annavaram, Jignesh M. Patel, and Edward S. Davidson. Data prefetching\nby dependence graph precomputation. In *ISCA* , 2001."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "putation*\n*enables*\n*effective*\n*cache*\n*prefetching for even irregular memory access behavior, by*\n*using an alternate thread on a multithreaded or multi-core*\n*architecture. This paper describes a system that constructs*\n*and runs precomputation based prefetching threads via*\n*event-driven dynamic optimization. Precomputation threads*\n*are dynamically constructed by a runtime compiler from the*\n*programs frequently executed hot traces, and are adapted*\n*to the memory behavior automatically. Both construction*\n*and execution of the prefetching threads happen in another*\n*thread, imposing little overhead on the main thread. This*\n*paper also presents several techniques to accelerate the pre-*\n*computation threads, including colocation of p-threads with*\n*hot traces, dynamic stride prediction, and automatic adap-*\n*tation of runahead and jumpstart distan",
			  "While in-\nlined prefetches are typically effective for simple addressing\npatterns (e.g., strided addresses), p-thread based prefetching\nhas the potential to handle more complex address patterns\n(e.g. pointer chasing), or accesses embedded in more com-\nplex control flow. This is because the prefetching address is\ncomputed via actual code extracted from the main thread.",
			  "ead.\nA successful precomputation-based prefetcher must ad-\ndress several challenges. It must be able to determine the\nproper distance by which the prefetching thread should lead\nthe main thread, and it should have the ability to control that\ndistance. It must create lightweight threads that can actually\nproceed faster than the main thread, so that they stay out in\nfront. It must prevent p-threads from diverging from the ad-\ndress stream of the main thread, or at least detect when it\nhas happened. This divergence may be the result of control\nflow or address value speculation in the p-thread. Runaway\nprefetching may unnecessarily displace useful data, resulting\nin more data cache m",
			  " sophistication of slice creation.\nMore recent work by Lu, et al. [12] dynamically con-\nstructs p-slices via a runtime optimizer running on an idle\ncore. A single user-level thread is multiplexed to detect the\nprograms phases, construct the p-thread code, and perform\nprecomputation prefetching.",
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "**Loop Re-rolling**  A hot trace may contain multiple\ncopies of the same code due to loop unrolling done during\nstatic compilation. We perform loop *re-rolling* for the p-\nslice (i.e. removing the redundant loop copies) to reduce\nduplicated computation inside a p-slice. This optimization\nincreases the granularity at which we can set the prefetch\nrun-ahead distance, since the prefetch distance is always an\nintegral number of iterations.",
			  "**Object-Based Prefetching**  We perform same-object\nbased prefetching, as in our prior work on inline prefetch-\ning [26]. Same-object prefetching clusters prefetches falling",
			  "**P-Thread Jump Starting**\nSometimes, the only way to get the prefetch thread ahead\nof the main thread is to give it a head start. Existing dy-\nnamic precomputation schemes (e.g.\n[12]) typically start\np-threads from the same starting point (same iteration) as the\nmain thread.",
			  "**6**\n**Results**\nThis section evaluates the cost and performance of our dy-\nnamically generated precomputation based prefetching tech-\nnique.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches).",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "Our approach is complementary to and does not interfere with existing hardware prefetchers since we target only delinquent irregular load instructions (those with no constant or striding address patterns).",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support.",
			  "Delinquent Irregular Loads",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://hlitz.github.io/papers/crisp.pdf",
			"excerpts": [
			  "We also compare CRISP to a hardware-only design referred to as IBDA which performs load slice extraction via iterative backwards dependency analysis. IBDA ...Read more"
			]
		  },
		  {
			"title": "Caching and Performance of CPUs - Jyotiprakash's Blog",
			"url": "https://blog.jyotiprakash.org/caching-and-performance-of-cpus",
			"excerpts": [
			  "Nonblocking Caches to Handle Multiple Misses:** Nonblocking or \"lockup-free\" caches take cache optimization a step further by allowing out-of-order execution. With a nonblocking cache, the CPU doesn't need to stall on a cache miss; instead, it can continue to fetch other instructions while waiting for the missing data. This technique, referred to as \"hit under miss\" or \"miss under miss,\" reduces effective miss penalties by overlapping misses and allowing more efficient cache utilizatio",
			  "ltibanked Caches to Increase Cache Bandwidth:** Instead of treating the cache as a single large block, **multibanked caches** split it into independent banks that can support simultaneous accesses. This approach effectively increases cache bandwidth by allowing multiple memory accesses at the same time.",
			  "For example, modern processors like the Intel Core i7 use multiple banks in their L1 cache, enabling up to two memory accesses per clock cycle. By using **sequential interleaving** , addresses are spread evenly across different banks, ensuring that memory accesses are well distributed, reducing contention, and improving cache performance."
			]
		  },
		  {
			"title": "Understanding the Backward Slices of Performance ...",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/2000/slice.isca.pdf",
			"excerpts": [
			  "backward slice (the subset of the program that relates to*\n*the instruction) of these performance degrading instructions, if*\n*small compared to the whole dynamic instruction stream, can be*\n*pre-executed to hide the instructions latenc",
			  " be effective with respect to a given instruction, a pre-execu-\ntion technique needs three things.",
			  "First, at an *initiation point* ahead\nof the instructions execution, the pre-execution technique needs to\nknow that the performance degrading instruction *will* be executed.",
			  "Second, it has to know which other instructions contribute to the\nperformance degrading instruction.",
			  "-fetches.\nPre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. U",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "The key to answering all of these questions lies in the backward\nslice of the performance degrading instruction.",
			  "Pre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. Unlike the previous two cases, this\npre-executed branch outcome (and perhaps target) needs to be\nbound to a particular dynamic branch instance to fully benefit from\nthe pre-execution.",
			  "n general, pre-execution amounts\nto guessing the existence of a future performance degrading\ninstruction and executing it (or what we think it will be) some time\nprior to its actual encounter in the machine, thereby at least par-\ntially hiding its latency"
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "In this section requirements for\neffective direct pre-execution are examined and hardware\nmechanisms supporting these requirements are described.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://llvm.org/devmtg/2017-03/assets/slides/software_prefetching_for_indirect_memory_accesses.pdf",
			"excerpts": [
			  "Software Prefetching for Indirect. Memory Accesses. Sam Ainsworth and Timothy M. Jones. Computer Laboratory. Page 2. What should we software prefetch? Stride ...Read more"
			]
		  },
		  {
			"title": "Long-range Prefetching of Delinquent Loads",
			"url": "http://cseweb.ucsd.edu/~tullsen/isca2001.pdf",
			"excerpts": [
			  "Speculative Precomputation, a tech-*\n*nique that uses idle thread contexts in a multithreaded ar-*\n*chitecture to improve performance of single-threaded appli",
			  "ecula-\ntive threads are spawned under one of two conditions: when\nencounteringa basic trigger, which occurs when a designated\ninstruction in the main thread reaches a particular pipeline\nstage (such as the commit stage), or a chaining trigger, when\none speculative thread explicitly spawns another.",
			  "A speculative thread is spawned by allocating a hardware\nthread context, copying necessary live-in values into its reg-\nister file, and providing the thread context with the address of\nthe first instruction of the threa",
			  "If a free hardware context\nis not available the spawn request is ignored.",
			  "Necessary live-in values are always copied into the thread\ncontext when a speculative thread is spawned.",
			  "peculative threads execute precomputation slices (p-\nslices), which are sequences of dependent instructions which\nhave been extracted from the non-speculative thread and\ncompute the address accessed by delinquent loads.",
			  "When\na speculative thread is spawned, it precomputes the address\nexpected to be accessed by a future delinquent load, and\nprefetches the data.",
			  "wo primary forms of Speculative Precom-*\n*putation are evaluat",
			  "Delinquent Loads",
			  "We find that in most programs the set of delinquent\nloads is quite small; commonly 10 or fewer static loads cause\nmore than 80% of L1 data cache misses.",
			  " precomputa-\ntion slices used by our work are constructed within an in-\n ...",
			  "ulative precom-\nputation could be thought of as a special prefetch mech-\nanism that effectively targets load instructions that tradi-\ntionally have been difficult to handle via prefetching, such\nas loads that do not exhibit predictable access patterns and\nchains of dependent load",
			  "Speculative threads can be spawned",
			  "hardware structure is called the Outstanding Slice\nCounter (OSC). This structure tracks, for a subset of delin-\nquent loads, the number of instances of delinquent loads\nfor which a speculative thread has been spawned but for\nwhich the main thread has not yet committed the corre-\nsponding load."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3319393",
			"excerpts": [
			  "S. Ainsworth and Timothy M. Jones. 2017. Software prefetching for indirect memory accesses. In *Proceedings of the International Symposium on Code Generation and Optimization (CGO17)* . Navigate to citation 1 citation 2",
			  "Sam Ainsworth and Timothy M. Jones. 2018. An event-triggered programmable prefetcher for irregular workloads. In *Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)* . Navigate to citation 1"
			]
		  },
		  {
			"title": "Orchestrated Scheduling and Prefetching for GPGPUs",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/orchestrated-gpgpu-scheduling-prefetching_isca13.pdf",
			"excerpts": [
			  "In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General. Purpose Graphics Processing Unit (GPGPU) ...Read more"
			]
		  },
		  {
			"title": "Evaluating and Mitigating Bandwidth Bottlenecks Across ...",
			"url": "https://users.cs.utah.edu/~vijay/papers/ispass17.pdf",
			"excerpts": [
			  "For instance, we observe that to prevent throttling of L1 cache, increasing the L1 bandwidth by increasing the MSHRs to handle more outstanding misses can lead ..."
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://hparch.gatech.edu/papers/lee_micro10.pdf",
			"excerpts": [
			  " **In some cases, blindly applying prefetching degrades perfor-**\n**mance. To reduce such negative effects, we propose an** ***adaptive***\n***prefetch throttling*** **scheme, which permits automatic GPGPU**\n**application- and hardware-specific adjustment. We show that**\n**adaptation reduces the negative effects of prefetching and can**\n**even improve performance. Overall, compared to the state-of-**\n**the-art software and hardware prefetching, our MT-prefetching**\n**improves performance on average by 16% (software pref.) / 15%**\n**(hardware pref.) on our benchmarks.**",
			  "IP may not be useful in two cases. The first case is when\ndemand requests corresponding to prefetch requests have al-\nready been generated. This can happen because warps are not\nexecuted in strict sequential order.",
			  ".\nThe second case is when the warp that is prefetching is\nthe last warp of a thread block and the target warp (i.e.\nthread block to which the target warp belongs) has been\nassigned to a different core.",
			  " Inter-thread Prefetching (IP):* One of the main differ-\nences between GPGPU applications and traditional applica-\ntions is that GPGPU applications have a significantly higher\nnumber of thread",
			  "er warp training:* Stream and stride detectors must be\ntrained on a per-warp basis, similar to those in simulta-\nneous multithreading architectures. This aspect is criti-\ncal since many requests from different warps can easily\nconfuse pattern detector",
			  "*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "The key idea behind hardware IP is that when an application\nexhibits a strided memory access pattern across threads at the\nsame PC, one thread generates prefetch requests for another\nthread.",
			  "able*\n*Hardware*\n*Prefetcher*\n*Training:*\nCurrent\nGPGPU applications exhibit largely regular memory access\npatterns, so one might expect traditional stream or stride\nperfetchers to work well. However, because the number of\nthreads is often in the hundreds, traditional training mecha-\nnisms do not scale.",
			  "Here, we describe extensions to the traditional training\npolicies, for program counter (PC) based stride prefetch-\ners [4, 11], that can overcome this limitation. This basic idea\ncan be extended to other types of prefetchers as well.",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  " *Stride promotion:* Since memory access patterns are\nfairly regular in GPGPU applications, we observe that\nwhen a few warps have the same access stride for a given\nPC, all warps will often have the same stride for the\nPC.",
			  "Based on this observation, when at least three PWS\nentries for the same PC have the same stride, we promote\nthe PC stride combination to the *global stride (GS)* table.",
			  "IV. U NDERSTANDING U SEFUL VS . H ARMFUL\nP REFETCHING IN GPGPU",
			  "o as** ***many-thread aware prefetching*** **(MT-prefetching) mecha-*",
			  "nisms.",
			  "access behavior among fine-grained threads.",
			  "For hardware MT-**",
			  "The key ideas behind our MT-prefetching schemes are\n(a) per-warp-training and stride promotion, (b) inter-thread\nprefetching, and (c) adaptive throttling.",
			  "mechanism.",
			  "In some cases, blindly applying prefetching degrades perfor-**",
			  "mance. To reduce such negative effects, we propose an** ***adaptive***",
			  "prefetch throttling*** **scheme, which permits automatic GPGPU**",
			  "application- and hardware-specific adjustment.",
			  "We show that**",
			  "adaptation reduces the negative effects of prefetching and can**",
			  "even improve performance.",
			  "III. P REFETCHING M ECHANISMS FOR GPGPU",
			  "This section describes our *many-thread aware* prefetching",
			  "(MT-prefetching) schemes, which includes both hardware",
			  "and software mechanisms.",
			  "To support these schemes, we\naugment each core of the GPGPUs with a prefetch cache and\na prefetch engine.",
			  "The prefetch cache holds the prefetched\nblocks from memory and the prefetch engine is responsible\nfor throttling prefetch requests (see Section V).",
			  "*A. Software Prefetching*",
			  "We refer to our software prefetching mechanism as *many-*",
			  " refer to our software prefetching mechanism as *many-*\n*thread aware software prefetching* (MT-SWP). MT-SWP con-\nsists of two components: conventional *stride prefetching* and\na newly proposed *inter-thread prefetching* (IP).",
			  "a newly proposed *inter-thread prefetching* (IP).",
			  "*1) Stride Prefetching:* This mechanism is the same as the",
			  "1) Stride Prefetching:* This mechanism is the same as the\ntraditional stride prefetching mechanism. The prefetch cache\nstores any prefetched blocks.",
			  "tions is that GPGPU applications have a significantly higher\nnumber of threads.",
			  "As a result, the execution length of each\nthread is often very short.",
			  "Figure 3 shows a snippet of sequen-\ntial code with prefetch instructions and the equivalent CUDA\ncode without prefetch instructions.",
			  "In the CUDA code, since\nthe loop iterations are parallelized and each thread executes\nonly one (or very few) iteration(s) of the sequential loop, there\nare no (or very few) opportunities to insert prefetch requests",
			  "This can happen because warps are not\nexecuted in strict sequential order.",
			  "For example, when T32\ngenerates a prefetch request for T64, T64 might have already\nissued the demand request corresponding to the prefetch\nrequest generated by T32.",
			  "hese prefetch requests are usu-\nally merged in the memory system since the corresponding\ndemand requests are likely to still be in the memory system",
			  "Unless inter-core merging occurs\nin the DRAM controller, these prefetch requests are useless.",
			  "This problem is similar to the out-of-array-bounds problem\nencountered when prefetching in CPU systems.",
			  "Nevertheless,\nwe find that the benefits of IP far outweigh its negative effects.",
			  "*B. Hardware Prefetching*",
			  "We refer to our hardware prefetching mechanism as\nthe *many-thread aware hardware prefetcher* (MT-HWP).",
			  "T-HWP has (1) enhanced prefetcher training algorithms\nthat provide improved scalability over previously proposed\nstream/stride prefetchers and (2) a hardware-based inter-\nthread prefetching (IP) mechanism.",
			  "This information is stored in a separate table called an\nIP table.",
			  "*3) Implementation:* Figure 6 shows the overall design of\nthe MT-HWP, which consists of the three tables discussed\nearlier: PWS, GS, and IP tables.",
			  " practice, overly aggressive prefetching can have a nega-\ntive effect on performance.",
			  " principal memory latency tolerance mechanism in a\nGPGPU is multithreading. Thus, if a sufficient number of\nwarps and/or enough computation exist, memory latency can"
			]
		  },
		  {
			"title": "CTA-aware Prefetching for GPGPU - Computer Engineering",
			"url": "https://ceng.usc.edu/techreports/2014/Annavaram%20CENG-2014-08.pdf",
			"excerpts": [
			  "Lee et al. [21] proposed a software and hardware\nbased many-thread aware prefetching which basically commands\nthreads to prefetch data for the other threads.",
			  "They exploit the\nfact that the memory addresses are referenced using thread id in\nmany GPU applications."
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/",
			"excerpts": [
			  "To optimize memory access on NVIDIA GPUs, prefetching can be employed in software when excess warps are insufficient to hide memory latency.",
			  "A synchronization within the loopfor example, `syncthreads` constitutes a memory fence and forces the loading of `arr` to complete at that point within the same iteration, not PDIST iterations later. The fix is to use asynchronous loads into shared memory, the simplest version of which is explained in the [Pipeline interface](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) section of the CUDA programmer guide.",
			  "Prefetching can be implemented by unrolling loops, storing prefetched values in registers or shared memory, and using techniques like batched or rolling prefetching, with the latter being more effective when combined with asynchronous memory copies.",
			  "Confirm that not all memory bandwidth is being used.",
			  "Confirm the main reason warps are blocked is **Stall Long Scoreboard** , which means that the SMs are waiting for data from DRAM.",
			  "This leads to the improved shared memory results shown in Figure 2. A prefetch distance of just 6, combined with asynchronous memory copies in a rolling fashion, is sufficient to obtain optimal performance at almost 60% speedup over the original version of the code."
			]
		  },
		  {
			"title": "Near-Side Prefetch Throttling - of Wim Heirman",
			"url": "https://heirman.net/papers/pact2018.pdf",
			"excerpts": [
			  " near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine. The MSHR tracks outstand-\ning cache misses triggered by both demand requests (application\nloads and stores), and prefetches; the extra state allows for detec-\ntion of late prefetches. The state machine periodically computes\nthe fraction of late prefetches, and updates the optimal prefetch\ndistance.\n",
			  "Maintaining a small fraction\n(e.g., 10%) of late prefetches does not harm performance as long as\nthe late prefetches are only late by a small amount, i.e., the demand\naccess is made only just before the prefetch request completes.",
			  "Near-Side Prefetch Throttling",
			  " We show that near-side throttling can be extended to mul-\ntiple prefetchers per core, where it will naturally throttle\nthose prefetchers that yield no useful requests, allowing for\na diverse set of prefetch algorithms to co-exis",
			  "e throttling detects bandwidth\nsaturation locally (through memory latency), so no global coordi-\nnation is needed between the per-core prefetchers in a many-core\narchitecture, or even between multiple prefetch algorithms on a\nsingle core.\n",
			  "ng detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost. This makes near-side throttling superior over traditional\nfar-side throttling as it is able to provide even slightly better per-\nformance (9.6% vs. 9.4%), at a far cheaper implementation cost,\nand is more widely applicable to other use cases such as software\nprefetching and control of multiple hardware prefetchers.",
			  "The basic concept of near-side prefetch throttling (NST) is to\ndetect late prefetches, and tune the prefetcher aggressiveness such\nthat the amount of late prefetches is balanced around a small but\nnon-zero fraction of all prefetches.",
			  "NEAR-SIDE PREFETCH THROTTLING**",
			  "ur\nsolution is cheap to implement in hardware, includes throttling on\noff-chip bandwidth saturation, applies to both hardware and soft-\nware prefetching, and can control multiple concurrent prefetchers\nwhere it will naturally allow the most useful prefetch algorithm\nto generate most of the requests",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "In a many-core processor, the prefetchers in each core can be con-\ntrolled independently based on their own specific late prefetch\nfraction. This allows for heterogenous applications or multi-\nprogramming workloads, and will tune each prefetchers distance\nto the specific access pattern it is experienci",
			  "Our proposed implementation of near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine.",
			  "sing detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Hardware Design of DRAM Memory Prefetching Engine for ...",
			"url": "https://www.mdpi.com/2227-7080/13/10/455",
			"excerpts": [
			  "Inter-warp prefetching mechanisms are based on the detection of stride patterns and base addresses in different warps. A many-thread-aware prefetching mechanism ...Read more"
			]
		  },
		  {
			"title": "APAC: An Accurate and Adaptive Prefetch Framework with ...",
			"url": "https://par.nsf.gov/servlets/purl/10251073",
			"excerpts": [
			  "Near-side prefetch throttling (NST) [9] only adjusts the aggressiveness of prefetching based on the fraction of late prefetchers, which has a relatively small ...Read more"
			]
		  },
		  {
			"title": "PPT - Many-Thread Aware Prefetching Mechanisms for GPGPU Application PowerPoint Presentation - ID:5741796",
			"url": "https://www.slideserve.com/phil/many-thread-aware-prefetching-mechanisms-for-gpgpu-application",
			"excerpts": [
			  "akash\n**[Motivation](https://image3.slideserve.com/5741796/motivation-l.jpg \"motivation\")**  Memory latency hiding through multithread prefetching schemes  Per-warp training and Stride promotion  Inter-thread Prefetching  Adaptive Throttling  Propose software and hardware prefetching mechanisms for a GPGPU architecture  Scalable to large number of threads  Robustness through feedback and throttling mechanisms to avoid degraded performance\n",
			  "rmance\n**[Memory Latency Hiding techniques](https://image3.slideserve.com/5741796/memory-latency-hiding-techniques-l.jpg \"memory latency hiding techniques\")**  Multithreading  Thread level and Warp level context switching  Utilization of complex cache memory hierarchies  Using L1, L2, DRAMs than accessing Global Memory each time  Prefetching  Insufficient thread-level parallelism  Memory request merging Thread1 Thread2 Thread1 Thread3",
			  "MT-HWP  Stride Promotion  Considering the stride pattern is the same across all warps for a given PC, PWS is monitored for three accesses  If found same stride, promote the PWS to Global Stride(GS) table, if not, retain in PWS  Inter-thread Prefetching  Monitor stride pattern across threads at the same PC, for 3 memory accesses  If found same, stride information is stored in the IP table",
			  "MT-HWP  Implementation  When there are hits in both GS and IP, GS is given preference because  Strides"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications | Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2010.44",
			"excerpts": [
			  "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract\nContent:\nWe consider the problem of how to improve memory latency tolerance in massively multithreaded GPGPUs when the thread-level parallelism of an application is not sufficient to hide memory latency. One solution used in conventional CPU systems is prefetching, both in hardware and software. However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously. This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms. Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads. For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, blindly applying prefetching degrades performance. To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment. We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Overall, compared to the state-of-the-art software and hardware prefetching, our MT-prefetching improves performance on average by 16%(software pref.) / 15% (hardware pref.) on our benchmarks.",
			  "Section Title: Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract",
			  "However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously.",
			  "This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms.",
			  "Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads.",
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism.",
			  "In some cases, blindly applying prefetching degrades performance.",
			  "To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment.",
			  "We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Content:"
			]
		  },
		  {
			"title": "[PDF] Near-side prefetch throttling: adaptive prefetching for high-performance many-core processors | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Near-side-prefetch-throttling%3A-adaptive-prefetching-Heirman-Bois/39e6013cbe45a288431ddb4269611a483c90bbb9",
			"excerpts": [
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "TLDR"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nDaniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, and Jir Bittner. 2021. A Survey on Bounding Volume Hierarchies for Ray Tracing. Computer Graphics Forum (2021).\n[Go",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Meister%2C+Shinji+Ogaki%2C+Carsten+Benthin%2C+Michael%C2%A0J.+Doyle%2C+Michael+Guthe%2C+and+Jir%C3%AD+Bittner.+2021.+A+Survey+on+Bounding+Volume+Hierarchies+for+Ray+Tracing.+Computer+Graphics+Forum+%282021%29.)",
			  "[32]",
			  "Bochang Moon, Yongyoung Byun, Tae-Joon Kim, Pio Claudio, Hye-Sun Kim, Yun-Ji Ban, Seung Woo Nam, and Sung-Eui Yoon. 2010. Cache-Oblivious Ray Reordering. ACM Transactions on Graphics (TOG) (2010).",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Bochang+Moon%2C+Yongyoung+Byun%2C+Tae-Joon+Kim%2C+Pio+Claudio%2C+Hye-Sun+Kim%2C+Yun-Ji+Ban%2C+Seung%C2%A0Woo+Nam%2C+and+Sung-Eui+Yoon.+2010.+Cache-Oblivious+Ray+Reordering.+ACM+Transactions+on+Graphics+%28TOG%29+%282010%29.)",
			  "[33]",
			  "Paul Arthur Navratil, Donald S. Fussell, Calvin Lin, and William R. Mark. 2007. Dynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In IEEE Symposium on Interactive Ray Tracing. 95104.\n[Dig",
			  "[Digital Library](/doi/10.1109/RT.2007.4342596)",
			  "[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FRT.2007.4342596)",
			  "[34]"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://info.computer.org/csdl/proceedings-article/micro/2010/05695538/12OmNzcPApv",
			"excerpts": [
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, ...Read more"
			]
		  },
		  {
			"title": "(PDF) Adaptive Prefetching for Fine-grain Communication in PGAS Programs",
			"url": "https://www.researchgate.net/publication/381186663_Adaptive_Prefetching_for_Fine-grain_Communication_in_PGAS_Programs",
			"excerpts": [
			  "In this work, we present an adaptive prefetching optimization that can be applied to PGAS programs with irregular memory access patterns. We ...Read more"
			]
		  },
		  {
			"title": "System level cache prefetching algorithms for complex ...",
			"url": "https://lup.lub.lu.se/student-papers/record/9159122/file/9159147.pdf",
			"excerpts": [
			  "These merges occur when a prefetch request is late and a demand request is coming at the same time. The throttling behaviour can be observed in table 1. Early ...Read more"
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching - Technical Blog - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/boosting-application-performance-with-gpu-memory-prefetching/209210",
			"excerpts": [
			  "This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.Read more"
			]
		  },
		  {
			"title": "Many-thread aware hardware prefetcher (MT-HWP). | Download Scientific Diagram",
			"url": "https://www.researchgate.net/figure/Many-thread-aware-hardware-prefetcher-MT-HWP_fig7_221005092",
			"excerpts": [
			  "h consists of the three tables discussed earlier: PWS, GS, and IP tables. The ",
			  "The IP and GS tables are indexed in parallel with a PC address.",
			  "adaptive MT-HWP pro- vides a 29% performance improvement over the baseline."
			]
		  },
		  {
			"title": "Some issues regarding the use of prefetch in the cuda kernel - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/some-issues-regarding-the-use-of-prefetch-in-the-cuda-kernel/334568",
			"excerpts": [
			  "e difference between cp.async and normal loads is the means of re-synchronization of the asynchronous operations.",
			  "For normal loads from global memory, the re-synchronization works with the help of the long scoreboard, as soon as the result registers are used in instructions dependent on the result.",
			  "For cp.async, the results are transferred into shared memory. You have to use cp.async.wait_group or cp.async.wait_all for re-synchronization."
			]
		  },
		  {
			"title": "Combining Local and Global History for High Performance ...",
			"url": "https://jilp.org/dpc/online/papers/00dimitrov.pdf",
			"excerpts": [
			  "To capture delta correlation, a delta buffer is included in the prefetch function, which keeps the delta information when a linked list is traversed in the GHB.Read more"
			]
		  },
		  {
			"title": "L5 - Advanced Memory Prefetching Techniques",
			"url": "https://coconote.app/notes/5551d401-c94c-4d86-bb70-c67c9e8bb912",
			"excerpts": [
			  "Delta correlation prefetchers track repeating delta patterns between accessed addresses to predict future accesses. Correlation-based ...Read more"
			]
		  },
		  {
			"title": "Pointer jumping - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Pointer_jumping",
			"excerpts": [
			  "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs.Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov Predictors",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=joseph_isca97.pdf",
			"excerpts": [
			  "The Markov prefetcher provides the best performance across nll\nthe applications, particularly when applied to both the instruction\nand data caches."
			]
		  },
		  {
			"title": "A Survey on Recent Hardware Data Prefetching ...",
			"url": "https://arxiv.org/pdf/2009.00715",
			"excerpts": [
			  "99] D. Joseph and D. Grunwald, Prefetching Using Markov Predictors, in *Proceedings of the International Symposium on*\n*Computer Architecture (ISCA)* , pp. 252263, 1997"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1999/jmp-ptr.isca.pdf",
			"excerpts": [
			  "bstract**\n*Current techniques for prefetching linked data structures*\n*(LDS) exploit the work available in one loop iteration or*\n*recursive call to overlap pointer chasing latency. Jump-*\n*pointers, which provide direct access to non-adjacent*\n*nodes, can be used for prefetching when loop and recur-*\n*sive procedure bodies are small and do not have sufficient*\n*work to overlap a long latency. This paper describes a*\n*framework for jump-pointer prefetching (JPP) that sup-*\n*ports four prefetching idioms: queue, full, chain, and root*\n*jumping and three implementations: software-only, hard-*\n*ware-only, and a cooperative software/hardware tech-*\n*nique.",
			  "n a suite of pointer intensive programs, jump-*\n*pointer prefetching reduces memory stall time by 72% for*\n*software, 83% for cooperative and 55% for hardware, pro-*\n*ducing speedups of 15%, 20% and 22% respecti",
			  "general purpose technique for tol-\nerating serialized latencies that result from LDS tra-\nversal.\nBy storing explicit jump-pointers to nodes\nseveral hops away, JPP overcomes the pointer-chasing\nproblem.\nIt is able to generate prefetch addresses\ndirectly, rather than in a serial fashion, and is effective\neven in situations where not enough work is available\nto hide latencies by scheduling.\n**",
			  "P implementations: software-only,\nhardware-only, and cooperative.\nFor those programs\nwith appreciable memory latency components, these\nimplementations reduce overall observed memory\nlatency by 72%, 55%, and 83%, respectively and\nachieve speedups of 15%, 22%, and 20%.\n",
			  "**Abstract**",
			  "*Current techniques for prefetching linked data structures*",
			  "\n*(LDS) exploit the work available in one loop iteration or*",
			  "*recursive call to overlap pointer chasing latency. Jump-*",
			  "*pointers, which provide direct access to non-adjacent*",
			  "*nodes, can be used for prefetching when loop and recur-*",
			  "*sive procedure bodies are small and do not have sufficient*",
			  "*work to overlap a long latency. This paper describes a*",
			  "*framework for jump-pointer prefetching (JPP) that sup-*",
			  "*ports four prefetching idioms: queue, full, chain, and root*",
			  "*jumping and three implementations: software-only, hard-*",
			  "*ware-only, and a cooperative software/hardware tech-*",
			  "*nique.*",
			  "*On a suite of pointer intensive programs, jump-*",
			  "*pointer prefetching reduces memory stall time by 72% for*",
			  "*software, 83% for cooperative and 55% for hardware, pro-*",
			  "*ducing speedups of 15%, 20% and 22% respectively.*",
			  "*\n**1 Introduction**",
			  "Linked data structures (LDS) are common in many appli-",
			  "cations, and their importance is growing with the spread of",
			  "object-oriented programming.",
			  "The popularity of LDS",
			  "stems from their flexibility, not their performance. LDS",
			  "access, often referred to as *pointer-chasing* , entails chains",
			  "of data dependent loads that serialize address generation",
			  "and memory access.",
			  "In traversing an LDS, these loads",
			  "often form the programs critical path.\nC",
			  "Consequently,",
			  "when they miss in the cache, they can severely limit paral-",
			  "lelism and degrade performance.",
			  "Prefetching is one way to hide LDS load latency and",
			  " ... \nefficiently [6] and to parallelize searches and reductions on",
			  "lists [9].",
			  "Discussions of maintaining recursion-avoiding",
			  "traversal *threads* in non-linear data structures can be found",
			  "in data structures literature [18]. As noted earlier, Luk and",
			  "Mowry [11] suggested the use of programmer controlled",
			  "jump-pointers for prefetching. We are not aware of any",
			  "implementations, actual or proposed,\nof hardware or",
			  "cooperative jump-pointer prefetching.",
			  "\n**6 Summary and Future Directions**\n",
			  "In this paper, we describe the general technique of jump-",
			  "pointer prefetching (JPP) for tolerating linked structure",
			  "(LDS) access latency. JPP is effective when limited work",
			  "is available between successive dependent accesses (e.g., a",
			  "***Figure***",
			  "***7.***",
			  "***Tolerating***",
			  "***longer***",
			  "***memory***",
			  "***latencies.***",
			  "*Execution times for health: the first group of bars uses*",
			  "*the base configuration (70 cycle memory latency), the*",
			  "*second and third simulate long memory latency (280*",
			  "cycles).*",
			  "*In terms of prefetching, the first two*",
			  "*configurations use a jump interval (the distance*",
			  "*\n*between a jump-pointers home and target nodes) of 8,*",
			  "the third uses an interval of 16.*",
			  "0.0",
			  "0.5",
			  "1.0",
			  "1.5",
			  "2.0",
			  "2.5",
			  "3.0",
			  "MemLat=70",
			  "Interval=8",
			  "Interval=8",
			  "BDSCH",
			  "BDSCH",
			  "BDSCH",
			  "MemLat=280",
			  "MemLat=280",
			  "Interval=16",
			  "Normalized Execution Time",
			  "memory latency",
			  "Compute time",
			  "**Legend: B:** Base",
			  "**D:** DBP",
			  "**S:** Software JPP",
			  "**C:** Cooperative JPP",
			  "**H:** Hardware JPP",
			  "tight pointer chasing loop) to enable aggressive scheduling"
			]
		  },
		  {
			"title": "The Efficacy of Software Prefetching and Locality ...",
			"url": "https://jilp.org/vol6/v6paper7.pdf",
			"excerpts": [
			  "In jump pointer prefetching, additional pointers are inserted into a dynamic data structure\nto connect non-consecutive link elements.",
			  "These jump pointers allow prefetch instructions to\nname link elements further down the pointer chain ( i.e. a prefetch distance, PD , away which\nis computed as in Sections 3.1 and 3.2) without sequentially traversing the intermediate links.",
			  "Consequently, prefetch instructions can overlap the fetch of multiple link elements simultaneously\nby issuing prefetches through the memory addresses stored in the jump pointers.",
			  "Jump pointer prefetching, however, cannot prefetch the first PD link nodes in a linked list\nbecause there are no jump pointers that point to these early nodes.",
			  "To enable prefetching of early\nnodes, jump pointer prefetching can be extended with prefetch arrays [7]. In this technique, an\narray of prefetch pointers is added to every linked list to point to the first PD link nodes.",
			  "Hence,\nprefetches can be issued through the memory addresses in the prefetch arrays before traversing\neach linked list to cover the early nodes, much like the prologue loops in affine array and indexed\narray prefetching prefetch the first PD array elements.",
			  "Figure 5 Part(A) illustrates the addition\nof a prologue loop that performs prefetching through a prefetch array.",
			  "6.3 ccmalloc and Prefetching",
			  "Software prefetching for pointer-chasing codes suffers high overhead to create and manage jump\npointers, as described in Section 5.2.",
			  "owever, jump pointers may not be necessary when prefetch-\ning is combined with ccmalloc memory allocation",
			  "Since intelligent allocation places link nodes\ncontiguously in memory, prefetch instructions can access future link nodes by simple indexing, just\nas for affine array accesses.",
			  "Figure 9 shows the effect of ccmalloc on nodes linked together by\npointers.",
			  "From the right-hand part of the figure, it is intuitive that a compiler can insert prefetches\nfor list nodes further down the list using the size of a node and the location of the first node.",
			  "This approach, which we call index prefetching [1, 37], was originally proposed in [8].",
			  "With in-\ndex prefetching, the jump pointers can be removed, thus eliminating all the overhead associated\nwith jump pointer prefetchin",
			  "To quantify this benefit, we created index prefetching versions for\nHealth and MST , and show the results for these benchmarks in Figure 18.",
			  "(We did not create an\nindex prefetching version for EM3D , our third pointer-chasing benchmark, since it already achieves\nhigh performance with normal software prefetching as shown in Figures 10 and 13)."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta- ...",
			"url": "https://jilp.org/vol13/v13paper2.pdf",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " Correlating Prediction\nTables (DCPT). DCPT builds upon two previously proposed prefetcher techniques, com-\nbining them and refining their ideas to achieve better perf"
			]
		  },
		  {
			"title": "Prefetching",
			"url": "https://ece752.ece.wisc.edu/lect15-prefetching.pdf",
			"excerpts": [
			  "Markov prefetching forms address correlations",
			  "Joseph and Grunwald (ISCA 97)",
			  "Uses global memory addresses as states in the Markov graph",
			  "Correlation Table *approximates* Markov graph",
			  "obal History Buffer (GHB)\n Holds miss address\nhistory in FIFO order",
			  "Global History Buffer",
			  "Delta-based prefetching leads to much smaller table than\nclassical Markov Prefetching",
			  "Delta-based prefetching can remove compulsory misses"
			]
		  },
		  {
			"title": "Data prefetching on in-order processors",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/2268c547-2c42-49d5-9e73-7578ebe3758e/content",
			"excerpts": [
			  "[14] K. J. Nesbit and J. E. Smith, Data cache prefetching using a global history buffer, Software, IEE Proceedings-, 2004. [15] S. Srinath, O. Mutlu, H ...Read more"
			]
		  },
		  {
			"title": "Feedback Mechanisms for Improving Probabilistic Memory ...",
			"url": "https://www.cs.utexas.edu/~lin/papers/hpca09.pdf",
			"excerpts": [
			  "The efficiency of stream prefetching has been improved by Nesbit and. Smith [18], who introduce the Global History Buffer to im- prove prefetch effectiveness ...Read more"
			]
		  },
		  {
			"title": "Data Access History Cache and Associated Data Prefetching ...",
			"url": "http://www.cs.iit.edu/~scs/assets/files/SC07_DAHC.pdf",
			"excerpts": [
			  "Nesbit and Smith proposed a global history buffer for data prefetching in [14] and. [15]. The similarity between their work and our work is that both attempt ..."
			]
		  },
		  {
			"title": "TDT4260 Computer Architecture Mini-Project",
			"url": "https://www.nichele.eu/files/nichele_tdt4260.pdf",
			"excerpts": [
			  "[14] M. Grannaes, M. Jahre and L. Natvig. Multi-level Hardware Prefetching. Using Low Complexity Delta Correlating Prediction Tables with Partial. Matching.Read more"
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer",
			"url": "https://www.researchgate.net/publication/3215463_Data_Cache_Prefetching_Using_a_Global_History_Buffer",
			"excerpts": [
			  "This research is to design a history table-based linear analysis ... This paper studies hardware prefetching for second-level (L2) caches."
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | Proceedings of the 10th International Symposium on High Performance Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1109/HPCA.2004.10030",
			"excerpts": [
			  "A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction.",
			  "The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.",
			  "Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods.",
			  "Global History Buffer prefetching can increase correlation prefetching performance by 20% and cut its memory traffic by 90%. Furthermore, the Global History Buffer can make correlations within a loads address stream, which can increase stride prefetching performance by 6%. "
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | IEEE Micro",
			"url": "https://dl.acm.org/doi/abs/10.1109/MM.2005.6",
			"excerpts": [
			  "By organizing data cache prefetch information in a new way, a GHB supports existing prefetch algorithms more effectively than conventional prefetch tables. It reduces stale table data, improving accuracy and reducing memory traffic. It contains a more complete picture of cache miss history and is smaller than conventional tables",
			  "The structure is based on a Global History Buffer that holds the most\nrecent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer",
			  "HPCA '04: Proceedings of the 10th International Symposium on High Performance Computer ArchitectureA new structure for implementing data cache prefetching is proposed and analyzed via",
			  "A new structure for implementing data cache prefetching is proposed and analyzed via"
			]
		  },
		  {
			"title": "DATA CACHE PREFETCHING USING A GLOBAL ...",
			"url": "https://minds.wisconsin.edu/bitstream/1793/11158/1/file_1.pdf",
			"excerpts": [
			  "As a circular buffer, the GHB prefetching\nstructure eliminates many problems associat-\ned with conventional tables. First, the GHB\nFIFO naturally gives table space priority to\nthe most recent history, thus eliminating the\nstale-data problem.",
			  "dex table entries contain point-\ners into the GHB.",
			  "The GHB is larger, with a size chosen to hold\na representative portion of the miss address\nstream. Last, and perhaps most important, a\ndesigner can use the ordered global history to\ncreate more-sophisticated prefetching meth-\nods than conventional stride and correlation\nprefetchin"
			]
		  },
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%.",
			  "In\nfigure 7 we show the average portion of deltas that can be\nrepresented with a given amount of bits across all SPEC2006\nbenchmarks.",
			  "Although the coverage steadily increases with the amount\nof bits used, speedup has a distinct knee at around 7 bits.",
			  "In figure 8 we show the geometric mean of speedups as\na function of the number of deltas per table entry.",
			  "One of the main differences between DCPT and PC/DC is\nthat DCPT stores deltas, while PC/DC stores entire addresses\nin its GHB.",
			  "the deltas are usually quite small, fewer\nbits are needed to represent a delta than a full address."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov predictors | Proceedings of the 24th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/264107.264207",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the *Markov prefetcher.* This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching *multiple reference predictions* from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "Prefetching using Markov predictors for ISCA 1997 - IBM Research",
			"url": "https://research.ibm.com/publications/prefetching-using-markov-predictors--1",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems.",
			  "In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs.",
			  "The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://people.ucsc.edu/~hlitz/papers/crisp.pdf",
			"excerpts": [
			  " Prefetching using Markov predictors.\n",
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using Markov predictors.",
			  "Tempo-\nral prefetchers [ 8 , 49 , 54 , 119 , 122 ] track the temporal order of\ncache line accesses based on Markov prefetching [ 55 ] introducing\nsignificant storage overheads in the order of megabytes in con-\ntrast to CRISP",
			  " Runahead prefetchers [ 6 , 33 , 48 , 82  84 , 89 , 95 ] and\nhelper threads [ 21 , 23 , 24 , 70 , 73 , 74 , 110 , 117 , 123 , 124 ] prefetch\nirregular memory accesses as in linked-list traversals, however,\nthey introduce significant hardware complexity or consume sepa-\nrate SMT-threads [ 34 ] whereas CRISP requires only minimal hard-\nware modifications. Bra",
			  "CRISP can be\ncombined with these prior approaches to increase coverage by\nreducing the miss penalty of irregular memory accesses. T"
			]
		  },
		  {
			"title": "Perceptron-Based Prefetch Filtering - Engineering People Site",
			"url": "https://people.engr.tamu.edu/djimenez/pdfs/ppf_isca2019.pdf",
			"excerpts": [
			  "7.2\nLookahead Prefetchers\nUnlike spatial prefetchers, lookahead prefetchers take program order\ninto account when they make predictions. Shevgoor et al. propose\nthe Variable Length Delta Prefetcher (VLDP) [ 35 ], which correlates\nhistories of deltas between cache line accesses within memory pages\nwith the next delta within that page. SPP [ 2 ] and KPCs prefetching\ncomponent [ 36 ] are more recent examples of lookahead prefetchers.\n",
			  "Ishii et al. propose the Access Map Pattern\nMatching prefetcher (AMPM) [ 11 ], which creates a map of all ac-\ncessed lines within a region of memory, and then analyzes this map\non every access to see if any fixed-stride pattern can be identified\nand prefetched that is centered on the current access.",
			  "In a single core configuration, PPF increases performance by\n3.78% compared to the underlying prefetcher, SPP.",
			  "In a multi-core\nsystem running a mixes of memory intensive SPEC CPU 2017 traces,\nPPF saw an improvement of 11.4% over SPP for a 4-core system,\nand 9.65% for an 8-core system.",
			  "Michaud proposes the\nBest-Offset Prefetcher [ 34 ], which determines the optimal offset to\nprefetch while considering memory latency and prefetch timeliness.",
			  "DRAM-Aware\nAMPM (DA-AMPM) [ 32 ] is a variant of AMPM that delays some\nprefetches so they can be issued together with others in the same\nDRAM row, increasing bandwidth utilization.",
			  "Pugsley et al. pro-\npose the Sandbox Prefetcher [ 33 ], which analyzes candidate fixed-\noffset prefetchers in a sandboxed environment to determine which is\nmost suitable for the current program phase."
			]
		  },
		  {
			"title": "Building Efficient Neural Prefetcher",
			"url": "https://www.memsys.io/wp-content/uploads/2023/09/3.pdf",
			"excerpts": [
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using markov predictors. In\n*Proceedings of the 24th annual international symposium on Computer architecture* .\n252",
			  "chun Kim, Seth H Pugsley, Paul V Gratz, AL Narasimha Reddy, Chris Wilker-\nson, and Zeshan Chishti. 2016. Path confidence based lookahead prefetching.\nIn *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture*\n*(MICRO)* . IEEE, 112."
			]
		  },
		  {
			"title": "Arsenal of Hardware Prefetchers",
			"url": "https://www.arxiv.org/pdf/1911.10349v1",
			"excerpts": [
			  "ature Path Prefetcher** (SPP) [ 5 ] stores the stride pat-\nterns in a compressed form in the signature table (ST). Each\nentry in the ST is used to index into the pattern table (PT),\nwhich is used to predict the next stride and also contains the\nconfidence for the current prefetch. The signature is then up-\ndated with the latest stride and is used to recursively lookup\nthe PT to predict more strides. This goes on until the confi-\ndence, which is multiplied with the last prefetch confidenc"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching For Linked Data ...",
			"url": "https://www.scribd.com/document/861770035/9",
			"excerpts": [
			  "This paper presents a framework for jump-pointer prefetching (JPP) aimed at improving the performance of linked data structures (LDS) by ...Read more"
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "Jump pointers, which provide direct access to non-adjacent nodes, can be used for prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures",
			  "New technique: Jump Pointer Prefetching",
			  "Creates parallelism",
			  "Hides arbitrary latency",
			  "Choice of implementation: software, hardware, cooperative",
			  "Problem: Pointer chasing latency",
			  "pointer loads",
			  "Jump pointer prefetching:\nOverlap pointer loads with each other anyway!"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  " uses the underlying data of the application, and provides an 11.3% speedup using no additional processor state. By adding less than 1/2% space 2 overhead to the second level cache, performance can be further increased to 12.6% across a range of \"real world\" applications.\n*",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems.",
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "There are a number of ways to\nidentify \"likely\" addresses.",
			  "Roth\net al.\nintroduced dependence-based techniques for capturing\nproducer-consumer load pairs [ 12].",
			  "his paper investigates a technique that predicts addresses in\npointer-intensive applications using a hardware only technique with\nno built-in biases toward the layout of the recursive data struc-\nSection Title: A state",
			  "ability to \"run ahead\" of an application has been shown to be a re-\nquirement for pointer-intensive applications [12], which tradition-\nally do not provide sufficient computational work for masking the\nprefetch latency.",
			  "me hybrid prefetch engines [13] do have the\nability to run several instances ahead of the processor, but require\napriori\nknowledge of the layout of the data structure, and in some\ncases, the traversal order of the structu",
			  "es. Content-based prefetching works by examining the content of data as it is moved from memory to the caches. Data values that are likely to be addresses are then translated and pushed to a prefetch buffer. Con",
			  "N\nIn early processor designs, the performance of the processor and\nmemory were comparable, but in the last 20 years their relative\nperformances have steadily diverged [4], with the performance im-\nprovemen"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  "Content-Directed Data Prefetching, a data*\nprefetching architecture that exploits the memory allocation used\nby operating systems and runtime systems to improve the perfor-\nmance of pointer-intensive applications constructed using modem\nlanguage systems. This technique is modeled after conservative\ngarbage collection, and prefetches \"likely\" virtual addresses ob-\nserved in memory references",
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "Prefetching:- - CSE IITM",
			"url": "https://www.cse.iitm.ac.in/~pritam/prefetching.pdf",
			"excerpts": [
			  "**PRITAM MAJUMDER, PACE LAB, CSE DEPT., IIT MADRAS** **9**\nCollins Jamison\nCalder Brad\nTullsen Dean M\nPointer Cache Assisted Prefetching\n2002\nMICRO"
			]
		  },
		  {
			"title": "CS 473 Homework 3 (due 12/20/03) Fall 2004",
			"url": "https://jeffe.cs.illinois.edu/teaching/473/hw3final.pdf",
			"excerpts": [
			  "Storing a complete binary search tree in the van Emde Boas layout allows us to perform any search in O(logB N) memory operations in the cache-oblivious model.Read more"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://diglib.eg.org/bitstreams/c41b2a27-076a-4252-a346-584b2deac5fd/download",
			"excerpts": [
			  "Yoon et al. [YM06] proposed a cache-oblivious. BVH layout for collision detection which applied to a k- d tree for ray tracing, resulted ...Read more",
			  "The common DFS layout performed worst for all node layouts in both memory areas. Excluding layouts that use statistics the equally simple to construct BFS ...Read more",
			  "BVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and p",
			  "reelet based DFS/BFS (TDFS/TBFS):** A treelet is a\nconnected sub-tree of a BVH. For this layout treelets of\nnodes that were accessed above a certain threshold are\nbuilt",
			  "The internal memory layout of a treelet can be cho-\nsen freely. By always adding nodes just to the front or the\nback of the merge queue we automatically obtain a treelet\nin DFS or BFS order. Finally the node order of the whole\ntree is obtained by lining up the nodes of all treelets.",
			  "A32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines)",
			  "A16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache line",
			  "**Table 1:** *Scenes used for benchmarking",
			  "ivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and par",
			  " cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.",
			  "**5.1. Node Layouts**",
			  "e classic BVH node data structure stores a bounding vol-\nume along with pointers to its children. We follow Aila et\nal. [ AL09 ], i.e., a node does not store its bounding box, but\nthe bounding boxes of its children. Both children are fetched\nand tested together, which is more efficient for GPUs due\nto increased instruction level parallelism and allows rough\nfront to back traversal. Depending on the data layout, the\nsize of such a node is at least 56 bytes (2 float values for\nminimum/maximum per dimension and child plus pointers).",
			  "*AoS:** 64 bytes, including 8 bytes padding (fitting 2 nodes\nin one 128B cache line",
			  "*SoA32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines",
			  "*SoA16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache lin",
			  "We also analyzed an SoA8 layout which fitted 16 nodes in\n7 cache lines. As it consistently performed much worse than\nthe other layouts, we excluded it from our experiments.",
			  "**5.2. Tree Layouts**",
			  "A tree layout describes how nodes are grouped in memory.",
			  "We analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:",
			  "DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf",
			  "BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches",
			  "vEB):** A cache-oblivious tree layout\n[ vEB75 ]",
			  "ext we describe our two proposed layouts depending on\nnode access statistics which use a threshold *p* ",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "We analyzed six different tree layouts. The first four layouts are two common layouts and two cache-efficient layouts. We further propose two more layouts.Read more"
			]
		  },
		  {
			"title": "[2209.09166] Cache-Oblivious Representation of B-Tree Structures",
			"url": "https://arxiv.org/abs/2209.09166",
			"excerpts": [
			  "CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout with packed memory array.",
			  "In the use of the vEB layout mostly search complexity was considered, so far.",
			  "We describe how to build an arbitrary tree in vEB layout if we can simulate its depth-first search.",
			  " The data structure allows searching with an optimal I/O complexity \\mathcal{O}(\\log_B{N}) and is stored in linear space. ",
			  "ave an amortized I/O complexity \\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)",
			  " amortized time complexity \\mathcal{O}(S\\cdot\\log^2 N) , where S is the size of the subtree and N the size of the whole stored tree. R",
			  "Rebuilding an existing subtree saves the multiplicative \\mathcal{O}(\\log^2 N) in both complexities if the number of vertices on individual tree levels is not changed; it is paid only for the inserted/removed vertices otherwise.",
			  "We propose a general data structure CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout ...Read more"
			]
		  },
		  {
			"title": "Cache-oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/paper.pdf",
			"excerpts": [
			  "The van Emde Boas layout proceeds recursively. Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree. Suppose first that *h* is\na power of 2. Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + 1. This breaks the tree into the *top recursive subtree* *A* of\nheight *h/* 2, and several *bottom recursive subtrees* *B* 1 , *B* 2 , . . . , *B* ** , each of height *h/* 2",
			  "eaf nodes have the same number of children, then the recursive subtrees all\nhave size roughly\n**\n*N* , and ** is roughly\n**\n*N* . The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2",
			  " memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail* . Each level of detail is a partition of the tree into disjoint recursive\nsubtrees. In the finest level of detail, 0, each node forms its own recursive subtree. In\nthe coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subt",
			  "ur layout is a modified version of Prokops layout for a complete binary tree\nwhose height is a power of 2 [40, pp. 6162]. We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48]. ",
			  "One useful consequence of our rounding scheme is the following.",
			  "Lemma 2.1. *At level of detail* *k* *all recursive subtrees except the one containing*\n ... \ntree, even though the relative order of elements changes very little. Instead we use a\npacked-memory array from Section 2.3 to store the van Emde Boas layout, allowing\nus to make room for changes in the tree.",
			  "n additional technical complication arises in maintaining pointers to nodes that\nmove during an update. We search in the top tree by following pointers from nodes\nto their children, represented by indices into the packed-memory array. When we\ninsert or delete an element in the packed-memory array, an amortized *O* (log 2 *N* ) ele-\nments move. Any element that is moved must let its parent know where it has go",
			  "Thus, each node must have a pointer to its parent, and so each node must also let\nits children know where it has moved.",
			  " nodes that move can have\n*O* (log 2 *N* ) children scattered throughout the packed-memory array, each in separate\nmemory blocks. Thus an insertion or deletion can potentially induce *O* (log 2 *N* ) mem-\nory transfers to update these disparately located pointers. We can afford this large\nCACHE-OBLIVIOUS B-TREES\n11\ncost in an amortized sense by adding another level of (log *N* ",
			  "verall structure of our cache-oblivious B-tree therefore has three levels. The\ntop level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array. The middle level is a collection o",
			  "The van Emde Boas layout proceeds recursively.",
			  "Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree.",
			  "Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + ",
			  "The memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail",
			  "Each level of detail is a partition of the tree into disjoint recursive\nsubtrees.",
			  "In the finest level of detail, 0, each node forms its own recursive subtree.",
			  "the coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subtree.",
			  "The key property of the van Emde Boas layout is that, at any\nlevel of detail, each recursive subtree is stored in a contiguous block of memory.",
			  "cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.",
			  "op level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array",
			  "The middle level is a collection of",
			  "We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.\nI",
			  " The cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors."
			]
		  },
		  {
			"title": "Cache Oblivious Search Trees via Binary ...",
			"url": "https://www.cs.au.dk/~gerth/papers/soda02.pdf",
			"excerpts": [
			  "The basic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22]. The static structures are maintained\nby global rebuilding, i.e. they are rebuilt each time the\ndynamic tree has doubled or halved in size.",
			  "ion.\nFor storing n elements, our proposal uses (1 +  ) n\ntimes the element size of memory, and performs searches\nin worst case O (log B n ) memory transfers, updates\nin amortized O ((log 2 n ) / ( B )) memory transfers, and\nrange queries in worst case O (log B n + k/B ) memory\ntransfers, where k is the size of the output.",
			  " For random searches, we\ncan expect the top levels of the trees to reside in cache.\nFor the remaining levels, a cache fault should happen at\nevery level for the BFS layout, approximately at every\nsecond level for the DFS layout (most nodes reside in the\nsame cache line as their left child), and every (log B n )\nlevels for the van Emde Boas layout.",
			  "In the last part of this paper, we try to assess\nmore systematically the impact of the memory layout\nof search trees by comparing experimentally the effi-\nciency of the cache-oblivious van Emde Boas layout with\na cache-aware layout based on multiway trees, and with\nclassical layouts such as Breath First Search (BFS),\nDepth First Search (DFS), and inorde",
			  " trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "Inside main memory, the BFS is best, but looses by\na factor of five outside. The tree optimized for page size\nis the best outside main memory, but looses by a factor\nof two inside. Remarkably, the van Emde Boas layout\nis on par with the best throughout the range.",
			  "4.3\nConclusion.\nFrom the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "It also appears that in the area of search trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  ".\nFigure 4 compares the time for random searches in\nimplicit layouts. For sizes up to cache size ( n = 2 16 ), it\nappears that the higher instruction count for navigating\nin an implicit layout dominates the running times: most\ngraphs are slightly higher than corresponding graphs\nin Figure 3, and the van Emde Boas layout (most\ncomplicated address arithmetic) is the slowest while the\nBFS layout (simplest address arithmetic) is fastest. ",
			  "In particular, our data structure avoids the\nuse of weight balanced B -trees, and can be implemented\nas just a single array of data elements, without the use of\npointers.",
			  "sic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22",
			  " For storing n elements, our data structure uses\n(1 +  ) n times the element size of memory. ",
			  "Searches are\nperformed in worst case O (log B n ) memory transfers,\nupdates in amortized O ((log 2 n ) / ( B )) memory trans-\nfers, and range queries in worst case O (log B n + k/B )\nmemory transfers, where k is the size of the output.",
			  "From the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "he van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "The basic idea of our data structure is to maintain a\ndynamic binary tree of height log n + O (1) using existing\nmethods, embed this tree in a static binary tree, which\nin turn is embedded in an array in a cache oblivious\nfashion, using the van Emde Boas layout of Prokop.",
			  "he\nvan Emde Boas layout , was proposed by Prokop [19,\nSection 10.2], and is related to a data structure of\nvan Emde Boas [21, 22]."
			]
		  },
		  {
			"title": "Cache-Oblivious Dynamic Search Trees Zardosht Kasheff",
			"url": "https://people.csail.mit.edu/bradley/papers/Kasheff04.pdf",
			"excerpts": [
			  "The tree is laid out in memory in a recursive\nfashion. Let *h* be the height of the binary tree. For simplicity, assume *h* is a power of 2. Let\n*N* be the number of nodes in the tree. We divide the tree into two sections. The first section\nis the top half containing a subtree, sharing the same root as the tree, of height *h/* 2 with\n**\n*N* nodes. The second section is the bottom half containing 2 *h/* 2 subtrees of height *h/* 2,\neach containing about\n**\n*N* nodes. This represents subtree *A* in Figure 2-3. The idea is to\n19\nfirst layout the top half recursively. Then layout the remaining 2 *h/* 2 subtrees recursively in\norder. This represents subtrees *B* 1 *, B* 2 *, . . . , B* *l* in Figure 2-3. In memory, the entire subtree\n*A* would be laid out first, followed by *B* 1 *, . . . , B* *l* . We assume the binary tree is full and\nbalanced. If *h* is not a power of 2, the bottom half of the tree is chosen such that its height\nis a power of 2. Figure 2-3 shows the layout of a binary tree with height 5.\n",
			  "The locations of the children of node *i* are 2 *i* and 2 *i* + 1. Thus, the\nlocation of children may be implicitly calculated. Implicit calculations of children makes the\n35",
			  "The tree is represented in memory as an array. The value at location *i* of the array\ncorresponds to some node of the tree. We need a way of computing the location of the left\nand right children of node *i* . One solution is to have the array store pointers, but pointers\ncost space. Instead, we wish to have an array such that the root of the tree is the first\nelement of the array, and for a given node located at array location *i* , the locations of the\nnodes two children are easily found. This chapter provides details.",
			  "ch that the tree\nmay be traversed with *O* (1+log *B* ( *N* )) memory transfers, which is asymptotically optimal [7].\n**"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  "The Eurographics Association and Blackwell Publishing 2006.",
			  "e compare our cache-efficient layouts with other layouts in the*\n*context of collision detection and ray tracing.",
			  "r benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applicatio",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  "he resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmost.",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C",
			  "Our layout*\n*computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the",
			  "The COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH.",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime appli",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applic",
			  ".\nHavran analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.2.\n**Layouts of geometric meshes:** Many algor",
			  "8.2. Comparison with Cache-Oblivious Mesh Layouts",
			  "We have tested the performance of the OBB-tree collision\n ... \ngraph. The edge creation methods for BVHs described in\nYoon et al. [YLPM05] do not adequately represent access\npatterns of the travers",
			  "Our greedy algorithm is\nbased on greedy heuristics to compute cache-coherent layouts\nbased on parent-child locality. T",
			  "There are several areas for future work. We would like to\nextend our probability formulation that predicts runtime data\naccess patterns of collision queries to consider other proximity\nqueries such as minimum separation distance. W",
			  " The Eurographics Association and Blackwell Publishing 2006.\n",
			  "an analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.",
			  "outs of geometric meshes:** Many algorithms and repre-\nsentations have been proposed to compute coherent layouts for\nspecialized applications. Rendering sequences (e.g., triangle\nstrips) [Dee95,Hop99] are used to improve rendering through-\nput by optimizing the vertex cache hits in the GPU. Isenburg\nand Gumhold [IG03] propose processing sequences, includ-\ning streaming meshes [IL04], as an extension of rendering se-\nquences for large-data processing. In these cases, global mesh\naccess is restricted to a fixed traversal order. Many algorithms\nuse space filling curves [Sag94] to compute cache-friendly\nlayouts of volumetric grids or height fields. These layouts\nare widely used to improve performance of image process-\ning [VG91] and terrain or volume visualization [PF01,L",
			  "We introduce a new probabilistic\nmodel to predict the runtime access patterns of BVHs based on\nlocalities.",
			  "ecifically, we utilize two types of localities during\ntraversal of a BVH: parent-child and spatial localities between\nthe accessed node",
			  "In this section, we define two localities that are used to com-\npute a cache-efficient layout of a BV",
			  "to achieve this\ngoal, we recursively compute the clusters. We first decompose\nthe BVH into a set of clusters and recursively decompose each\ncluster. In this case, the cache block boundaries can lie any-\nwhere within a layout that corresponds to the nodes of these\nclusters. Therefore, we need to compute a cache-efficient or-\ndering of the clusters computed at each level of recursion.\nOur algorithm has two different components that handle\nparent-child and spatial localities. In particular, the first part\nof our algorithm decomposes a BVH into a set of clusters that\nminimize the cache misses for parent-child locality. The clus-\nters are classified as a root cluster and child clusters. The root\ncluster contains the root node of the BVH and child clusters\nare created for each node outside the root cluster whose par-\nent node is in the root cluster (see the middle image in Fig.\n4). The second part of the algorithm computes an ordering of\nthe clusters and stores the root cluster at the beginning of the\nordering. The ordering of child clusters is computed by con-\nsidering their spatial locality. Then, we can merge two child\nclusters if it can further decrease the size of the working set.\nWe recursively apply this two-fold procedure to compute an\nordering of all the BVs in the BVH"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://diglib.eg.org/bitstream/handle/10.1111/cgf142662/v40i2pp683-712.pdf",
			"excerpts": [
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged.",
			  "he latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order",
			  " authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH. The key difference is the use of two different types of\nclusters to further reduce bandwidth and cache misses.",
			  "s first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit withi",
			  "In this report, we review the basic principles of bounding volume hierarchies as well as advanced state of the art methods with a focus on the construction and ...Read more"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies | Request PDF",
			"url": "https://www.researchgate.net/publication/220507680_Cache-Efficient_Layouts_of_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "Yoon and Manocha [YM06] proposed a node layout algorithm known as cache-oblivious BVH (COLBVH) that recursively decomposes clusters of nodes and works without prior knowledge of the cache, such as the block size.",
			  "In initialization, each node is assigned the probability that the node is accessed, given that the cluster's root is already accessed."
			]
		  },
		  {
			"title": "CacheEfficient Layouts of Bounding Volume Hierarchies - Yoon - 2006 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2006.00970.x",
			"excerpts": [
			  "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			  "Section Title: Cache-Efficient Layouts of Bounding Volume Hierarchies > Abstract",
			  "*We present a novel algorithm to compute cache-efficient layouts of bounding volume hierarchies (BVHs) of polygonal models. Our approach does not make any assumptions about the cache parameters or block sizes of the memory hierarchy. We introduce a new probabilistic model to predict the runtime access patterns of a BVH. Our layout computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the number of cache misses and the size of the working set. Our algorithm also works well for spatial partitioning hierarchies including kd-trees. We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of millions of triangles. We compare our cache-efficient layouts with other layouts in the context of collision detection and ray tracing. In our benchmarks, our layouts consistently show better performance over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the underlying algorithms or runtime applications.*",
			  "Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Hierarchy and Geometric Transformations"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "The VEB layout is complex\nto setup and maintain for trees and difficult to apply to graphs in\ngeneral.",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "Figure 2 shows graphically how this might be done. Assume\nan algorithm *P* *i* that aims to copy a tree while traversing it, into\nblocks that fit into the cache at level *i* . Using breadth first search,\nit can discover the entire subtree that fits into a block at level *i*",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "A third class of techniques (including the one in this paper) are\nused at runtime. One approach is to control memory allocation.",
			  "The van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7].",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "ble 1.** Cachegrind miss rates",
			  "Figure 4.** Binary Search Tree Performan",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.",
			  "For storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "The source code of the programs, our scripts and tools, and the data we present here are available online under ftp.brics.dk/RS/01/36/Experiments/.",
			  "Section Title: Cache Oblivious Search Trees via Binary Trees of Small Height > Abstract",
			  "Content:\nWe propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.\nFor storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.\nThe basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.\nWe also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "Van Emde Boas tree",
			"url": "https://en.wikipedia.org/wiki/Van_Emde_Boas_tree",
			"excerpts": [
			  "A van Emde Boas tree also known as a vEB tree or van Emde Boas priority queue, is a tree data structure which implements an associative array with m-bit ...Read more"
			]
		  },
		  {
			"title": "The Cost of Cache-Oblivious Searching",
			"url": "https://www3.cs.stonybrook.edu/~bender/newpub/2011-algorithmica-BenderBrFa-co-searching.pdf",
			"excerpts": [
			  "atic cache-oblivious search tree is built as follows: Embed a complete binary tree with\nN nodes in memory, conceptually splitting the tree at half its height, thus obtaining (\n\nN ) subtrees each\nwith (\n\nN ) nodes. Lay out each of these trees contiguously, storing each recursively in memory. This type\nof recursive layout is commonly referred to in the literature as a van Emde Boas layout because it is remi-\nniscent of the recursive structure of the van Emde Boas tree [37,38]. The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26",
			  " The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26]. ",
			  "We present the following results:",
			  "We present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.",
			  " Intuitively, the improvement of uneven splitting, as compared to the even splitting in the standard van\nEmde Boas layout, is likely to be due to the generation of a variety of subtree sizes at each recursive\nlevel of the layout. Such a variety will on any search path reduce the number of subtrees that can have\nparticularly bad sizes compared to the block size B",
			  "Finally, we demonstrate that it is harder to search in the cache-oblivious model than in the DAM model.\nPreviously the only lower bound for searching in the cache oblivious model was the log B N lower bound\nfrom the DAM model. We prove a lower bound of lg e log B N memory transfers for searching in the\naverage case in the cache-oblivious model.",
			  "\n We then present a class of generalized van Emde Boas layouts that optimizes performance through\nthe use of uneven splits on the height of the tree. For any constant  > 0, we optimize the layout\nachieving a performance of [lg e +  + O (lg lg B/ lg B )] log B N + O (1) expected memory transfers. ",
			  "In this section we give a tight analysis of the cost of searching in a binary tree stored using the van Emde. Boas layout [31]. As mentioned earlier, in the vEB ...Read more",
			  "s.\nWe present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.\n ",
			  "in the vEB layout, the tree is split evenly by height, except for\nroundoff. Thus, a tree of height h is split into a top tree of height  h/ 2  and bottom tree of height  h/ 2  . It\nis known [15,22] that the number of memory transfers for a search is 4 log B N in the worst case ; we give a\nmatching configuration showing that this analysis is tight. We then consider the average-case performance\nover all starting positions of the tree in memory, and we show that the expected search cost is 2(1 +\n3 /\n\nB ) log B N + O (1) memory transfers, which is tight within a 1 + o (1) factor. We assume that the data\nstructure begins at a random position in memory; if there is not enough space, then the data structure\nwraps around to the first location in memory.\nA",
			  "trees.\nThe generalized vEB layout is as follows: Suppose the complete binary tree contains N  1 = 2 h  1\nnodes and has height h = lg N . Let a and b be constants such that 0 < a < 1 and b = 1  a . Conceptually\nwe split the tree at the edges below the nodes of depth  ah  . This splits the tree into a top recursive subtree\nof height  ah  , and k = 2  ah  bottom recursive subtrees of height  bh  . Thus, there are roughly N a bottom\nrecursive subtrees and each bottom recursive subtree contains roughly N b nodes. We map the nodes of the\ntree into positions in the array by recursively laying out the subtrees contiguously in memory. The base case\nis reached when the trees have one node, as in the standard vEB layout.\nWe",
			  "We find the values of a and b that yield a layout whose memory-transfer cost is arbitrarily close to\n[lg e + O (lg lg B/ lg B )] log B N + O (1) for a = 1 / 2   and large enough N . We focus our analysis on the first\nlevel of detail where recursive subtrees have size at most the block size B . In our analysis memory transfers\ncan be classified in two types. There are V path-length memory transfers , which are caused by accessing\ndifferent recursive subtrees in the level of detail of the analysis, and there are C page-boundary memory"
			]
		  },
		  {
			"title": "Memory Layouts for Binary Search",
			"url": "https://cglab.ca/~morin/misc/arraylayout/",
			"excerpts": [
			  "`veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature.",
			  "In many settings B-trees (with a properly\nchosen value of B) are best. In others, the Eytzinger layout\nwins. In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "In many settings B-trees (with a properly\nchosen value of B) are best.",
			  "In others, the Eytzinger layout\nwins.",
			  "In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "Which of these array memory layouts is fastest?",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "For an example, consider the following two graphs, generated\nby running the same code on two different Intel machines.",
			  "In\nthe left graph, the Eytzinger layout is almost as slow as a\nplain sorted array while the van Emde Boas and B-tree layouts\nare more than twice as fast.",
			  "In the right graph, the Eytzinger layout and b-tree are the\nfastest, the sorted array is still the slowest, and the vEB layout\nis somewhere in betweeen (for array sizes).",
			  "veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature"
			]
		  },
		  {
			"title": "Cache oblivious search trees via binary trees of small height | Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms",
			"url": "https://dl.acm.org/doi/10.5555/545381.545386",
			"excerpts": [
			  "Section Title: Cache oblivious search trees via binary trees of small height",
			  "ng Brodal](# \"Gerth Stlting Brodal\") Gerth Stlting Brodal\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81409594931) , [Rolf Fagerberg](# \"Rolf Fagerberg\") Rolf Fagerberg\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100166398) , [Riko Jacob](# \"Riko Jacob\") Riko Jacob\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100438116) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386)\n[SODA '02: Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms](/doi/proceedings/10.5555/545381)\nPages 39 - 48\nPublished : 06 January 2002 [Publication History](#) To view Publication History, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386) [](# \"Check for updates on crossmark\")\n... citation ... Downloads\n[](#) To get citation alerts, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F54",
			  "For storing *n* elements, our proposal uses (1 + ) *n* times the element size of memory, and performs searches in worst case *O* (log *B* *n* ) memory transfers, updates in amortized *O* ((log 2 *n* )/( *B* )) memory transfers, and range queries in worst case *O* (log *B* *n + k/B* ) memory transfers, where *k* is the size of the output.The ",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log *n+O* (1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "We also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "[PDF] Cache Oblivious Search Trees via Binary Trees of Small Height | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache-Oblivious-Search-Trees-via-Binary-Trees-of-Brodal-Fagerberg/da35c4651414b03abe079b5c8454948c59f372c0",
			"excerpts": [
			  "Cache Oblivious Search Trees via Binary Trees of Small Height",
			  "A version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds is proposed, and can be implemented as just a single array of data elements, without the use of pointers.",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/",
			"excerpts": [
			  "We present a novel algorithm to compute cache-efficient layouts of\nbounding volume hierarchies (BVHs) of polygonal\nmodels.",
			  "We does not make any\nassumptions about the cache parameters or block sizes of the memory hierarchy.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH.",
			  "Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%\nwithout any modification of the underlying algorithms or runtime\napplications.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing.",
			  "Paper: [Cache-Efficient Layouts of Bounding Volume Hierarchies](CELBVH.pdf) , Computer graphics forum (Eurographics), volume 25, issue 3, 2006, pp. 507-516",
			  "Section Title: by [Sung-Eui Yoon](http://jupiter.kaist.ac.kr/~sungeui/) and [Dinesh Manocha](http://www.cs.unc.edu/~dm/) .",
			  "llision Detection between Hugo and 1M Power Plant Models:** The hugo robot model is placed inside the power plant model, whose\noverall shape is shown on the right. We are able to achieve 35%--2600%\nperformance improvement in collision detection by using our cache-efficient\nlayouts of the OBBTree over other tested layouts.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH. Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing. In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%",
			  "without any modification of the underlying algorithms or runtime\napplications.",
			  "We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of\nmillions of triangles.",
			  " In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%"
			]
		  },
		  {
			"title": "[PDF] CacheEfficient Layouts of Bounding Volume Hierarchies | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache%E2%80%90Efficient-Layouts-of-Bounding-Volume-Yoon-Manocha/77fdc59d6f28a3eb5da7c9fc134205a8f498f306",
			"excerpts": [
			  "A novel algorithm to compute cacheefficient layouts of bounding volume hierarchies (BVHs) of polygonal models and a new probabilistic model to predict the runtime access patterns of a BVH is introduced",
			  "Published in Computer graphics forum 1 September 2006\n",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/",
			"excerpts": [
			  "Demaine, and Martin Farach-Colton, Cache-Oblivious B-Trees, SIAM Journal on Computing, volume 35, number 2, 2005, pages 341358. Abstract: This paper presents ...Read more"
			]
		  },
		  {
			"title": "Concurrent Cache-Oblivious B-Trees - People",
			"url": "https://people.csail.mit.edu/bradley/papers/BenderFiGi05.pdf",
			"excerpts": [
			  "e first review the performance models used to analyze cache-\nefficient data structures, and then review three variations on serial\ncache-oblivious B-trees.",
			  "ernal-memory data structures, such as B-trees, are tradition-\nally analyzed in the *disk-access model (DAM)* [1], in which internal\nmemory has size *M* and is divided into blocks of size *B* , and exter-\nnal memory (disk) is arbitrarily larg",
			  "Performance in the DAM\nmodel is measured in terms of the number of block transfers.",
			  "Thus\nB-trees implement searches asymptotically optimally in the DAM\nmodel.",
			  "The *cache-oblivious model* [15,26] is like the DAM model in that\nthe objective is to minimize the number of data transfers between\ntwo levels.",
			  "A CO B-tree [8] achieves nearly optimal locality of reference at\nevery level of the memory hierarchy. It optimizes simultaneously\nfor first- and second-level cache misses, page faults, TLB misses,\ndata prefetching, and locality in the disk subsystem."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*",
			  "To mitigate\nthis, we modified the memory allocator implementation in\nEmbree [8] to allocate BVH in a single 1GB hugepage, which\nled to an end-to-end rendering speedup of 6.4%.",
			  "Divergent memory access is a major source of latency.",
			  "d stall* cycles reveals that most of the memory stalls are\ncaused by TLB misses. This is possible due to the virtually-\nindexed physically-tagged nature of L1 data cache, where TLB\nmisses can delay tag access latency and thereby the hit latency\nof the cache.",
			  "Ray sorting improves memory\ndivergence by mapping coherent rays to adjacent threads, and\nthis is evidenced by the fewer number of L1 sectors per\nrequest.",
			  "IMT execution divergence** severely limits per-\nformance of software-based GPU ray tracing. For our CUDA\nworkload, 44% of the threads in a single SIMD unit is inactive\nduring execution. This is because threads mapped to divergent\nrays unavoidably execute divergent branches or terminate early\nduring traversal, causing them to become inactive due to the\nlockstep execution of SI"
			]
		  },
		  {
			"title": "OPTIMIZING QUERY TIME IN A BOUNDING VOLUME ...",
			"url": "https://benedikt-bitterli.me/bvh-report.pdf",
			"excerpts": [
			  "To reduce the likelihood of this occurring, we implemented\na special tree layout from literature, the van Emde Boas\nordering[ **?** ], which is a cache-oblivious layout designed to\nkeep certain subtrees close together in memory.",
			  "The van Emde Boas ordering of a single node is the node\nitself.",
			  "The van Emde Boas ordering of a tree *T* of depth *d* *T*\nis the van Emde Boas ordering of the top subtree ending at\ndepth ** *d* *T*\n2 ** followed by the van Emde Boas ordering of all\nchild subtrees rooted at depth ** *d* *T*\n2 ** + 1 .\nIn",
			  ".\nInformally, this guarantees that any subtree is likely to\nbe stored in a contiguous memory segment; in other words,\nthe traversal algorithm is likely to work in a locally contigu-\nous memory segment for many traversal steps before mak-\ning a large jump through memory.",
			  "Although this should increase performance in theory, in\npractice, no performance improvement can be observed. It\nappears that TLB misses do not play a significant role in the\ntraversal performance."
			]
		  },
		  {
			"title": "Cache-Oblivious Algorithms and Data Structures",
			"url": "https://erikdemaine.org/papers/BRICS2002/paper.pdf",
			"excerpts": [
			  "A comparison of cache aware and cache oblivious static search trees using program in- strumentation. In Experimental Algorithmics: From Algorithm Design to.Read more"
			]
		  },
		  {
			"title": "Cache-oblivious algorithms and data structures",
			"url": "https://scispace.com/pdf/cache-oblivious-algorithms-and-data-structures-jjcrutokhi.pdf",
			"excerpts": [
			  "Prokop in [60] proposed static cache-oblivious search trees with search cost\nO (log B N ) I/Os, matching the search cost of standard (cache-aware) B-trees [17].",
			  "\nThe search trees of Prokop are related to a data structure of van Emde Boas [67,\n68], since the recursive layout of a search tree generated by Prokops scheme re-\nsembles the layout of the search trees of van Emde Boas.",
			  "The constant in the\nO (log B N ) search cost was studied in [21], where it is proved that no cache-\noblivious algorithm can achieve a performance better than log 2 e  log B N I/Os,\ni.e. a factor  1 . 44 slower than a cache-aware algorithm.",
			  "Dynamic B-trees were first presented by Bender et al. [22] achieving searches\nin O (log B N ) I/Os and updates requiring amortized O (log B N ) I/Os.",
			  "A cache-oblivious dictionary based on exponential search trees was presented\nin [19]."
			]
		  },
		  {
			"title": "Interactive Visualization and Collision Detection using ...",
			"url": "https://sgvr.kaist.ac.kr/~sungeui/thesis/phd_thesis_yoon_2005.pdf",
			"excerpts": [
			  "We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layou",
			  "erall, our approach offers the following\nbenefits:\n1. **Generality:** Our algorithm is general and applicable to all kind of BVHs. It\ndoes not require any knowledge of cache parameters or block sizes of a memory\nhierarchy.\n159\n2. **Applicability:** Our algorithm does not require any modification of BVH-based\nalgorithms or the runtime application. We simply compute cache-oblivious lay-\nouts of BVHs without making any assumptions about the applications.\n3. **Improved performance:** Our layouts reduce the number of cache misses during\ntraversals of BVHs. We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layouts. Main improve"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.\nNext we describe our ",
			  " Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  "In global memory we have\nachieved a runtime reduction by 1%6%. We gained a 30%\n40% runtime reduction compared to the baseline in global\nmemory when the BVH is stored in texture memory simi-\nlar to Aila et al. [ ALK12",
			  " **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  "*Address Cluster (AC):** A continuous address space that can be\nreferenced by a small pointer. If the original BVH can address\n2 *n* nodes, the maximum size of an AC is 2 *n* */* 2 .",
			  "We achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address spac",
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "4. Two-Level Clustering",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft",
			  " consider that storing *P* *Le ft* allows less nodes per\ncache line. The size *|* *P* *|* can be up to 4 bytes, which is small\ncompared to conventional BVH encodings used by previous wo",
			  "However, a node pair can be quantized to as small as 8 bytes using\nDFL (Sec. 6.1 ), whereas clustering would require up to 12 bytes.",
			  "C* can maintain the node size of the depth-first layout (or\neven reduce it), while the *CC* reorders nodes within the same *AC* for\nthe best cache utilization.",
			  "5. Algorithm",
			  "In order to effectively reduce the working set, we must carefully\nselect the nodes for each cluster. We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ].",
			  " We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ]. They attempt to order the\nnodes according to the most likely traversal path based on *parent-*\n*child* and *spatial locality* .",
			  "ssuming that all traversal paths start at the root of the tree, their\n*COLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next",
			  "4.1. Glue Nodes",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  "lue nodes* only generate bandwidth when the\nchild node is traversed, not when accessing the parent node.",
			  "Glue Nodes",
			  "7.2. Bandwidth Analysis",
			  "*A novel node layout and addressing scheme*",
			  " achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address space. We\nthen choose the order of nodes inside these clusters to maxi-\nmize the cache line locality. We introduce a new node type to\nreference address-space changes during traversal. This keeps the\nnode sizes uniform, which is more suited for a fixed function\nhardware.",
			  "ct node ordering** schemes can eliminate a few child point-\ners from the BVH. Depth-first layout (DFL) places the left child\ndirectly after the parent node, therefore only the the right pointer\nis required. Alternatively, two sibling nodes can be stored sequen-\ntially [ AL09 ]. Besides compression, these layouts can also improve\ncache locality, since child nodes are often tested together following\nthe parent during traversal. Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent",
			  "ubtree partitioning** methods first decompose the BVH into clus-\nters of nodes, each containing one or more subtrees. By optimizing\nthe node order for multiple traversal paths it can further improve\ncache locality. Moreover, the size of the child pointers within clus-\nters may be reduced. This optimization was presented for BSP trees\nby Havran [ Hav97 ]. Gil and Itai [ GI99 ] showed that cache local-\nity for tree traversal can be significantly improved if the clusters\nof nodes are generated top-down, by greedily merging the children\nwith the highest probability. Yoon et al. [ YM06 ] applied this theory\nto kd-tree based ray travers",
			  "**8-byte Internal Node:**",
			  "6 x 6 bits per plane",
			  "6 bits",
			  "2 bits",
			  "2 x 10 bits",
			  "Figure 3: A 2D illustration of our quantized storage of sibling nodes\nwith parent-plane sharing (top). The layout of our internal nodes\n(bottom). We store 2 bits to indicate leaves, one low-precision\npointer per child, and 6 plane offsets (z-axis not shown). Finally,\nour reuse mask is set to 1 if the corresponding plane belongs to the\nleft child.",
			  "Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent.\n**",
			  " compare our method (green) with the standard ODFL (gray).\nWhen scaling the L2 cache with a fixed L1, we see a different\ntrend: as the capacity of L2 increases, the reduction achieved by our\nmethod slowly diminishes. Our explanation is that the outstanding\nmisses from L2 become less and less coherent and since more of the\nfrequently traversed nodes reside inside L2, the clustering heuris-\ntic cannot predict the outgoing access pattern anymore. There is\nanother interesting trend regarding the utilization of the traversal\nunit, which increases with the L2 capacity.",
			  "hen describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footprint* . In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft* can be omit",
			  "ache Cluster (CC):** A small set of nodes that fits within a\ncache line, created within an *AC* ",
			  " Two-Level Clustering*",
			  " Two-Level Clustering*"
			]
		  },
		  {
			"title": "The Ultimate Guide to Bounding Volume Hierarchies",
			"url": "http://www.lufei.ca/posts/BVH.html",
			"excerpts": [
			  "Alternatively, proposals including Cache-Oblivious BVH (COLBVH) 19 and Swapped Subtrees (SWST) 20 organize the tree into subtree clusters in memory.Read more"
			]
		  },
		  {
			"title": "Further Reading",
			"url": "https://pbr-book.org/4ed/Primitives_and_Intersection_Acceleration/Further_Reading",
			"excerpts": [
			  "Vaidyanathan et al. ( [2016](:Vaidyanathan2016) ), who introduced a\nreduced-precision representation of the BVH that still guarantees\nconservative intersection tests with respect to the original BVH.",
			  "Liktor\nand Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node\nrepresentation based on clustering nodes that improves cache performance\nand reduces storage requirements for child node pointers.",
			  "Ylitie et\nal. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs\ninto wider BVHs with more children at each node, from which they derived a\ncompressed BVH representation that shows a substantial bandwidth reduction\nwith incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )\ndeveloped an algorithm for efficiently traversing such wide BVHs using a\nsmall stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing\nsets of adjacent leaf nodes of BVHs under the principle that most of the\nmemory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described\nan approach that saves both computation and storage by taking advantage of\nshared planes among the bounds of the children of a BVH node.",
			  "Other work in the area of space-efficient BVHs includes that of",
			  "reduced-precision representation of the BVH that still guarantees",
			  "conservative intersection tests with respect to the original BVH.",
			  "and reduces storage requirements for child node pointers. Ylitie et",
			  "with incoherent rays. Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "small stack. Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "Section Title: Further Reading > Grids > Bounding Volume Hierarchies",
			  "Liktor",
			  "Liktor",
			  "and Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node",
			  "representation based on clustering nodes that improves cache performance",
			  "representation based on clustering nodes that improves cache performance",
			  "and reduces storage requirements for child node pointers.",
			  "Ylitie et",
			  "al. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "with incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "developed an algorithm for efficiently traversing such wide BVHs using a",
			  "small stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "sets of adjacent leaf nodes of BVHs under the principle that most of the",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "an approach that saves both computation and storage by taking advantage of",
			  "an approach that saves both computation and storage by taking advantage of",
			  "shared planes among the bounds of the children of a BVH node.",
			  "shared planes among the bounds of the children of a BVH node."
			]
		  },
		  {
			"title": "High-Performance Graphics 2016",
			"url": "https://diglib.eg.org/collections/c76a314b-e5ee-4fc4-8162-19c780bda7db",
			"excerpts": [
			  "Reduced precision bounding volume ... Moreover, as BVH nodes become comparably small to practical cache line sizes, the BVH is cached less efficiently.Read more"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://www.researchgate.net/publication/284233414_Performance_Comparison_of_Bounding_Volume_Hierarchies_and_Kd-Trees_for_GPU_Ray_Tracing",
			"excerpts": [
			  "The BVH node takes 64 bytes: 24 bytes for each of the children axis-aligned bounding boxes (AABB), two 4 byte offsets to the left and right ...Read more"
			]
		  },
		  {
			"title": "[PDF] Bandwidth-efficient BVH layout for incremental hardware traversal | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Bandwidth-efficient-BVH-layout-for-incremental-Liktor-Vaidyanathan/23e7f72b23dbbc376dd46dd681e0edcde34a6037",
			"excerpts": [
			  "This paper introduces a novel memory layout and node addressing scheme and map it to a system architecture for fixed-function ray traversal and demonstrates a significant reduction in memory bandwidth, compared to previous approaches."
			]
		  },
		  {
			"title": "What is a GPU warp? | Modular",
			"url": "https://docs.modular.com/glossary/gpu/warp",
			"excerpts": [
			  "Threads in a warp can access contiguous memory locations efficiently through memory coalescing. The hardware automatically synchronizes threads within a warp ..."
			]
		  },
		  {
			"title": "Introduction to the HIP programming model  HIP 7.1.52802 Documentation",
			"url": "https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html",
			"excerpts": [
			  "Coalescing memory accesses means aligning and organizing these accesses so that multiple threads in a warp can combine their memory requests into the fewest ...Read more"
			]
		  },
		  {
			"title": "definition - In CUDA, what is memory coalescing, and how is it achieved? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/5041328/in-cuda-what-is-memory-coalescing-and-how-is-it-achieved",
			"excerpts": [
			  "A coalesced memory transaction is one in which all of the threads in a half-warp access global memory at the same time.Read more"
			]
		  },
		  {
			"title": "Accessing same global memory address within warps - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/accessing-same-global-memory-address-within-warps/66574",
			"excerpts": [
			  "If a warp accesses the same addresses several times, then the memory instruction is coalesced. Internally, NVIDIA GPUs only support gather instructions.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://meistdan.github.io/publications/raysorting/paper.pdf",
			"excerpts": [
			  " employed techniques such as path tracing produce increasingly\nincoherent ray sets due to scattering on diffuse and glossy surfaces.",
			  "\nTracing incoherent rays is much more costly than tracing coherent\nones due to higher memory bandwidth, higher cache miss rate,\nand computational divergence. ",
			  ". A number of techniques were pro-\nposed to mitigate this issue that usually use the wavefront path\ntracing combined with ray reordering, packet tracing, or ray space\nhierarchies.",
			  "The core of the ray tracing based algorithms is evaluating ray\nscene intersections, which is often referred to as the trace kernel. In\nour paper, we revisit the basic problem of sorting rays to produce\ncoherent subsets of rays in order to accelerate the trace kernel. We",
			  "We\nfocus on methods that are fully agnostic to the particular trace ker-\nnel and the employed acceleration data structure.",
			  "Such techniques\nalready appeared in the literature [Aila and Karras 2010; Costa et al .\nI3D 20, May 57, 2020, San Francisco, CA, USA\n",
			  "USA\nMeister, Bokansk, Guthe, and Bittner\n2015; Moon et al . 2010; Reis et al . 2017], but we feel there is a need\nfor their thorough comparison and deeper analysis.",
			  "\nWe aim at the following contributions:\n We summarize previously published methods for ray reorder-\ning suitable for GPU ray tracing.",
			  "We propose a method for sorting key computation that aims\nto maximize ray coherence by using a novel termination\npoint estimation technique.\n",
			  "We show the current limits of the trace acceleration using an\n ...",
			  "\nRays in three-dimensional space can be represented as points in\na five-dimensional space (ray space), where three dimensions rep-\nresent ray origins, and two dimensions represent ray directions.",
			  "cing on GPUs, Aila and\nLaine [2009] also evaluated a hash-based sorting criterion based on\ninterleaving ray origin and normalized ray direction. At that time,\nthe sorting overhead was too large to improve the overall rendering\ntime. Ano",
			  ".\nIn a case when thread divergence occurs on GPU, the whole warp\nof threads is blocked until all its rays finish the traversal. Aila and\nLaine [2009] proposed to increase SIMD efficiency by replacing al-\nready finished rays with new ones from a global queue.",
			  " Techniques\nsuch as speculative traversal slightly increase the redundancy of ray\nintersection tests because they work on possibly terminated rays.\n",
			  "Moon et al .\n[2010]. They propose to sort rays using an estimated termination\npoint that is calculated by ray tracing a simplified scene that fits\ninto the main memory. The approach is, however, only suitable for\nout-of-core ray tracing due to the expensive hit point estimation.",
			  "the sorting overhead was too large to improve the overall rendering\ntime."
			]
		  },
		  {
			"title": "Designing Fast Architecture-Sensitive Tree Search on",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2043652.2043655",
			"excerpts": [
			  "We explore latency hiding techniques for CPUs and GPUs to improve instruction throughput, resulting in better SIMD utilization. This article is an extended ...Read more"
			]
		  },
		  {
			"title": "Cpu Cache",
			"url": "https://paul.bone.id.au/blog/2019/05/01/cpu-cache/",
			"excerpts": [
			  "In other words, the cache manages 64-byte long (and aligned) blocks, or lines of memory. Managing cache in lines improves its use, since if you ...Read more"
			]
		  },
		  {
			"title": "Global memory access - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/global-memory-access/44288",
			"excerpts": [
			  "In general if you can have each thread load a 128 bit segment(16 bytes) then this will usually be faster than a 32 bit(4 bytes) or 64(8 bytes) bit word per ...Read more"
			]
		  },
		  {
			"title": "Memory transaction size - CUDA",
			"url": "https://forums.developer.nvidia.com/t/memory-transaction-size/8856",
			"excerpts": [
			  "In device 1.2+ (G200), you can use a transaction size as small as 32 bytes as long as each thread accesses memory by only 8-bit words.Read more"
			]
		  },
		  {
			"title": "Adapting Tree Structures for Processing with SIMD ...",
			"url": "https://openproceedings.org/2014/conf/edbt/ZeuchFH14.pdf",
			"excerpts": [
			  "We adapt the. B+-Tree and prefix B-Tree (trie) by changing the search al- gorithm on inner nodes from binary search to k-ary search. The k-ary search enables ...Read more",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search",
			  "The remainder of this paper is structured as follows. Sec-\ntion 2 covers preliminaries of our work. First, we discuss the\nSIMD chipset extension of modern processors and their op-\nportunities. Furthermore, we outline the *k-ary search* idea\nas the foundation for our work. Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search.",
			  "The optimized Seg-\nTrie provides a constant 14 fold speedup independently of\ntree depth and an eight fold reduced memory consumption\ncompared to the original *B* + -Tree.",
			  "Like the *B* + -Tree using binary search, the Seg-Tree adds one\nnode to the traversal path for each increase in tree depth.",
			  "The smallest data type that can currently be processed by\nthe *SIMD Extensions* is 8-bit [2]. This restriction limits a\nfurther increase in tree depth."
			]
		  },
		  {
			"title": "A High Throughput B+tree for SIMD Architectures",
			"url": "https://www.ece.lsu.edu/lpeng/papers/tpds-20-1.pdf",
			"excerpts": [
			  "AbstractB+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent.Read more",
			  "armonia, a\nnovel B+tree structure, to bridge the gaps between B+tree\nand SIMD architectures.",
			  "The key region stores the nodes with its keys in a breadth-\nfirst order",
			  "The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region.",
			  "Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality.",
			  "Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "aluations on a 28-core INTELCPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than\nthat of CPU-based HB+Tree [1], a recent state-of-the-art solution. And on a Volta TITAN VGPU, it can achieve up to 3.6 billion queries per\nsecond, which is about 3.4X faster than that of GPU-based HB+Tree.",
			  "The key region stores the nodes with its keys in a\nbreadth-first order.",
			  "To make it more efficient, Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "ap in Memory Access Requirement. Each B+tree query\nneeds to traverse the tree from root to leaf. This traversal\nbrings lots of indirect memory accesses, which is propor-\ntional to tree height",
			  "Gap in Memory Divergence. Since the target leaf node of a\nquery is generally random, multiple queries may traverse the",
			  "e\ngaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodes with its keys in a\nbreadth-first order. The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region. Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality. To",
			  "Partially-Sorted Aggregation (PSA)\n",
			  "narrowed thread-group traversal (NTG)."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "fast architecture sensitive tree search on modern CPUs ...",
			"url": "http://kaldewey.com/pubs/FAST__SIGMOD10.pdf",
			"excerpts": [
			  "t **FAST** (Fast Architecture Sensitive Tree)\nsearch algorithm that exploits high compute in modern processors\nfor index tree traversal. FAST is a binary tree, managed as a hier-\narchical tree whose elements are rearranged based on architecture\nfeatures like page size, cache line size, and SIMD width of underly-\ning hardware. We",
			  "eliminate the impact of latency with\nhierarchically blocked tree, software pipelining, and prefetches.",
			  "Having eliminated memory latency impact, we show how to ex-",
			  "cache line have the minimum number of cache misses, they found",
			  "that TLB misses are much higher than on trees with large node\nsizes, thus favoring large node sizes. Chen at al. [9] also concluded\nthat having a B+-tree node size larger than a cache line performs\nbetter and proposed pB+-trees, which tries to minimize the increase\nof cache misses of larger nodes by inserting software prefetches.",
			  "In order to efficiently use the compute performance of processors,\nit is imperative to eliminate the latency stalls, and store/access trees\nin a SIMD friendly fashion to further speedup the run-time.",
			  "*Hierarchical Blocking**\nWe advocate building binary trees (using the keys of the tuple)\nas the index structure, with a layout optimized for the specific ar-\nchitectural features.",
			  "For tree sizes larger than the LLC, the per-\nformance is dictated by the number of cache lines loaded from the\nmemory, and the hardware features available to hide the latenc",
			  "architecture features like page size, cache\nline size, and SIMD width of the underlying hardware.",
			  "this paper, we present FAST, an extremely fast architecture\nsensitive layout of the index tree. FAST is a binary tree logically\norganized to optimize for architecture features like page size, cache\nline size, and SIMD width of the underlying hardware. FAST elimi-\nnates impact of memory latency, and exploits thread-level and data-\nlevel parallelism on both CPUs and GPUs to achieve 50 million\n(CPU) and 85 million (GPU) queries per second, 5X (CPU) and\n1.7X (GPU) faster than the best previously reported performance\non the same architectures.",
			  "ompression techniques have been\nused to overcome disk I/O bottleneck by increasing the effective\nmemory capacity [15, 17, 20]. The transfer unit between mem-\nory and processor cores is a cache line. Compression allows each\ncache line to pack more data and increases the effective memory\nbandwidth. This increased memory bandwidth can improve query\nprocessing speed as long as decompression overhead is kept mini-\nmal [19, 3"
			]
		  },
		  {
			"title": "(PDF) FAST: fast architecture sensitive tree search on modern CPUs and GPUs",
			"url": "https://www.researchgate.net/publication/221213860_FAST_fast_architecture_sensitive_tree_search_on_modern_CPUs_and_GPUs",
			"excerpts": [
			  "FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware.",
			  " FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures.",
			  "FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.",
			  "In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree."
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "The memory bus of your GPU isn't simply 48 bytes wide (which would be quite cumbersome as it is not a power of 2). Instead, it is composed of 6 memory channels of 8 bytes (64 bits) each. Memory transactions are usually much wider than the channel width, in order to take advantage of the memory's burst mode. Good transaction sizes start from 64 bytes to produce a size-8 burst, which matches nicely with 16 32-bit words of a half-warp on compute capability 1.x devices.",
			  "128 byte wide transactions are still a bit faster, and match the warp-wide 32-bit word accesses of compute capability 2.0 (and higher) devices. Cache lines are also 128 bytes wide to match. Note that all of these accesses must be aligned on a multiple of the transaction width in order to map to a single memory transaction.",
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM.",
			  "When you are operating in caching mode, you can't really control the granularity of requests to global memory below 128 bytes at a time, anyway."
			]
		  },
		  {
			"title": "The granularity of L1 and L2 caches - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/the-granularity-of-l1-and-l2-caches/290065",
			"excerpts": [
			  ")\nI am currently studying CUDA.\nAcorrding to the 2022 CUDA C Programming Guide, A cache line is 128 bytes and maps to a 128 byte aligned segment in device memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte memory transactions, whereas memory accesses that are cached in L2 only are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.",
			  "In modern GPUs (say, Pascal and newer) both the L1 and L2 cache can be populated sector-by-sector. The minimum granularity is 1 sector or 32 bytes. The cache line tag, however, applies to 4 sectors (in each case) that comprise the 128-byte cache line. You can adjust L2 cache granularity."
			]
		  },
		  {
			"title": "Coalesced Memory Read Question - CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/coalesced-memory-read-question/41565",
			"excerpts": [
			  "From CUDA Best Programming Guide I know that GPU reading 128 byte word for one warp transaction. Thats mean, each warp(32 threads) can easy read any 4 byte data type in coalesced way during one cycle.",
			  "Im no expert but I think youre right. If you had 32 float3s, it would take 3 warp cycles, no matter how you arrange the data.",
			  "SoA approach would still take 3 warp cycles as well. No matter how you slice it, youre getting a full read.",
			  "Yes, multiple transactions will be issued.",
			  "SoA as a general recommendation is a good idea, but there shouldnt be any problem (no difference in efficiency) with loading a float4 per thread."
			]
		  },
		  {
			"title": "CUDA Performance Optimization",
			"url": "https://juser.fz-juelich.de/record/915940/files/04-mhrywniak-perf_opt.pdf",
			"excerpts": [
			  "To achieve coalesced global memory access: Usually: Fix your access pattern. Try to use shared memory (but first, check cache behavior). Look for different way ..."
			]
		  },
		  {
			"title": "An Evaluation of B-tree Compression Techniques | The VLDB Journal | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1007/s00778-025-00950-8",
			"excerpts": [
			  "Among them, B-tree compression is an important technique introduced as early as the 1970s to improve both space efficiency and query performance ...Read more"
			]
		  },
		  {
			"title": "fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu- ...",
			"url": "https://scispace.com/pdf/fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu-42t1vrahpt.pdf",
			"excerpts": [
			  "e present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive int",
			  "r breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack. ",
			  "This algorithm amortizes the cost of node access\npattern among the rays.",
			  "Ray sorting timings are not taken\ninto account when we evaluate [ AL09 ] performance."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2013/Laine-MegakernelsConsideredHarmful.pdf",
			"excerpts": [
			  "llows further optimizations**\n**Collecting requests in operation-specific queues and scheduling**\n**them individually**\n**This is the big one!** **Really hard to do in the megakernel approach**\n****\n**Path state must reside in memory**\n**A simple loop-based method can keep it in registers**\n**Not as bad as it sounds if we use a good memory layout (SOA",
			  "**Step 2: Per-Operation Queues**\n**Allocate a queue for each primitive operation request**\n**Extension ray casts**\n**Shadow ray casts**\n**New path generation**\n**Material evaluations**\n***With separate queues for individual materials***\n**Place requests compactly (i.e., no gaps) into queues**\n**When executing, use one thread per request**\n**Every thread will have an item to work on**\n**Every thread will be doing the same thing, so theres very**\n**little execution divergence!**"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "It is shown that this type of hash table can benefit from the\ncache locality and multi-threading more than ordinary hash tables.",
			  "Moreover, the batch design\nfor hash tables is amenable to using advanced features of modern processors such as prefetching\nand SIMD vectorization.",
			  "While state-of-the-art research and open-source projects on batch hash\ntables made efforts to propose improved designs by better usage of mentioned hardware features,\ntheir approaches still do not fully exploit the existing opportunities for performance improvements.",
			  "Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing.",
			  "To allow developers to fully take\nadvantage of its performance, we recommend a high-level batch API design.",
			  "Our experimental\nresults show the superiority and competitiveness of this approach in comparison with the alternative\nimplementations and state-of-the-art for the data-intensive workloads of relational join processing,\nset operations, and sparse vector processing.",
			  "The SIMD is a hardware\nfeature that allows the simultaneous execution of an operation on a vector of values.",
			  "On\nthe other hand, prefetching is a hardware feature that allows the program to request future\nmemory accesses in advance and asynchronous to the other computations.",
			  "We will cover the\nmore-detailed definitions of these two concepts later in this section.",
			  " 1 ], Horton [ 9 ] and Cuckoo++[ 23 ] have focused on improving the\nperformance of batch hash tables by applying SIMD and prefetching techniques to a specific\ntype of SIMD-aware batch hash table designs called Bucketized Cuckoo Hash Tables (BCHTs)",
			  " Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing. ",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "Modern CPUs support hardware and software prefetch-\ning. Prefetching improves the performance of a program by amortizing the costs of memory\naccess over tim",
			  "In hash tables, regardless of the hashing scheme, accessing entries is based on the value\nof the computed hash for each provided key.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable.",
			  " \nFigure 6 depicts a generic and high-level algorithm for combining prefetching with vertical\nvectorization (based on the assumption that we take the group-prefetching approach instead\nof standard prefetching",
			  "By having a group of keys as input, before starting the vertical\nvectorization, we define a loop over the group keys (prefetching loop).",
			  "foreach\ngroup in array by GROUP_SIZE {",
			  "// prefetching\nstage",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "Conditions of coalescing global memory into few transactions - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/conditions-of-coalescing-global-memory-into-few-transactions/109481",
			"excerpts": [
			  "The (L2) cache can act as a coalescing buffer by collecting write activity from multiple instructions, before it is written out to DRAM, in presumably a minimized set of transactions. This is possible in part because L2 has write-back, not write-through, behavior.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp."
			]
		  },
		  {
			"title": "CUDA on WSL User Guide  CUDA C++ Best Practices Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/",
			"excerpts": [
			  "A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.",
			  "On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.",
			  "On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.",
			  "The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the *k* -th thread accesses the *k* -th word in a 32-byte aligned array. Not all threads need to participate."
			]
		  },
		  {
			"title": "Memory Latency - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/memory-latency",
			"excerpts": [
			  "eturning to Little's Law, we notice that it assumes that the full bandwidth be utilized, meaning, that all 64 bytes transferred with each memory block are useful bytes actually requested by an application, and not bytes that are transferred just because they belong to the same memory block. When any amount of data is accessed, with a minimum of one single byte, the entire 64-byte block that the data belongs to is actually transferred. To make sure that all bytes transferred are useful, it is necessary that accesses are *coalesced* , i.e. requests from different threads are presented to the [memory management unit](/topics/computer-science/memory-management-unit \"Learn more about memory management unit from ScienceDirect's AI-generated Topic Pages\") (MMU) in such a way that they can be packed into accesses that will use an entire 64-byte block",
			  " Typical latencies are 4 cycles for the L1 cache, 12 cycles for the [L2 cache](/topics/computer-science/l2-cache \"Learn more about L2 cache from ScienceDirect's AI-generated Topic Pages\") , and roughly 150-200 cycles for main memory. This memory hierarchy enhances memory performance in two ways. On one hand, it reduces the memory latency for recently used data. On the other hand, it reduces the number of accesses to the main memory, thereby limiting the usage of the network interconnect and the bandwidth demand. Indeed, accesses to L1 and L2 caches do not require network activation because they are part of the [processor socket](/topics/computer-science/processor-socket \"Learn more about processor socket from ScienceDirect's AI-generated Topic Pages\")",
			  "The size of memory transactions varies significantly between Fermi and the older versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction size would start off at 128 bytes per memory access. This would then be reduced to 64 or 32 bytes if the total region being accessed by the coalesced threads was small enough and within the same 32-byte aligned block. This memory was not cached, so if threads did not access consecutive memory addresses, it led to a rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2, 3, 4, , 31 and thread 1 reads addresses32, 32, 34, , 63, they will not be coalesced. In fact, the hardware will issue one read request of at least 32 bytes for each thread. The bytes not used will be fetched from memory and simply be discarded. Thus, without careful consideration of how memory is used, you can easily receive a tiny fraction of the actual bandwidth available on the device.",
			  "The situation in Fermi and Kepler is much improved from this perspective. Fermi, unlike compute 1.x devices, fetches memory in transactions of either 32 or 128 bytes. A 64-byte fetch is not supported. By default every memory transaction is a 128-byte cache line fetch. Thus, one crucial difference is that access by a stride other than one, but within 128 bytes, now results in cached access instead of another memory fetch. This makes the GPU model from Fermi onwards considerably easier to program than previous generations.",
			  "The 16 LSUs distributes 64 of the 128 bytes to the registers used by the first half-warp of warp 0. In the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register used by the other half-warp. However, warp 0 still can not progress as it has only one of the two operands it needs for the multiply. It thus does not execute and the subsequent bytes arriving from the coalesced read of a for the other warps are distributed to the relevant registers for those warps.",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data. For most programs, the data brought into the cache will then allow a cache hit on the next [loop iteration](/topics/computer-science/loop-iteration \"Learn more about loop iteration from ScienceDirect's AI-generated Topic Pages\") .",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data.",
			  "ple, the MMU can only find 10 threads that read 10 4-byte words from the same block, 40 bytes will actually be used and 24 will be discarded. It is clear that coalescing is extremely important to achieve high memory utilization, and that it is much easier when the access pattern is regular and contiguous. Th"
			]
		  },
		  {
			"title": "CUDA C++ Programming Guide (Legacy)  CUDA C++ Programming Guide",
			"url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/",
			"excerpts": [
			  "By default page-locked host memory is allocated as cacheable. It can optionally be allocated as *write-combining* instead by passing flag `cudaHostAllocWriteCombined` to `cudaHostAlloc()` . Write-combining memory frees up the hosts L1 and L2 cache resources, making more cache available to the rest of the application. In addition, write-combining memory is not snooped during transfers across the PCI Express bus, which can improve transfer performance by up to 40%.",
			  "ng from write-combining memory from the host is prohibitively slow, so write-combining memory should in general be used for memory that the host only writ",
			  "Using CPU atomic instructions on WC memory should be avoided because not all CPU implementations guarantee that functionality.",
			  "An access policy window specifies a contiguous region of global memory and a persistence property in the L2 cache for accesses within that region.",
			  "The code example below shows how to set an L2 persisting access window using a CUDA Stream.",
			  "When a kernel subsequently executes in CUDA `stream` , memory accesses within the global memory extent `[ptr..ptr+num_bytes)` are more likely to persist in the L2 cache than accesses to other global memory locations.",
			  "The `hitRatio` parameter can be used to specify the fraction of accesses that receive the `hitProp` property.",
			  "For example, if the L2 set-aside cache size is 16KB and the `num_bytes` in the `accessPolicyWindow` is 32KB:",
			  "With a `hitRatio` of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.",
			  "Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://www.researchgate.net/publication/2623917_Making_B-Trees_Cache_Conscious_in_Main_Memory",
			"excerpts": [
			  "CSB+Tree (Rao and Ross 2000) improves key density and reduces cache accesses and misses by storing only the address of the first child and ...Read more"
			]
		  },
		  {
			"title": "TRAVERSING A BVH CUT TO EXPLOIT RAY COHERENCE",
			"url": "https://www.scitepress.org/Papers/2011/33634/33634.pdf",
			"excerpts": [
			  "algorithms used for traversing a subtree are due to. (Aila and Laine, 2009). They are the persistent packet and the persistent while-while and will be ..."
			]
		  },
		  {
			"title": "How do cache lines work?",
			"url": "https://stackoverflow.com/questions/3928995/how-do-cache-lines-work",
			"excerpts": [
			  "Modern PC memory modules transfer 64 bits (8 bytes) at a time, in a burst of eight transfers, so one command triggers a read or write of a full cache line from ...Read more"
			]
		  },
		  {
			"title": "Search Lookaside Buffer: Efficient Caching for Index Data ...",
			"url": "https://wuxb45.github.io/papers/slb.pdf",
			"excerpts": [
			  "The CPU cache can leverage access locality to keep the most frequently used part of an index in it for fast access. However, the traversal on the index to a ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Proceedings of the 2001 ACM symposium on Applied computing",
			"url": "https://dl.acm.org/doi/10.1145/372202.372329",
			"excerpts": [
			  "The B+-Tree is the most popular index structure in database systems. In this paper, we present a fast B+-Tree ... Read More  Storage systems for movies-on ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Batch-construction-of-B%2B-trees-Kim-Won/a4f0cb5e06927a905cccb25056730b2a95e48d06",
			"excerpts": [
			  "An algorithm for batchconstructing the B+-tree, the most widely-used index structure in database systems, which achieves up to 28 times performance gain ..."
			]
		  },
		  {
			"title": "Fast Divergent Ray Traversal by Batching Rays in a BVH",
			"url": "https://jbikker.github.io/literature/Fast%20Divergent%20Ray%20Traversal%20by%20Batching%20Rays%20in%20a%20BVH%20-%202016.pdf",
			"excerpts": [
			  "In this work we propose a batching traversal scheme\ncalled RayCrawler.",
			  "Our scheme operates on a hierarchy of\nBVHs by splitting an existing BVH into two separate layers,\ncreating a top-level tree and multiple small trees that fit in the\nL2 cache of modern CPUs.",
			  "The Top-BVH traversal stack of the ray is\nstored to resume traversal later on.",
			  "he traversal algorithm starts by first traversing each\nray depth-first through the Top-BVH; once the ray reaches\na leaf node of the Top-BVH, the ray is batched at the Leaf-\nBVH that the leaf node is pointing to and the traversal of the\nray is suspended.",
			  "This system tries to amortize the cost of retrieving a Leaf-\nBVH from memory by traversing many batched rays through\nthe Leaf-BVH once it has been loaded into cache",
			  "hed rays. The scheme\nachieves modest speedups compared to a single-ray traver-\nsal algorithm for secondary rays and proves that a batching\nscheme can outperform a naive single-ray traversal approach\nfor highly divergent rays",
			  "he comparisons in this work are made with the algo-\nrithms implemented in the Embree framework version 2.7.1",
			  "s section gives a short overview of the data structure used\nthroughout the paper and an overview of the traversal al-\ngorithm. The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched",
			  "We achieve this by splitting a regular 4-wide BVH in two\nlayers. The rays are batched in the leaf nodes of the top\nlayer before traversing the bottom layer of the data struc-\ntur",
			  " The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched rays."
			]
		  },
		  {
			"title": "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/3170492.3136044",
			"excerpts": [
			  "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms",
			  "GPCE 2017: Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and ExperiencesIn order to achieve the highest possible performance, the ray traversal and intersection\nroutines at the core of every high-performance ray tracer are usually hand-coded,\nheavily optimized, and implemented separately for each hardware platformeven ...[Read More](# \"",
			  "Stackless traversal algorithms for ray tracing acceleration structures require significantly\nless storage per ray than ordinary stack-based ones. This advantage is important for\nmassively parallel rendering methods, where there are many rays in flight. ...[Read More]",
			  "Efficient stack-less BVH traversal for ray tracing",
			  "SCCG '11: Proceedings of the 27th Spring Conference on Computer GraphicsWe propose a new, completely iterative traversal algorithm for ray tracing bounding\nvolume hierarchies that is based on storing a parent pointer with each node, and on\nusing simple state logic to infer which node to traverse next."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://minkhollow.ca/Courses/461/Notes/Trees/Resources/rao.pdf",
			"excerpts": [
			  "we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "CSS-Trees augment binary search\nby storing a directory structure on top of the\nsorted list of elements.",
			  "We proposed a new index\nstructure called Cache-Sensitive Search Trees\n(CSS-Tree) that has even better cache behavior\nthan a B + -Tree. ",
			  "With a large amount of RAM, most of the\nindexes can be memory resident.",
			  "Our experiments are performed for 4-byte keys\nand 4-byte child pointers. Theoretically, B + -Trees\nwill have 30% more cache misses than CSB + -Trees.",
			  "In this paper, we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "As the gap between CPU and memory speed is\nwidening, CSB + -Trees should be considered as a re-\nplacement for B + -Trees in main memory database",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees."
			]
		  },
		  {
			"title": "Cache Craftiness for Fast Multicore Key-Value Storage",
			"url": "https://pdos.csail.mit.edu/papers/masstree:eurosys12.pdf",
			"excerpts": [
			  "Masstree uses a combination of old and new techniques\nto achieve high performance [8, 11, 13, 20, 2729]. It\nachieves fast concurrent operation using a scheme inspired\nby OLFIT [11], Bronson *et al.* [9], and read-copy up-\ndate [28]. Lookups use no locks or interlocked instructions,\nand thus operate without invalidating shared cache lines and\nin parallel with most inserts and update",
			  "Masstree shares a single tree among all cores to avoid load\nimbalances that can occur in partitioned designs.",
			  "The tree\nis a trie-like concatenation of B + -trees, and provides high\nperformance even for long common key prefixes, an area in\nwhich other tree designs have trouble.",
			  "Masstree uses a\nwide-fanout tree to reduce the tree depth, prefetches nodes\nfrom DRAM to overlap fetch latencies, and carefully lays\nout data in cache lines to reduce the amount of data needed\nper node.",
			  "Masstree achieves six to ten million operations per\nsecond on parts AC of the benchmark, more than 30 ** as\nfast as VoltDB [5] or MongoDB [2",
			  "e contributions of this paper are as follows. First, an\nin-memory concurrent tree that supports keys with shared\nprefixes efficiently. Second, a set of techniques for laying\nout the data of each tree node, and accessing it, that reduces\nthe time spent waiting for DRAM while descending the tree.\nThird, a demonstration that a single tree shared among mul-\ntiple cores can provide higher performance than a partitioned\ndesign for some workloads. Fourth, a complete design that\naddresses all bottlenecks in the way of million-query-per-\nsecond performance",
			  "Masstree provides high\nconcurrency from the start."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory",
			"url": "https://dl.acm.org/doi/pdf/10.1145/335191.335449",
			"excerpts": [
			  "ache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a\nvariant of B + -Trees that stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node.",
			  "We propose a new indexing technique called\nCache Sensitive B + -Trees (CSB + -Trees).",
			  "t stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node. The rest of the children can\nbe found by adding an offset to that address. ",
			  "Since only\none child pointer is stored explicitly, the utilization of\na cache line is high.",
			  "In Section 2 we survey related work on cache\noptimization.\nIn Section 3 we introduce our\nnew CSB + -Tree and its variants.",
			  "Full CSB + -Trees are better than B + -Tree in all\naspects except for space.\nWhen space overhead\nis not a big concern,\nFull CSB + -Tree is the\nbes"
			]
		  },
		  {
			"title": "Cache craftiness for fast multicore key-value storage | Proceedings of the 7th ACM european conference on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/2168836.2168855",
			"excerpts": [
			  "J. Rao and K. A. Ross. Making B+-trees cache conscious in main memory. SIGMOD Record, 29:475--486, May 2000.",
			  ". Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: A cache-sensitive parallel external sort. The VLDB Journal, 4(4):603--627, 1995.\n"
			]
		  },
		  {
			"title": "What is Memory Coalescing? | GPU Glossary",
			"url": "https://modal.com/gpu-glossary/perf/memory-coalescing",
			"excerpts": [
			  "Memory coalescing takes advantage of the internals of DRAM technology to enable\nfull bandwidth utilization for certain access patterns. Each time a DRAM address\nis accessed, multiple consecutive addresses are fetched together in parallel in\na single clock. For a bit more detail, see Section 6.1 of [the 4th edition of Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311) ;\nfor comprehensive detail, see Ulrich Drepper's excellent article [*What Every Programmer Should Know About Memory*](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf) .\nThe access and transfer of these consecutive memory locations is referred to as\na *DRAM burst* . If multiple concurrent logical accesses are serviced by a single\nphysical burst, the access is said to be *coalesced* . Note that a physical\naccess is part of a memory transaction, terminology you may see elsewhere in\ndescriptions of memory coalescing.\nOn CPUs, a similar mapping of bursts onto cache lines improves access\nefficiency. As is common in GPU programming, what is automatic cache behavior in\nCPUs is here programmer-managed."
			]
		  },
		  {
			"title": "Request Hedging vs Request Coalescing: A Software Engineers Guide to Optimizing Distributed Systems | by Sourav Chaurasia | Medium",
			"url": "https://medium.com/@mr.sourav.raj/request-hedging-vs-request-coalescing-a-software-engineers-guide-to-optimizing-distributed-fdcc6590ba9d",
			"excerpts": [
			  "**Request Coalescing** , also known as request deduplication or the singleflight pattern, is a resource optimization technique that merges multiple identical concurrent requests into a single execution. When multiple clients request the same resource simultaneously, only one actual request is executed, and all requesters share the result.",
			  "**Benefits:**",
			  "**Significant resource savings** : Can reduce duplicate executions by 7090% in high-concurrency scenarios.",
			  "**Improved cache efficiency** : Better hit rates when multiple requests need the same data.",
			  "**Prevents thundering herd** : Avoids overwhelming backend services during spikes.",
			  "**Lower infrastructure costs** : Reduced CPU, memory, and network usage.",
			  "**Drawbacks:**",
			  "**No individual latency improvement** : Doesnt help single request performance.",
			  "**Implementation complexity** : Requires careful key generation and cleanup.",
			  "**Memory overhead** : Must track pending requests and their futures.",
			  "**Potential bottlenecks** : Shared operations can become single points of failure.",
			  "**Discords Message Storage:** Discord uses request coalescing to manage its trillion-message database efficiently, preventing duplicate queries for the same message data.",
			  "**CDN Cache Filling:** Content delivery networks coalesce multiple requests for the same uncached resource, fetching it once and serving all waiting clients.",
			  "**Authentication Token Refresh:** When multiple requests need to refresh an expired token simultaneously, coalescing ensures only one refresh operation occurs.",
			  "**Database Query Optimization:** High-traffic applications coalesce identical database queries to reduce load and improve response times.",
			  "**Memory vs CPU Balance** Coalescing trades memory (for tracking pending requests) against CPU and network resources (avoiding duplicate work).",
			  "**Key Strategy Impact:** The effectiveness depends heavily on your key generation strategy:\nToo specific: Minimal coalescing benefit.\nToo general: Risk of sharing inappropriate results.\nOptimal: Balance between specificity and reuse.",
			  "\n**Cleanup Strategies:** Implement proper cleanup to prevent memory leaks",
			  "**Resource efficiency matters** : Infrastructure costs or capacity are constraints.",
			  "**High concurrency** : Many clients are requesting identical data simultaneously.",
			  "**Expensive operations** : Computationally intensive or slow database queries.",
			  "**Cache scenarios** : Filling caches or warming up cold data.",
			  "Request hedging and request coalescing represent two fundamental approaches to optimizing distributed systems. Hedging prioritizes user experience through latency reduction, while coalescing prioritizes system efficiency through resource optimization."
			]
		  },
		  {
			"title": "Two-Minute Tech Tuesdays - Request Coalescing - Resources",
			"url": "https://info.varnish-software.com/blog/two-minutes-tech-tuesdays-request-coalescing",
			"excerpts": [
			  "This episode of Two Minute Tech Tuesdays is about request coalescing, a core feature in Varnish that is used to reduce the stress on origin servers when multiple requests are trying to fetch the same uncached content.",
			  "What happens when multiple clients request the same uncached content from Varnish? Does Varnish open up the same amount of requests to the origin, and potentially destabilize the entire platform under heavy load (the Thundering Herd effect)? Luckily Varnish is not sensitive to the Thundering Herd. It identifies requests to the same uncached resource, queues them on a waiting list, and only sends a single request to the origin. As the origin responds, Varnish will satisfy the entire waiting list in parallel, so there's no [head-of-line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking) : everyone gets the content at exactly the same time. So request coalescing will effectively merge multiple potential requests to the origin into a single request.",
			  "The advantages are pretty straightforward: * Less pressure on the origin server",
			  "Less latency for queued clients",
			  "In terms of domains of application, request coalescing is useful for: * Long tail content that doesn't always end up in the cache",
			  "This only applies to cacheable content. Uncacheable content cannot take advantage of request coalescing. With uncacheable content we mean content that uses set cookie headers, or that has cache control response headers that deliberately bypass the cache.",
			  "With serialization we mean items on the waiting list being processed serially, rather than in parallel.",
			  "doing so we avoid potential serialization. With serialization we mean items on the waiting list being processed serially, rather than in parallel. This has a very detrimental effect on the performance and the quality of experience for the user, because in this case there actually is head-of-line blocking. Luc",
			  "Luckily Varnish Configuration Language (VCL) has provisions for that:",
			  "set beresp.ttl = 120s;",
			  "set beresp.uncacheable = true;"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)",
			  "Decoupled Access/Execute (DAE)",
			  "Idea: Decouple operand\naccess and execution via\ntwo separate instruction\nstreams that communicate\nvia ISA-visible queues .",
			  "Smith,  Decoupled Access/Execute\nComputer Architectures ,  ISCA 1982,\nACM TOCS 1984.",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Advantages:\n+ Execute stream can run ahead of the access stream and vice\nversa\n+ If A is waiting for memory, E can perform useful work\n+ If A hits in cache, it supplies data to lagging E\n+ Queues reduce the number of required registers\n+ Limited out-of-order execution without wakeup/select complexi",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream.",
			  "Control flow synchronization (like branches) is handled via a separate branch queue.",
			  "The compiler analyzes the program, identifies operations belonging to each stream, and generates two distinct instruction sequences that explicitly communicate via queue operations inserted by the compiler.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Stop Crying Over Your Cache Miss Rate: Handling ...",
			"url": "https://www.epfl.ch/labs/lap/wp-content/uploads/2019/06/AsiaticiFeb19_StopCryingOverYourCacheMissRateHandlingEfficientlyThousandsOfOutstandingMissesInFpgas_FPGA19.pdf",
			"excerpts": [
			  "could generate even more\nrequests per cycle with no fundamental limitations on the total num-\nber of in-flight operations",
			  "a shared MHA to maximize the merging opportunities.",
			  "In general, the cache requires 15 block RAMs per 32 kB per cache way, the MSHR buffer requires 1 block RAM per 512 MSHRs per cuckoo hash table for storage plus ...Read more"
			]
		  },
		  {
			"title": "Addressing Isolation Challenges of Non-blocking Caches ...",
			"url": "https://www.ittc.ku.edu/~heechul/papers/taming-rtsj17.pdf",
			"excerpts": [
			  "Miss Status Holding\nRegisters (MSHRs), which track the status of outstanding cache-misses, can be a sig-\nnificant source of contention that is not addressed by conventional cache partitioning.",
			  "e propose to dynamically control the num-\nber of usable MSHRs in the private L1 caches",
			  "We add two\nhardware counters *TargetCount* and *V alidCount* for each L1 cache controller.",
			  "The *V alidCount* tracks the number of total valid MSHR entries (i.e., entries with\noutstanding memory requests) of the cache and is updated by the hardware.",
			  "The\n*TargetCount* defines the maximum number of MSHRs that can be used by the core\nand is set by the system software (OS).",
			  " By control-\nling the value of *TargetCount* , the OS can effectively control the cores local MLP.",
			  "The added area and logic complexity is minimal as we only need two additional\ncounter registers and one comparator logic.",
			  "The degree of parallelism supported by\na memory subsystem is called *Memory-Level Parallelism (MLP)* [13].",
			  "Non-blocking\ncaches are essential to provide high MLP in multicore processors.",
			  "When a cache-miss occurs on a non-blocking cache, the cache controller records\nthe miss on a special register, called Miss Status Holding Register (MSHR) [27],",
			  "The request is managed at a cache-line\ngranularity. Multiple misses to the same cache-line are merged and notified together\nby a single MSHR entry.",
			  "The MSHR entry is cleared when the corresponding mem-\nory request is serviced from the lower-level memory hierarchy. ",
			  " other words, cache hit re-\n ... \ndynamic CPU and memory frequency scaling. Table 1 shows the basic characteristics\nof the five CPU architectures we used in our experime",
			  "3.2 Memory-Level Parallelism\nWe first identify memory-level parallelism (MLP) of the multicore architectures using\nan experimental method described in [11].",
			  "The method uses a pointer-chasing micro-\nbenchmark shown in Figure 3 to identify memory-level parallelism.",
			  "Latency\nis a pointer chasing synthetic benchmark, which accesses a randomly shuffled single\nlinked list. Due to data dependencies, Latency can only generate one outstanding re-\nquest at a time."
			]
		  },
		  {
			"title": "Scalable Cache Miss Handling for High Memory-Level ...",
			"url": "https://iacoma.cs.uiuc.edu/iacoma-papers/micro06_mshr.pdf",
			"excerpts": [
			  " a line is *primary* if there is currently no outstand-\ning miss on the line and, therefore, a new MSHR needs to be allo-\ncated. ",
			  "A miss is *secondary* if there is already a pending miss on the\nline. In this case, the existing MSHR for the line can be augmented to\nrecord the new miss, and no request is issued to memory. In this case,\nthe MSHR for a line keeps information for all outstanding misses on\nthe line.",
			  " each miss, it contains a *subentry* (in contrast to an *en-*\n*try* , which is the MSHR itself",
			  "Once an MHA exhausts its MSHRs or subentries, it\n*locks-up* the cache (or the corresponding cache bank).",
			  "From then on,\nthe cache or cache bank rejects further requests from the processor.",
			  "This may eventually lead to a processor stall.",
			  "They used a design where each cache bank has its own\nMSHR fi le (Figure 1(b)), but did not discuss the MSHR itself"
			]
		  },
		  {
			"title": "CudaDMA | Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis",
			"url": "https://dl.acm.org/doi/10.1145/2063384.2063400",
			"excerpts": [
			  "Section Title: CudaDMA: optimizing GPU memory bandwidth via warp specialization > Abstract\nContent:\nAs the computational power of GPUs continues to scale with Moore's Law, an increasing number of applications are becoming limited by memory bandwidth. We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories. Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms. DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout. To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns. Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Research",
			"url": "https://research.nvidia.com/publication/2011-11_cudadma-optimizing-gpu-memory-bandwidth-warp-specialization",
			"excerpts": [
			  "We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic micro-benchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "Leveraging Warp Specialization for High Performance on GPUs",
			"url": "https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf",
			"excerpts": [
			  "e crucial insight for warp specialization is that while con-\ntrol divergence within a warp results in performance degradation,\ndivergence between warps does not.",
			  "med barriers pro-\nvide two operations: *arrive* and *sync* . Arrive is a non-blocking op-\neration which registers that a warp has arrived at a barrier and then\ncontinues execution. Sync is a blocking operation that waits until\nall the necessary warps have arrived or synced on the barrie",
			  "Warp-specialized partitioning provides a useful mechanism for\nDSL compilers when grappling with computations that exhibit\nboth irregularity and large working sets.",
			  "The mapping compilation stage is responsible for taking in an\narbitrary dataflow graph of operations and mapping it onto the\nspecified number of warps and available GPU memories.",
			  "It is important to\nnote that named barriers support synchronization between arbitrary\nsubsets of warps within a CTA, including allowing synchronization\nbetween a single pair of warps as in this example.",
			  "rtitioning computations us-\ning warp specialization allows Singe to deal efficiently with the\nirregularity in both data access patterns and computation.",
			  "Consumer Warp",
			  "Producer Warp",
			  "(signal begin)",
			  "(wait until ready)",
			  "bar.sync",
			  "bar.sync",
			  "(wait until begin)",
			  "bar.arrive",
			  "bar.arrive",
			  "(signal ready)",
			  "producer-consumer named barrier is used\nto indicate to the QSSA warps when the needed values from the\nnon-QSSA warps have been written into shared memory.",
			  "arp-specialized programs also require more expressive syn-\nchronization mechanisms",
			  "Warp specialization exploits the division\nof a thread block into warps to partition computations into sub-\ncomputations such that each sub-computation is executed by a\ndifferent warp within a thread block.",
			  "However, by using inline PTX statements, a CUDA program\nhas access to a more expressive set of intra-CTA synchronization\nprimitives referred to as *named barriers*",
			  "Using arrive and sync operations, programmers can encode\nproducer-consumer relationships in warp-specialized programs.",
			  "gure 2 illustrates using two named barriers to coordinate move-\nment of data from a producer warp (red) to a consumer warp (blue)\nthrough a buffer in shared memory.",
			  "he\nconsumer warp signals the buffer is ready by performing a non-\nblocking arrive operation.",
			  "Since the arrive is non-blocking, the\nconsumer warp is free to perform additional work while waiting\nfor the buffer to be filled.",
			  "At some point the consumer warp blocks\non the second named barrier waiting for the buffer to be full.",
			  "The\nproducer warp signals when the buffer is full using a non-blocking\narrive operation on the second named barrier.",
			  "Named Barrier 0",
			  "Named Barrier 1"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  " root cause of this entanglement is the require-\nment encouraged by the CUDA programming model that\nthreads of a CTA perform both memory accesses and com-\nputation. By creating specialized warps that perform inde-\npendent compute and memory operations we can tease apart\nthe issues that affect memory performance from those that\naffect compute performan",
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps.",
			  "arp specialization allows subsets of threads within\na CTA to have their behavior tuned for a particular purpose\nwhich enables more efficient consumption of constrained re-\nsources",
			  "If single buffering is not exploiting enough MLP to keep\nthe memory system busy, an alternative is to create two\nbuffers with two sets of DMA warps for transferring data.\nWe call this two-buffer technique double buffering ",
			  "DMA warps also improve programmer productivity\nby decoupling the need for thread array shapes to match\ndata layout.",
			  "Named Barrier 0",
			  "Named Barrier 1",
			  "compute warps",
			  "DMA warps",
			  "start_async_dma()",
			  "wait_for_dma_fnish()",
			  "wait_for_dma_start()",
			  "(bar.sync)",
			  "(bar.sync)",
			  "fnish_async_dma()",
			  "(bar.arrive)",
			  "(bar.arrive)",
			  "The\nproducer/consumer nature of our synchronization mecha-\nnisms allow the programmer to employ a variety of tech-\nniques to overlap communication and computation",
			  "e accomplish fine-grained synchronization by using in-\nlined PTX assembly to express named barriers .",
			  "med\nbarriers are hardware resources that support a barrier oper-\nation for a subset of warps in a CTA and can be identified\nby a unique name (e.g. immediate value in PTX).",
			  "There are\ntwo named barriers associated with every cudaDMA object.\nTwo barriers are required to track whether the data buffer\nin shared memory is full or empty.",
			  "e use the PTX in-\nstruction bar.arrive , which allows a thread to signal its ar-\nrival at a named barrier without blocking the threads execu-\ntion",
			  "This functionality is useful for producer-consumer\nsynchronization by allowing a producer to indicate that a\ndata transfer has finished filling a buffer while permitting\nthe producer thread to continue to perform work.",
			  "Similarly,\na consuming thread can use the same instruction to indicate\nthat a buffer has been read and is now empty.",
			  "For blocking\noperations we use the PTX instruction bar.sync to block\non a named barrier.",
			  "CudaDMA has shown the benefits of emulating an asyn-\nchronous DMA engine in software on a GP",
			  "The CudaDMA approach to GPU programming is therefore\ngeneral enough to be applied to any CUDA program.",
			  "CudaDMA encapsulates this technique in order to make it\nmore generally available to a range of application workloads.",
			  "By decoupling the compute and\nDMA warps, CudaDMA enables this approach without\nsacrificing memory system performance.",
			  "CudaDMA enables the\nprogrammer to decouple the shape of data from how the\ndata is transferred by creating specialized DMA warps for"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more",
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
			"url": "https://arxiv.org/html/2510.14719v2",
			"excerpts": [
			  "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT ...Read more"
			]
		  },
		  {
			"title": "Using Littles Law to Measure System Performance | Kevin Sookocheff",
			"url": "https://sookocheff.com/post/modeling/littles-law/",
			"excerpts": [
			  "Little's Law is a useful tool for software architecture because it provides a simple way to measure the effect of changes to a system.Read more"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "t.\nThere is also a \"Branch From\nQueue\" (BFQ) instruction that is conditional on\nthe branch outcome at the head of the branch queue\ncoming from the\nopposite processor.\n",
			  ".\nThus conditional branches\nappear in the two processors as complementary\npairs.",
			  " deadlock can occur if\nboth the AEQ and EAQ are full and both processors\nare blocked by the full queues, or if both queues\nare empty and both processors are blocked by the\nempty queues.\n"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. ",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Little's Law as Viewed on Its 50th Anniversary",
			"url": "https://people.cs.umass.edu/~emery/classes/cmpsci691st/readings/OS/Littles-Law-50-Years-Later.pdf",
			"excerpts": [
			  ".\nLittles Law says that the average number of items in a\nqueuing system, denoted *L* , equals the average arrival rate\nof items to the system, ** , multiplied by the average waiting\ntime of an item in the system, *W* . Thus,\n*L* = ** *W* *0*\nF",
			  "\nThe two biggest fields in which Littles Law regularly\ncomes into play are operations management (OM) and\ncomputer architecture (computers).",
			  "Latency is a key performance measure for computers\n(low is good) and has an unspoken average implicitly\nattached, but *response time* seems more meaningful for a\nlayperson.",
			  "e\ncomputer architect wants to understand what is going on so\nas best to design the system.",
			  "Computer memory comes in various *tiers* with different\ninherent response times. The tiers range from CPU cache\n(fast), to DRAM (dynamic random access memory) (not\nquite as fast), to RAM (random access memory) (a lit-\ntle slower), all the way to solid-state drives and finally to\nnetwork attached storage (banks of hard disk drives).",
			  "The server CPU contains microprocessors, each of which\ncontains cores, perhaps 8 or 16 of them. Each is a von\nNeumann computer. Now we are coming to the queues. The\npiece of code the computer is executing is aptly called a\nthread (of instructions). The author visualizes it as a list of\ninstructions running down his penciled code sheet. Oh, oh,\nthe instruction calls for a piece of data that is somewhere\nelse in memory, electronically far away from the thread.\nThe thread STOPS and sends out a request for that data."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://dl.acm.org/doi/abs/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982. ... 1984. ISSN ...Read more",
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings ... James E Smith profile image James E. Smith. Department of Electrical and ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Overview and Code Examples",
			"url": "https://www.nvidia.com/content/PDF/GDC2011/Brucek_KhailanySC11.pdf",
			"excerpts": [
			  "Warp Specialization. CudaDMA enables warp specialization: DMA warps. Maximize MLP. Compute warps. No stalls due to memory. CudaDMA objects manage warp ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://alastairreid.github.io/RelatedWork/papers/smith:tocs:1984/",
			"excerpts": [
			  "Decoupled access/execute computer architectures. James E. Smith [doi] [Google Scholar] [DBLP] [Citeseer]. ACM Transactions on Computer Systems 2(4)Read more"
			]
		  },
		  {
			"title": "(PDF) A decoupled access-execute architecture for ...",
			"url": "https://www.researchgate.net/publication/326637256_A_decoupled_access-execute_architecture_for_reconfigurable_accelerators",
			"excerpts": [
			  "PDF | Mapping computational intensive applications on reconfigurable technology for acceleration requires two main implementation parts: (a) ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114% ( ...Read more"
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "This paper demonstrates significant performance gains on a commodity 8-core multicore machine running a variety of codes transformed with DSWP+. Formats ...Read more"
			]
		  },
		  {
			"title": "Scalable-Grain Pipeline Parallelization Method for Multi- ...",
			"url": "https://inria.hal.science/hal-01513778/file/978-3-642-40820-5_23_Chapter.pdf",
			"excerpts": [
			  "Some approaches partition individual instructions across pro- cessors, such as the decoupled software pipeline (DSWP) method [10], while.Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "1.1 Problem. Decoupled software pipelining [5] presents an easy way to automatically ex- tract thread-level parallelism for general loops in any program.Read more"
			]
		  },
		  {
			"title": "decoupled-software-pipelining-creates-parallelization- ...",
			"url": "https://scispace.com/pdf/decoupled-software-pipelining-creates-parallelization-tyrvhzup00.pdf",
			"excerpts": [
			  "An automatic parallelization technique, DSWP splits the loop body into several stages distributed across multiple threads, and executes them in a pipeline."
			]
		  },
		  {
			"title": "\nParallel Techniques of the Sequential Codes Based on Multi-core",
			"url": "https://scialert.net/fulltext/?doi=itj.2013.1673.1684",
			"excerpts": [
			  "DSWP parallelizes a loop by partitioning the body of the loop into a sequence of pipeline stages. Each stage is then executed by a separate thread. The threads ...Read more"
			]
		  },
		  {
			"title": "PIPELINED MULTITHREADING TRANSFORMATIONS AND ...",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_ram.pdf",
			"excerpts": [
			  "tolerate variable latency and to overcome scope restrictions imposed by single PC ar-\nchitectures, a program transformation called *decoupled software pipelining* (DSWP) is pre-\nsented in this chapter",
			  "SWP avoids heavy hardware usage by attacking the fundamental\nproblem of working within a single-threaded execution model, and moving to a multi-\nthreaded execution model.",
			  "Only useful, about-to-execute instructions are even brought into\nthe core. The rest are conveniently left in the instruction cache or their results committed\nand retired from the processor core.",
			  "Unlike the single-threaded multi-core techniques pre-\nsented in Table 2.1, DSWP is an entirely *non-speculative* techniqu",
			  "Each DSWP thread\nperforms useful work towards program completion.",
			  "The concurrent multithreading model of DSWP means that each participating thread\ncommits its register and memory state concurrently and independently of other threads.",
			  "The current thread model for DSWP is as follows. Execution begins as a single thread,\ncalled *primary thread* . It spawns all necessary *auxiliary threads* at the beginning of a\nprogram",
			  "When the primary thread reaches a DSWPed loop, auxiliary threads are set up\nwith necessary loop live-in values.",
			  "Similarly, upon loop termination, loop live-outs from\nauxiliary threads have to be communicated back to the primary thread."
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization. They also promise much wider flexi-\nbility than data-parallelism-centric methods for processors, aiming\nfor the effective parallelization of general-purpose application",
			  "In this paper, we provide another method to decouple control-\nflow regions of serial programs into concurrent tasks, exposing\npipeline and data parallelism",
			  "The power and simplicity of the\nmethod rely on the restriction that all streams should retain a\nsynchronous semantics [8].",
			  "ics [8]. It amounts to checking the sufficient\ncondition that the source and target of any decoupled dependence\nare control-dependent on the same node in the control dependence\ntree (this assumes structured control flow).",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "For example, when\nthere are no dependences between loop iterations of a DSWP stage,\nthe incoming data can be distributed over multiple data-parallel\nworker threads dedicated to this stage, while the outgoing data can\nbe merged to proceed with downstream pipeline stages."
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformati",
			  "WP operates by partitioning the instructions of a\nloop among a sequence of loops. The new loops are concurrently\nexecuted on different threads, with dependences among them flow-\ning in a single direction, thus forming a pipeline of threads.",
			  "The performance results indicate\nthe potential of this technique to exploit iteration-level parallelism\nin loops that cannot be parallelized as DOALL.",
			  "e evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining with the Synchronization Array",
			"url": "https://liberty.princeton.edu/Publications/pact04_dswp.pdf",
			"excerpts": [
			  " software pipelining (DSWP), a technique that stati-*\n*cally splits a single-threaded sequential loop into multi-*\n*ple non-speculative threads, each of which performs use-*\n*ful computation essential for overall program correctne",
			  "g threads execute on thread-parallel architec-*\n*tures such as simultaneous multithreaded (SMT) cores or*\n*chip multiprocessors (CMP), expose additional instruction*\n*level parallelism, and tolerate latency better than the orig-*\n*inal single-threaded RDS lo",
			  "accomplish this,\nthe paper presents the *synchronization array* which ap-\npears to the ISA as a set of queues capable of supporting\nboth out-of-order execution and speculative issue",
			  "e traversal and computation threads behave as a tra-\nditional decoupled producer-consumer pair.",
			  "DSWP threads these loops\nfor parallel execution on SMT or CMP processors.",
			  "ever, for this threading technique to be effective, a very\nlow overhead communication and synchronization mecha-\nnism between the threads is required. "
			]
		  },
		  {
			"title": "CoroBase: Coroutine-Oriented Main-Memory Database ...",
			"url": "http://vldb.org/pvldb/vol14/p431-he.pdf",
			"excerpts": [
			  "Data stalls are a major overhead in main-memory database engines\ndue to the use of pointer-rich data structures. ",
			  ". Lightweight corou-\ntines ease the implementation of software prefetching to hide data\nstalls by overlapping computation and asynchronous data prefetch-\ning. ",
			  "Coroutine-to-transaction models transactions as coroutines\nand thus enables inter-transaction batching, avoiding application\nchanges but retaining the benefits of prefetching. W",
			  ". We show that\non a 48-core server, CoroBase can perform close to 2  better for\nread-intensive workloads and remain competitive for workloads\nthat inherently do not benefit from software prefetching.\n*",
			  "CoroBase is open-source at https://github.com/sfu-dis/corobase ."
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://medium.com/nlnetlabs/adapting-radix-trees-15fe7d27c894",
			"excerpts": [
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the ...Read more"
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/pf_final.pdf",
			"excerpts": [
			  "For index searches, pB+-Trees reduce this problem by having wider nodes than the natural data transfer size,. e.g., eight vs. one cache lines (or disk pages).",
			  "To accelerate searches, pB+-Trees use prefetching to e ectively create wider nodes than the natural data trans- fer size: e.g., eight vs. one cache lines or ...Read more",
			  "prefetching pointer-linked data structures (i.e. linked-lists, trees, etc.) in general-purpose applications. Assuming that three nodes worth of computation ..."
			]
		  },
		  {
			"title": "The Taming of the B-Trees - ScyllaDB",
			"url": "https://www.scylladb.com/2021/11/23/the-taming-of-the-b-trees/",
			"excerpts": [
			  "There is a good and pretty cheap optimization to mitigate this spike that we've called linear root. The leaf root node grows on demand, ...Read more"
			]
		  },
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "The [first]() is based on the memory layout of a B-tree, and, depending on the array size, it is up to 8x faster than `std::lower_bound` while using the same space as the array and only requiring a permutation of its eleme",
			  "[second]() is based on the memory layout of a B+ tree, and it is up to 15x faster than `std::lower_bound` while using just 6-7% more memory  or 6-7% **of** the memory if we can keep the original sorted array.",
			  "o distinguish them from B-trees  the structures with pointers, hundreds to thousands of keys per node, and empty spaces in them  we will use the names *S-tree* and *S+ tree* respectively to refer to these particular memory layouts [1](:1) .",
			  ":\nTo find the lower bound, we need to fetch the $B$ keys in a node, find the first key $a_i$ not less than $x$, descend to the $i$-th child  and continue until we reach a leaf node. There is some variability in how to find that first key. For example, we could do a tiny internal binary search that makes $O(\\log B)$ iterations, or maybe just compare each key sequentially in $O(B)$ time until we find the local lower bound, hopefully exiting from the loop a bit early.",
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://blog.nlnetlabs.nl/adapting-radix-trees/",
			"excerpts": [
			  "The Adaptive Radix Tree (ART)",
			  "Section Title: The Adaptive Radix Tree (ART) > *Adaptively sized nodes*",
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the respective number of references to child nodes, and are grown/shrunk as needed.",
			  "Fixed sizes are used to minimize the number of memory (de)allocations.",
			  "Nodes of sizes up to 48, map keys onto edges using two separate vectors.",
			  "Nodes of size 16 make use of 128-bit SIMD instructions ( [SSE2](https://en.wikipedia.org/wiki/SSE2) and [NEON](https://en.wikipedia.org/wiki/ARM_architecture(Neon)) ) to map a key onto an edge to improve performance.\n",
			  "Section Title: The Adaptive Radix Tree (ART) > Path compression",
			  "Inner nodes that have only one child are merged with their parent and each node reserves a fixed number of bytes to store the prefix.",
			  "If more space is required, lookups simply skip the remaining number of bytes and compare the search key to the key of the leaf once it arrives there.",
			  "Section Title: The Adaptive Radix Tree (ART) > Lazy expansion",
			  "Inner nodes are created only to distinguish at least two leaf nodes. Paths are truncated. Pointer tagging is used to tell inner nodes apart from leaf nodes."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://db.in.tum.de/~leis/papers/ART.pdf",
			"excerpts": [
			  "**ode16:** This node type is used for storing between 5 and\n16 child pointers. Like the Node4 , the keys and pointers\nare stored in separate arrays at corresponding positions, but\nboth arrays have space for 16 entries.",
			  "ART adapts the\nrepresentation of every individual node, as exemplified in\nFigure 1. By adapting each inner node *locally* , it optimizes\n*global* space utilization and access efficiency at the same ",
			  "Radix trees consist of two types of nodes: Inner nodes,\nwhich map partial keys to other nodes, and leaf nodes, which\nstore the values corresponding to the keys. The most efficient\nrepresentation of an inner node is as an array of 2 *s* pointer",
			  "A useful property of radix trees is that the order of the keys\nis not random as in hash tables; rather, the keys are ordered\nbitwise lexicographically.",
			  "Two additional techniques, path\ncompression and lazy expansion, allow ART to efficiently\nindex long keys by collapsing nodes and thereby decreasing\nthe tree height.",
			  "e use a small number of node types, each with a different\nfanout. Depending on the number of non-null children, the\nappropriate node type is used. ",
			  "the space consumption per key is bounded\nto 52 bytes, even for arbitrarily long keys. We show\nexperimentally, that the space consumption is much lower\nin practice, often as low as 8.1 bytes per key.",
			  " **ode4:** The smallest node type can store up to 4 child\npointers and uses an array of length 4 for keys and another\narray of the same length for pointers. The keys and pointers\nare stored at corresponding positions and the keys are sorted.",
			  "The height (and complexity) of radix trees depends on\nthe length of the keys but in general not on the number\nof elements in the tree.",
			  "Instead of using\na list of key/value pairs, we split the list into one key part\nand one pointer part. This allows to keep the representation\ncompact while permitting efficient search:",
			  "**ode48:** As the number of entries in a node increases,\nsearching the key array becomes expensive. Therefore, nodes\nwith more than 16 pointers do not store the keys explicitly.\n",
			  " lookup**\n**performance surpasses highly tuned, read-only search trees, while**\n**supporting very efficient insertions and deletions as we",
			  "en though ARTs performance**\n**is comparable to hash tables, it maintains the data in sorted**\n**order, which enables additional operations like range scan and**\n**pref"
			]
		  },
		  {
			"title": "Adaptive Radix Tree",
			"url": "https://pages.cs.wisc.edu/~yxy/cs764-f22/slides/L16.pdf",
			"excerpts": [
			  "**Key idea** : Use a small node type\nwhen only a small number of\nchildren pointers exist",
			  "t\nKey Idea: Adaptive Radix Tree",
			  "**Node4** and **Node16**\n**Node48**\n**Node256**\n 256 child pointers indexed with\npartial key byte directly\n",
			  "Inner Node Structure",
			  "\n**Node4** and **Node16**\n Store up to 4 (16) partial keys\nand the corresponding pointers\n Each partial key is one byte\n Use SIMD instructions to\naccelerate key search\n**No",
			  "**Node48**",
			  "**Node256**",
			  "256 entries indexed with partial\nkey byte directly",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "**Lazy expansion** : remove path to\nsingle leaf",
			  "**Path compression** : merge one-way\nnode into child node",
			  "ART requires at most **52 bytes** of memory to index a key"
			]
		  },
		  {
			"title": " Beating hash tables with trees? The ART-ful radix trie | Paper Trail ",
			"url": "https://www.the-paper-trail.org/post/art-paper-notes/",
			"excerpts": [
			  "The most significant change that ART makes to the standard trie structure is that it introduces the\nability to change the datastructure used for each internal node depending on how many children the\nnode actually has, rather than how many it might have.",
			  ".\nPointers are assumed to be 8 bytes, so a single `Node4` is 36 bytes, so sits in a single cache\nline. The search loop can also be unrolled. Finally, by not early-exiting from the loop, we can hint\nto the compiler that it need not use a full branch, but can just use a conditional `cmov` [predicated instruction]",
			  "Nodes with from 5 to 16 children have an identical layout to `Node4` , just with 16 children per node:\nKeys in a `Node16` are stored sorted, so binary search could be used to find a particular key. Since\nthere are only 16 of them, its also possible to search all the keys in parallel using SIMD. What\nfollows is an annotated version of the algorithm presented in the papers Fig 8.\nThis is superior to binary-search: no branches (except for the test when bitfield is 0), and all the\ncomparisons are done in parallel.",
			  "The next node can hold up to three times as many keys as a `Node16` . As the paper says, when there\nare more than 16 children, searching for the key can become expensive, so instead the keys are\nstored implicitly in an array of 256 indexes. The entries in that array index a separate array of up\nto 48 pointers.\nThe idea here is that this is superior to just storing an array of 256 `Node` pointers because you\ncan store 48 children in 640 bytes (where 256 pointers would take 2k). Looking up the pointer does\ntake an extra indirection:\nThe paper notes that in fact only 6 bytes (i.e. \\(log_2(48)\\)) are needed for each index; both in\nthe paper and here its simpler to use a byte per index to avoid any shifting and masking.",
			  " final node type is the traditional trie node, used when a node has between 49 and 256 children.\nLooking up child pointers is obviously very efficient - the most efficient of all the node types -\nand when occupancy is at least 49 children the wasted space is less significant (although not 0 by\nany stretch of the imagination)"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://www.eecs.umich.edu/techreports/cse/02/CSE-TR-468-02.pdf",
			"excerpts": [
			  "A design decision that is consistently used in cache-conscious tree-based indices is defining the node size\nto be equal to the size of the L2 data cache line.",
			  "nalogous\nto the traditional B+-tree where a node size is equal to a disk page to minimize the number of page ac-\ncesses during a search, the node size for the CSB\n -tree is set equal to a processor cache line to minimize\nthe number of cache miss",
			  "The work by Rao and Ross has been extended in recent years in a number\nof different ways, including handling variable key length attributes efficiently [4] and for architectures that\nsupport prefetching [9].",
			  "In a recent paper, Chen, Gibbons and Mowry [9] examined the cache behavior of B+trees and CSB\n -\ntrees. They conclude that the CSB\n -trees produce very deep trees which cause many cache misses as a\nsearch traverses down the tree. They propose a prefetching-based solution, in which the node size of a\nB+tree is larger than the cache line size, and special prefetching instructions are manually inserted into th",
			  ". The CSB\n -tree eliminates child node pointers in the non-leaf\nnodes, allowing additional keys to be stored in a node which improves cache line utilization. An"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://pages.cs.wisc.edu/~jignesh/publ/cci.pdf",
			"excerpts": [
			  "uthors also investi-\ngated dynamic indexing techniques in the main-memory environ-\nment in [27], proposing a cache-conscious variation of the tradi-\ntional B+-tree, called the CSB + -tree. The CSB + -tree eliminates\nchild node pointers in the non-leaf nodes, allowing additional keys\nto be stored in a node which improves cache line utilizatio",
			  "the\nnode size for the CSB + -tree is set equal to a processor cache line to\nminimize the number of cache misses.",
			  "In a recent paper, Chen, Gibbons and Mowry [10] examined\nthe cache behavior of B+trees and CSB + -trees. They conclude\nthat the CSB + -trees produce very deep trees which cause many\ncache misses as a search traverses down the tree. They propose a\nprefetching-based solution, in which the node size of a B+tree is\nlarger than the cache line size, and special prefetching instructions\nare manually inserted into the B+tree code to prefetch cache lines\nand avoid stalling the processor.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  "We also recommend larger node\nsizes for the CSB + -tree, but our recommendation is not based on\nusing special hardware prefetch instructions. Rather, we recognize\nthat node size influences a number of different factors besides cache\nmisses, and that overall performance is improved by carefully con-\nsidering the effect of node size on all these factor",
			  " [27] is an adaptation of the ubiquitous B + -tree for\nmain memory databases [27]. The CSB + -tree is an important data\nstructure for memory-resident databases as it has been shown to\noutperform other cache-conscious, tree-based indices as well as tra-\nditional, tree-based indices in memory-resident databases",
			  "sing a first-order analytical model of the search performance,\nwe show that the conventional choice of setting the node size\nequal to the cache line size is often suboptimal. This design\nchoice focuses on reducing the number of cache misses, but\nignores the effect on the number of instructions that are exe-\ncuted, the number of conditional branches mispredicted, and\nthe number of TLB misses",
			  "The work by Rao and Ross\nhas been extended in recent years in a number of different ways,\nincluding handling variable key length attributes efficiently [5] and\nfor architectures that support prefetching [10].",
			  "Chen, Gibbons, Mowry, and Valentin also propose a version of\ntheir prefetching B+-tree optimized for disk pages, called Fractal\npB+-trees\n[11]."
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/238799599_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "As the speed gap between main memory and modern processors continues to widen, the cache behavior becomes more important for main memory database systems (MMDBs).",
			  "Indexing technique is a key component of MMDBs. Unfortunately, the predominant indexes  B+-trees and T-trees  have been shown to utilize cache poorly, which triggers the development of many cache-conscious indexes, such as CSB+-trees and pB+-trees.",
			  "The J+-tree stores all the keys in its leaf nodes and keeps the reference values of leaf nodes in a Judy structure, which makes J+-tree not only hold the advantages of Judy (such as fast single value search) but also outperform it in other aspects.",
			  "For example, J+-trees can achieve better performance on range queries than Judy. The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans.",
			  "asstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary l",
			  "Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. O"
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.semanticscholar.org/paper/The-adaptive-radix-tree%3A-ARTful-indexing-for-Leis-Kemper/6abf5107efc723c655956f027b4a67565b048799",
			"excerpts": [
			  "\nMain memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree Expand",
			  "Section Title: The adaptive radix tree: ARTful indexing for main-memory databases",
			  "[Viktor Leis](/author/Viktor-Leis/1787012) , [A. Kemper](/author/A.-Kemper/144122431) , [Thomas Neumann](/author/Thomas-Neumann/143993045)",
			  "Computer Science",
			  "[View on IEEE](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812 \"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812\")",
			  "[www-db.in.tum.de](http://www-db.in.tum.de/%7Eleis/papers/ART.pdf \"http://www-db.in.tum.de/%7Eleis/papers/ART.pdf\")",
			  "Save to Library Save",
			  "Create Alert Alert",
			  "Cite",
			  "Share",
			  "442 Citations",
			  "Content:"
			]
		  },
		  {
			"title": "Beautiful branchless binary search | Hacker News",
			"url": "https://news.ycombinator.com/item?id=35737862",
			"excerpts": [
			  "A significant speedup of a classic BS could be achieved by switching to vectorized search when the remaining range has a width of 3-4 SIMD lines."
			]
		  },
		  {
			"title": "An Eight-Dimensional Systematic Evaluation of Optimized ...",
			"url": "http://www.vldb.org/pvldb/vol11/p1550-schulz.pdf",
			"excerpts": [
			  "e implemented the algorithms described above and opti-\nmized the resulting code by eliminating branches, unrolling\nloops and adding software prefetching",
			  " vec-\ntorized the sequential, binary and uniform k-ary search us-\ning AVX2 instruction",
			  "One way to eliminate branches is by branch predication,\ni.e., both sides of a branch are executed and only the effects\nof the side that was actually needed are kept.",
			  "he x86 ar-\nchitecture supports branch predication via conditional move\ninstructions ( cmov )",
			  "el et al. [12] propose to use SIMD to parallelize in-\ndex computations and key comparisons of a k-ary search by\nfitting the *k* ** 1 separator elements and their indices in a\nsingle SIMD regist",
			  "Assuming AVX2 with 32-bit indices and keys, *k* = 9.",
			  "e vectorized implementa-\ntions were again tuned like the scalar variants.",
			  "The sequential search is vectorized by simply loading and\ncomparing *m* consecutive keys in parallel each iteration,\nwhere *m* is the number of keys per SIMD w",
			  "The binary search can be vectorized by viewing the array\nas a sequence of SIMD word sized blocks."
			]
		  },
		  {
			"title": "Avx2/branch-optimized binary search in .NET  GitHub",
			"url": "https://gist.github.com/buybackoff/f403be01486220baba8a9d4fe22c3cf6",
			"excerpts": [
			  "Binary search is theoretically optimal, but it's possible to speed it up substantially using AVX2 and branchless code even in .NET Core.",
			  "Memory access is the limiting factor for binary search. When we access each element for comparison a cache line is loaded, so we could load a 32-byte vector almost free, check if it contains the target value, and if not - reduce the search space by `32/sizeof(T)` elements instead of 1 element. This gives quite good performance improvement (code in `BinarySearch1.cs` and results in the table 1 below).",
			  "The linear search was not using AVX2, and for linear AVX2 should definitely work, shouldn't it!? With vectorized linear search and some additional branching optimization the performance is improved by *additional* 30-50% for the most relevant N (code in `BinarySearch2.cs` and results in the table 2 below)."
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Section Title: k-ary search on modern processors > Abstract and Figures",
			  "2.3Binary Search With SIMD Instructions",
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "The adaptive radix tree | Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013)",
			"url": "https://dl.acm.org/doi/10.1109/ICDE.2013.6544812",
			"excerpts": [
			  "Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck.",
			  "To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory.",
			  "Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well.",
			  "ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes.",
			  "Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.",
			  "Authors : [Viktor Leis]() , [Alfons Kemper]() , [Thomas Neumann]() [Authors Info & Claims]()",
			  "Pages 38 - 49",
			  "Contents",
			  "Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches."
			]
		  },
		  {
			"title": "How I implemented an ART (Adaptive Radix Trie) data structure in Go to increase the performance of my database 2x | by Farhan Ali Khan | techlog | Medium",
			"url": "https://medium.com/techlog/how-i-implemented-an-art-adaptive-radix-trie-data-structure-in-go-to-increase-the-performance-of-a8a2300b246a",
			"excerpts": [
			  "The paper introduces [ART](https://db.in.tum.de/~leis/papers/ART.pdf) , an adaptive radix tree (trie) for efficient indexing in main memory. It quotes the lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. It maintains the data in sorted order, which enables additional operations like range scan and prefix lookup."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf",
			"excerpts": [
			  "Tree puts all the child\nnodes for a given node contiguously in an array and\nstores only the pointer to the first child node.",
			  "a CSB + -Tree has fewer\npointers per node than a B + -Tree.",
			  "By having fewer\npointers per node, we have more room for keys and\nhence better cache performance.",
			  "We achieve this\ngoal by balancing the best features of the two index\nstructures.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "like a CSS-Tree, which requires batch updates, a\nCSB + -Tree is a general index structure that sup-\nports efficient incremental updates.",
			  "SB + -Trees need to maintain the property that\nsibling nodes are contiguous, even in the face of\n ... \nsystem). As observed in [RR99, CLH98], B + -Trees\nwith node size of a cache line have close to optimal\nperf",
			  "S-Trees were proposed in [RR99].\nThey improve on B + -Trees in terms of search per-\nformance because each node contains only keys,\nand no pointers.",
			  "Child nodes are identified by\nperforming arithmetical operations on array offsets.",
			  "Compared with B + -Trees, CSS-Trees utilize more\nkeys per cache line, and thus need fewer cache\naccesses and fewer cache misses.",
			  "The use of arithmetic to identify children requires\na rigid storage allocation policy.",
			  "This approach allows good\nutilization of a cache line.",
			  "We get away with fewer pointers by using a\nlimited amount of arithmetic on array offsets,\ntogether with the pointers, to identify child nodes.",
			  " describe\nvariants with more pointers per node. T",
			  "Cache Sensitive B + -Trees (CSB + -Trees)",
			  "Cache Sensitive B + -Trees (CSB + -Trees)"
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/314796211_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "Graefe and Larson [2001] summarized techniques of improving cache performance on B-tree indexes. Hankins and Patel [2003] explored the effect of node size on the performance of CSB+-trees and found that using node sizes larger than a cache line size (i.e., larger than 512 bytes) produces better search performance. While trees with nodes that are of the same size as a cache line have the minimum number of cache misses, they found 2 1B refers to 1 billion. ...",
			  "Both methods show that the best search performance is achieved by using a node size larger than 160 bytes. The best results are received when the node size is up to 3072 bytes [Han03] . For an insert operation the effect of node size is the opposite. ...",
			  " -tree has been further optimized using a variety of techniques, such as prefetching [4], storing only partial keys in nodes [5], and choosing the node size more carefully",
			  "Hankins and Patel [HP03] report that by tuning the node size of a CSB + -tree to be significantly bigger than cache line size they are able to reduce the number of instructions executed, TLB misses and branch mispredictions. ...",
			  "Hankins and Patel [37] studied the node size of CSB + -Trees on a Pentium III machine and concluded that it is desirable to use larger node size to reduce the number of tree levels because every level of the tree experiences a TLB miss and a fixed instruction overhead.",
			  "Chen et al. proposed pB+-tree [7] to use larger index nodes and rely on prefetching instructions to bring index nodes into cache before nodes are accessed.",
			  "Masstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary length.",
			  "In this paper, we present NitroGen, a framework for utilizing code generation for speeding up index traversal in main memory database systems."
			]
		  },
		  {
			"title": "A case study for Adaptive Radix Tree index",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S0306437921001228",
			"excerpts": [
			  "In the previous sections, we proposed a solution for cracking the Adaptive Radix Tree (ART), a popular IMDB index. ... Viktor Leis, Alfons Kemper, Thomas Neumann, ..."
			]
		  },
		  {
			"title": "A Case Study for Adaptive Radix Tree Index",
			"url": "https://arxiv.org/pdf/1911.11387",
			"excerpts": [
			  "Leis, A. Kemper, and T. Neumann. 2013. The adaptive radix tree: ARTful indexing for main-memory databases. In In Proceedings of the 29th ..."
			]
		  },
		  {
			"title": "FB -tree: A Memory-Optimized B -tree with Latch-Free Update",
			"url": "https://www.vldb.org/pvldb/vol18/p1579-li.pdf",
			"excerpts": [
			  "The Adaptive Radix Tree (ART) adaptively uses four different node layouts ... [43] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive ..."
			]
		  },
		  {
			"title": "Adaptive Hybrid Indexes",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/146253/3514221.3526121.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "[31] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix. Tree: ARTful Indexing for Main-Memory Databases. In ICDE, Vol. 13. 3849. [32] ..."
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.researchgate.net/publication/261087784_The_adaptive_radix_tree_ARTful_indexing_for_main-memory_databases",
			"excerpts": [
			  "Leis et al. [42] proposed a fast and space-efficient inmemory trie called ... Alfons Kemper  Thomas Neumann. The two areas of online transaction ..."
			]
		  },
		  {
			"title": "Fractal Prefetching B -Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/fpbtree.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effec- tively optimizes both CPU cache and disk performance on modern processors, ...Read more"
			]
		  },
		  {
			"title": "Amanieu/brie-tree: SIMD-optimized B+ ...",
			"url": "https://github.com/Amanieu/brie-tree",
			"excerpts": [
			  "A fast B+ Tree implementation that uses integer keys. The API is similar to the standard library's BTreeMap with some significant differences:.Read more"
			]
		  },
		  {
			"title": "ARTful indexing",
			"url": "http://daslab.seas.harvard.edu/classes/cs265/files/presentations/CS265_presentation_Sinyagin.pdf",
			"excerpts": [
			  "(P) The Adaptive Radix Tree: ARTful indexing for main-memory databases. Viktor Leis, Alfons Kemper, Thomas Neumann. International Conference on Data ..."
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "This is a follow up on a [previous article](https://algorithmica.org/en/eytzinger) about using Eytzinger memory layout to speed up binary search.",
			  "B-trees generalize the concept of binary search trees by allowing nodes to have more than two children.",
			  "Instead of single key, a B-tree node contains up to \\(B\\) sorted keys may have up to \\((B+1)\\) children, thus reducing the tree height in \\(\\frac{\\log_2 n}{\\log_B n} = \\frac{\\log B}{\\log 2} = \\log_2 B\\) times.",
			  "They were primarily developed for the purpose of managing on-disk databases, as their random access times are almost the same as reading 1MB of data sequentially, which makes the trade-off between number of comparisons and tree height beneficial.",
			  "In our implementation, we will make each the size of each block equal to the cache line size, which in case of `int` is 16 elements.",
			  "s.\nNormally, a B-tree node also stores \\((B+1)\\) pointers to its children, but we will only store keys and rely on pointer arithmetic, similar to the one used in Eytzinger array:\nThe root node is numbered \\(0\\) .\nNode \\(k\\) has \\((B+1)\\) child nodes numbered \\(\\{k \\cdot (B+1) + i\\}\\) for \\(i \\in [1, B]\\) .\nKeys are stored in a 2d array in non-decreasing order. If the length of the initial array is not a multiple of \\(B\\) , the last block is padded with the largest value if its data type.",
			  "Back in the 90s, computer engineers discovered that you can get more bang for a buck by adding circuits that do more useful work per cycle than just trying to increase CPU clock rate which [cant continue forever](https://en.wikipedia.org/wiki/Speed_of_light) .",
			  "This worked [particularly well](https://finance.yahoo.com/quote/NVDA/) for parallelizable workloads like video game graphics where just you need to perform the same operation over some array of data. This this is how the concept of *SIMD* became a thing, which stands for *single instruction, multiple data* .",
			  "ta* .\nModern hardware can do [lots of stuff](https://software.intel.com/sites/landingpage/IntrinsicsGuide) under this paradigm, leveraging *data-level parallelism* . For example, the simplest thing you can do on modern Intel CPUs is to:\nload 256-bit block of ints (which is \\(\\frac{256}{32} = 8\\) ints),\nload another 256-bit block of ints,\nadd them together,\nwrite the result somewhere else\nand this whole transaction costs the same as loading and adding just two intswhich means we can do 8 times more work. Magic!\n",
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD",
			  "data-intensive applications like sorting [2, 4],\nhash-based search [11], and relational query processing [5,",
			  "12] can also benefit from SIMD instructions.",
			  "This paper\ncomplements these techniques by providing efficient meth-\nods for sort-based search.\n",
			  "We classify the instructions into instructions\nfor data loading, element-wise instructions, and horizontal",
			  "e assume throughout\nthat the registers of the processor of interest are vectors of\n*k* ** 1 scalars",
			  "Table 1 lists the worst-case number of iterations performed\nby binary search (Bin), SIMDized binary search (Bin[ *k* ** 1])\nand *k* -ary search for various dataset sizes and values of *k",
			  "learly, *k* -ary search is the more attractive the larger the\ndataset and the larger the value of *k* .",
			  "uture gen-\nerations of processors will support much larger values of *k*\nand thus provide further efficiency gains",
			  "r example, Intel\nrecently announced that its upcoming processors will sup-\nport the AVX instruction set [6] with 256-bit vector registers\n( *k* = 9 for 32-bit keys) for the 2010 processor generation and\nup to 512-bit vector registers ( *k* = 17) for later generations.",
			  "Similarly, the upcoming Larrabee GPGPU [9]a hybrid be-\ntween a GPU and a multi-core CPUprovides 16-way vec-\ntor registers ( *k* = 17) for integer, single-precision float, and\ndouble-precision float instructi",
			  "**2.**",
			  "**PREREQUISITES**",
			  "**2.1**",
			  "**Binary Search**",
			  "Binary search is a dichotomic divide-and-conquer search",
			  "Binary search is a dichotomic divide-and-conquer search",
			  " restrict our attention to the case where the keys are of a\ntype natively supported by the underlying processor archi-\ntectures, i.e., integer and floating point types.",
			  "**3.2**",
			  "**On a Sorted Array**",
			  "p S1 consists of two parts: (a) calculate the indexes\nof the separators and (b) load them into *R* . Substep (a) is\nrequired because the *k* ** 1 separators are stored in non-\ncontiguous memory locations, see Figure 4",
			  "In this case, balanced search trees appear to be the method\nof choice. We conjecture that SIMD instructions will also\nbe valuable to speed up search on those trees, but this is\nbeyond the scope of our current work.",
			  "**4.2**",
			  "**IBM Cell BE: PPE**",
			  "The first experiment was conducted on a PPE core of the\nCell Broadband Engine in Sonys Playstation3.",
			  "gure 6a shows the results of our PPE experiments. For\nsmall datasetsup to 2 16 keysBin performs best. The rea-\nson is that the 128-bit AltiVec unit with its vector registers\noperates concurrently with the scalar integer/floating-point\nregisters and there is no way to directly move data between\nboth types of registers.",
			  "rs.\nTherefore, scalar replication and\nthe horizontal-sum instructions are expensive on the Pow-\nerPC platform. As a consequence, the benefit of fewer iter-\n",
			  "Performance Improvements and Energy Efficiency"
			]
		  },
		  {
			"title": "k-ary search on modern processors | Proceedings of the Fifth International Workshop on Data Management on New Hardware",
			"url": "https://dl.acm.org/doi/10.1145/1565694.1565705",
			"excerpts": [
			  "This paper presents novel tree-based search algorithms that exploit the SIMD instructions found in virtually all modern processors. The algorithms are a natural extension of binary search: While binary search performs one comparison at each iteration, thereby cutting the search space in two halves, our algorithms perform *k* comparisons at a time and thus cut the search space into *k* pieces. On traditional processors, this so-called *k* -ary search procedure is not beneficial because the cost increase per iteration offsets the cost reduction due to the reduced number of iterations. On modern processors, however, multiple scalar operations can be executed simultaneously, which makes *k* -ary search attractive. In this paper, we provide two different search algorithms that differ in terms of efficiency and memory access patterns. Both algorithms are first described in a platform independent way and then evaluated on various state-of-the-art processors. Our experiments suggest that *k* -ary search provides significant performance improvements (factor two and more) on most platforms."
			]
		  },
		  {
			"title": "DEX: Scalable Range Indexing on Disaggregated Memory",
			"url": "https://www.vldb.org/pvldb/vol17/p2603-lu.pdf%3C/ee%3E",
			"excerpts": [
			  "The adaptive radix tree:\nARTful indexing for main-memory databases."
			]
		  },
		  {
			"title": "[PDF] Improving index performance through prefetching | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/99d4b6749d93726e007da067c7b36cb06321d1a3",
			"excerpts": [
			  "### Prefetching J+-Tree: A Cache-Optimized Main Memory Database Index Structure",
			  "The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans and can provide better performance on both time (search, scan, update) and space aspects.",
			  "### Fractal prefetching B+-Trees: optimizing both cache and disk performance",
			  "Fractal prefetching B+-Trees are proposed, which embed \"cache-optimized\" trees within \"disk-optimization\" trees, in order to optimize both cache and I/O performance.",
			  "### Redesigning database systems in light of cpu cache prefetching",
			  "This thesis investigates a different approach: reducing the impact of cache misses through a technique called cache prefetching, and presents a novel algorithm, Inspector Joins, that exploits the free information obtained from one pass of the hash join algorithm to improve the performance of a later pass.",
			  "### BS-tree: A gapped data-parallel B-tree",
			  "BS-tree is proposed, an in-memory implementation of the B+-tree that adopts the structure of the disk-based index, setting the node size to a memory block that can be processed fast and in parallel using SIMD instructions.",
			  "### Making B+- trees cache conscious in main memory",
			  "CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node, and introduces two variants of CSB+, which can reduce the copying cost when there is a split and preallocate space for the full node group to reduce the split cost."
			]
		  },
		  {
			"title": "M.Sc. Thesis",
			"url": "https://skemman.is/bitstream/1946/7489/1/MSc_Arni-Mar-Jonsson.pdf",
			"excerpts": [
			  "Prefetching B + -tree (pB + -tree) [9] is another main-memory indexing method, which\nis almost identical to the B + -tree. It uses prefetching (see Section 2.3.3), which allows it\nto have nodes wider than a cache-line, without having to wait an entire cache-miss latency\nfor each cache-line accessed. Wider nodes result in shallower trees, and the benefits of\nhaving a shallow tree usually outweigh the prefetching overhead.",
			  "any modern CPUs have parallel memory systems. They can fetch multiple cache-lines\nfrom main-memory at the same time. Programs can instruct the CPU to fetch a given\ncache-line by issuing so-called prefetch instructions. Multiple prefetch instructions can\nbe executed at the same time. For example, the Alpha 21264 CPU has a 150 cycle cache-\nmiss latency. It can, however, fetch 15 cache-lines at the same time, meaning that a\ncache-line can be delivered to cache every 10 cycles. If a program knows what cache-line\nit will need 150 cycles later, it can prefetch it and 150 cycles later it is available in c",
			  "This way the perceived cache-miss latency is 0 cycles, even though the actual cache-miss\nlatency is unchanged",
			  "**Node Layout and Prefetching**",
			  "Pointer elimination and prefetching as described in this chapter are complementary tech-\nniques [9]. It is possible to increase the node width of the CSB + -tree and prefetching\nnodes like the pB + -tree does, resulting in the Prefetching CSB + -Tree (pCSB + -tree).\nThat way, you get better performance than the pB + -tree, since the branch factor is slightly\nhigher.",
			  "The cache-performance of the pCSB + -tree can be found using the same method as pB + -\ntree and doubling the branch factor.",
			  "Indicespredominantly B + -treesare a key performance component of DBMSs. Un-\nfortunately, however, B + -trees have been shown to utilize cache memory poorly [9], trig-\ngering the development of many cache-conscious indices. The CSS-tree [26] and CSB + -\ntree [25] improve cache performance by not storing pointers to all the children of a node,\neffectively compacting the index structure and improving locality.",
			  "In their seminal paper, Ailamaki et al. [2] showed that less than half of the CPU\ntime for commercial DBMSs is spent on computations.",
			  "he Prefetching CSB + -Tree",
			  "he Prefetching CSB + -Tree",
			  "**General Description**",
			  "**General Description**",
			  "Chapter 7",
			  "**Conclusions**",
			  "In this thesis we have studied the performance of the pB + -tree on the Itanium 2 processor."
			]
		  },
		  {
			"title": "(PDF) Improving Index Performance through Prefetching",
			"url": "https://www.researchgate.net/publication/2528098_Improving_Index_Performance_through_Prefetching",
			"excerpts": [
			  "This paper proposes and evaluates Prefetching B -Trees#, which use prefetching to accelerate two important operations on B -Tree indices: searches and range scans.",
			  "To accelerate searches, pB -Trees use prefetching to e create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B -Tree, thereby decreasing the number of expensive misses when going from parenttochild without signi increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.2.5 for main-memory B -Trees.",
			  "In addition, it outperforms and is complementary to #Cache-SensitiveB -Trees.",
			  "To accelerate range scans, pB -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys.",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://www.yumpu.com/en/document/view/21981799/the-adaptive-radix-tree-artful-indexing-for-main-memory-",
			"excerpts": [
			  "The Adaptive Radix Tree: ARTful Indexing for Main-Memory ..."
			]
		  },
		  {
			"title": "Adaptive Radix Tree (ART) Index - All things DataOS",
			"url": "https://dataos.info/resources/stacks/flash/art/",
			"excerpts": [
			  "Node Types in ART: Node4: Supports up to 4 child pointers. Node16: Supports up to 16 child pointers. Node48: Supports up to 48 child pointers. Node256 ...Read more"
			]
		  },
		  {
			"title": "Adaptive Radix Tree for Databases | PDF | Cpu Cache",
			"url": "https://www.scribd.com/document/189896702/Art",
			"excerpts": [
			  "In this work, we present the adaptive radix tree (ART) which is a fast and space-efcient in-memory indexing structure specically tuned for modern hardware.Read more"
			]
		  },
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "However, CSS-Trees are designed for decision support workloads with relatively static data.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Nevertheless, for applications that require incremental updates, traditional B + -Trees perform well.",
			  "Our goal is to make B + -Trees as cache conscious as CSS-Trees without increasing their update cost too much.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "The rest of the children can be found by adding an offset to that address.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "CSB + -Trees support incremental updates in a way similar to B + -Trees.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Content:",
			  "Content:",
			  "We also introduce two variants of CSB + -Trees. Segmented CSB + -Trees divide the child nodes into segments.",
			  "Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node.",
			  "Segmented CSB + -Trees can reduce the copying cost when there is a split since only one segment needs to be moved.",
			  "Full CSB + -Trees preallocate space for the full node group and thus reduce the split cost.",
			  "Our performance studies show that CSB + -Trees are useful for a wide range of applications."
			]
		  },
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This works well for simple access patterns, like iterating over array in increasing or decreasing order, but for something complex like what we have here its not going to perform well.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree .",
			  "This blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.",
			  "The goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!",
			  "The source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Speeding up independent binary searches by interleaving them  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/",
			"excerpts": [
			  "Each data access is done using fewer than 10 instructions in my implementation, which is far below the number of cycles and small compared to the size of the instruction buffers, so finding ways to reduce the instruction count should not help.",
			  "redit: This work is the result of a collaboration with Travis Downs and Nathan Kurz, though all of the mistakes are mine.\nD",
			  "Daniel Lemire, \"Speeding up independent binary searches by interleaving them,\" in *Daniel Lemire's blog* , September 14, 2019, https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/ .",
			  "The condition move instructions are pretty much standard and old at this point.",
			  "On Cannon Lake, however, you should be able to do better.",
			  "9 on Cannon Lake, 7 on Skylake, 5 on Skylark"
			]
		  },
		  {
			"title": "cwisstable/DESIGN.md at main",
			"url": "https://github.com/google/cwisstable/blob/main/DESIGN.md",
			"excerpts": [
			  "The Abseil implementation implements SwissTable via the raw_hash_set type, which provides sufficient extension points that all of the various flavors of hash ...Read more"
			]
		  },
		  {
			"title": "Facebook open-sources F14 algorithm for faster and memory-efficient hash tables",
			"url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/facebook-open-sources-f14-algorithm-for-faster-and-memory-efficient-hash-tables?srsltid=AfmBOorcYqrbSM-yIqjNiyPD-2c3or2pVOwzIgzCg7M0yVcU3kq050zF",
			"excerpts": [
			  "F14 helps the hash tables provide a faster way for maintaining a set of keys or map keys to values, even if the keys are objects, like strings.Read more"
			]
		  },
		  {
			"title": "A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort | Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/abs/10.1145/2588555.2610522",
			"excerpts": [
			  "This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy.Read more"
			]
		  },
		  {
			"title": "abseil / Swiss Tables Design Notes",
			"url": "https://abseil.io/about/design/swisstables",
			"excerpts": [
			  "Within Swiss tables, the result of the hash function produces a 64-bit hash\nvalue. We split this value up into two parts:\nH1, a 57 bit hash value, used to identify the element index within the table\nitself, which is truncated and modulated as any normal hash value would be for\nlookup and insertion purposes.\nH2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "Swiss tables hold a densely packed array of metadata, containing presence\ninformation for entries in the table. This presence information allows us to\noptimize both lookup and insertion operations. This metadata adds one byte of\noverhead for every entry in the table.",
			  "The metadata of a Swiss table stores presence information (whether the element\nis empty, deleted, or full). Each metadata entry consists of one byte, which\nconsists of a single control bit and the 7 bit H2 hash. The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "When searching for items in the table we use [SSE instructions](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) to scan for\ncandidate matches. The process of finding an element can be roughly summarized\nas follows:\nUse the H1 hash to find the start of the bucket chain for that hash.\nUse the H2 hash to construct a mask.\nUse SSE instructions and the mask to produce a set of candidate matches.\nPerform an equality check on each candidate.\nIf no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates. Note that a deleted element does not cease\nprobing, though an empty element would.\nSteps 2+3 can be summarized visually as:\nEquivalent code for this lookup appears below:\nThis process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "Use the H2 hash to construct a mask.",
			  "Perform an equality check on each candidate.",
			  "For performance reasons, it is important that you use a hash function that\ndistributes entropy across the entire bit space well (producing an [avalanche effect]",
			  "This process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "This metadata adds one byte of\noverhead for every entry in the table.",
			  "H2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "Use the H1 hash to find the start of the bucket chain for that hash.",
			  "Use SSE instructions and the mask to produce a set of candidate matches.",
			  "If no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates.",
			  "The H2 hash bits are stored separately within the metadata section of\nthe table."
			]
		  },
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "For a keys hash, split into:\n`H1` : upper 57 bits to compute the group index\n`H2` : lower 7 bits as the fingerprint stored in the control byte",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "It uses an **open addressing** hash table structure.",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "Linear Probing ( `i + j` ): Move one slot ( `j` ) at a time from the original hash position ( `i` )",
			  "Quadratic Probing( `i + j` ): Move `j` slots from the original hash position ( `i` )",
			  "Swiss Table has the following key characteristics:",
			  "It combines **linear probing** with the **Robin Hood hashing algorithm** to resolve collisions.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split)",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split) > Lookup flow (high level):",
			  "Use `H1` to pick a starting group.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > Open Addressing Hash Table"
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Swiss Tables boast improvements to efficiency and provide C++11 codebases early\naccess to APIs from C++17 and C++20.",
			  "\nThese hash tables live within the [Abseil `container` library](",
			  "We are extremely pleased to announce the availability of the new Swiss Table family of hashtables in Abseil and the `absl::Hash` hashing framework that allows easy extensibility for user defined types.",
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "The magic of SwissTables** is in the implementation detail of how it determines if the group contains the key or not. Instead of iterating over all the slots in the group, **SwissTables** leverage the Control Word!",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Swiss Tables, popularised by Google's Abseil C++ library, takes a different approach, primarily using **open addressing** (instead of chaining) with a clever variation of **linear probing** leveraging a dedicated metadata array."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "The vector search is coded using SIMD intrinsics, SSE2 on x86_64 and\nNEON on aarch64. These instructions are a non-optional part of those\nplatforms (unlike later SIMD instruction sets like AVX2 or SVE), so no\nspecial compilation flags are required. The exact vector operations\nperformed differs between x86_64 and aarch64 because aarch64 lacks a\nmovemask instruction, but the F14 algorithm is the same.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate).",
			  "F14 performs collision resolution if a chunk overflows or if two keys both pass the filtering step. The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "The lower bits of the full hash code determine the chunk. The upper bits are used to filter which slots in a chunk might hold the search key.",
			  "Chunking is an effective strategy because the chance that 15 of the tables keys will map to a chunk with 14 slots is much lower than the chance that two keys will map to one slot. For instance, imagine you are in a room with 180 people. The chance that one other person has the same birthday as you is about 50 percent, but the chance that there are 14 people who were born in the same fortnight as you is much lower than 1 percent.",
			  "Below is a plot of the likelihood that an algorithm wont find a search key in the very first place it looks. The happiest place on the graph is the bottom right, where the high load factor saves memory and the lack of collisions means that keys are found quickly with predictable control flow. Youll notice that the plot includes lines for both F14 ideal and F14 with 7-bit tag. The former includes only chunk overflow, while the latter reflects the actual algorithm. Theres a 1/128 chance that two keys have the same 7-bit tag even with a high-quality hash function.",
			  "The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "Collisions are the bane of a hash table: Resolving them creates unpredictable control flow and requires extra memory accesses. Modern processors are fast largely because of pipelining  each core has many execution units that allow the actual work of instructions to overlap."
			]
		  },
		  {
			"title": "Database Processing-in-Memory: An Experimental Study",
			"url": "https://pages.cs.wisc.edu/~yxy/cs839-s20/papers/p334-kepe.pdf",
			"excerpts": [
			  "ns. The hash join and\naggregation require the *gather* and *scatter* SIMD memory\ninstructions to load and store multiple entries of hash tables.",
			  "n par-\nticular, we present a new SIMD sorting algorithm that re-\nquires fewer memory instructions compared to the state of\nthe art [21]. For each operator, we gauge the latency and en-\nergy spend to process TPC-H and Zipf distribution dataset",
			  "Finally, the sorting operation and sort-merge join require the\n*min/max* and *shuffle* SIMD instructions.",
			  "**SIMD units** Unified func. units (integer + floating-point) @1 GHz;",
			  "**4.**",
			  "**IMPLEMENTATION DETAILS OF THE**",
			  "**SIMD QUERY OPERATORS**",
			  "In a nutshell, the implementations of the selection and\nprojection operators require SIMD *load* and *store* memory\ninstructions."
			]
		  },
		  {
			"title": "Efficient SIMD and MIMD parallelization of hash-based aggregation by conflict mitigation | Proceedings of the International Conference on Supercomputing",
			"url": "https://dl.acm.org/doi/10.1145/3079079.3079080",
			"excerpts": [
			  "To address this problem, we design a variant of basic bucket hashing and a *bucketized* aggregation procedure that can utilize both SIMD and MIMD parallelism efficiently. Our approach first adds distinct offsets to input rows on different SIMD lanes, which reduces the possibility of different lanes accessing identical slot in the hash table.",
			  "For parallelization across cores, we adopt separate hash tables and optimize with parallel reduction and a hybrid approach.",
			  "Section Title: Efficient SIMD and MIMD parallelization of hash-based aggregation by conflict mitigation > Recommendations",
			  "### SIMD Vectorized Hashing for Grouped Aggregation",
			  "Advances in Databases and Information SystemsAbstractGrouped aggregation is a commonly used analytical function. The common implementation\nof the function using hashing techniques suffers lower throughput rate due to the\ncollision of the insert keys in the hashing techniques. During collision, the ..."
			]
		  },
		  {
			"title": "40x faster hash joiner with vectorized execution",
			"url": "https://www.cockroachlabs.com/blog/vectorized-hash-joiner/",
			"excerpts": [
			  "In most SQL engines, including CockroachDB's current engine, data is processed a row at a time: each component of the plan (e.g. a join or a distinct) asks its input for the next row, does a little bit of work, and prepares a new row for output. This model is called the \"Volcano\" model, based off of a paper by Goetz Graefe.\nB",
			  "By contrast, in the vectorized execution model, each component of the plan processes an entire batch of columnar data at once, instead of just a single row.",
			  "This idea is written about in great detail in the excellent paper [MonetDB/X100: Hyper-Pipelining Query Execution](http://cidrdb.org/cidr2005/papers/P19.pdf) , and it's what we've chosen to use in our new execution engine.",
			  "\nAlso, as you might have guessed from the word \"vectorized\", organizing data in this batched, columnar fashion is the primary prerequisite for using SIMD CPU instructions, which operate on a vector of data at a time. ",
			  "Now that we have a taste of what vectorized execution and hash joiners are, let's take it one step further and combine the two concepts. The challenge is to break down the hash join algorithm into a series of simple loops over a single column, with as few run-time decisions, if statements and jumps as possible. Marcin Zukowski described one such algorithm to implement a many-to-one inner hash join in his paper \" [**Balancing Vectorized Query Execution with Bandwidth-Optimized Storage**](https://dare.uva.nl/search?identifier=5ccbb60a-38b8-4eeb-858a-e7735dd37487) \". This paper laid invaluable groundwork for our vectorized hash join operator."
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best.",
			  ".\n**Hash Tables.** Richter et al . [ 66 ] conduct an analysis of scalar\nhashing schemes and derive a decision guide that focuses on the\nworkload at hand. They implement a variant of VLP on AVX2. Poly-\nchroniou et al . [ 65 ] differentiate horizontal and vertical vectoriza-\ntion. Vertical vectorization looks up multiple keys in parallel, which\nrequires scatter/gather operations and bulk inserts/lookups. Hori-\nzontal vectorization serves as a drop-in replacement for scalar hash\ntables. Pietrzyk et al . [ 63 ] implement a conflict detection-aware ver-\nsion of vertical VLP. Behrens et al . [ 13 ] use OpenCL to implement\nvertical VFP. Metas F14 [ 20 , 21 ] and Googles Abseil containers [ 28 ]\nare industry implementations of BBC using SSE/NEON.",
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better."
			]
		  },
		  {
			"title": "Faster Go maps with Swiss Tables",
			"url": "https://go.dev/blog/swisstable",
			"excerpts": [
			  "This improvement to probing behavior allowed both the Abseil and Go implementations to increase the maximum load factor of Swiss Table maps ...Read more",
			  "Compute hash(key) and break the hash into two parts: the upper 57-bits (called h1 ) and the lower 7 bits (called h2 ).  The upper bits ( h1 ) ...Read more"
			]
		  },
		  {
			"title": "Deep Dive Into GO 1.24 Swiss Table-Part1 | by Rajesh Samala | Medium",
			"url": "https://medium.com/@samal.rajesh/deep-dive-into-go-1-24-swiss-table-part1-0d96e49630ee",
			"excerpts": [
			  "One of its key optimizations is the use of SIMD hardware to perform parallel operations on multiple control bytes simultaneously.",
			  "Slots where `h2` does match are potential matches, but we must still check the entire key, as there is potential for collisions (1/128 probability of collision with a 7-bit hash, so still quite low).",
			  "It is high performance hash-table, it replace earlier hashmap which is based on open addressing with linear probing. It enhanced map type with open addressing, quadratic probing, SIMD optimizations and cache aligned memory layouts.",
			  "Swiss table achieves the faster lookups, lower memory usage and better scalability.",
			  "Each group contains 8-slots and an 8-byte control words(64bit, 1byte per slots)",
			  "The control word encodes:",
			  "State: Empty(0xff), occupied or deleted",
			  "H2: A 7 bit fragment of 64-bit hash, it used for quick slot filtering.",
			  "from abseil.io",
			  "For example, a 64-bit control word(8-bytes, each byte represent a slot) comparison can be completed in a single SIMD operation, allowing the table to process multiple slots in parallel, significantly improving lookup and insertion performance."
			]
		  },
		  {
			"title": "SwissTable: A High-Performance Hash Table Implementation - DEV Community",
			"url": "https://dev.to/huizhou92/swisstable-a-high-performance-hash-table-implementation-1knc",
			"excerpts": [
			  "SwissTable uses a new metadata control mechanism to significantly reduce unnecessary `key` comparisons and leverages SIMD instructions to boost throughput.",
			  "In `swisstable` , `ctrl` is an array of `metadata` , corresponding to the `group[K, V]` array. Each `group` has 8 `slots` .",
			  "The hash is divided into `57 bits` for H1 to determine the starting `groups` , and the remaining `7 bits` called H2, stored in `metadata` as the hash signature of the current key for subsequent search and filtering.",
			  "The key advantage of `swisstable` over traditional hash tables lies in the metadata called `ctrl` . Control information includes:",
			  "Whether a slot is empty: `0b10000000`",
			  "Whether a slot has been deleted: `0b11111110`",
			  "The key's hash signature (H2) in a slot: `0bh2`",
			  "Multiplying `h2` by `0x0101010101010101` to get a uint64, allowing simultaneous comparison with 8 `ctrl` values.",
			  "The process of adding data in `swisstable` involves several steps:",
			  "Calculate the hash value and split it into `h1` and `h2` . Using `h1` , determine the starting groups.",
			  "Use `metaMatchH2` to check the current group's `metadata` for a matching `h2` . If found, further check for the matching key and update the value if they match.",
			  "If no matching key is found, use `metaMatchEmpty` to check for empty `slots` in the current group. Insert the new key-value pair if an empty slot is found and update the `metadata` and `resident` count.",
			  "If no empty slots are available in the current group, perform linear probing to check the next `groups` ."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases",
			"url": "https://15721.courses.cs.cmu.edu/spring2016/papers/p1493-polychroniou.pdf",
			"excerpts": [
			  "Our\nvectorization principle is to process a different key per SIMD\nlane using gathers to access the hash table.",
			  "Hash tables are used in database systems to execute joins\nand aggregations since they allow constant time key lookups.",
			  "The vectorized implementation of probing a hash table\nusing a linear probing scheme is shown in Algorithm 5."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases | Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/10.1145/2723372.2747645",
			"excerpts": [
			  " Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[D",
			  " Section Title: Rethinking SIMD Vectorization for In-Memory Databases > References",
			  "Content:\n[Digital Library](/doi/10.14778/2002938.2002940)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.14778%2F2002938.2002940)\n[23]\nR. Pagh et al. Cuckoo hashing. J. Algorithms, 51(2):122--144, May 2004.\n[Digital Library](/doi/10.1016/j.jalgor.2003.12.002)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016%2Fj.jalgor.2003.12.002)\n[24]\nH. Pirk et al. Accelerating foreign-key joins using asymmetric memory channels. In ADMS, 2011.\n[Google Scholar](https://scholar.google.com/scholar?q=H.+Pirk+et+al.+Accelerating+foreign-key+joins+using+asymmetric+memory+channels.+In+ADMS%2C+2011.)\n[25]\nO. Polychroniou et al. High throughput heavy hitter aggregation for modern SIMD processors. In DaMoN, 2013.\n[Digital Library](/doi/10.1145/2485278.2485284)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2485278.2485284)\n[26]\nO. Polychroniou et al. A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort. In SIGMOD, pages 755--766, 2014.\n[Digital Library](/doi/10.1145/2588555.2610522)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2588555.2610522)\n[27]\nO. Polychroniou et al. Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[Digital Library](/doi/10.1145/2619228.2619234)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2619228.2619234)\n[28]"
			]
		  },
		  {
			"title": "Library-based Prefetching for Pointer-intensive Applications",
			"url": "http://csl.stanford.edu/~christos/publications/2006.library_prefetch.manuscript.pdf",
			"excerpts": [
			  "In general, compiler-based prefetching has to handle the difficulty of data- structure layout analysis and pointer disambiguation."
			]
		  },
		  {
			"title": "A prefetching indexing scheme for in-memory database ...",
			"url": "https://www.sciencedirect.com/science/article/pii/S0167739X24000840",
			"excerpts": [
			  "There are two typical cache prefetching strategies: reducing the number of cache misses and reducing the impact of cache misses. Generally, a database workload ..."
			]
		  },
		  {
			"title": "Optimal Prefetching in Random Trees",
			"url": "https://inria.hal.science/hal-03361953v2/document",
			"excerpts": [
			  "Prefetching is a basic technique underlying many computer science applications. Its main purpose is to reduce the time needed to access some ..."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms and Data Structures",
			"url": "https://www.cs.cornell.edu/courses/cs612/2005sp/lectures/hitesh.pdf",
			"excerpts": [
			  "Different kinds of layouts. In-order. Breadth-first. Depth-first van Emde Boas van Emde Boas Layout : Main Idea. Store recursive sub-trees in contiguous memory.Read more"
			]
		  },
		  {
			"title": "A speculation-friendly binary search tree",
			"url": "https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/cpe.4883",
			"excerpts": [
			  "We introduce a speculation-friendly tree (s-tree for short) as a tree that transiently breaks its balance structural invariant without hampering the abstraction ...Read more"
			]
		  },
		  {
			"title": "Fractal Prefetching B+-Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/CMU-CS-02-115.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effectively optimizes both CPU cache and disk performance on modern processors, for ...Read more"
			]
		  },
		  {
			"title": "Improving index performance through prefetching | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/abs/10.1145/376284.375688",
			"excerpts": [
			  "This paper proposes and evaluate *Prefetching B* + - *Trees* (pB + -Trees), which use prefetching to accelerate two important operations on B + -Tree indices: searches and range scans.",
			  "To accelerate searches, pB + -Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B + -Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B + -Trees.",
			  "In addition, it outperforms and is complementary to Cache-Sensitive B + -Trees.",
			  "To accelerate range scans, pB + -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB + -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "ur results show that this technique yields over a *sixfold* speedup on range scans of 1000+ keys",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | Proceedings of the seventh international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/237090.237190",
			"excerpts": [
			  "Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors.",
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures.",
			  "Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme ( *greedy prefetching* ) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000.",
			  "Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45% for the applications we study."
			]
		  },
		  {
			"title": "Lecture 22 Prefetching Recursive Data Structures",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s16/www/lectures/L22-Prefetching-Pointer-Structures.pdf",
			"excerpts": [
			  "Summary of Prefetching Algorithms\n\nGreedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "**Carnegie Mellon**\n**Lecture 22**\n**Prefetching Recursive Data Structures**\nMaterial from: C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data\nStructures. In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "Eliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45%",
			  "Add new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n\nTrade space & time for better control over prefetching distances",
			  " History-Pointer Prefetching",
			  "\nCreation order equals major traversal order in treeadd & perimeter",
			  " and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in tr",
			  " Data-Linearization Prefetching",
			  "No pointer dereferences are required",
			  "Map nodes close in the traversal to contiguous memory",
			  "hree schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetch",
			  " improves performance significantly for half of Olden",
			  "Greedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "call: Example Code with Prefetching Arrays\n15-745: Prefetching Pointer Structures\n3\n**for (i = 0; i < 3; i++)**\n**for (j = 0; j < 100; j++)**\n**A[i][j] = B[j][0] + B[j+1][0];**\nOriginal Code\n**prefetch(&B[0][0]);**",
			  "Applicable because a list structure does not change over time",
			  "Improved accuracy outweighs increased overhead in this case",
			  "**H** = history-pointer prefetching",
			  "Health",
			  "Health",
			  "Performance of Data-Linearization Prefetching",
			  " hence data linearization is done without data restructuring",
			  "9% and 18% speedups over greedy prefetching through:",
			  " 94%->78% in perimeter, 87%->81% in treeadd\n",
			  " while maintaining good coverage factors:",
			  " 100%->80% in perimeter, 100%->93% in treeadd",
			  "**G** = greedy prefetching",
			  "**G** = greedy prefetching",
			  "**D** = data-linearization prefetching",
			  "15-745: Prefetching Pointer Structures",
			  "15-745: Prefetching Pointer Structures",
			  "load stall",
			  "load stall",
			  "load stall",
			  "store stall",
			  "store stall",
			  "store stall",
			  "inst. stall",
			  "inst. stall",
			  "inst. stall",
			  "busy",
			  "busy",
			  "busy",
			  "Three schemes to overcome the pointer-chasing problem:",
			  "Carnegie Mellon",
			  "\n**preorder(treeNode * t){**\n**if (t != NULL){**\n**pf(t->left);**\n**pf(t->right);**\n**process(t->data);**\n**preorder(t->left);**\n**preorder(t->right);**\n**}**\n*",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "**O** = original",
			  "Conclusions",
			  "Automated greedy prefetching in SUIF",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**"
			]
		  },
		  {
			"title": "Automatic compiler-inserted prefetching for pointer-based ...",
			"url": "https://ieeexplore.ieee.org/document/752654/",
			"excerpts": [
			  "As the disparity between processor and memory speeds continues to grow, memory latency is becoming an increasingly important performance bottleneck.",
			  "While software-controlled prefetching is an attractive technique for tolerating this latency, its success has been limited thus far to array-based numeric codes.",
			  "In this paper, we expand the scope of automatic compiler-inserted prefetching to also include the recursive data structures commonly found in pointer-based applications.",
			  "We propose three compiler-based prefetching schemes, and automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler.",
			  "Our experimental results demonstrate that compiler-inserted prefetching can offer significant performance gains on both uniprocessors and large-scale shared-memory multiprocessors."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  " We claim that we can achieve $O(\\log_B N)$ page accesses, but without having to know $B$ ahead of time.",
			  "The data structure were using is a good old balanced Binary Search Tree.",
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page.",
			  "One application of the recursive van Embde Boas layout",
			  "Suppose the page size is $B$. Every time step, the height of atoms halves.",
			  "Were interested in the height of the atoms at the first time step where an entire atom can fit in a page.",
			  "Since the number of vertices in a complete binary tree grows exponentially with height, that happens when atoms have height $\\Theta(\\log B)$. T",
			  ". Then, analysing the layout at this resolution, we can fit any atom (which all have the same height of $\\Theta(\\log B)$) into the cache with 1 page load*.",
			  "Then, now consider what happens in a search. A search basically consists of a path from the root of the BST to some leaf*. This path will spend some time in the first atom, until it reaches the leaf of the atom and goes into the next atom, and so forth.",
			  ". Since the path always start at the root vertex of an atom and ends on a leaf, it will spend $\\Theta(\\log B)$ steps in that atom.",
			  "Since the overall search path is $\\log N$ steps long, well need $O(\\frac{\\log N}{\\log B}) = O(\\log_B N)$ atoms, and thats the number of page accesses we need."
			]
		  },
		  {
			"title": "Lab note #044 Sailing by cache-oblivious data structures",
			"url": "https://interjectedfuture.com/lab-notes-044-sailing-by-cache-oblivious-data-structures/",
			"excerpts": [
			  "Cache-oblivious data structures base their Big-O on the number of cache line transfers between different levels of the memory hierarchy to minimize its growth as the data set gets bigger.",
			  "Through it, I learned about the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "If cache-oblivious works as advertised, why haven't I heard of any modern databases using it as an index?",
			  "the idea is 25 years old."
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "http://pdl.cmu.edu/PDL-FTP/Database/CMU-CS-00-177.pdf",
			"excerpts": [
			  "Luk and Mowry proposed three solutions to the pointer-chasing problem 13, 14 ... We then prefetch the next chunk ahead in the jump-pointer array.Read more"
			]
		  },
		  {
			"title": "[PDF] Automatic Compiler-Inserted Prefetching for Pointer-Based Applications | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Automatic-Compiler-Inserted-Prefetching-for-Luk-Mowry/be36f6c5b363116faf91e6eb62f5585b226656a5",
			"excerpts": [
			  "The scope of automatic compiler-inserted prefetching is expanded to also include the recursive data structures commonly found in pointer-based applications, ..."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Our goal: In this paper, we aim to provide an effective, bandwidth- efficient, and low-cost solution to prefetching linked data structures by 1) overcoming the ...Read more",
			  "is paper proposes a low-cost hardware/software cooperative*\n*technique that enables bandwidth-efficient prefetching of linked data*\n*structure",
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,",
			  "The prefetcher brings cache blocks into the L2 (last-\nlevel) cache, since we use an out-of-order execution machine that can\ntolerate short L1-miss latencies",
			  "far ahead of the demand miss\nstream the prefetcher can send requests is determined by the *Prefetch*\n*Distance* parameter.",
			  "ch relies on the observation that most virtual addresses share com-\n ... \ning and object metadata. In *PLDI* , 2004."
			]
		  },
		  {
			"title": "Lecture 21 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s11/public/lectures/L21-Data-Prefetching.pdf",
			"excerpts": [
			  "\nI.\nPrefetching for Arrays\nII. Prefetching for Recursive Data Structures\nReading: ALSU 11 11 4\n**Carnegie Mellon**\nReading: ALSU 11.11.4\nAdvanced readings (optional):\nT.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm for\nPrefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.\nC.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures. In\nProceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.\nTodd C. Mowry\n15-745: Data Prefetching\n1\nThe Memory Latency Problem\n**Carnegie Mellon**\n\n processor speed >>  memory speed\n\ncaches are not a panacea\nTodd C. Mowry\n15-745: Data Prefetching\n2\nUniprocessor Cache Performance on Scientific Code\n\nApplications from SPEC, SPLASH, and NAS Parallel.\nM\nb\nt\nt\ni\nl f MIPS R4000 (100 MH )\n**Carnegie Mellon**\n\nMemory subsystem typical of MIPS R4000 (100 MHz):\n 8K / 256K direct-mapped caches, 32 byte lines\n miss penalties: 12 / 75 cycles\n\n8 of 13 spend > 50% of time stalled for memory\nTodd C. Mowry\n15-745: Data Prefetching\n3\nPrefetching for Arrays: Overview\n\nTolerating Memory Latency\n\nPrefetching Compiler Algorithm and Results\n\nImplications of These Results\n**Carnegie Mellon**\nTodd C. Mowry\n15-745: Data",
			  "Performance of History-Pointer Prefetching\n**O** = original\n\nApplicable because a list structure does not change over time\n**O**\noriginal\n**G** = greedy prefetching\n**H** = history-pointer prefetching\nHealth\n**Carnegie Mellon**\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)\n\nImproved accuracy outweighs increased overhead in this case",
			  "Performance of Data-Linearization Prefetching\n**O** = original\n**G**\nd\nf t hi\n\nCreation order equals major traversal order in treeadd & perimeter\nhence data linearization is done without data restructuring\n**G** = greedy prefetching\n**D** = data-linearization prefetching\n**Carnegie Mellon**\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in treeadd",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "**Lecture 21**",
			  "**Compiler Algorithms for Prefetching Data**",
			  "**Compiler Algorithms for Prefetching Data**"
			]
		  },
		  {
			"title": "Lectures 26-27 Compiler Algorithms for Prefetching Data",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s15/public/lectures/L26-27-Data-Prefetching.pdf",
			"excerpts": [
			  "Propose 3 schemes to overcome the pointer-chasing problem:",
			  "**Compiler Algorithms for Prefetching Data**",
			  "II. Prefetching for Recursive Data Structures",
			  "f Data-Linearization Prefetching",
			  "59",
			  "Conclusions",
			  "Conclusions",
			  "Propose 3 schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetching",
			  "Automated greedy prefetching in SUIF",
			  "Automated greedy prefetching in SUIF",
			  "T.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm fo",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures.",
			  "fewer unnecessary prefetches:",
			  "94%->78% in perimeter, 87%->81% in treeadd",
			  "while maintaining good coverage factors:",
			  "100%->80% in perimeter, 100%->93% in treeadd",
			  "**Lectures 26-27**",
			  "Reading: ALSU 11.11.4",
			  "Advanced readings (optional):",
			  "Prefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.",
			  "In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "1",
			  "\ncaches are not a panacea",
			  "Uniprocessor Cache Performance on Scientific Code",
			  "Applications from SPEC, SPLASH, and NAS Parallel.\n",
			  "\nMemory subsystem typical of MIPS R4000 (100 MHz):",
			  " 8K / 256K direct-mapped caches, 32 byte lines\n",
			  " miss penalties: 12 / 75 cycles",
			  "8 of 13 spend > 50% of time stalled for memory",
			  "3",
			  "Prefetching for Arrays: Overview",
			  "Tolerating Memory Latency",
			  "Prefetching Compiler Algorithm and Results",
			  "Implications of These Results",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "4",
			  "2",
			  "2",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "Coping with Memory Latency",
			  "**Reduce Latency:**",
			  " Locality Optimizations\n",
			  " reorder iterations to improve cache reuse\n",
			  "**Tolerate Latency:**",
			  " move data close to the processor before it is needed\n",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry"
			]
		  },
		  {
			"title": "Lecture 27 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s12/public/lectures/L27-Data-Prefetching-1up.pdf",
			"excerpts": [
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n\n1\n\n",
			  "reedy Prefetching\n\nOur proposals:\n\n\nuse existing pointer(s) in ni to approximate &ni+d\n\na dd new pointer(s) to ni to a pproximate &ni+d\n\nni\nni+d\n\nn\n\na new p oin",
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n",
			  "History-Pointer Prefetching\n\n\nAdd new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n",
			  "Data-Linearization Prefetching\n\n\nNo pointer dereferences are required\n\nMap nodes close in the traversal to contiguous memory\n",
			  "Performance of Compiler-Inserted Greedy Prefetching\n\nload stall\nO = Original\n\nG = Compiler-Inserted Greedy Prefetching\n\nload stall\nstore stall\ninst. stall\nbusy\n\n\nEliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45",
			  "Performance of History-Pointer Prefetching\n\nO = original\n\nG = greedy prefetching\n\nH = history-pointer prefetching\n\n\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n\nHealth\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "Performance of Data-Linearization Prefetching\n\nO = original\nG = greedy prefetching\n\nD = data-linearization prefetching\n\n\nCreation order equals major traversal order in treeadd & perimeter\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n\n 94%->78% in perimeter, 87%->81% in treeadd\n\n while maintaining good coverage factors:\n\n 100% >80% in perimeter  100% >93% in treeadd\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance.",
			  "In the evaluation, the proposed solution achieves an average speedup of 1.37  over a set of memory-intensive benchmarks.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/635508.605427",
			"excerpts": [
			  "C.-K. Luk and T. Mowry. Compiler-based prefetching for recursive data structures. In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 222-233, Cambridge, Massachusetts, October 1996. ACM.",
			  "D. Joseph and D. Grunwald. Prefetching using markov predictors. In Proceedings of the 24th Annual International Symposium on Computer Architecture, pages 252-263, Denver, Colorado, June 1997. ACM."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[30] A. Roth, A. Moshovos, and G. S. Sohi. Dependence based prefetching\nfor linked data structures. In *ASPLOS-8* , 1998.",
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown",
			  "Dependence based\nprefetching for linked data structures, *",
			  "indirections through memory as seen in sparse tensor algebra\nand graphs [ 13  15 ], and more generally applications with\npointer chasing [ 10 , 11 ]. The target application influences the\ndata access pattern that the prefetcher tries to identify and\nprefetch for. For example, a common access pattern in sparse\ntensor algebra and graph computations is An [ *...* A1 [ A0 [ i ]] *...* ] .\nCorr",
			  "Yu et al. [ 13 ] (a.k.a. IMP) tries to detect\naccess patterns given by Y [ Z [ i ]] (2-level IMP) and X [ Y [ Z [ i ]]]\n(3-level IMP), for striding loop variable i , and prefetch\ndata by assuming that Y [ Z [ i + ** ]] and X [ Y [ Z [ i + ** ]]] will\nbe needed in the future. Ainsw"
			]
		  },
		  {
			"title": "Dependence Based Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1998/asplos-prefetch-lds.pdf",
			"excerpts": [
			  "k and Mowry [12] proposed and evaluated a greedy compiler\nalgorithm for scheduling software prefetches for linked data struc-\ntures. They showed this scheme to be effective for certain pro-\ngrams, citing instruction overhead and the generation of useless\nprefetches as performance degradation factors for other",
			  "ry [12] presented a case for history-pointer prefetch-\ning, which augments linked structure nodes with prefetching\npointer fields, and data-linearization, in which LDS are program-\nmatically laid out at runtime to allow sequential prefetch machin-\nery to capture their traversal.",
			  "eir\nalgorithm uses type information to identify recurrent pointer\naccesses, including those accessed via arrays, and may have advan-\ntages in tailoring a prefetch schedule to a particular traversal.",
			  "It collects these loads along with the\ndependence relationships that connect them and constructs a\ndescription of the steps the program has followed to traverse the\nstructure.",
			  "Predicting that the program will continue to follow these\nsame steps, a small prefetch engine takes this description and spec-\nulatively executes it in parallel with the original progra",
			  "Linked data structures (LDS) such as lists and trees are used in\nmany important applications."
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Later Luk and\nMowry proposed a compiler-based prefetching scheme for\nrecursive data structures [22].",
			  "This requires extra storage at\nruntime.",
			  "Jump Pointer [29], which has\nbeen used widely to break the serial LDS (linked data struc-\nture) traversal, stores pointers several iterations ahead in the\nnode currently visite",
			  "In a recent work, Gau-\ntam Doshi et al. [14] discussed the downside of software\nprefetching and exploited the use of rotating registers and\npredication to reduce the instruction overhead",
			  "Profile Guided Software Prefetching",
			  "Software prefetching is ineffective in pointer-based pro-\ngrams. To address this problem, Chi K. Luk et al. presented\na Profile Guided Post-Link Stride Prefetching [23] using a\nstride profile to obtain prefetching guidance for the com-\npil",
			  "[22] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching\nFor Recursive Data Structures. In *ASPLOS-7* , pages 222\n233. ACM Press, 1996",
			  "[23] C.-K. Luk, R. Muth, H. Patil, R. Weiss, P. G. Lowney, and\nR. Cohn. Profile-Guided Post-link Stride Prefetching. In\n*ICS-16* , pages 167178. ACM Press, 2002.",
			  "24] T. C. Mowry, M. S. Lam, and A. Gupta. Design and Evalua-\ntion of A Compiler Algorithm for Prefetching. In *ASPLOS-*\n*5* , pages 6273. ACM Press, ",
			  "[25] T. C. Mowry and C.-K. Luk. Predicting Data Cache Misses\nin Non-Numeric Applications Through Correlation Profil-\ning. In *Micro-30* , pages 314320. IEEE Computer Society\nPress, 199"
			]
		  },
		  {
			"title": "Dynamic Hot Data Stream Prefetching for General-Purpose ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s09/www/papers/prefetch_hds.pdf",
			"excerpts": [
			  "Jump pointers are a software technique for prefetching linked data\nstructures, overcoming the array-and-loop limitation.",
			  "Artificial\njump pointers are extra pointers stored into an object that point to\nan object some distance ahead in the traversal order.",
			  "d. Natural jump pointers are existing pointers in the\ndata\nstructure\nused\nfor\nprefetching.\n",
			  "ng.\nFor\nexample,\ngreedy\nprefetching makes the assumption that when a program uses an\nobject o, it will use the objects that o points to, in the near future,\nand hence prefetches the targets of all pointer fields.",
			  " These\ntechniques were introduced by Luk and Mowry in [22] and refined\nin [5, 18].",
			  "n\ndependence-based prefetching, producer-consumer pairs of loads\nare identified, and a prefetch engine speculatively traverses and\nprefetches them [26].",
			  "\nThe hardware technique that best corresponds to history-pointers is\ncorrelation-based prefetching. As originally proposed, it learns\ndigrams of a key and prefetch addresses: when the key is observed,\nthe prefetch is issued [6]. J"
			]
		  },
		  {
			"title": "2003 Workshop on Duplicating, Deconstructing and ... - PHARM",
			"url": "https://pharm.ece.wisc.edu/wddd/2003/wddd2003_proceedings.pdf",
			"excerpts": [
			  " ware prefetch when bandwidth is limited; with sufficient\nbandwidth software prefetch is the most successful strategy.\nHowever, their research also shows that the combination of\ncache-conscious allocation and software prefetch might not\nlead to further performance improvements, instead it coun-\nteracts changes in bandwidth or latency. Their results are\nsimilar to ours, although we have implemented a different\nsoftware prefetch that does not require any extra memory.\nSeveral researchers have studied hardware prefetch,\nor hybrid schemes, and successfully adapted hardware\nprefetch to pointer-based data structures with irregular ac-\ncess behavior. However, they generally require more hard-\nware than those evaluated in this study. Hardware support\nhas been investigated by the use of lock-up free prefetching,\n[13], and prefetch buffers, [10], and general prefetching in\nhardware is described in [20, 21] together with other cache\nmemory aspects. Karlsson et al., [11], propose a technique\nfor prefetching pointer-based data structures, either in soft-\nware combined with hardware or in software alone, by im-\nplementing prefetch arrays, making it possible to prefetch\nboth short data structures and longer data structures without\nknowing the traversal path. Roth et al. have investigated\nmore adaptable strategies for hybrid prefetch schemes, us-\ning dependence graphs, [18], and jump pointer prefetching,\n[19]. In [19], Roth et al. evaluate a framework for jump-\n ... \n[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999.\n[20] Alan J. Smith. Cache memories. *ACM Computing*\n*Surveys* , 14:3:473530, September 1982.\n[21] Steven P. VanderWiel and David Lilja. Data prefetch\nmechanisms.\n*ACM Computing Surveys* , 32:2:174\n199, June 2000.\n[22] Chengqiang Zhang and Sally A. McKee. Hardware-\nonly stream prefetching and dynamic access order-\ning. In *International Conference on Supercomputing* ,\npages 167175, 2000.\n[23] L. Zhang, S. McKee, W. Hsieh, and J. Carter. Pointer-\nbased prefetching within the impulse adaptable mem-\nory controller: Initial results. In *Proceedings of the*\n*Workshop on Solving the Memory Wall Problem* , June\n2000.\n[24] Craig B. Zilles. Benchmark health considered harm-\nful. *Computer Architecture News* , 29:3, 2001.\n13\n**Comparison of State-Preserving vs. Non-State-Preserving Leakage Control**\n**in Caches**\nDharmesh Parikh\n\n, Yan Zhang\n\n, Karthik Sankaranarayanan\n\n, Kevin Skadron\n\n, Mircea Stan\n\n Dept. ",
			  "[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999."
			]
		  },
		  {
			"title": "APT-GET | Proceedings of the Seventeenth European Conference on Computer Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3492321.3519583",
			"excerpts": [
			  "imin Chen, Anastassia Ailamaki, Phillip B Gibbons, and Todd C Mowry. 2004. Improving Hash Join Performance through Prefetching. In *Proceedings of the 20th International Conference on Data Engineering.* 116.",
			  "mir Roth, Andreas Moshovos, and Gurindar S Sohi. 1998. Dependence based prefetching for linked data structures. *ACM SIGOPS Operating Systems Review* 32, 5 (1998), 115--126",
			  "mison D Collins, Hong Wang, Dean M Tullsen, Christopher Hughes, Yong-Fong Lee, Dan Lavery, and John P Shen. 2001. Speculative pre-computation: Long-range prefetching of delinquent loads. In *Proceedings 28th Annual International Symposium on Computer Architecture.* IEEE, 14--25."
			]
		  },
		  {
			"title": "MetaSys-open-source-cross-layer-metadata- ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/MetaSys-open-source-cross-layer-metadata-management_taco22-arxiv22.pdf",
			"excerpts": [
			  "[30] Chi-Keung Luk and T. C. Mowry. Cooperative Prefetching: Compiler and Hardware Support for Effective Instruction Prefetching in Modern\nProcessors. In *MICRO* , 1998.",
			  "[31] Trishul M Chilimbi and Martin Hirzel. Dynamic Hot Data Stream Prefetching for General-Purpose Programs. In *PLDI* , 2002."
			]
		  },
		  {
			"title": "References - Engineering Information Technology",
			"url": "https://user.eng.umd.edu/~blj/memory/Book-References.pdf",
			"excerpts": [
			  "A. Roth, A. Moshovos, and G. Sohi. 1998. Dependence\nbased prefetching for linked data structures. In\nThe 8th Int. Conf. on Architectural Support for\nProgramming Languages and Operating Systems\n(ASPLOS), pp. 115126, October 1998.",
			  "A. Roth and G. S. Sohi. 1999. Effective jump-pointer\nprefetching for linked data structures. In Proc. 26th Int.\nSymp. on Computer Architecture (ISCA), Atlanta, GA,\nMay 1999.",
			  "E. Rotenberg, S. Bennett, and J. E. Smith. 1996.\nTrace cache: A low latency approach to high\nbandwidth instruction fetching. In Proc. 29th Ann.\nACM/IEEE Int. Symp. on Microarchitecture (MICRO-\n29), pp. 2435, Paris, France, December 1996"
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/248209.237190",
			"excerpts": [
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the ...Read more"
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/384265.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "Dependence based prefetching for linked data structures > Abstract",
			  "Content:\nWe introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different BVH ...",
			"url": "https://www.dominikwodniok.de/publications/Wodniok_EGPGV2013.pdf",
			"excerpts": [
			  "The best performing combination is the TDFS BVH layout with a threshold of 0.6 using the. AoS node layout. It is only marginally faster than the base- line ...Read more"
			]
		  },
		  {
			"title": "Why is SOA (Structures of Arrays) faster than AOS?",
			"url": "https://www.reddit.com/r/C_Programming/comments/9jg7hy/why_is_soa_structures_of_arrays_faster_than_aos/",
			"excerpts": [
			  "I have heard that SOA (Structures of Arrays) is faster due to CPU caching then AOS (Array of Structures). But that doesn't really make sense to me.Read more"
			]
		  },
		  {
			"title": "c - cache locality for a binary tree - Stack Overflow",
			"url": "https://stackoverflow.com/questions/31281905/cache-locality-for-a-binary-tree",
			"excerpts": [
			  "If you want data to be collocated to take advantage of cache locality a simple alternative is to allocate an array of the struct and then allocate from that ...Read more"
			]
		  },
		  {
			"title": "Help with BVH memory access optimizations",
			"url": "https://www.reddit.com/r/GraphicsProgramming/comments/1f90xbi/help_with_bvh_memory_access_optimizations/",
			"excerpts": [
			  "pointer chasing and frequent cache misses.",
			  "flatten the tree.",
			  "Note that `sizeof(BVHNode)` is exactly 32, so a node and one of its children should be pulled together to a cache line as far as I understand. This has made the program twice as much slower!",
			  "Surprisingly this did not improve performance. It is actually comparable to the pointer-based implementation.",
			  "My understanding is that this makes traversal slow due to pointer chasing and frequent cache misses.",
			  "So I quickly moved to flatten the tree."
			]
		  },
		  {
			"title": "Binary search is a pathological case for caches - Paul Khuong: some Lisp",
			"url": "https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/",
			"excerpts": [
			  "Binary search is a pathological case for caches",
			  "\nBinary search suffers from a related ailment when executed on medium\nor large vectors of almost power-of-two size (in bytes)",
			  "Two vectors of 64KB each, allocated at 0x10000 and 0x20000, have the unfortunate property that, for each index, the data at that index in both vectors will map to the same cache lines in a direct-mapped cache with 1024 lines of 64 bytes each.",
			  "the L1D cache has 512 lines of 64 bytes each, L2 4096\nlines, and L3 196608 lines (12 MB). Crucially, the L1 and L2 caches are\n8-way set-associative, and the L3 16-way.",
			  "There are workarounds, on both the hardware and software sides.",
			  "cache lines are power-of-two sizes, as are L1 and L2 set (buckets) counts."
			]
		  },
		  {
			"title": "Locality, B-trees, and splay trees",
			"url": "https://www.cs.cornell.edu/courses/cs312/2004sp/lectures/lec24.html",
			"excerpts": [
			  "Having caches only helps if when the processor needs to get some data, it is\nalready in the cache. Thus, the first time the processor access the memory, it\nmust wait for the data to arrive. On subsequent reads from the same location,\nthere is a good chance that the cache will be able to serve the memory request\nwithout involving main memory. Of course, since the cache is much smaller than\nthe main memory, it can't store all of main memory. The cache is constantly\nthrowing out information about memory locations in order to make space for new\ndata. The processor only gets speedup from the cache if the data fetched from\nmemory is still in the cache when it is needed again. When the cache has the\ndata that is needed by the processor,  it is called a **cache hit** . If\nnot, it is a **cache miss** . The ratio of the number of hits to misses is\ncalled the **cache hit ratio** .",
			  "Caches improve performance when memory accesses exhibit: reads from memory\ntends to request the same locations repeatedly, or at least memory locations\nnear previous requests. A tendency to revisit the same or nearby locations is\nknown as **locality** . Computations that exhibit locality will have a relatively high cache hit\nratio.",
			  "at caches actually store chunks of memory rather than individual words of\nmemory. So a series of memory reads to nearby memory locations are likely to\nmostly hit in the cache. When there is a cache miss, a whole sequence of memory\nwords is requested from main memory at once, because it is cheaper to read\nmemory that way. The cache records cached memory locations in units of **cache\nlines** whose size depends on the size of the cache (typically 4 - 32 words).",
			  "The same idea can be applied to trees. Binary trees are not good for locality\nbecause a given node of the binary tree probably occupies only a fraction of a\ncache line. **B-trees** are a way to get better locality. As in the hash\ntable trick above, we store several elements in a single node -- as many as will\nfit in a cache line."
			]
		  },
		  {
			"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
			"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
			"excerpts": [
			  "is seemingly simple decision can result in **10-100x performance differences** in modern computing system",
			  "The choice between AoS and SoA affects everything from CPU cache efficiency to SIMD vectorization capabilities to GPU memory coalescing patterns.",
			  "Memory Access Pattern Animation"
			]
		  },
		  {
			"title": "Array of Structs and Struct of Arrays | *^^*  HWisnu's blog   ^^",
			"url": "https://hwisnu.bearblog.dev/array-of-structs-and-struct-of-arrays/",
			"excerpts": [
			  "s (AoS) can be less cache-friendly because accessing multiple fields of a single element requires loading multiple cache lines. ",
			  "oA is almost 30% faster based on this simple example, the difference will get significantly magnified with more complex scenario, sometimes SoA (optimized) is 10x faster compared to AoS as shown in this [great article"
			]
		  },
		  {
			"title": "AoS and SoA - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
			"excerpts": [
			  "**Structure of arrays** ( **SoA** ) is a layout separating elements of a [record](/wiki/Record_(computer_science) \"Record (computer science)\") (or 'struct' in the [C programming language](/wiki/C_(programming_language) \"C (programming language)\") ) into one parallel array per [field](/wiki/Field_(computer_science) \"Field (computer science)\") . [[ 1 ]]() The motivation is easier manipulation with packed [SIMD instructions](/wiki/SIMD_instruction \"SIMD instruction\") in most [instruction set architectures](/wiki/Instruction_set_architecture \"Instruction set architecture\") , since a single [SIMD register](/wiki/SIMD_register \"SIMD register\") can load [homogeneous data](/w/index.php?title=Homogeneous_data&action=edit&redlink=1 \"Homogeneous data (page does not exist)\") , possibly transferred by a wide [internal datapath](/wiki/Internal_datapath \"Internal datapath\") (e.g. [128-bit](/wiki/128-bit \"128-bit\") ). If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line. The downside is requiring more [cache ways](/wiki/Cache_way \"Cache way\") when traversing data, and inefficient [indexed addressing](/wiki/Indexed_addressing \"Indexed addressing\") .",
			  "For example, to store *N* points in 3D space using a structure of arrays:",
			  "array of structures",
			  " ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved. This is often more intuitive, and supported directly by most [programming languages](/wiki/Programming_languages \"Programming languages\") .",
			  "AoS vs. SoA presents a choice when considering 3D or [4D vector](/wiki/4D_vector \"4D vector\") data on machines with four-lane SIMD hardware. SIMD ISAs are usually designed for homogeneous data, however some provide a [dot product](/wiki/Dot_product \"Dot product\") instruction [[ 5 ]]() and additional permutes, making the AoS case easier to handle.",
			  "Although most [GPU](/wiki/Graphics_processing_unit \"Graphics processing unit\") hardware has moved away from 4D instructions to scalar [SIMT](/wiki/Single_instruction,_multiple_threads \"Single instruction, multiple threads\") pipelines, [[ 6 ]]() modern [compute kernels](/wiki/Compute_kernel \"Compute kernel\") using SoA instead of AoS can still give better performance due to memory coalescing. [[ 7 ]]()",
			  "SoA is mostly found in languages, libraries, or [metaprogramming](/wiki/Metaprogramming \"Metaprogramming\") tools used to support a [data-oriented design](/wiki/Data-oriented_design \"Data-oriented design\") . Examples include:\n\"Data frames\", as implemented in [R](/wiki/R_(programming_language) \"R (programming language)\") , [Python](/wiki/Python_(programming_language) \"Python (programming language)\") 's Pandas package, and [Julia](/wiki/Julia_(programming_language) \"Julia (programming language)\") 's DataFrames.jl package, are interfaces to access SoA like AoS.\nThe Julia package StructArrays.jl allows for accessing SoA as AoS to combine the performance of SoA with the intuitiveness of AoS."
			]
		  },
		  {
			"title": "\n\tBuilding BVH4 or BVH8 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/td-p/1132356",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure ..."
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies(originally ...",
			"url": "https://www.embree.org/papers/2018-HPG-compressedleafbvh.pdf",
			"excerpts": [
			  "For each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "brees fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is\nstored as 2 ** 3 bytes, for the boxs *lower* and *upper* bounds,\nrequiring 48 bytes for all 8 children. Including the 8 child\npointers, this sums to a total of 136 bytes, slightly more than\nhalf an uncompressed BVH8 multi-node",
			  "sting 1: Illustration of the three BVH node types.**\n**Top: Embrees regular BVH8 nodes contain 8 point-**\n**ers and float boxes (256 bytes). Middle: Embrees**\n**quantized QBVH8 nodes contain 8 pointers, 8 quan-**\n**tized bounding boxes, and 6 floats to specify the**\n**dequantization domain (136 bytes). Bottom: At the**\n**leaf level, our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the no",
			  " resulting BVHwhich we call a Compressed-\nLeaf BVH ( CLBVH )has two multi-node types: regular BVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original BVH8 multi-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1).\nSince",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "Exactly\nwhat primitive data is stored in a leaf depends on the BVH\ntype: for triangles, it is either a list of triangle4 structures,\nfully pre-gathered vertices of four triangles in SoA layout;\nor a list of triangle4i with four triangles worth of vertex\nindices.",
			  "our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the nod",
			  " eliminating the primitive pointers for the leaf nodes our\ncompressed multi-leaf node requires only 72 bytes. Compared\nto an uncompressed BVH8 multi-node (256 bytes) this yields\na compression factor of over 3 ** .",
			  "We implement and evaluate the previously discussed strategy\nwithin a modified version of Embree 3.0.",
			  "mpressed-Leaf*\n*Bounding Volume Hierarchies* (CLBVH), which strike a bal-\nance between compressed and non-compressed BVH layouts",
			  "\nOur CLBVH layout introduces dedicated compressed multi-\nleaf nodes where most effective at reducing memory use, and\nuses regular BVH nodes for inner nodes and small, isolated\nleaves. We show that when implemented within the Embree\nray tracing framework, this approach achieves roughly the\nsame memory savings as Embrees current compressed BVH\nlayout, while maintaining almost the full performance of its\nfastest non-compressed BVH.",
			  "In Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "s fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is",
			  "cus on compressing just the leaf nodes, by introduc-\ning dedicated *compressed multi-leaf nodes* . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\ntraverse interior nodes.\n**",
			  "**2**\n**RELATED WORK**\nAcceleration structures for ray tracing have a long history;\na survey on the general concepts can be found in Havrans\nthesis [ 4 ]. Today, most ray tracers use some sort of BVH,\ntypically with a branching factor of 4 or 8, and in some\ncases 16 [ 2 , 3 , 7 , 9 , 10 ]. While in the past each ray tracer\nimplemented its own acceleration structures and traversal\nmethods, the last few years have seen the emergence of\ncommonly accepted ray tracing libraries such as Embree\nfor CPUs [ 10 ], and OptiX for GPUs [ 7 ], both of which use\nwide BVHs.",
			  "n**\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf."
			]
		  },
		  {
			"title": "Hardware-Accelerated Dual-Split Trees",
			"url": "https://hwrt.cs.utah.edu/papers/hardware_dual-split_trees.pdf",
			"excerpts": [
			  "A wider node (4 or 8 children) is attractive to current CPU\nor GPU architectures as computations on these wider nodes can benefit from SIMD computation.",
			  " BVH8 node has 200 bytes (192 bytes\nfor child bounding boxes, 4 bytes for offset, and 4 bytes for node types).",
			  "BVH4 and BVH8 nodes\nstore degenerate bounding boxes for empty children."
			]
		  },
		  {
			"title": "Building BVH4 or BVH8",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/m-p/1132356/highlight/true",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure and"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray ...",
			"url": "https://jo.dreggn.org/home/2008_qbvh.pdf",
			"excerpts": [
			  "The result is a large BVH node with a size of 128 bytes which\nperfectly matches the caches of modern computer hardware.",
			  "The 4 bounding boxes are stored in structure-of-arrays (SoA)\nlayout for direct processing in SIMD registers.",
			  "The leaf bounding box is already\nstored in the parent and the leaf data can be encoded directly\ninto the corresponding child integer.",
			  "We use the sign of the child index to encode whether a node\nis a leaf or an inner node.",
			  "The tree data is kept in a linear array of memory\nlocations, so the child pointer can be stored as integer indices\ninstead of using platform-dependent pointers.",
			  "Four\nchildren are created for each node.",
			  "Given a ray, the stack-based traversal algorithm starts by si-\nmultaneously intersecting the ray with the four bounding boxes\ncontained in the root SIMD BVH Node using SIMD instructions",
			  "The pointers of the children with a non-empty bounding box\nintersection are sorted and then pushed on the stack.",
			  "The ray is prepared prior to traversal by replicating the values\nfor the maximum ray distance ( tfar ), the origin, and reciprocal\nof the direction across a SIMD register."
			]
		  },
		  {
			"title": "Exploiting Local Orientation Similarity for Efficient Ray ...",
			"url": "https://www.embree.org/papers/2014-HPG-hair.pdf",
			"excerpts": [
			  "a BVH with a branching factor of four that will allow for al-\nways intersecting four child-bounds in parallel (Section 3.4 ).",
			  "This data-parallel intersection in particular requires that ev-\nery group of four sibling nodes have to be of the same type:\nIf only one prefers OBBs, all four nodes have to be OBB\nnodes.",
			  "**Node References.** A node stores 64-bit *node references* to\npoint to its children. These node references are decorated\npointers, where we use the lower 4 bits to encode the type of\ninner node we reference (AABB node or OBB node) or the\nnumber of hair segments pointed to by a leaf node. During\ntraversal we can use simple bit operations to separate the\nnode type information from the aligned pointer.",
			  "**AABB nodes.** For nodes with axis aligned bounds, we store\nfour bounding boxes in a SIMD friendly structure-of-array\nlayout (SOA). In addition to the single-precision floating\npoint coordinates for the four AABBs this node also stores\nthe four 64-bit node references, making a total of 128 bytes\n(exactly two 64-byte cache lines)."
			]
		  },
		  {
			"title": "Faster Incoherent Ray Traversal Using 8-Wide AVX ...",
			"url": "https://www.cs.ubbcluj.ro/~afra/publications/afra2013tr_mbvh8.pdf",
			"excerpts": [
			  ";\nThe node size for MBVH4 is 128 bytes, and for MBVH8\nit is 256 bytes.\nT",
			  "\nThe multi-triangles, similar to the bounding boxes in the\nnodes, have a fixed-size SoA layout to facilitate SIMD pro-\ncessing. The triangle data is pregathered to maximize inter-\nsection performance at the cost of higher memory usage.\n**"
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231581/06-1025-benthin.pdf",
			"excerpts": [
			  ".\n**4.1**\n**Node Compression and Decompression**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf.",
			  "**struct** BVH8MultiNode {\nbox3f childBounds [8]; //one float box per child\nuint64 childRef [8]; }; // child pointers",
			  "**struct** QBVH8MultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nuint64 childRef [8]; }; // child pointers",
			  "**struct** CLBVHMultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nLeafPrimData childPrims [0]; // implicit pointer\n}; // leaf data stored right behind this node",
			  "ion**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf. E",
			  "**4**\n**IMPLEMENTATION**\nWe implement and evaluate the previously discussed strategy within\na modified version of Embree 3.0.",
			  "The key to improving fast ray tracing is the use of acceleration data\nstructures. Though indispensable for performance, such structures\nrequire both time and memory to be built and stored. In particu-\nlar, the memory overhead of the acceleration structure can be a",
			  "roduces dedicated compressed multi-leaf nodes where most effec-\ntive at reducing memory use, and uses regular BVH nodes for inner\nnodes and small, isolated leaves. We show that when implemented\nwithin the Embree ray tracing framework, this approach achieves\nroughly the same memory savings as Embrees compressed BVH\nlayout, while maintaining almost the full performance of its fastest\nnon-compressed BVH.\n**CC"
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH ...Read more"
			]
		  },
		  {
			"title": "[PDF] Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://www.semanticscholar.org/paper/Efficient-incoherent-ray-traversal-on-GPUs-through-Ylitie-Karras/7d4816055b1a9aecb75f36e5e6ab5948b1354ed1",
			"excerpts": [
			  "Efficient incoherent ray traversal on GPUs through compressed wide BVHs  H. Ylitie, Tero Karras, S. Laine  Published in High Performance Graphics 28 July 2017 ..."
			]
		  },
		  {
			"title": "(PDF) Compressed-leaf bounding volume hierarchies",
			"url": "https://www.researchgate.net/publication/326762122_Compressed-leaf_bounding_volume_hierarchies",
			"excerpts": [
			  "Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH layouts. Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "Typically, wide BVHs use a data layout where all of an\nindividual nodes N children are stored together in a consec-\nutive block, typically in a SoA data layout. This allows for\naddressing all of a parents N children with a single pointer\nand aids vectorization, but slightly confuses the terminology\nof what a node in a wide BVH actually i",
			  "Throughout the\npaper we will refer to each group of N siblings as a N-wide\nmulti- node, with each sibling consisting of N individual nodes",
			  "s . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\n ... ",
			  " summary, our resulting BVHwhich we call a Compressed-\nLeaf BVH (\nCLBVH\n)has two multi-node types: regular\nBVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original\nBVH8\nmulti-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1)",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "CLBVH-fast\nuses our\nCLBVH\nnodes with regular,\nuncompressed leaf data, while\nCLBVH-compact\nperforms the\nleaf data compression described in Section 4.5.\nWhen looking at only the\nQBVH8\nand\nCLBVH-fast\nvariants,\nthe relative memory savings of both are higher, since they\nare no longer as dominated by the leaf data cost.",
			  "For an additional roughly 5% of performance,\nCLBVH-compact\ncan save even more memory, reaching, on average, a nearly\n3  reduction in total memory "
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call *Compressed-Leaf Bounding Volume Hierarchies* (CLBVH), which strike a balance between compressed and non-compressed BVH layouts.",
			  "Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "We show that when implemented within the Embree ray tracing framework, this approach achieves roughly the same memory savings as Embree's compressed BVH layout, while maintaining almost the full performance of its fastest non-compressed BVH."
			]
		  },
		  {
			"title": "Carsten Benthin, Ingo Wald, Sven Woop, Attila fra",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CompressedLeafBoundingVolumeHierarchies.pdf",
			"excerpts": [
			  "Tweak top-down BVH builder to primarily generate all-leaf multi-nodes",
			  "Store referenced primitive data directly behind all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "Leaf-Data 0",
			  "Leaf-Data 1",
			  "Leaf-Data 2",
			  "Leaf-Data 3",
			  "Leaf-Data 4",
			  "Leaf-Data 5",
			  "Leaf-Data 6",
			  "Leaf-Data 7",
			  "EVEN FURTHER COMPRESSION CLBVH\n(compact)",
			  "Extract shared features in geometry data in all-leaf multi-node",
			  "Object IDs, vertices, vertex indices, shader IDs, etc."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://pdfs.semanticscholar.org/f65c/b99458aaf6c9f609e7950711188ea97a025e.pdf",
			"excerpts": [
			  "8-wide BVH constructed with SAH-optimal widening",
			  "Compressed node storage format",
			  "Quantization grid position and scale\nstored in parent node",
			  "Quantize child node AABBs to a local grid",
			  "Quantize child node AABBs to a local grid"
			]
		  },
		  {
			"title": "[PDF] Accelerating Graph Analytics on CPU-FPGA Heterogeneous Platform | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Accelerating-Graph-Analytics-on-CPU-FPGA-Platform-Zhou-Prasanna/5cf2f8dd9491b2cf0c42b0d2a6ccdae5c764f906",
			"excerpts": [
			  "Graphicionado: A high-performance and energy-efficient accelerator for graph analytics  Computer Science, Engineering. 2016 49th Annual IEEE/ACM International ..."
			]
		  },
		  {
			"title": "Two-way skewed-associative caches | by Arpit Gupta | Medium",
			"url": "https://medium.com/@arpitguptarag/two-way-skewed-associative-caches-9bab2c36dcee",
			"excerpts": [
			  "Dr Andre Seznec describes a new design skewed-associative caches which helps in improving miss rate while not compromising with the frequency at the same time.Read more"
			]
		  },
		  {
			"title": "US20160188476A1 - Hardware prefetcher for indirect access patterns \n      - Google Patents",
			"url": "https://patents.google.com/patent/US20160188476A1/en",
			"excerpts": [
			  "Two techniques address bottlenecking in processors. The first is indirect prefetching. The technique can be especially useful for graph analytics and sparse ..."
			]
		  },
		  {
			"title": "ShadowLoad: Injecting State into Hardware Prefetchers",
			"url": "https://misc0110.net/web/files/shadowload_asplos25.pdf",
			"excerpts": [
			  "ShadowLoad exploits the missing isolation of hardware prefetcher state between applications or privilege levels on. Intel and AMD CPUs up to ( ...Read more"
			]
		  },
		  {
			"title": "Looking Under the Hood of CPU Cache Prefetching",
			"url": "https://dbis.cs.tu-dortmund.de/storages/dbis-cs/r/papers/2024/sw-prefetching-survey/sw-prefetching.pdf",
			"excerpts": [
			  "Both Intel and AMD note that these non-temporally prefetched cache lines are prioritized for quicker eviction [4, 13]. 2.3 Limitations. Modern ...Read more",
			  "Interplay with Hardware Prefetchers",
			  " saw that software prefetching has its limits when it is nec-\nessary to prefetch large data blocks at once. To circumvent this\nlimitation and associated costs, a collaboration between software\nand hardware prefetching emerges as a promising solution, form-\ning a symbiotic relationship: Software prefetching takes the initial\nstep by early hinting about unpredictable data accesses; hardware\nprefetchers can take over, incrementally fetching the remaining\ndata line by line. In this section, we delve into their cooperative\ndynamics, emphasizing how strategic software prefetching *stimu-*\n*lates* hardware prefetchers beyond the typical dependence on load\nmiss",
			  "ardware Prefetcher Patterns.* In our in-depth analysis, we con-\ncentrate on Intel processors, which offer valuable insights into\nprefetching dynamics by implementing sampling of data addresses\nand access latency through the Linux *perf* interface [ 1 ]. Intel uti-\nlizes multiple core-local prefetchers that serve the L 1 and L 2 caches\nacross both modern and legacy processor generations [ 13 , 32 ]. The\nL1d prefetchers monitor load streams within a single cache line,\nsuccessively requesting the next line. Meanwhile, L 2 prefetchers\nstudy the patterns of cache line requests traversing through the\nL 2 cache. Here, the *stream prefetcher* initiates prefetches based on\na sequence of consecutive cache line requests, while the *adjacent*\n*prefetcher* consistently loads a pair of neighboring cache lines at\nonce. This architectural design prompts two critical lines of inquiry:\nDo software and hardware prefetchers have any interaction? If yes,\nwhat are the impacts of different prefetch instructions?",
			  "Discussion.* The measured results allow us to conclude that hardware\nand software prefetchers interact with each other, even on different\nhardware platforms. Leveraging this interaction becomes essential\nwhen large amounts of data should be transferred from memory to\nthe CPU cachegiven the limitations of LFB s highlighted in Sec-\ntion 3.2. Consider the *range scan* of tree structures as an example:\nAlthough accessed in a logical order, leaf nodes are scattered in\nmemory and accessed in a seemingly random fashion from the\nmemory subsystems point of view. While software prefetching can\nsignal to the hardware what leaf nodes will be accessed shortly, thus\ninitiating the transfer into the cache, the hardware prefetcher can\neffectively take over once it recognizes the sequential access pat-\ntern. This principle also extends to morsel-driven database engines,\nsuch as HyPer [ 17 ] and Umbra [ 29 , 36 ], which process fine-grained\npackages of tuples"
			]
		  },
		  {
			"title": "Graphicionado: A High-Performance and Energy-Efficient ...",
			"url": "https://taejunham.github.io/data/graphicionado_micro16.pdf",
			"excerpts": [
			  "asily perform next-line prefetches and get the data into the\naccelerator before they are needed. We extend the *sequential*\n*vertex read* and *edge read* modules (stage P1 and P3 in Fig. 6)\nto prefetch and buffer up to *N* cachelines ( *N* = 4 is used for\nour evaluation) and configure them to continue fetching the\nnext cacheline from memory as long as the buffer is not ful",
			  "**exploits not only data structure-centric datapath specialization,**\n**but also memory subsystem specialization, all the while taking ad-**\n**vantage of the parallelism inherent in this domain. G",
			  "icionado\ncan process graph analytics workloads with reasonable effi-\nciency. However, thus far it is a single pipeline with theoretical\nmaximum throughput limited to one edge per cycle in the\n*Processing* phase and one vertex per cycle in the *Apply*\nphase",
			  "e on-chip storage allowed dramatic\nreduction of the communication latency and bandwidth by con-\nverting the frequent and inefficient random off-chip data com-\nmunication to on-chip, efficient, finer-granularity scratchpad\nmemory accesse",
			  "This paper makes the following contributions:",
			  " A specialized graph analytics processing hardware pipeline\nthat employs datatype and memory subsystem special-\nizations while offering workload-specific reconfigurable\nblocks called Graphicionado",
			  "Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "The Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "= 4 is used for\nour evaluation"
			]
		  },
		  {
			"title": "Graphicionado",
			"url": "https://mrmgroup.cs.princeton.edu/slides2/graphicionado_slide.pdf",
			"excerpts": [
			  "Graphicionado : A high-performance , energy-efficient graph analytics HW\naccelerator which overcomes the limitations of software frameworks while\nretaining the programmability benefit of SW frameworks",
			  "Graphicionado utilizes an on-chip storage to avoid wasting off-chip BW",
			  "Partition a graph before the execution; Then, process each subgraph\nat a time",
			  "Scaling On-Chip Memory Usage",
			  "Only half of the vertices to be stored in on-chip storage at a time"
			]
		  },
		  {
			"title": "adaptive prefetching on POWER7",
			"url": "https://people.ac.upc.edu/fcazorla/articles/vjimenez_pact_2012.pdf",
			"excerpts": [
			  "ardware data prefetch is a well-known technique to help\nalleviate the so-called *memory wall* problem [31]",
			  ", 7, 15] is programmable and al-\nlows the user to set different parameters (knobs) that control\nits behavior: i) *prefetch depth* , how many lines in advance\nto prefetch, ii) *prefetch on stores* , whether to prefetch store\noperations, and iii) *stride-N* , whether to prefetch streams\nwith a stride larger than one cache block.",
			  "The prefetcher is\ncontrolled via the data stream control register (DSCR).",
			  "The\nLinux kernel exposes the register to the user through the sys\nvirtual filesystem [22], allowing the user to set the prefetch\nsetting on a per-thread basis.",
			  "In this paper,\nwe present an adaptive prefetch scheme that dynamically\nadapts the prefetcher configuration to the running work-\nload, aiming to improve performan",
			  "e use the IBM\nPOWER7 [27] as the vehicle for this study, since: (i) this\nrepresents a state-of-the-art high-end processor, with a ma-\nture data prefetch engine that has evolved significantly since\nthe POWER3 time-frame; and (ii) this product provides\nfacilities for accurate measurement of performance metrics\nthrough a user-visible performance counter interface",
			  "POWER7 contains a programmable prefetch engine that\nis able to prefetch consecutive data blocks as well as those\nseparated by a non-unit stride [27].",
			  "The processor system is\nprovided to customers with a default prefetch setting that\nis targeted to improve performance for most applications."
			]
		  },
		  {
			"title": "Making Data Prefetch Smarter: Adaptive Prefetching on POWER7 | Conference Paper | PNNL",
			"url": "https://www.pnnl.gov/publications/making-data-prefetch-smarter-adaptive-prefetching-power7",
			"excerpts": [
			  "Hardware data prefetch engines are integral parts of many general purpose server-class microprocessors in the eld to- day. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default conguration during system bring-up and dynamic reconguration of the prefetch engine is not an autonomic feature of current machines.",
			  "In fact, they may actually de- grade performance due to useless bus bandwidth consump- tion and cache pollution.",
			  "In this paper, we present an adap- tive prefetch scheme that dynamically modies the prefetch settings in order to adapt to the workload requirements.",
			  "We implement and evaluate adaptive prefetching in the con- text of an existing, commercial processor, namely the IBM POWER7.",
			  "Our adaptive prefetch mechanism improves per- formance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively.",
			  "Categories",
			  "*Revised: July 23, 2014 | Published: September 19, 2012*"
			]
		  },
		  {
			"title": "Adaptive prefetching on power7: Improving performance and power consumption for ACM TOPC - IBM Research",
			"url": "https://research.ibm.com/publications/adaptive-prefetching-on-power7-improving-performance-and-power-consumption",
			"excerpts": [
			  "We present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to workloads' requirements.",
			  "Adaptive prefetching is also able to reduce power consumption in some cases.",
			  "showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively.",
			  "First we characterize\"in terms of performance and power consumption\"the prefetcher in that processor using microbenchmarks and SPEC CPU2006.We then present our adaptive prefetch mechanism showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Making data prefetch smarter: Adaptive prefetching on power 7 for PACT 2012 - IBM Research",
			"url": "https://research.ibm.com/publications/making-data-prefetch-smarter-adaptive-prefetching-on-power-7",
			"excerpts": [
			  "ware data prefetch engines are integral parts of many general purpose server-class microprocessors in the field today. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default configuration during system bring-up and dynamic reconfiguration of the prefetch engine is not an autonomic feature of current machines. Conceptually, however, it is easy to infer that commonly used prefetch algorithms, when applied in a fixed mode will not help performance in many cases. In fact, they may actually degrade performance due to useless bus bandwidth consumption and cache pollution. In this paper, we present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to the workload requirements. We implement and evaluate adaptive prefetc",
			  "Our adaptive prefetch mechanism improves performance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Skewed-Associative Caches: CS752 Final Project",
			"url": "https://pages.cs.wisc.edu/~chalpin/about/resume/cs752.pdf",
			"excerpts": [
			  "Skewed-associative caches were proposed as a way to decrease the miss rate, while\nnot further increasing the size or associativity.",
			  "In a single level cache system, skewing\nfunctions are useful to help decrease the miss rate, and cause fewer requests to main\nmemory.",
			  "They could be used in the L1 cache, but since they add a small amount of logic\nto the critical path of cache access, they do not meet the criteria of fast L1 cache access\nstated above.",
			  "We will explore the claim by Seznec that a two-way skewed-associative cache\nwill perform as well as a four-way set associative cache.",
			  "The basic requirement is that *A* 1 ** *A* 2 results in values\nappropriate to address the number of blocks in a bank. ",
			  "The shuffle function is simply a reordering of the wires."
			]
		  },
		  {
			"title": "Cache architecture research  ALF",
			"url": "https://team.inria.fr/alf/members/andre-seznec/cache-architecture-research/",
			"excerpts": [
			  "The skewed associative cache is a new organization for multi-bank caches.",
			  "Skewed-associative caches have been shown to have two major advantages over conventional set-associative caches.",
			  "First, at equal associativity degrees, a skewed-associative cache typically exhibits the same hardware complexity as a set-associative cache, but exhibits lower miss ratio.",
			  "This is particularly significant for BTBs and L2 caches for which a significant ratio of conflict misses occurs even on 2-way set-associative caches.",
			  "Second, the behavior of skewed-associative caches is quite insensitive to the precise data placement in memory.",
			  "We also  showed that the skewed associative structure offers a unique opportunity to build TLBs supporting multiple page sizes."
			]
		  },
		  {
			"title": "Minimally-skewed-associative caches",
			"url": "https://ieeexplore.ieee.org/document/1180765/",
			"excerpts": [
			  "Skewed-associativity is a technique that reduces the miss ratios of CPU caches by applying different indexing functions to each way of an associative cache.",
			  "Although several of the new architectures were welcomed by the industry, such as victim caches which are employed in AMD processors, skewed-associativity was not, presumably because implementation of the original scheme is complex and, most probably, involves access-time penal-ties among other costs",
			  ". The purpose of this work is to evaluate a simplified, easy to implement version that we call minimally-skewed-associativity (MSkA).",
			  "Miss ratios are not as good as those for full skewing, but they are still advantageous.",
			  "Minimal-skewing is thus proposed as a way to improve the hit/miss performance of caches, often without producing access-time delays or increases in power consumption as other techniques do (for example, using higher associativities).",
			  "**Published in:** [14th Symposium on Computer Architecture and High Performance Computing, 2002. Proceedings.](/xpl/conhome/8407/proceeding)",
			  "**Date of Conference:** 28-30 October 2002"
			]
		  },
		  {
			"title": "GraphP: Reducing Communication for PIM-based Graph ...",
			"url": "http://alchem.usc.edu/portal/static/download/graphp.pdf",
			"excerpts": [
			  "Martonosi,. Graphicionado: A high-performance and energy-efficient ac- celerator for graph analytics, in Microarchitecture (MICRO),. 2016 49th Annual IEEE ...Read more"
			]
		  },
		  {
			"title": "Seminar on Computer Architecture",
			"url": "https://pdfs.semanticscholar.org/efc1/28130e3b2170ca9102abbeb62f7984d1a81a.pdf",
			"excerpts": [
			  "DDR3-OoO HMC-OoO HMC-MC Tesseract Tesseract. LP. Tesseract. LP + MTP. +56%. +25%. 9.0x. 11.6x. 13.8x. Average Performance. Page 18. Evaluation Results. 18.Read more"
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://inria.hal.science/hal-02991204/document",
			"excerpts": [
			  "Our more recent tests have shown that the Emu hardware can achieve up to 1.6 GB/s per node and 12.8 GB/s on 8 nodes for the STREAM benchmark, ...Read more"
			]
		  },
		  {
			"title": "Improving Streaming Graph Processing Performance using ...",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3466752.3480096",
			"excerpts": [
			  "The protocol FSM takes appropriate actions on the MSHR status change: the task is forwarded to a FIFO buffer to the cache controller  and the ...Read more"
			]
		  },
		  {
			"title": "Energy characterization of graph workloads - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S221053792030189X",
			"excerpts": [
			  "For these reasons, there are several ongoing research efforts exploring custom architectures to enhance graph processing. Some solutions include custom processing elements that decouple computation from communication (e.g., Graphicionado [1], Lincoln Lab graph processor [2]), while other designs explore near memory processing (e.g., Tesseract [3], GraphPIM [4], GraphP [5]).",
			  "Some systems, like the Cray XMT [6], [7], exploit multithreading to tolerate, rather than reduce, latencies even at a large scale.",
			  "The EMU system [8] exploits the concept of migrating threads near to the data.",
			  "The DARPA Hierarchical Identify Verify and Exploit (HIVE) program [9] is looking to build a graph analytics processor that can process (streaming) graphs faster and at much lower power than current processing technology.",
			  "A particular focus of HIVE is to optimize both performance and power (i.e., the efficiency), trying to reach 1000 times the TEPS/W (traversed edges per second per Watt) of current designs (such as GPUs and conventional CPUs).",
			  "Ideally, a processor 1000faster in the same power envelope of a current design, or a processor as fast as a current one, but consuming 1/1000th of the power, would both reach the project objective."
			]
		  },
		  {
			"title": "A Scalable Processing-in-Memory Accelerator for Parallel ...",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/tesseract-pim-architecture-for-graph-processing_isca15.pdf",
			"excerpts": [
			  "ign a programmable PIM accelerator for large-scale*\n*graph processing called Tesseract. Tesseract is composed of*\n*(1) a new hardware architecture that fully utilizes the available*\n*memory bandwidth, (2) an efficient method of communication*\n*between different memory partitions, and (3) a programming*\n*interface that reflects and exploits the unique hardware de-*\n*sign. It also includes two hardware prefetchers specialized for*\n*memory access patterns of graph processing, which operate*\n*based on the hints provided by our programming model. Our*\n*comprehensive evaluations using five state-of-the-art graph*\n*processing workloads with large real-world graphs show that*\n*the proposed architecture improves average system perfor-*\n*mance by a factor of ten and achieves 87% average energy*\n*reduction over conventional s",
			  "* We provide case studies of how five graph processing work-\nloads can be mapped to our architecture and how they\ncan benefit from it. Our evaluations show that Tesseract\nachieves 10x average performance improvement and 87%\naverage reduction in energy consumption over a conven-\n ... ",
			  "\nOur prefetching mechanisms, when employed together, en-\nable Tesseract to achieve a 14x average performance improve-\nment over the DDR3-based conventional system, while min-\nimizing the storage overhead to less than 5 KB per core (see\nSection 4.1). Me",
			  "The reason why conventional systems fall behind Tesseract\nis that they are limited by the low off-chip link bandwidth\n( 102.4 GB/s in DDR3-OoO or 640 GB/s in HMC-OoO/-MC)\nwhereas our system utilizes the large internal memory band-\nwidth of HMCs ( 8 TB/s ). 10 "
			]
		  },
		  {
			"title": "Retrospective: A Scalable Processing-in-Memory Accelerator ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/Tesseract_50YearsOfISCA-Retrospective_isca23.pdf",
			"excerpts": [
			  "15 paper [1] provides a new pro-**\n**grammable processing-in-memory (PIM) architecture and system**\n**design that can accelerate key data-intensive applications, with**\n**a focus on graph processing wor",
			  "our accelerator system, Tesseract, using 3D-stacked memories**\n**with logic layers, where each logic layer contains general-purpose**\n**processing cores and cores communicate with each other using a**\n**message-passing programming mod",
			  " the first to completely design**\n**a near-memory accelerator system from scratch such that it is**\n**both generally programmable and specifically customizable to**\n**accelerate important applications, with a case study on major**\n**graph processing workloads. Ensuing work in academia and**\n**industry showed that similar approaches to system design can**\n**greatly benefit both graph processing workloads and other**\n**applications, such as machine learning, for which ideas from**\n**Tesseract seem to have been influential"
			]
		  },
		  {
			"title": "[2306.15577] Retrospective: A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing",
			"url": "https://arxiv.org/abs/2306.15577",
			"excerpts": [
			  ". We built our accelerator system, Tesseract, using 3D-stacked memories with logic layers, where each logic layer contains general-purpose processing cores and cores communicate with each other using a message-passing programming model.",
			  " Our ISCA 2015 paper provides a new programmable processing-in-memory (PIM) architecture and system design that can accelerate key data-intensive applications, with a focus on graph processing workloads. ",
			  "r major idea was to completely rethink the system, including the programming model, data partitioning mechanisms, system support, instruction set architecture, along with near-memory execution units and their communication architecture, su"
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://dl.acm.org/doi/10.1145/2749469.2750386",
			"excerpts": [
			  "The key modern enabler for PIM is the recent advancement of the 3D integration technology that facilitates stacking logic and memory dies in a single package, which was not available when the PIM concept was originally examined.",
			  "Tesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design.",
			  "In this work, we argue that the conventional concept of processing-in-memory (PIM) can be a viable solution to achieve such an objective.",
			  "In order to take advantage of such a new technology to enable memory-capacity-proportional performance, we design a programmable PIM accelerator for large-scale graph processing called Tesseract.",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model.",
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems."
			]
		  },
		  {
			"title": "An Initial Characterization of the Emu Chick",
			"url": "https://fruitfly1026.github.io/static/files/ipdpsw18-hein.pdf",
			"excerpts": [
			  "The Emu Chick prototype is still in active development. The\ncurrent hardware iteration uses an Arria 10 FPGA on each\nnode card to implement the Gossamer cores, the migration\nengine, and the stationary cores.",
			  ".\nThe Emu is a cache-less system built around nodelets that\neach execute lightweight threads and migrate threads to data\nrather than moving data through a traditional cache hierarchy.\nThi",
			  "e current prototype hardware uses**\n**FPGAs to implement cache-less Gossamer cores for doing**\n**computational work and a stationary core to run basic operating**\n**system functions and migrate threads betw",
			  "Pointer chasing on the Xeon architecture performs poorly\nfor several reasons. For small block sizes, the memory system\nbandwidth is used inefficiently. An entire 64-byte cache line\nmust be transferred from memory, but only 16 bytes will\nbe used. The best performance is achieved with a block size\nbetween 256 and 4096 elements.",
			  "Performance on Emu remains mostly flat regardless of block\nsize. Emus memory access granularity is 8 bytes, so it never\ntransfers unused data in this benchmark. As long as a block\nfits within a single nodelets local memory channel, there is no\npenalty for random access within the block.",
			  "The Emu Chick is a prototype system designed**\n**around the concept of migratory memory-side processing. Rather**\n**than transferring large amounts of data across power-hungry,**\n**high-latency interconnects, the Emu Chick moves lightweight**\n**thread contexts to near-memory cores before the beginning**\n*",
			  "The\nEmu architecture is designed from the ground up to support\nhigh bandwidth utilization and efficiency for demanding data\nanalysis workloads.",
			  "he Emu Chick provides stable,**\n**predictable performance with 80% bandwidth utilization on a**\n**random-access pointer chasing benchmark with weak lo"
			]
		  },
		  {
			"title": "A Microbenchmark Characterization of the Emu Chick",
			"url": "https://arxiv.org/pdf/1809.07696",
			"excerpts": [
			  "The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring\nlarge amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to\nnear-memory cores before the beginning of each memory read.",
			  "Emu bandwidth is currently limited by CPU speed and thread\ncount rather than DDR bus transfer rates. However even with\nthis prototype system we can observe improvements in other\nbenchmarks where the memory access pattern is not as linear\nand predictable as it is with STREAM.",
			  "Pointer chasing on the Xeon architecture performs poorly\nfor several reasons. For small block sizes, the memory system\nbandwidth is used ine ffi ciently. An entire 64-byte cache line\nmust be transferred from memory, but only 16 bytes will be used.",
			  "e pointer chasing benchmark in Section 4.2 achieves a\nstable 60-65% bandwidth utilization across a wide range of lo-\ncality parameters. These pointer chasing results and data layout\nstudies show how random accesses with SpMV can be improved\nand while performance of SpMV does not quite match a well-\noptimized x86 implementation, these optimizations can provide\na template for future benchmarking and application development\nand show how application memory layouts and smart thread\nmigration can be used to maximize performance on the Emu\nsystem",
			  "results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more e ffi ciently than a more\ntraditional, cache-based architecture although bandwidth usage su ff ers for computationally intensive workloads like SpMV.",
			  "Moreover,\nthe Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access\npointer chasing benchmark with weak locality.",
			  "The main high-level finding is that an Emu-style architecture\ncan more e ffi ciently utilize available memory bandwidth while\nreducing the variability of that bandwidth to the memory access\npatter"
			]
		  },
		  {
			"title": "Programming the EMU Architecture: Algorithm Design ...",
			"url": "https://sc18.supercomputing.org/proceedings/tech_poster/poster_files/post213s2-file2.pdf",
			"excerpts": [
			  "**EMU-Chick System:** 8 nodes, 64GB memory per node.",
			  "**Node:** Each node has 8x nodelets, an array of DRAMs, a migration engine, PCI-\nExpress interfaces, and a stationary core (SC), accompanied with an SSD.",
			  "**Nodelet:** A nodelet contains 2x Gosamer cores (GC), each of which supports 64\nconcurrent in-order, single-issue hardware threads",
			  "**Memory:** Each node has a 64-byte channel DRAM, divided into eight 8-byte\nnarrow-channel-DRAMs (NC-DRAM).",
			  "**EMU Architecture**",
			  "Using higher number of memory blocks in mw_malloc2d will force threads to\nmigrate more frequently across nodelets.",
			  "Allocating fewer blocks will distribute memory into fewer nodelets and will\nrestrict parallelism.",
			  "\n Flat spawn: The parent thread sequentially iterates over the for loop and spawns\nchildren. Thread creation complexity is O(n) , where n is the number of threads.\n",
			  " Tree spawn: The process of creating children in EMU can be parallelized by using\na recursive, hierarchal spawn. Thread creation complexity is O(log n).\nT",
			  "Three factors play a major role in performance on EMU systems:",
			  "Each nodelet in EMU-chick can support up to 256 live threads and also can hold\ncontext information of up to 500 threads.",
			  "Threads that exceed this level will fail to spawn and execute as a regular function.",
			  "The optimal number of threads per nodelet varies by the load.",
			  "**Conclusion**",
			  "EMU presents an unorthodox approach to solve the increasingly worsening\nmemory bottleneck problem in HPC.",
			  "There are unique architectural considerations that need to be addressed while\ndeveloping for the EMU platform.",
			  "**Level-Synchronous BFS for EMU**",
			  "We use compressed sparse row representation (CSR) to represent graphs in\nmemory.",
			  "BFS-EMU follows a hierarchical launch strategy",
			  " First, visit_node_block is spawned for every node block, so that further\nnode related operations become local to the spawned children.",
			  " Then, visit_node is spawned for each node in the nodelet. Each thread of\nthis function further migrates to the nodelet where the portion of the edges\narray holding current nodes neighbor indicies resides.",
			  " Finally, for each neighbor, visit_neighbor is spawned. Each thread of\nthis function migrates to the nodelet where the corresponding indicies for the\ncosts and masks arrays are located. They are updated with proper values.",
			  "2\n1\n3\n4\n1\n2\n3\n4",
			  "**Want to collaborate with us on our EMU system?**",
			  "**Contact us:** **https://excl.ornl.gov/contact/**",
			  "[1] Kogge Dysart et.al Highly scalable near memory processing with migrating\nthreads on the emu system architecture. In Proceedings of the Sixth Workshop on\nIrregular Applications: Architectures and Algorithms (IA3), ACM, 2016",
			  "[2] Mehmet E. Belviranli, Seyong Lee and Jeffrey S. Vetter. \"Designing Algorithms\nfor the EMU Migrating-threads-based Architecture.\" In High Performance Extreme\nComputing Conference (HPEC), IEEE, 2018",
			  "**References**"
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3418077",
			"excerpts": [
			  "Nodelets combine narrowly banked memory with highly multi-threaded, cacheless Gossamer cores to provide a memory-centric environment for migrating threads.Read more"
			]
		  },
		  {
			"title": "\n        Graphicionado: A high-performance and energy-efficient accelerator for graph analytics\n      -  Princeton University",
			"url": "https://collaborate.princeton.edu/en/publications/graphicionado-a-high-performance-and-energy-efficient-accelerator/",
			"excerpts": [
			  "Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76-6.54x speedup while consuming 50-100x less energy compared to a state-of-The-Art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.",
			  "Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs."
			]
		  },
		  {
			"title": "(Open Access) Graphicionado: a high-performance and energy-efficient accelerator for graph analytics (2016) | Tae Jun Ham | 295 Citations",
			"url": "https://scispace.com/papers/graphicionado-a-high-performance-and-energy-efficient-3kyusmd4up?citations_page=45",
			"excerpts": [
			  "Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks, for high-performance, energy-efficient processing of graph analytics workload",
			  "s Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs.",
			  "Our results show that Graphicionado achieves a 1.76  6.54x speedup while consuming 50  100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.",
			  "Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks, for high-performance, energy-efficient processing of graph analytics workloads."
			]
		  },
		  {
			"title": "Papers on Graph Analytics - Julian Shun - MIT",
			"url": "https://jshun.csail.mit.edu/graph.shtml",
			"excerpts": [
			  "[Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics](https"
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://ieeexplore.ieee.org/document/7284059/",
			"excerpts": [
			  ":\nTesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design. I",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model.",
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems.",
			  "**Published in:** [2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)](/xpl/conhome/7270878/proceeding)",
			  "**Date of Conference:** 13-17 June 2015"
			]
		  },
		  {
			"title": "Tesseract Pim Architecture For Graph Processing - Isca15 | PDF | Cpu Cache | Computer Data Storage",
			"url": "https://www.scribd.com/document/663293050/tesseract-pim-architecture-for-graph-processing-isca15",
			"excerpts": [
			  "Tesseract is composed of\n(1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efci ent method of communic ation\nbetween different memory partitions, and (3) a programming\ninterface that reects and exploits the unique hardware de-\nsign.",
			  "It also includes two hardware prefetchers specialized for\nmemory access patterns of graph processing, which operate\nbased on the hints provided by our programming model.",
			  "Our\ncomprehensive evaluations using ve state-of-the-art graph\nprocessing workloads with large real-world graphs show that\nthe proposed architecture improves average system perfor- mance by a factor of ten and achieves 87% average energy\nreduction over conventional systems.\n"
			]
		  },
		  {
			"title": "[1809.07696] A Microbenchmark Characterization of the Emu Chick",
			"url": "https://arxiv.org/abs/1809.07696",
			"excerpts": [
			  "The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring large amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each memory read.",
			  "The current prototype hardware uses FPGAs to implement cache-less \"Gossamer cores for doing computational work and a stationary core to run basic operating system functions and migrate threads between nodes.",
			  "n this multi-node characterization of the Emu Chick, we extend an earlier single-node investigation (Hein, et al. AsHES 2018) of the the memory bandwidth characteristics of the system through benchmarks like STREAM, pointer chasing, and sparse matrix-vector multiplicati",
			  "We compare the Emu Chick hardware to architectural simulation and an Intel Xeon-based platform.",
			  "Our results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more efficiently than a more traditional, cache-based architecture although bandwidth usage suffers for computationally intensive workloads like SpMV.",
			  "Moreover, the Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access pointer chasing benchmark with weak locality."
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://arxiv.org/pdf/1901.02775",
			"excerpts": [
			  "The Emu Chick prototype implements migratory memory-side processing in a novel hardware\nsystem.",
			  "Rather than transferring large amounts of data across the system interconnect, the Emu\nChick moves lightweight thread contexts to near-memory cores before the beginning of each\nremote memory read.",
			  "the Emu architecture is designed to scale applications with poor data locality to supercomputing\nscale by more effectively utilizing available memory bandwidth and by dedicating limited power\nresources to networks and data movement rather than caches.",
			  "The\nkey differentiators for the Emu architecture are the use of cache-less processing cores, a high-radix\nnetwork connecting distributed memory, and PGAS-based data placement and accesses.",
			  "In short,\nthe Emu architecture is designed to scale applications with poor data locality to supercomputing\nscale by more effectively utilizing available memory bandwidth and by dedicating limited power\nresources to networks and data movement rather than caches."
			]
		  },
		  {
			"title": "Runtime Systems and Scheduling Support for High-End ...",
			"url": "https://etd.ohiolink.edu/acprod/odb_etd/ws/send_file/send?accession=osu1338324367&disposition=inline",
			"excerpts": [
			  "Next, we described a general dynamic scheduling framework for data parallel loops geared towards CPU-GPU heterogeneous machines. Our approach involves ..."
			]
		  },
		  {
			"title": "Data Prefetch Mechanisms",
			"url": "https://www.ece.lsu.edu/tca/papers/vanderwiel-00.pdf",
			"excerpts": [
			  "Data prefetching has been proposed as a technique for hiding the access latency of data referencing patterns that defeat caching strategies."
			]
		  },
		  {
			"title": "Frontiers | Runtime support for CPU-GPU high-performance computing on distributed memory platforms",
			"url": "https://www.frontiersin.org/journals/high-performance-computing/articles/10.3389/fhpcp.2024.1417040/full",
			"excerpts": [
			  "This paper presents an effort to address the challenges of efficiently utilizing distributed heterogeneous computing nodes in a portable and performant way. It introduces a heterogeneous tasking framework as an extensible layer of portability.",
			  "gn and implementation of a heterogeneous tasking framework for the development of performance portable applications that can scale from single-core to multi-core, multi-device ( *CPUs, GPUs* ) platforms efficiently, *without any code refac",
			  "A novel integration of a distributed runtime with the heterogeneous tasking framework to provide an end-to-end solution that scales over distributed heterogeneous computing nodes while exposing a high-level and abstract programming model.",
			  "A series of memory, scheduling, and threading performance optimizations that achieve significant improvements, up to 300% on a single GPU and linear scalability on a multi-GPU platform, that are directly applicable to similar systems and applications.",
			  "The memory captured by a hetero_object should mainly be accessed and modified through hetero_tasks for optimal performance.",
			  "the application can also explicitly request access to the underlying data on the host after specifying the type of access requested to maintain coherence. This method will trigger (if needed) an asynchronous transfer from the device with the most recent version of the data and immediately return a future.",
			  "Heterogeneous tasks (hetero_tasks) are opaque structures that consolidate the parameters characterizing a computational task.",
			  "Heterogeneous tasks are executed asynchronously by the tasking framework. A task submitted for execution is appended to a list of task execution requests. The control is then immediately returned to the application, which can continue to issue more tasks or execute other work.",
			  "However, once tasks reach the runnable list, they are ensured that all of their dependencies have been resolved and, thus, the scheduler can run them in parallel and in any order.",
			  "With the introduction of more heterogeneous computing devices and workloads, it is expected that scheduling and load balancing will only become more complicated. To provide flexibility for different use cases, the actual implementation of the scheduler is designed to be modular and separate from the rest of the heterogeneous tasking framework.",
			  "The abstract scheduler class allows the development of as simple or complex custom data structures and policies as the user might need.",
			  "The heterogeneous tasking framework is implemented in the C++ programming language leveraging its performance and object-oriented design. It is developed in three software layers to allow easy integration with new device types, programming APIs, and scheduling policies (see [Figure 4]",
			  "gure 4** . A high-level representation of the heterogeneous tasking framework software stack and operations. The operations performed by the Core Runtime include (a) Memory monitoring to keep track of the available device memory and deallocate unused objects when running low on resources, (b) Memory transfer and task execution request handling that dispatches such requests when it is safe, (c) Memory coherence among different copies of the same hetero_object in multiple devices, and (d) Task scheduling to optimize for a reduction in memory transfers and optimize overall execution time.",
			  "The device API maps high-level abstractions like hetero_objects and hetero_tasks to actual device-specific constructs.",
			  "To fully utilize the bandwidth capabilities of the respective hardware, NVIDIA GPUs require that host data to be transferred to the GPU should reside in a page-locked memory region.",
			  "ptimization gives a significant boost that ranges between 30% and 145% for different matrix sizes ( [Figure 8]() ; TF-PageLocked)",
			  "The most significant improvement of this optimization is manifested for the smallest matrix case (64  64) with another 60%.",
			  "The results of this optimization are substantial in most matrix sizes when the overlap between memory transfers and kernel executions can be large enough.",
			  "In PREMA, messages are one-sided and asynchronous and are received implicitly to invoke a designated task on their target. Thus, the receiver cannot specify a memory region where the message buffer shall be stored (like MPI).",
			  "The cache allowed us to attain performance within 10% overhead of that achieved by the MPI ( [Figure 10]() ; PREMA+CUDA). "
			]
		  },
		  {
			"title": "CoreTuner: Predicting and Scheduling Framework for Optimizing the Joint Allocation of CPU and GPU in Training Cluster | Proceedings of the 54th International Conference on Parallel Processing",
			"url": "https://dl.acm.org/doi/10.1145/3754598.3754634",
			"excerpts": [
			  "Periodic scheduling, through the CPU influence model and the OCCC metric, for the different resource sensitivities of the tasks, allocates resources to the tasks that make the best use of them and maximizes the shortening of the overall task execution duration of the cluster with limited resources.",
			  "In addition, in the face of the huge solution space brought about by the CPU-GPU combination, a caching mechanism is designed based on the similarity between the allocation schemes to increase the number of search rounds in a limited time to improve the quality of the solution.",
			  "This performance degradation is mainly due to the increased parallel computing and scheduling overhead.",
			  "The OCCC shows distinct heterogeneous characteristics.",
			  "Figure [2]() illustrates the OCCC for various models and batch sizes in an environment with a single P40 GPU, as well as the OCCC for the AlexNet model across different GPU allocation schemes."
			]
		  },
		  {
			"title": "A Scheduling and Runtime Framework for a Cluster ...",
			"url": "https://ieeexplore.ieee.org/document/7161504/",
			"excerpts": [
			  "We present a runtime system for simple and efficient programming of CPU+GPU clusters.",
			  "The programmer focuses on core logic, while the system undertakes task allocation, load balancing, scheduling, data transfer, etc.",
			  "Our programming model is based on a shared global address space, made efficient by transaction style bulk-synchronous semantics.",
			  "This model broadly targets coarse-grained data parallel computation particularly suited to multi-GPU heterogeneous clusters.",
			  "Our runtime system achieves a performance of 5.61 TFlop/s while multiplying two square matrices of 1.56 billion elements each over a 10-nodecluster with 20 GPUs.",
			  "This performance is possible due toa number of critical optimizations working in concert. These include perfecting, pipelining, maximizing overlap between computation and communication, and scheduling efficiently across heterogeneous devices of vastly different capacities."
			]
		  },
		  {
			"title": "Prefetch Instruction - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/prefetch-instruction",
			"excerpts": [
			  "Hardware-initiated prefetching is performed by hardware stream prefetchers located in the last-level (L2) caches, which dynamically detect access patterns and begin prefetching future addresses in those patterns.",
			  "Hardware prefetchers can throttle themselves in response to software prefetching and remain mostly inactive if not effective for a given application.",
			  "Automatic compiler prefetching is generally recommended, with manual prefetching reserved for cases where compiler-generated prefetches are insufficient, such as complex [data processing](../phys",
			  "Tuning parameters for manual prefetching include prefetch distance and cache level hints; systematic variation of these parameters, such as varying prefetch distances and hint values, is described as a methodology for refining prefetch behavior.",
			  "In multi-core and many-core processors, prefetch requests from one core can delay demand requests from another due to memory bandwidth contention, and this interference becomes more significant as the number of cores increases.",
			  "software prefetching allows application writers to provide hints to the memory system.",
			  "prefetching can significantly increase instruction throughput and reduce pipeline stalls, its effectiveness depends on careful tuning of parameters such as prefetch distance and cache level hints to avoid cache pollution and excessive memory bandwidth usage. Com",
			  "The hardware prefetchers can throttle themselves in response to software prefetching, so even if hardware prefetching is not effective for a certain application, it does not need to be disabled because it will remain mostly inactive.",
			  "Note that the hardware/software boundary is not firm."
			]
		  },
		  {
			"title": "Optimizing Memory Access: A Practical Guide to Scheduling Algorithms | by Mohit Mishra | ILLUMINATION | Medium",
			"url": "https://medium.com/illumination/optimizing-memory-access-a-practical-guide-to-scheduling-algorithms-01a8de0c50da",
			"excerpts": [
			  "Section Title: **Optimizing Memory Access: A Practical Guide to Scheduling Algorithms** > **FR-FCFS vs. PAR-BS vs. Adaptive Thread-Aware Scheduling**\nContent:\nWhile First Ready-First Come First Serve (FR-FCFS) offers a foundational approach to memory scheduling, pursuing enhanced performance and fairness demands exploring more advanced techniques. This article goes into two contenders: **Parallelism-Aware Batch Scheduling (PAR-BS) and Adaptive Thread-Aware Scheduling** , unveiling their innovative mechanisms and highlighting their potential to revolutionize memory management.",
			  "FR-FCFS is a memory scheduling algorithm that balances fairness and performance optimization. It prioritizes requests based on row buffer hits and request age.",
			  "The algorithm maintains separate queues for row buffer hits and misses, with each request tagged with a timestamp upon arrival. This approach minimizes the overhead of row activations and pre-charges while ensuring older requests are not starved."
			]
		  },
		  {
			"title": "Lecture 5: September 18 5.1 Scheduling",
			"url": "https://lass.cs.umass.edu/~shenoy/courses/fall13/lectures/Lec05_notes.pdf",
			"excerpts": [
			  "First-Out (FIFO), algorithm is the simplest scheduling algorithm.\nProcesses are dispatched in order according to their arrival time on the ready queue. Being a non-preemptive\ndiscipline, once a process has a CPU, it runs to completion. The FCFS scheduling is fair in the formal sense\nor human sense of fairness but it is unfair in the sense that long jobs make short jobs wait and unimportant\njobs make important jobs wait. FCFS is more predictable than most of other schemes since the scheduling\norder can be easily understood and the code for FCFS scheduling is simple to write and understand. However,\nthe FCFS scheme is not useful in scheduling interactive processes because it cannot guarantee good response\ntime. One of the major drawbacks of this scheme is that the average time is often quite long. Short jobs\nor jobs that frequently perform I/O can have very high waiting times since long jobs can monopolize the\nCPU. FCFS originally didnt have a job relinquish the CPU when it was doing I/O. We assume that a FCFS\nscheduler will run when a process is doing I/O, but it is still non-preemptive. The First-Come-First-Served\nalgorithm is rarely used as a master scheme in modern operating systems but it is often embedded within\nother schemes.\n**Advantage:** simple\n**Disadvantage:** poor performance for short jobs or tasks performing frequent I/O operations\n**5.3**\n**Round Robin Scheduling (RR)**\nRound-robin (RR) is one of the simplest preemptive scheduling algorithms for processes in an operating\nsystem. RR assigns short time slices to each process in equal portions and in order, cycling through the\nprocesses. Round-robin scheduling is both simple and easy to implement, and starvation-free (all processes\nwill receive some portion of the CPU). Round-robin scheduling can also be applied to other scheduling\nproblems, such as data packet scheduling in computer networks. However, round robin does not support\nprioritizing jobs, meaning that less important tasks will receive the same CPU time as more important ones.\nMost time sharing systems actually use variations of round robin for scheduling.\nExample: The time slot could be 100 milliseconds. If job1 takes a total time of 250ms to complete, the\n*Lecture 5: September 18*\n5-3\nround-robin scheduler will suspend the job after 100ms and give other jobs their time on the CPU. Once the\nother jobs have had their equal share (100ms each), job1 will get another allocation of CPU time and the\ncycle will repeat. This process continues until the job finishes and needs no more time on the CPU.\nA challenge with the RR scheduler is determining how to determine the time slice to allocate tasks. If the\ntime slice is too large, then RR performs similarly to FCFS. However, if the time slice is too small, then the\noverhead of performing context switches so frequently can reduce overall performance. The OS must pick\na time slice which balances these tradeoffs. A general rule is that the time for a context switch should be\nroughly 1% of the time slice.\n**Advantage:** Fairness (each job gets an equal amount of the CPU)\n**Disadvantage:** Average waiting time can be bad (especially when the number of processes is large)\n**5.4**\n**Shortest Job First (SJF)**\nShortest Job First (SJF) is a scheduling policy that selects the waiting process with the smallest (expected)\namount of work (CPU time) to execute next. Once a job is selected it is run non-preemptively. Shortest\njob first is advantageous because it provably minimizes the average wait time. However, it is generally not\npossible to know exactly how much CPU time a job has remaining so it must be estimated. SJF scheduling\nis rarely used except in specialized environments where accurate estimations of the runtime of all processes\nare possible. This scheduler also has the potential for process starvation for processes which will require\na long time to complete if short processes are continually added. A variant of this scheduler is Shortest\nRemaining Time First (SRTF) which is for jobs which may perform I/O. The SRTF scheduler brings task\nback to the run queue based on the estimated time remaining to run after an I/O activity. SRTF also picks\nthe shortest job (like SJF), but it runs it for a quantum (time slice), then does a context switch to the next\nshortest remaining time job. This means that SRTF is unlike SJF in that it is a preemptive scheduler.\n**Advantage:** Minimizes average waiting time. Works for preemptive and non-preemptive schedulers. Gives\npriority to I/O bound jobs over CPU bound jobs.\n**Disadvantage:** Cannot know how much time a job has remaining. Long running jobs can be starved for\nCPU.\n**5.5**\n**Multilevel Feedback Queues (MLFQ)**\nThe Multilevel feedback queue scheduling algorithm is what is used in Linux and Unix systems. The MLFQ\nalgorithm tries to pick jobs to run based on their observed behavior. It does this by analysing whether a\njob runs for its full time slice or if it relinquishes the CPU early to do I/O. This allows the scheduler to\ndetermine if a job is I/O bound or CPU bound so that they can be treated differently. This allows the\nscheduler to approximate the behavior of a SJF scheduler without requiring perfect information about job\ncompletion times. While over long periods of time a job may change from I/O to CPU bound and back,\nover a short period it will tend to behave in one manner or the other.\nThe MLFQ scheduler uses several different ready queues and associates a different priority with each queue.\nEach queue also has a time slice associated with it; the highest priority queue has the smallest time slice.\nWhen a new job arrives, it is assigned to the highest priority queue by default. At each scheduling decision,\nthe Algorithm attempts to choose a process from the highest priority queue that is not empty. Within the\nhighest priority non-empty queue, jobs are scheduled with a round robin scheduler.\nWhen a job relinquishes the CPU it is because either it has used its full time quantum, or because it has\n5-4\n*Lecture 5: September 18*\npaused to perform an I/O activity. The scheduler uses this information to change the priority of the job;\nCPU bound jobs using their full time quantum are moved down one priority level, while I/O bound jobs that\ngive up their time slice are moved to a one level higher priority queue. This approach gives I/O bound tasks\nbetter response times since they are given higher priority when they need the CPU. The MLFQ scheduler has\na starvation problem just like SJF if there is a continual stream of small jobs. There are potential schemes to\nprevent starvation, for example: low priority tasks can periodically be bumped up to higher priority queues\nif they have not been run in a long time.\n**Advantage:** Adaptive behavior can distinguish between CPU and I/O bound tasks and schedule them\naccordingly.\n**Disadvantage:** More complicated. Uses past behavior to predict future.\n**5.6**\n**Lottery Scheduling**\nLottery Scheduling is a probabilistic scheduling algorithm for processes in an operating system. Processes\nare each assigned some number of lottery tickets, and the scheduler draws a random ticket to select the\nnext process. The distribution of tickets need not be uniform; granting a process more tickets provides it a\nrelative higher chance of selection. This technique can be used to approximate other scheduling algorithms,\nsuch as SJF and Fair-share scheduling. Allocating more tickets to a process gives it a higher priority since\nit has a greater chance of being scheduled at each scheduling decision.\nLottery scheduling solves the problem of starvation. Giving each process at least one lottery ticket guarantees\nthat it has a non-zero probability of being selected at each scheduling operation. On average, CPU time is\nproportional to the number of tickets given to each job. For approximating SJF, most tickets are assigned to\nshort running jobs and fewer to longer running jobs. To avoid starvation, every job gets at least one ticket.\nImplementations of lottery scheduling should take into consideration that there could be a large number of\ntickets distributed among a large pool of threads",
			  "evel Feedback Queues (MLFQ)**\nThe Multilevel feedback queue scheduling algorithm is what is used in Linux and Unix systems. The MLFQ\nalgorithm tries to pick jobs to run based on their observed behavior. It does this by analysing whether a\njob runs for its full time slice or if it relinquishes the CPU early to do I/O. This allows the scheduler to\ndetermine if a job is I/O bound or CPU bound so that they can be treated differently. This allows the\nscheduler to approximate the behavior of a SJF scheduler without requiring perfect information about job\ncompletion times. While over long periods of time a job may change from I/O to CPU bound and back,\nover a short period it will tend to behave in one manner or the other.\nThe MLFQ scheduler uses several different ready queues and associates a different priority with each queue.\nEach queue also has a time slice associated with it; the highest priority queue has the smallest time slice.\nWhen a new job arrives, it is assigned to the highest priority queue by default. At each scheduling decision,\nthe Algorithm attempts to choose a process from the highest priority queue that is not empty. Within the\nhighest priority non-empty queue, jobs are scheduled with a round robin scheduler.\nWhen a job relinquishes the CPU it is because either it has used its full time quantum, or because it has\n5-4\n*Lecture 5: September 18*\npaused to perform an I/O activity. The scheduler uses this information to change the priority of the job;\nCPU bound jobs using their full time quantum are moved down one priority level, while I/O bound jobs that\ngive up their time slice are moved to a one level higher priority queue. This approach gives I/O bound tasks\nbetter response times since they are given higher priority when they need the CPU. The MLFQ scheduler has\na starvation problem just like SJF if there is a continual stream of small jobs. There are potential schemes to\nprevent starvation, for example: low priority tasks can periodically be bumped up to higher priority queues\nif they have not been run in a long time.\n**Advantage:** Adaptive behavior can distinguish between CPU and I/O bound tasks and schedule them\naccordingly.\n**Disadvantage:** More complicated. Uses past b"
			]
		  },
		  {
			"title": "05. Scheduling Algorithms",
			"url": "https://www.cl.cam.ac.uk/teaching/2223/OpSystems/materials/05-SchedulingAlgorithms.pdf",
			"excerpts": [
			  "7\nPriority scheduling\n Associate integer priority with process, and schedule the highest\npriority ( ~ lowest number) process, e.g.,",
			  "\n Average waiting time now\n1 + 5 + 0 + 1 + 5 + 10 + 1 + 5 + 10 + 2 + 1\n5\n= 41\n5 ",
			  "Dynamic priority scheduling",
			  "\nRound Robin\n A pre-emptive scheduling scheme for time-sharing systems\n",
			  "Multilevel Queues\n Partition Ready queue into many queues\nfor different types of process, e.g.,\n Foreground/interactive processes\n Background/batch processes\n Eac",
			  "ms\n21\nRound Robin\n A pre-emptive scheduling scheme for time-sharing systems\n Give each process a **quantum** (or time-slice) of CPU time e.g., 10100 milliseconds\n Once quantum elapsed, process is pre-empted and appended to the ready queue\n",
			  "4\nFirst-Come First-Served (FCFS)\n Schedule depends purely on the order in which processes arrive\n Simplest possible scheduling algorithm"
			]
		  },
		  {
			"title": "Fair and Efficient Scheduling in Data Ferrying Networks",
			"url": "http://conferences.sigcomm.org/co-next/2007/papers/papers/paper13.pdf",
			"excerpts": [
			  "To be able to\nprovide bandwidth guarantees at all times, the sum of\nrates allocated to all kiosks is not allowed to exceed\nthe capacity of the system.",
			  "Bundles leaving the regulator are deemed *eligible*\nfor subsequent scheduling and are buffered in per-kiosk\nqueues.",
			  "If the token bucket is empty, bundles are tem-\nporarily buffered in a pre-bucket queue with a fixed ca-\n3 T",
			  "If the pre-bucket queue is full, newly arriving\nbundles are dropped.",
			  "The use of TB regulators *decouples* fairness and delay\nminimization. The scheduler only considers the set of\neligible bundles and focuses solely on delay minimiza-\ntion, without any regard to fairnes",
			  "The use of TB regulators *decouples* fairness and delay\nminimization. The scheduler only considers the set of\neligible bundles and focuses solely on delay minimiza-\ntion, without any regard to fairnes",
			  "Moreover, every time new bundles leave\nthe regulator, a utility-maximizing scheduler is invoked\nto compute a schedule for all unserved bundles.",
			  "n\nother words, we associate with each bundle some util-\nity, which captures the value gained from delivering\nit as a function of its delay. The scheduler computes\na schedule that maximizes the total utility",
			  " show\nthat if we define the utility function in a certain way,\nthe optimal scheduling problem can be formulated as\na minimum cost network flow problem, for which effi-\ncient algorithms exist.",
			  "r sharing of bandwidth in packet-switching net-\nworks has been well studied in the context of tradi-\ntional networks. The well known max-min fairness cri-\nterion, is closely approximated by a number of packe-\ntized scheduling algorithms ",
			  "r work is different in two main\naspects.\nFirst, our notion of fairness is defined on a\nlonger time scale. While classical scheduling disciplines\ntry to achieve max-min allocation of bandwidth at time\nintervals as short as possible, we focus on long-term fair-\nness which is exactly what token bucket regulator can\nprovide.",
			  "We are willing to allocate a disproportionately\nlarge portion of bandwidth to some users at one time\nand compensate for other users at another, provided\nthe bandwidth is allocated fairly in the long run.",
			  "Second, besides\nensuring fair allocation of bandwidth, our scheme also\ntries to minimize end-to-end delay."
			]
		  },
		  {
			"title": "Two Optimization Methods for Raytracing",
			"url": "https://vvise.iat.sfu.ca/user/data/papers/rtopt.pdf",
			"excerpts": [
			  "Previously bounding volume hierarchies have been used to speed raytracing. A new method to speed the traversal of a bounding volume hierarchy is presented.Read more"
			]
		  },
		  {
			"title": "Exploiting Caches to Maximize Locality in Graph Processing",
			"url": "https://people.csail.mit.edu/sanchez/papers/2017.cgs.agp.pdf",
			"excerpts": [
			  "The key idea of this paper is to exploit the cache hierarchy to find a graph traversal schedule that results in high locality. The cache has plentiful ..."
			]
		  },
		  {
			"title": "Batch Query Processing and Optimization for Agentic Workflows",
			"url": "https://arxiv.org/html/2509.02121v2",
			"excerpts": [
			  "Tool coalescing:* Many workflows issue identical or overlapping tool calls (e.g., SQL retrieval queries) across agents and sessions. Coalescing these requests to a single physical execution amortizes overhead and enables shared reuse of results (Facebook, [2019](https://arxiv.org/html/2509.02121v2.bib43 \"DataLoader: data loading utility for javascript\") ; Giannikis et al. , [2012](https://arxiv.org/html/2509.02121v2.bib54 \"SharedDB: killing one thousand queries with one stone\") )",
			  "rk reduction via request coalescing . Beyond executing tasks, the Processor reduces redundant work across concurrent workflows. For tool operators, it canonicalizes an operator *signature* (operator type plus normalized arguments) and\nmerges pending tasks with identical signatures into a single physical execution. The resulting output is then fanned out to all dependent logical nodes. This coalescing is particularly effective for high-fanout agents that issue repeated SQL templates or identical retrieval calls.",
			  "Coordinator prioritizes CPU tasks that unlock the immediate GPU frontier. Concretely, ready tool nodes are ordered by increasing DAG depth to the next unsatisfied LLM node (shallower, more critical prerequisites first).",
			  "The Processor enforces bounded concurrency per tool backend (e.g., per-DB connection pool or HTTP client) and applies backpressure when queues exceed configured thresholds, stabilizing latency under bursty workloads.",
			  " batching, cache sharing/prefetching, and overlap of compute and communication, within a unified DAG scheduler that respects inter-operator dependencies and avoids pathological stalls (e.g., GPU underutilization while CPU tools are backlogged, o",
			  "Disabling this module forces the backend to process the raw, unreduced stream of requests, exposing the system to the full complexity of the workload and causing severe I/O congestion.",
			  "Halo models each request as a DAG spanning GPU LLM operators and CPU tool operators, and applies cost-based scheduling and placement to overlap execution, preserve locality, and exploit cross-request batching and sharing."
			]
		  },
		  {
			"title": "Making efficient Gremlin upserts with fold()/coalesce()/unfold() - Amazon Neptune",
			"url": "https://docs.aws.amazon.com/neptune/latest/userguide/gremlin-efficient-upserts-pre-3.6.html",
			"excerpts": [
			  "Section Title: Making efficient Gremlin upserts with `fold()/coalesce()/unfold()`",
			  "For example, the following query upserts a vertex by first looking for the specified\nvertex in the dataset, and then folding the results into a list.",
			  "In the first traversal\nsupplied to the `coalesce()` step, the query then unfolds this list.",
			  "If the\nunfolded list is not empty, the results are emitted from the `coalesce()` .",
			  "If, however, the `unfold()` returns an empty collection because the vertex\ndoes not currently exist, `coalesce()` moves on to evaluate the second\ntraversal with which it has been supplied, and in this second traversal the query creates\nthe missing vertex."
			]
		  },
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads."
			]
		  },
		  {
			"title": "Dual Streaming for Hardware-Accelerated Ray Tracing",
			"url": "https://hwrt.cs.utah.edu/papers/hwrt_hpg17.pdf",
			"excerpts": [
			  "The streaming processor uses many Thread Multiprocessors\n(TMs) for computation, which share chip-wide stream units.",
			  "The streaming processor uses many Thread Multiprocessors\n(TMs) for computation, which share chip-wide stream units.",
			  "e stream scheduler marshals the data required\nfor ray traversal to prevent TPs from accessing main memory di-\nrectly for both scene and ray stream data.",
			  "he stream scheduler\nalso tracks the current state of traversal, including the *working set*\nof active scene segments, the mapping of TMs to scene segments,\nand the status of the scene and ray streams",
			  "al streaming provides a new ray traversal order that resolves\nsome of the decades-old problems of high-performance ray tracing:\n *Random access to main memory during traversal is avoided",
			  "This is particularly important for large scenes and incoherent\nrays (such as secondary rays).",
			  "Memory latency is hidden by perfect prefetching.* A traditional\nsolution hides memory latency by adding more threads, which\n .",
			  "e *ray stream* consists of all rays in flight\ncollected as a queue per scene segment they intersect.",
			  "Rays at the same depth\nare traced as a wavefront, so each additional bounce requires an\nadditional pass.",
			  "he complete streaming processor is built from many TMs which\nshare access to several global units: the *stream scheduler* , the *scene*\n*buffer* , and the *hit record ",
			  "he complete streaming processor is built from many TMs which\nshare access to several global units: the *stream scheduler* , the *scene*\n*buffer* , and the *hit record "
			]
		  },
		  {
			"title": "Divergence-Aware Warp Scheduling",
			"url": "https://engineering.purdue.edu/tgrogers/publication/rogers-micro-2013/rogers-micro-2013.pdf",
			"excerpts": [
			  "Divergence-Aware Warp Scheduling (DAWS), which\nintroduces a divergence-based cache footprint predictor to estimate\nhow much L1 data cache capacity is needed to capture intra-warp\nlocality in loops.",
			  "WS uses these predictions to schedule warps\nsuch that data reused by active scalar threads is unlikely to ex-\nceed the capacity of the L1 data cache.",
			  "memory divergence detec-\ntor is used to classify static load instructions as convergent or diver-\ngent.",
			  "oalescing reduces the num-\nber of memory requests by merging accesses from multiple lanes\ninto cache line sized chunks when there is spatial locality across\nthe warp [1]",
			  "Memory divergence oc-\ncurs when coalescing fails to reduce the number of memory re-\nquests generated by an instruction to two or l",
			  "Divergence-Aware Warp Scheduling\nis a novel technique that proactively uses predictions to prevent\ncache thrashing before it occurs and aggressively increases cache\nsharing between warps as their thread activity decreases.",
			  "ergence-Aware\nWarp Scheduling uses this predicted code behaviour in combina-\ntion with live thread activity information to make more locality-\naware scheduling decisions.",
			  "ergence-Aware\nWarp Scheduling uses this predicted code behaviour in combina-\ntion with live thread activity information to make more locality-\naware scheduling decisions."
			]
		  },
		  {
			"title": "Enabling advanced GPU features in PyTorch  Warp Specialization  PyTorch",
			"url": "https://pytorch.org/blog/warp-specialization/",
			"excerpts": [
			  "Warp specialization (WS) is a GPU programming technique where warps (a group of 32 threads on NVIDIA GPUs) within a threadblock are assigned distinct roles or tasks. This approach optimizes performance by enabling efficient execution of workloads that require task differentiation or cooperative processing. It enhances kernel performance by leveraging an asynchronous execution model, where different parts of the kernel are managed by separate hardware units. Data communication between these units, facilitated via shared memory on the NVIDIA H100, is highly efficient. Compared to a uniform warp approach, warp specialization allows the hardware multitasking warp scheduler to operate more effectively, maximizing resource utilization and overall performance.",
			  "Using GEMM as an example, a typical uniform warp approach on the H100 GPU involves 8 warps per thread block collectively computing a tile of the output tensor. These 8 warps are divided into two warp groups (WG), with each group cooperatively computing half of the tile using efficient warp-group-level MMA (WGMMA) instructions, as illustrated in Figure 1.",
			  "ask Partitioning** : The entire kernel is automatically divided into asynchronous tasks based on predefined heuristics. The compiler determines how to utilize one producer warp group and a user-specified number of consumer warp groups to execute the kernel. It assigns task IDs to specific anchor operations, which then influence the task assignments for remaining operations through asynchronous task ID propagation and dependency analysi",
			  "ta Partitioning for Multiple Consumer Groups** : Efficiently partitioning data among multiple consumer groups is key to optimizing workload distribution. On the H100 GPU, the compiler, by default, attempts to partition the input tensor `A` along the `M` dimension, allowing each consumer group to compute half of the output tensor independently. This strategy, known as [cooperative partitioning](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md) , maximizes efficiency under most conditions. However, if this split leads to inefficienciessuch as producing a workload smaller than the native WGMMA instruction sizethe compiler dynamically adjusts and partitions along the `N` dimension instead"
			]
		  },
		  {
			"title": "Thread Cluster Memory Scheduling: Exploiting Differences ...",
			"url": "https://www.cs.cmu.edu/~harchol/Papers/micro2010.pdf",
			"excerpts": [
			  "This paper presents a new memory scheduling algorithm that addresses system throughput and fairness separately with the goal of achieving the best of both. The ...Read more"
			]
		  },
		  {
			"title": "Coordinated Control of Multiple Prefetchers in Multi-Core ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/coordinated-prefetching_micro09.pdf",
			"excerpts": [
			  "Our goal in this paper is to develop a hardware framework that enables large performance improvements from prefetching in CMPs by significantly reducing ...Read more"
			]
		  },
		  {
			"title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
			"url": "https://arxiv.org/html/2504.19365v3",
			"excerpts": [
			  "To avoid redundant requests, AGILE coalesces identical data requests issued by different threads, which is essential because user threads may independently request the same data chunk from SSDs.",
			  "To avoid redundant requests, AGILE coalesces identical data requests issued by different threads, which is essential because user threads may independently request the same data chunk from SSDs.",
			  "Then, AGILE selects one thread to forward the request to the second-level coalescing stage.",
			  "Then, AGILE selects one thread to forward the request to the second-level coalescing stage.",
			  "he second level is handled by the AGILE software cache (Section [3.4](https://arxiv.org/html/2504.19365v3.SS4 \"3.4. AGILE Software Cache  3. AGILE Design & Implementation  AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration\") ), which filters remaining redundant requests that are not eliminated in the first warp-level coalescing stag",
			  "he second level is handled by the AGILE software cache (Section [3.4](https://arxiv.org/html/2504.19365v3.SS4 \"3.4. AGILE Software Cache  3. AGILE Design & Implementation  AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration\") ), which filters remaining redundant requests that are not eliminated in the first warp-level coalescing stag",
			  "AGILE prioritizes the warp-level coalescing since accessing the shared software cache requires atomic operations to maintain consistency, which creates critical sections and serializes execution. This serialization can cause stalls and different execution paths for threads in a warp, which introduces warp divergence and degrades overall GPU performance.",
			  "AGILE prioritizes the warp-level coalescing since accessing the shared software cache requires atomic operations to maintain consistency, which creates critical sections and serializes execution. This serialization can cause stalls and different execution paths for threads in a warp, which introduces warp divergence and degrades overall GPU performance.",
			  "For async_issue(src,dst) , which mimics cp.async (Nvidia, [2025d](https://arxiv.org/html/2504.19365v3.bib37) ) or cuda::memcpy_async (Nvidia, [2025g](https://arxiv.org/html/2504.19365v3.bib40) ) in CUDA and no warp-level coalescing is performed.",
			  "Even if threads in a warp request the same data, each thread will still obtain its own copy of the requested data.",
			  "Even if threads in a warp request the same data, each thread will still obtain its own copy of the requested data.",
			  "t:\nFlash memory cannot be accessed randomly, and data is managed at a coarse-grained page level, typically 4KB per page (Gal and Toledo, [2005](",
			  "Therefore, the software cache line should align with the SSDs granularity. This alignment can avoid redundant I/Os when multiple threads access different parts of the same SSD page concurrently.",
			  "To ensure correctness during accessing the same cache line simultaneously, atomic operations are required to avoid conflicts and data hazards.",
			  "AGILE promises to offer flexibility in the software cache policy, and therefore, eliminating the potential for deadlock caused by the software cache is necessary.",
			  "A common scenario resulting in a deadlock is simultaneous threads accessing multiple cache lines.",
			  "To prevent redundant SSD accesses, once a thread checks the software cache and the requested data is foundi.e., a cache hit occursaccess to the corresponding cache lines must be atomic to avoid eviction before accesses in process are completed.",
			  "When multiple threads\nblock cache line eviction while requesting new cache lines, a deadlock could occur."
			]
		  },
		  {
			"title": "Optimizing Storage Performance with Calibrated Interrupts",
			"url": "https://www.usenix.org/system/files/osdi21-tai.pdf",
			"excerpts": [
			  " Typically, interrupt coalescing addresses interrupt storms\nby batching requests into a single interrupt. Batching, how-\n** Denotes co-first authors with equal contribution.\never, creates a trade-off between request latency and the inter-\nrupt rate. For the workloads we inspected, CPU utilization in-\ncreases by as much as 55% without coalescing (Figure 12 (d)),\nwhile under even the minimum amount of coalescing, request\nlatency increases by as much as 10 ** for small requests, due\nto large timeouts. Interrupt coalescing is disabled by default\nin Linux, and real deployments use alternatives ( 2 ).\nThis paper addresses the challenge of dealing with expo-\nnentially increasing interrupt rates without sacrificing latency.\nWe initially implemented adaptive coalescing for NVMe, a dy-\nnamic, device-side-only approach that tries to adjust batching\nbased on the workload, but find that it still adds unneces-\nsary latency to requests ( 3.2 ). This led to our core insight\nthat device-side heuristics, such as our adaptive coalescing\nscheme, cannot achieve optimal latency because the device\nlacks the semantic context to infer the requesters intent: is the\nrequest latency-sensitive or part of a series of asynchronous\nrequests that the requester completes in parallel? Sending this\nvital information to the device bridges the semantic gap and\nenables the device to interrupt the requester when appropriate.\nWe call this technique *calibrating* 1 interrupts (or simply,\n ... \nsemantic information is easily accessible in the storage stack\nand available at submission time.",
			  "We initially implemented adaptive coalescing for NVMe, a dy-\nnamic, device-side-only approach that tries to adjust batching\nbased on the workload, but find that it still adds unneces-\nsary latency to requests ( 3.2 ). This led to our core insight\nthat device-side heuristics, such as our adaptive coalescing\nscheme, cannot achieve optimal latency because the device\nlacks the semantic context to infer the requesters intent: is the\nrequest latency-sensitive or part of a series of asynchronous\nrequests that the requester completes in parallel? Sending this\nvital information to the device bridges the semantic gap and\nenables the device to interrupt the requester when appropriate.\n",
			  "ll this technique *calibrating* 1 interrupts (or simply,\n ... \nsemantic information is easily accessible in the storage stack\nand available at submission t",
			  ".\n**3.2**\n**Adaptive Coalescing**\nIdeally, an interrupt coalescing scheme adapts dynamically to\nthe workload. Figure 4 shows that even if the timeout granu-\nlarity in the NVMe specification were smaller, it is still *fixed* ,\nwhich means that interrupts will be generated when the work-\nload does not need interrupts ( *c* 1 ** *c* 8 ), while completions\nmust wait for the timeout to expire ( *c* 9 ) when the workload\ndoes need interrupts.\nInstead, as shown in the bottom row of Figure 4 , the adap-\ntive coalescing strategy in cinterrupts observes that a device\nshould generate a single interrupt for a burst, or a sequence\n ... "
			]
		  },
		  {
			"title": "Optimizing Storage Performance with Calibrated Interrupts",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3505139",
			"excerpts": [
			  ":\n*Comparison to NVMe Coalescing.* The adaptive strategy outperforms various NVMe coalescing configurations, even those with smaller timeouts, across different workloads. We compare the adaptive strategy, configured with $thr=32$ , $\\Delta =6$ , to no coalescing (default), nvme100, which uses a timeout of 100 $\\mu$ s, the smallest possible in standard NVMe, nvme20, which uses a theoretical timeout of 20 $\\mu$ s, and nvme6, which uses a theoretical timeout of 6 $\\mu$ s. All NVMe coalescing configurations have threshold set to 32.\nWe",
			  "The adaptive strategy outperforms various NVMe coalescing configurations, even those with smaller timeouts, across different workloads.",
			  "*YCSB-E.* Scans are interesting because their latency is determined by the completion of requests that can span multiple submission boundaries. Table [5]() shows throughput results for YCSB-E with different scan lengths, and Figure [24]() shows latency CDFs for scans of length 16 and 256.",
			  "Similar to the other YCSB workloads, the adaptive strategy again can almost match the throughput of cinterrupts, because it is designed for batching. At higher scan lengths, factors such as application-level queueing begin affecting scan throughput, reducing the benefit of cinterrupts.",
			  "Excessive interrupt generation limits default throughput to 86%-89% of cinterrupts."
			]
		  },
		  {
			"title": "Multi-stage coordinated prefetching for present-day processors | Proceedings of the 28th ACM international conference on Supercomputing",
			"url": "https://dl.acm.org/doi/10.1145/2597652.2597660",
			"excerpts": [
			  "Data prefetching is an important technique for hiding memory latency. Latest microarchitectures provide support for both hardware and software prefetching.",
			  "Based on our study of the interaction between the host architecture and prefetching, we find that coordinated multi-stage prefetching that brings data closer to the core in stages, yields best performance.",
			  "On SandyBridge, the mid-level cache hardware prefetcher and L1 software prefetching coordinate to achieve this end, whereas on Xeon Phi, pure software prefetching proves adequate.",
			  "We implement our algorithm in the ROSE source-to-source compiler framework."
			]
		  },
		  {
			"title": "Maximizing Hardware Prefetch Effectiveness with Machine ...",
			"url": "https://userweb.cs.txstate.edu/~burtscher/papers/hpcc15.pdf",
			"excerpts": [
			  " can help the user gain up to 96% of the achievable speedup\nprovided by the hardware prefetchers.",
			  "First, it\nenables us to use existing hardware more effectively.",
			  "Second, it\ndoes not require any source-code changes of the program being\noptimized.",
			  "stly,\nthe framework relies on open-source technologies, making it\nis easy to extend and port to other architectures",
			  "Prefetching is a widely explored area, and the benefits of\nprefetching are broadly documented and studied."
			]
		  },
		  {
			"title": "BLISS: Balancing Performance, Fairness and Complexity ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bliss-memory-scheduler_ieee-tpds16.pdf",
			"excerpts": [
			  "utlu and Moscibroda propose PARBS [4], an applica-\ntion-aware memory scheduler that batches the oldest\nrequests from applications and prioritizes the batched\nrequests, with the goals of preventing starvation and\nimproving fairness",
			  "ithin each batch, PARBS ranks indi-\nvidual applications based on the number of outstanding\nrequests of each application and, using this total rank order,\nprioritizes requests of applications that have low-memory-\nintensity to improve system throughpu",
			  "im et al. [6] observe that applications that receive low\nmemory service tend to experience interference from applica-\ntions that receive high memory service. Based on this observa-\ntion, they propose ATLAS, an application-aware memory\nscheduling policy that ranks individual applications based on\nthe amount of long-term memory service each receives and\nprioritizes applications that receive low memory service, with\nthe goal of improving overall system throughp",
			  "Thread cluster memory scheduling (TCM) [7] ranks\nindividual applications by memory intensity such that\nlow-memory-intensity applications are prioritized over\nhigh-memory-intensity applications (to improve system\nthroughput). Kim et al. [7] also observed that ranking all\napplications based on memory intensity and prioritizing\n ...",
			  "RFCFS has the lowest average request latency among\nall the schedulers. This is expected since FRFCFS maximizes\nDRAM throughput by prioritizing row-buffer hits. Hence,\nthe number of requests served is maximized overall (across\nall applications). However, maximizing throughput (i.e.,\nminimizing overall average request latency) degrades the\nperformance of low-memory-intensity applications, since\nthese applications requests are often delayed behind row-\nbuffer hits and older requests. This results in degradation in\nsystem performance and fairness, as shown in Fig. 4.",
			  "nd, ATLAS and TCM, memory schedulers that priori-\ntize\nrequests\nof\nlow-memory-intensity\napplications\nby\nemploying a full ordered ranking achieve relatively low aver-\nage latency. This is because these schedulers reduce the\nlatency of serving requests from latency-critical, low-mem-\nory-intensity applications significantly. Furthermore, priori-\ntizing low-memory-intensity applications requests does not\nincrease the latency of high-memory-intensity applications\nsignifican",
			  "ons, we conclude that BLISS achieves the best\nperformance and a good trade-off between fairness and per-\nformance for most of the workloads we examine."
			]
		  },
		  {
			"title": "DESIGNING EFFICIENT MEMORY SCHEDULERS FOR ...",
			"url": "https://users.cs.utah.edu/~rajeev/pubs/nil-thesis.pdf",
			"excerpts": [
			  "2.3.1\nFirst Read First-Come-First-Served (FR-FCFS)\nProposed by Rixner et al. [12], this is the most popular memory scheduling\nalgorithm that has been explored in detail in academia and also implemented in\nalmost all commercial memory schedulers today. The basic idea of this scheduler is\nto allow requests that require less time to be serviced, by virtue of being a row-hit, to\npreempt older row-miss requests. Evidently, this has higher performance and better\nenergy characteristics than a first-come first-served (FCFS) policy.",
			  "2.3.3\nParallelism-Aware Batch Scheduling (PARBS)\nProposed by Moscibroda et al. [34], this scheme tries to maintain the fairness and\nquality-of-service notions introduced in STFM and, in addition, aims at improving\nthe system throughput. The scheduler first forms batches of requests by grouping\nconsecutive outstanding requests in the memory request buffers and services all re-\nquests in a batch before moving over to the next batch. By grouping requests into\n17\nbatches, the scheme avoids starvation of threads at a very fine granularity and ensures\nsteady and fair progress across all threads. Within a batch, row-hits are prioritized\nover row-misses and threads with few requests or those that display high-bank-level\nparallelism are prioritized over others to minimize the service time of a bat",
			  "2.3.4\nAdaptive per-Thread Least-Attained-Service\nMemory Scheduling (ATLAS)\nProposed by Kim et al. [13], ATLAS is a scheme that allows multiple memory-\ncontrollers to coordinate their scheduling decisions to improve throughput. Execution\ntime is split into long epochs. During each epoch, the memory-controllers keep track of\nthe level of service received by each thread from the memory system. At the beginning\nof the next epoch, this information is accumulated at a central coordinator, which\nincreases the priorities of the threads that received the least service in the previous\nepoch. This information is propagated to the memory-controllers and thereafter, the\nselected threads are prioritized.",
			  "2.3.5\nThread-Cluster Memory Scheduling (TCM)\nProposed by Kim et al. [14], TCM argues that techniques such as STFM, PAR-BS,\nand ATLAS are unable to provide adequate fairness and high throughput because they\nuse the same policy for all threads. In contrast, TCM uses the memory behavior of the\nthread to decide its priority. First, it prioritizes requests from non-memory-intensive\nthreads over memory-intensive ones during memory scheduling. After making the\nobservation that unfairness in memory scheduling techniques stems from interference\namong memory-intensive threads, TCM periodically shuffles the priority order among\nsuch threads to increase fairness. However, not all threads get to enjoy all priority lev-\nels; instead, threads with higher bank-level parallelism are prioritized over streaming\nthreads that have high row-buffer locality.",
			  "4.6.3\nPAR-BS:\nThe PAR-BS scheme [34] forms batches of requests in a CPUs memory controller\nand issues requests from a batch to the memory system. The express motivation\nbehind the batch-formation is fairness and as a result, a batch in PAR-BS will include\nrequests from many threads and have different batches for different banks.\nOur\nbatching scheme does exactly the opposite and groups requests from a warp together\nto reduce the latency divergence of a warp. In addition, we arbitrate between batches\nbased on a bank-aware shortest job first policy to reduce wait time for warps, which\nis different from PAR-BS, which uses a MLP-based SJF policy for thread priorities."
			]
		  },
		  {
			"title": "The Blacklisting Memory Scheduler",
			"url": "https://users.ece.cmu.edu/~lsubrama/pubs/tr-2015-004.pdf",
			"excerpts": [
			  "rank order incurs high hardware complexity, as we demon-\nstrate in Section 7.2, slowing down the memory scheduler\nsignificantly (by 8x for TCM compared to FRFCFS), while\nalso increasing its area (by 1.8x).",
			  "Second, a total-order ranking is unfair to\napplications at the bottom of the ranking stack.",
			  "e implement them in Register Transfer Level\n(RTL), using Verilog. We synthesize the RTL implementations\nwith a commercial 32 nm standard cell library, using the\nDesign Compiler tool from Synopsys.\n",
			  "BLISS\nachieves 5% better weighted speedup, 25% lower maximum\nslowdown and 19% better harmonic speedup than the best",
			  "First, FRFCFS has the lowest average request latency\namong all the schedulers. This is expected since FRFCFS\nmaximizes DRAM throughput by prioritizing row-buffer hits.",
			  "Second, ATLAS and TCM, memory schedulers that prior-\nitize requests of low-memory-intensity applications by em-\nploying a full ordered ranking achieve relatively low av-\nerage latency.",
			  "we conclude that BLISS achieves the best trade-off between\nperformance, fairness and simplicity."
			]
		  },
		  {
			"title": "ATLAS: A scalable and high-performance scheduling ...",
			"url": "https://ieeexplore.ieee.org/document/5416658/",
			"excerpts": [
			  "ATLAS (Adaptive per-Thread Least-Attained-Service memory scheduling), a fundamentally new memory scheduling technique that improves system throughput without requiring significant coordination among memory controllers. The key idea is to periodically order threads based on the service they have attained from the memory controllers so far, and prioritize those threads that have attained the least service over others in each period."
			]
		  },
		  {
			"title": "GraphIt: a high-performance graph DSL",
			"url": "https://commit.csail.mit.edu/papers/2018/graphit.pdf",
			"excerpts": [
			  "This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics ..."
			]
		  },
		  {
			"title": "Inter-core Prefetching for Multicore Processors Using ...",
			"url": "https://cseweb.ucsd.edu/~swanson/papers/ASPLOS2011Prefetching.pdf",
			"excerpts": [
			  "This paper describes and evaluates helper threads that run on separate cores of a multicore and/or multi-socket computer system. Multiple threads running on ...Read more"
			]
		  },
		  {
			"title": "Exploring Memory Access Patterns for Graph Processing ...",
			"url": "https://arxiv.org/abs/2010.13619",
			"excerpts": [
			  "In this work, we propose a simulation environment for the analysis of graph processing accelerators based on simulating their memory access patterns."
			]
		  },
		  {
			"title": "Irregular accesses reorder unit: improving GPGPU memory ...",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/f1ddd05c-f631-449d-bf34-8de6a3105a44/content",
			"excerpts": [
			  "The IRU reorders data processed by the threads on irregular accesses to\nimprove memory coalescing, i.e., it tries to assign data elements to threads as to\nproduce coalesced accesses in SIMT groups.",
			  "The IRU is a compact and efficient hardware\nunit integrated into the Memory Partition (MP) of the GPU architecture as shown\nin Fig. 4 a, which incurs in very small energy and area overheads.",
			  "The GPU architecture with our IRU improves memory coalescing by a factor of\n1.32x and reduces NoC traffic by 46%, which result in 1.33x speedup and 13%\nenergy savings on average for a diverse set of graph-based applications.",
			  "raffic between SM and MP is reduced to as low as 23% for the *human* benchmark\non PR, overall reducing NoC traffic to 54% of the original interconnection traffic",
			  "IRU filtering further\nreduces accesses by removing/merging duplicated elements, that avoids additional\nmemory accesses.",
			  "IRU coalescing and\nfiltering improvement for these operations reduces L2 accesses but not L1 accesses,\nexplaining the larger reduction in L2 accesses compared to L1 for SSSP and PR.",
			  "ig. 14** Improvement in memory coalescing achieved with the IRU over the Baseline GPU system (GTX\n980 GPU with parameters shown in Table 2 )",
			  "\n**Fig. 13** Normalized interconnection traffic between SM (Streaming Multiprocessors) and MP (Memory\nPartitions) for the IRU enabled GPU system over the Baseline GPU system (GTX 98"
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "the\ndependence relationships between loads and stores, and prefetch\nthe subsequent nodes on their basis [ 4 ].",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "line design consists of a simple five stage\npipelined processor designed in ECE4750 interfaced with a blocking\ncache, which in turn is connected to memor",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system.",
			  "oosely-coupled processors have an advantage*\n*in that fine-grain resources, such as processor and L1 cache re",
			  "sources, are not contended by the application and helper threads,",
			  "*present techniques to alleviate this. O",
			  "r approach exploits large*\n*loop-based code regions and is based on a new synchronization",
			  "mechanism between the application and helper threads.",
			  "This*\n*mechanism precisely controls how far ahead the execution of t"
			]
		  },
		  {
			"title": "Security and Performance Aspects of HugePages Configuration",
			"url": "https://linuxgd.medium.com/security-and-performance-aspects-of-hugepages-configuration-72f138dc7d09",
			"excerpts": [
			  "Since HugePages are larger than standard pages, they might contain more sensitive data, increasing the risk of unauthorized access if security ...Read more"
			]
		  },
		  {
			"title": "caching - What is TLB shootdown? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/3748384/what-is-tlb-shootdown",
			"excerpts": [
			  "The actions of one processor causing the TLBs to be flushed on other processors is what is called a TLB shootdown. Copy link CC BY-SA 3.0. ...Read more"
			]
		  },
		  {
			"title": "\n\tThe term \"TLB shootdown\" was - Intel Community\n",
			"url": "https://community.intel.com/t5/Software-Tuning-Performance/Do-the-terms-tlb-shootdown-and-tlb-flush-refer-to-the-same-thing/m-p/1155420",
			"excerpts": [
			  "The \"shootdown\" refers to the software coordination of TLB invalidations, so it can be counted by the OS -- eg, \"cat /proc/interrupts | grep TLB\" in Linux.Read more"
			]
		  },
		  {
			"title": "Mitigating the Performance Impact of TLB Shootdowns ...",
			"url": "https://www.researchgate.net/publication/220884710_DiDi_Mitigating_the_Performance_Impact_of_TLB_Shootdowns_Using_a_Shared_TLB_Directory",
			"excerpts": [
			  "In VMs, TLB shootdown cost is even greater due to the ICR MSR write VM exits associated to sending IPIs and TLB shootdown preemption. ... Despite ...Read more"
			]
		  },
		  {
			"title": "A.12.numactl | Performance Tuning Guide | Red Hat Enterprise Linux | 7 | Red Hat Documentation",
			"url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-tool_reference-numactl",
			"excerpts": [
			  "Numactl lets administrators run a process with a specified scheduling or memory placement policy. Numactl can also set a persistent policy for shared memory ...Read more"
			]
		  },
		  {
			"title": "Understanding Huge Pages | Netdata\n",
			"url": "https://www.netdata.cloud/blog/understanding-huge-pages/",
			"excerpts": [
			  "Section Title: Understanding Huge Pages > Huge Pages",
			  "Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size. They help reduce the pressure on the Translation Lookaside Buffer (TLB) and lower the overhead of managing memory in systems with large amounts of RAM.",
			  "Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size. They help reduce the pressure on the Translation Lookaside Buffer (TLB) and lower the overhead of managing memory in systems with large amounts of RAM."
			]
		  },
		  {
			"title": "Cache-Aware Virtual Page Management - UWSpace",
			"url": "https://uwspace.uwaterloo.ca/bitstreams/71c06c89-7b69-42af-8e59-7f7134aa151c/download",
			"excerpts": [
			  "Page colouring assigns every bin a colour (a unique ID) and attempts to allocate memory\nof as many different colours as possible. In this way, the memory allocator spreads the\nvirtual pages across the CPUs cache.",
			  "Linuxs Hugepages serve as an experimental verification of this hypothesis. By using\nextremely large page sizes, the amount of bins is reduced to a single one. The experiment\nshows decreased variability as well as improved CPI measurements due to a reduction",
			  "The buffer strategy utilizes Linuxs Hugepages, which results in a perfect page-colouring as\nwell as a reduction in TLB misses.",
			  "Page colouring\nallows whole pages of data to be carefully positioned within the cache, providing the\nmechanism with which pages are placed into various partitions.",
			  "The Power 7 architecture particularly improves in-\nstruction throughput performance due to page colouring. With a best case performance\nimprovement of 66 %, the Power 7 architecture demonstrates the significance of cache-\naware memory allocation.",
			  "The initial colouring is done at boot-time, allowing\nthe operating system to allocate its own data-structures in a cache-aware manner.",
			  "The experiment\nshows decreased variability as well as improved CPI measurements due to a reduction\n55\nin conflict page mappin",
			  "Linuxs Hugepages serve as an experimental verification of this hypothesis. By using\nextremely large page sizes, the amount of bins is reduced to a single one.",
			  "A shared CPU cache has been shown to suffer excessive conflict misses when\nsubjected to certain combinations of workloads [26]."
			]
		  },
		  {
			"title": "Huge Pages are a Good Idea (evanjones.ca)",
			"url": "https://www.evanjones.ca/hugepages-are-a-good-idea.html",
			"excerpts": [
			  "On my Intel 11th generation Core i5-1135G7 (Tiger Lake) from 2020, using 2 MiB huge pages is 2.9 faster. I also tried 1 GiB pages, which is 3.1 faster than 4 kiB pages, but only 8% faster than 2 MiB pages.",
			  "In 2021, Google published a [paper about making their malloc implementation (TCMalloc) huge page aware (called Temeraire)](https://www.usenix.org/conference/osdi21/presentation/hunter) . They report this improved average requests-per-second throughput across their fleet by 7%, by increasing the amount of memory that is backed by huge pages.",
			  "Notably, when the Linux kernel's transparent huge page implementation was first introduced, it was enabled by default, which caused many performance problems.",
			  "Today's default to use huge pages only for applications that opt-in (aka madvise) should improve this.",
			  "The Linux kernel's implementation of transparent huge pages has been the source of performance problems.",
			  "Unfortunately, using larger pages is not without its disadvantages. Notably, when the Linux kernel's transparent huge page implementation was first introduced, it was enabled by default, which caused many performance problems. See the section below for more details. Today's default to use huge pages only for applications that opt-in (aka madvise) should improve this. The kernel's policies for managing huge pages have also changed since then, and are hopefully better now. At the very least, the fact that Google uses transparent huge pages for all their applications is some evidence that this can work for a wide variety of workloads."
			]
		  },
		  {
			"title": "CPU performance  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/cpu-benchmarks/",
			"excerpts": [
			  "Section Title: Latency & pointer chasing [Link to heading]() > Pointer chasing [Link to heading]()",
			  "Content:\nWe start with a simple *pointer chasing* experiment: we create a large array in\nwhich each position contains the index of another position and then follow the chain.\nIn particular, we ensure that the array is a random *derangement* , or rather, a permutation\nthat is just one long cycle, so that memory\ncannot be prefetched by the hardware prefetcher."
			]
		  },
		  {
			"title": "Don't shoot down TLB shootdowns!",
			"url": "https://nadav.amit.zone/publications/pdfs/amit2020tlb.pdf",
			"excerpts": [
			  "TLB shootdowns are costly, as they invoke a com-\nplex protocol which burdens all processors in the system\nand must wait for the acknowledgement of remote cores,\nrequiring several thousand cycles to complet",
			  "We introduce four general techniques to improve\nshootdown performance: (1) concurrently flush initiator and\nremote TLBs, (2) early acknowledgement from remote cores,\n(3) cacheline consolidation of kernel data structures to reduce\ncacheline contention, and (4) in-context flushing of userspace\nentries to address the overheads introduced by Spectre and\nMeltdown mitigations.",
			  "e apply our optimizations to Linux 5.2.8 and show that\nwe obtain significant performance improvements in both mi-\ncrobenchmarks and real-world workloads such as Sysbench\nand Apache",
			  "TLB flushes and shootdowns are mechanisms used by operat-\ning systems to synchronize page table entries (PTEs) cached\nin TLBs with the underlying page table",
			  "order to perform the shootdown, the initiator sends an IPI\nwith *work* , a data structure which indicates which entries\nneed to be flushed, to remote cores, which perform flushes\nlocally and send an acknowledgement back to the initiator\nby clearing a bit that the initiator spin-waits on.",
			  "Translation Lookaside Buffers (TLBs) are critical for build-\ning performant virtual memory systems.",
			  "The x86 architecture provides several mechanisms to control\nthe contents of the TLB."
			]
		  },
		  {
			"title": "Optimizing the TLB Shootdown Algorithm with Page ...",
			"url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
			"excerpts": [
			  "OS Solutions and Shortcomings**\nTo reduce TLB related overheads, OSes employ\nseveraltechniques to avoidunnecessaryshootdowns,\nreduce their time, and avoid TLB misses.\n",
			  "A common method to reduce shootdown time is\nto batch TLB invalidations if they can be deferred [ 21 ,\n47 ]. Batching, however, cannot be used in many\ncases, for example when a multithreaded application\nchanges the access permissions of a single page.",
			  "Linux tries to balance between the overheads of\nTLB flushes and TLB misses when a core becomes\nidle, using a lazy TLB invalidation scheme.",
			  "Per-Core Page Tables**\nCurrently, the state-of-the-art software solution for\nTLB shootdowns is setting per-core page tables, and\naccording to the experienced page-faults track which\ncores usedeachPTE [ 11 , 19 ]. When a PTE invalidation\nis needed, a shootdown is sent only to cores whose\npage tables hold the invalidated PT"
			]
		  },
		  {
			"title": "AMD Optimizes EPYC Memory with NUMA",
			"url": "https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/AMD-Optimizes-EPYC-Memory-With-NUMA.pdf",
			"excerpts": [
			  "NUMA reduces\nmemory latencies and reduces cross-die data traffic,\nbecause Intels Manhattan Mesh (Figure 4) has no\ndiagonal connections.\n",
			  "With dual socket configurations, latency for memory\naccess between sockets will have a significant latency\npenalty when memory accesses cross a socket-to-socket interconnect, whether that interconnect\nis AMD Infinity Fabric or Intel QPI.",
			  "EPYC can support 2 TB."
			]
		  },
		  {
			"title": "AMD's EPYC 9355P: Inside a 32 Core Zen 5 Server Chip",
			"url": "https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core",
			"excerpts": [
			  "NPS1 mode stripes memory accesses across all 12 of the chips memory controllers, presenting software with a monolithic view of memory at the cost of latency.",
			  "Different NUMA configurations can subdivide EPYC 9355P, associating cores with the closest memory controllers to improve latency.",
			  "An individual NPS4 node achieves 117.33 GB/s to its local memory pool, and just over 107 GB/s to the memory on the other three nodes."
			]
		  },
		  {
			"title": "NUMA, numactl, and taskset: A Practical Guide for AI/ML Engineers | by Tony Seah | Nov, 2025 | Medium",
			"url": "https://tonyseah.medium.com/numa-numactl-and-taskset-a-practical-guide-for-ai-ml-engineers-0fbacb450f22?source=rss------ia-5",
			"excerpts": [
			  "**What it does** : Bind a process (or PID) to specific CPU cores.",
			  "**Limitation** : Only controls CPU affinity, **not** memory placement.",
			  "Remember: this only controls **where the threads run** , not **where the memory is allocated** .",
			  "`taskset` is the lightweight hammer for CPU affinity.",
			  "Run a process on specific CPU cores:",
			  "This is great for:\nEnsuring dataloader or preprocessing threads dont migrate across all cores\nKeeping noisy neighbors away from your main service\nPinning a latency-sensitive inference server to a subset of cores\n"
			]
		  },
		  {
			"title": "Memory Performance of AMD EPYC Rome and Intel ...",
			"url": "https://research.spec.org/icpe_proceedings/2022/proceedings/p165.pdf",
			"excerpts": [
			  "The main memory bandwidth of a single NUMA node of a CLX\nprocessor is higher than on Rome, as CLX has three memory chan-\nnels per NUMA node, Rome tw",
			  "transferring data to/from a remote\nsocket comes at a high additional latency penalty."
			]
		  },
		  {
			"title": "Check L1/L2 prefetching settings in XPS 8700 BIOS",
			"url": "https://www.dell.com/community/en/conversations/locked-topics-desktops-general/help-needed-check-l1l2-prefetching-settings-in-xps-8700-bios/647f498df4ccf8a8de1f3cc3",
			"excerpts": [
			  "In Linux you can enable or disable the hardware prefetchers using msr-tools http://www.kernel.org/pub/linux/utils/cpu/msr-tools/. The ...Read more"
			]
		  },
		  {
			"title": "A Closer Look at Intel Resource Director Technology (RDT)",
			"url": "https://dl.acm.org/doi/10.1145/3534879.3534882",
			"excerpts": [
			  "The RDT framework comprises five components: Cache Allocation Technology (CAT) and Code and Data Prioritization (CDP) for managing shared cache, Memory ...Read more"
			]
		  },
		  {
			"title": "Performance tuning at the edge using Cache Allocation Technology",
			"url": "https://www.redhat.com/en/blog/performance-tuning-at-the-edge",
			"excerpts": [
			  "Resource Control ( resctrl ) is a kernel interface for CPU resource allocation using Intel Resource Director Technology. The resctrl interface ...Read more"
			]
		  },
		  {
			"title": "A Closer Look at Intel Resource Director Technology (RDT)",
			"url": "https://cs-people.bu.edu/rmancuso/files/papers/CloserLookRDT_RTNS22.pdf",
			"excerpts": [
			  "For cache management, the partitioning is way-based, whereas the main memory bandwidth controls limit the amount of band- width extracted on a ...Read more"
			]
		  },
		  {
			"title": "Enabling memory access isolation in real-time cloud ...",
			"url": "https://www.sciencedirect.com/science/article/pii/S1383762123000279",
			"excerpts": [
			  "Once the partitioning of the LLC is implemented, memory bandwidth regulation is still necessary to limit memory access contention.Read more"
			]
		  },
		  {
			"title": "Assessing Intel's memory bandwidth allocation for resource ...",
			"url": "https://www.researchgate.net/publication/361053277_Assessing_Intel's_memory_bandwidth_allocation_for_resource_limitation_in_real-time_systems",
			"excerpts": [
			  "MBA Memory Bandwidth Allocation. MPAM Memory System Resource Partitioning and Monitoring.Read more"
			]
		  },
		  {
			"title": "What is Cache Coloring and How Does it Work?",
			"url": "https://www.lynx.com/blog/what-is-cache-coloring",
			"excerpts": [
			  "Cache coloring suffers from a number of difficulties that, while not insurmountable, make it difficult and risky for RTOS vendors to implement.",
			  "Linus Torvalds is strongly against cache coloring for Linux.",
			  "Hey, there have been at least four different major cache coloring trials for the kernel over the years. This discussion has been going on since the early nineties. And _none_ of them have worked well in practice.",
			  "The real degradation comes in just the fact that cache coloring itself is often expensive to implement and causes nasty side effects like bad memory allocation patterns, and nasty special cases that you have to worry about (ie special fallback code on non-colored pages when required)."
			]
		  },
		  {
			"title": "caching - cache coloring on slab memory management in Linux kernel - Stack Overflow",
			"url": "https://stackoverflow.com/questions/15016359/cache-coloring-on-slab-memory-management-in-linux-kernel",
			"excerpts": [
			  "The final task of the slab allocator is optimal hardware cache use. If there is space left over after objects are packed into a slab, the remaining space is used to color the slab. Slab coloring is a scheme that attempts to have objects in different slabs use different lines in the cache. By placing objects at a different starting offset within the slab, objects will likely use different lines in the CPU cache, which helps ensure that objects from the same slab cache will be unlikely to flush each other."
			]
		  },
		  {
			"title": "Disclosure of H/W prefetcher control on some Intel processors",
			"url": "https://radiable56.rssing.com/chan-25518398/article18.html",
			"excerpts": [
			  "The above mentioned processors support 4 types of h/w prefetchers for prefetching data. There are 2 prefetchers associated with L1-data cache (also known as DCU) and 2 prefetchers associated with L2 cache. There is a Model Specific Register (MSR) on **every core** with address of 0x1A4 that can be used to control these 4 prefetchers. Bits 0-3 in this register can be used to either enable or disable these prefetchers. Other bits of this MSR are reserved.",
			  "If any of the above bits are set to 1 on a core, then that particular prefetcher on that core is disabled. Clearing that bit (setting it to 0) will enable the corresponding prefetcher. Please note that this MSR is present in every core and changes made to the MSR of a core will impact the prefetchers only in that core. If hyper-threading is enabled, both the threads share the same MSR.",
			  "Most BIOS implementations are likely to leave all the prefetchers enabled (i.e MSR 0x1A4 value at 0) as prefetchers are either neutral or positively impact the performance for a large number of applications. However, how these prefetchers may impact your application is going to be highly dependent on the data access patterns in your application."
			]
		  },
		  {
			"title": "uarch-configure/intel-prefetch/intel-prefetch-disable.c at master  deater/uarch-configure  GitHub",
			"url": "https://github.com/deater/uarch-configure/blob/master/intel-prefetch/intel-prefetch-disable.c",
			"excerpts": [
			  "/* The key is MSR 0x1a4 */",
			  "/* bit 0: L2 HW prefetcher */",
			  "/* bit 1: L2 adjacent line prefetcher */",
			  "/* bit 2: DCU (L1 Data Cache) next line prefetcher */",
			  "/* bit 3: DCU IP prefetcher (L1 Data Cache prefetch based on insn address) */",
			  "/* This code uses the /dev/msr interface, and you'll need to be root. */"
			]
		  },
		  {
			"title": "Thinking about A New Mechanism for Huge Page ...",
			"url": "https://www.cse.unsw.edu.au/~cs9242/19/exam/paper2.pdf",
			"excerpts": [
			  "ABSTRACT. The Huge page mechanism is proposed to reduce the TLB misses and benefit the overall system performance. On the system with large memory capacity, ..."
			]
		  },
		  {
			"title": "Hmm, with AMD Threadripper, you're already looking at TLB issues at these L3 siz... | Hacker News",
			"url": "https://news.ycombinator.com/item?id=26468499",
			"excerpts": [
			  "Case in point: AMD Zen2 has 2048 TLB entries (L2), under a default (in Linux and Windows) of 4kB per TLB entry. That's 8MBs of TLB before your ...Read more"
			]
		  },
		  {
			"title": "Reduce NUMA balance caused TLB-shootdowns in a VM [LWN.net]",
			"url": "https://lwn.net/Articles/940764/",
			"excerpts": [
			  "Reduce NUMA balance caused TLB-shootdowns in a VM. From: Yan Zhao <yan.y.zhao-AT-intel.com>. To: linux-mm-AT-kvack.org, linux-kernel-AT-vger ...Read more"
			]
		  },
		  {
			"title": "Understanding TLB from CPUID results on Intel",
			"url": "https://stackoverflow.com/questions/58128776/understanding-tlb-from-cpuid-results-on-intel",
			"excerpts": [
			  "The TLB information for Ice Lake and Goldmont Plus processors is present in leaf 0x18. This leaf provides more flexibility in encoding TLB ..."
			]
		  },
		  {
			"title": "More than 20% UPS gain on Linux with huge pages (AMD Zen 2) : r/factorio",
			"url": "https://www.reddit.com/r/factorio/comments/j68o2w/more_than_20_ups_gain_on_linux_with_huge_pages/",
			"excerpts": [
			  "A TLB entry stores the mapping between virtual and physical address. TLB misses are quite expensive and require walking the page table. Zen 2 ...Read more"
			]
		  },
		  {
			"title": "arm64: support batched/deferred tlb shootdown during page reclamation/migration [LWN.net]",
			"url": "https://lwn.net/Articles/938347/",
			"excerpts": [
			  "arm64: support batched/deferred tlb shootdown during page reclamation/migration ... Comments and public postings are copyrighted by their creators ...Read more"
			]
		  },
		  {
			"title": "LUF(Lazy Unmap Flush) reducing tlb numbers over 90%",
			"url": "https://lwn.net/Articles/973209/",
			"excerpts": [
			  "I'm suggesting a new mechanism, LUF(Lazy Unmap Flush), defers tlb flush until folios that have been unmapped and freed, eventually get allocated again.Read more"
			]
		  },
		  {
			"title": "Zen 2 - Microarchitectures - AMD - WikiChip",
			"url": "https://en.wikichip.org/wiki/amd/microarchitectures/zen_2",
			"excerpts": [
			  "he fully-associative L1 instruction TLB contains 64 entries and holds 4-Kbyte, 2-Mbyte, or 1-Gbyte page table entries. The 512 entries of the 8-way set-associative L2 instruction TLB can hold 4-Kbyte and 2-Mbyte page table entries. 1-Gbyte pages are *smashed* into 2-Mbyte entries in the L2 ITLB. A hardware page table walker handles L2 ITLB misses",
			  "TLB**\n64 entry L1 TLB, fully associative, all page sizes\n512 entry L2 TLB, 8-way set associative\n4-Kbyte and 2-Mbyte pages\nParity protecte",
			  "A two-level translation lookaside buffer (TLB) assists load and store address translation. The fully-associative L1 data TLB contains 64 entries and holds 4-Kbyte, 2-Mbyte, and 1-Gbyte page table entries. The L2 data TLB is a unified 12-way set-associative cache with 2048 entries, up from 1536 entries in Zen, holding 4-Kbyte and 2-Mbyte page table entries, as well as page directory entries (PDEs) to speed up DTLB and ITLB table walks. 1-Gbyte pages are *smashed* into 2-Mbyte entries but installed as 1-Gbyte entries when reloaded into the L1 TLB."
			]
		  },
		  {
			"title": "Skylake (server) - Microarchitectures - Intel - WikiChip",
			"url": "https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)",
			"excerpts": [
			  "TLBs:\nITLB\n4 KiB page translations:\n128 entries; 8-way set associative\ndynamic partitioning\n2 MiB / 4 MiB page translations:\n8 entries per thread; fully associative\nDuplicated for each thread",
			  "DTLB\n4 KiB page translations:\n64 entries; 4-way set associative\nfixed partition\n2 MiB / 4 MiB page translations:\n32 entries; 4-way set associative\nfixed partition\n1G page translations:\n4 entries; 4-way set associative\nfixed partition",
			  "STLB\n4 KiB + 2 MiB page translations:\n1536 entries; 12-way set associative. (Note: STLB is incorrectly reported as \"6-way\" by CPUID leaf 2 (EAX=02H). Skylake erratum SKL148 recommends software to simply ignore that value.)\nfixed partition\n1 GiB page translations:\n16 entries; 4-way set associative\nfixed partition"
			]
		  },
		  {
			"title": "Huge Pages and PostgreSQL | CYBERTEC PostgreSQL | Services & Support",
			"url": "https://www.cybertec-postgresql.com/en/huge-pages-postgresql/",
			"excerpts": [
			  " Linux this problem can be mitigated by using Huge Pages. Since page tables are organized hierarchically, they enable us to summarize allocations in much larger pages than the default. The size of huge pages are architecture-dependent, on x86 systems we can usually expect 2MB or 1GB sizes, IBM POWER allows 64kB, 16MB and 16 GB.\n*",
			  "**Lookups for specific virtual addresses of memory allocations are then much faster** and hopefully more independent of the entries found in the TLB.",
			  "On x86, when configuring huge pages, the 2MB page size is the default. You can easily get your system current settings via `/proc/meminfo` :",
			  "Hugepagesize: 2048 kB",
			  "For people not familiar with the meaning of huge pages, here is a short overview of what they are and which problems they try to solve.",
			  "On any modern system, applications don't use physical memory directly. Instead, they use a virtual memory address model to make it easier to handle memory allocation and avoid the complexity of computing and mapping physical addresses into the application memory space."
			]
		  },
		  {
			"title": "PostgreSQL and Huge Pages: Boosting Database Performance the Right Way | by Tomasz Gintowt | Medium",
			"url": "https://tomasz-gintowt.medium.com/postgresql-and-huge-pages-boosting-database-performance-the-right-way-32a27b25a819",
			"excerpts": [
			  "**Huge Pages** solve this problem by using **larger memory pages** , typically **2 MB** (or even 1 GB on some systems).",
			  "This reduces overhead and improves performance for memory-intensive applications like PostgreSQL.",
			  "Section Title: PostgreSQL and Huge Pages: Boosting Database Performance the Right Way > Why PostgreSQL benefits from Huge Pages",
			  "In Linux, memory is divided into small chunks called **pages** , usually **4 KB** each. When PostgreSQL allocates shared memory, it might use **thousands or millions of pages** . Managing so many small pages increases overhead  both in memory management and TLB (Translation Lookaside Buffer) lookups inside the CPU.",
			  "PostgreSQL uses a **shared memory segment** ( `shared_buffers` ) for caching data.",
			  "If you have a large shared_buffers setting  for example, several gigabytes  enabling Huge Pages helps by:",
			  "Reducing CPU overhead in managing small pages.",
			  "Preventing memory fragmentation.",
			  "Providing a small but consistent **performance boost** (15% on most systems).",
			  "Making memory usage more predictable."
			]
		  },
		  {
			"title": "PostgreSQL Performance Tuning: Ultimate Guide to Optimize Your Database Server",
			"url": "https://www.enterprisedb.com/postgres-tutorials/introduction-postgresql-performance-tuning-and-optimization",
			"excerpts": [
			  "By default, the page size on Linux is 4KB. A typical PostgreSQL instance may allocate many GBs of memory, leading to potential performance problems due to the small page size.",
			  "Enabling huge pages on Linux will boost PostgreSQL performance as it will allocate large blocks (huge pages) of memory altogether.",
			  "By default, huge pages are not enabled on Linux, which is also suitable for PostgreSQLs default huge_pages setting try, which means use huge pages if available on the OS, otherwise no.",
			  "There are two aspects to setting up huge pages for PostgreSQL: Configuring the OS and configuring PostgreSQL."
			]
		  },
		  {
			"title": "High Performance and Scalable GPU Graph Traversal",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2011-08_High-Performance-and/BFS%20TR.pdf",
			"excerpts": [
			  "Most parallel BFS algorithms are level-synchronous: each level may be processed in parallel as long as the sequential ordering of levels is preserved. An.Read more"
			]
		  },
		  {
			"title": "Mach-RT: A Many Chip Architecture for Ray Tracing",
			"url": "https://users.cs.utah.edu/~elvasiou/papers/vasiou_HPG19_machrt.pdf",
			"excerpts": [
			  "Other shared large-area units consist of L1 data cache, instruction cache, Ray. Staging Buffer and large execution units such as floating point di- vision.Read more"
			]
		  },
		  {
			"title": "A Case Study for Ray Tracing Cores: Performance Insights ...",
			"url": "https://xiaodongzhang1911.github.io/Zhang-papers/TR-25-2.pdf",
			"excerpts": [
			  "Following the similar idea, we develop our RT-based BFS, which is shown in Algorithm 1 . The\nunvisited vertices are explored in iterations, and inside an iteration, neighbors of the vertices\nvisited in the last iteration are traversed. A CPU-side global synchronization is executed to remove\nredundant neighbors between iterations. The difference from a CUDA-based BFS method is that\nwe need to convert the graph into a BVH (Line 1) and then expand the queue of unvisited vertices\nusing RT cores (Line 8). As a result, the two keys of RT-based BFS are how to construct a BVH\nrepresenting the graph, and how to issue rays realizing the neighbor visiting."
			]
		  },
		  {
			"title": "Efficient Ray Tracing Kernels for Modern CPU Architectures",
			"url": "https://jcgt.org/published/0004/04/05/paper-lowres.pdf",
			"excerpts": [
			  "he efficiency of packet traversal has motivated algorithms that attempt to ex-\ntract coherent subsets out of incoherent ray batches, such as stream filtering and\nrelated methods [ Overbeck et al. 2008 ], where ray batches are intersected with the\ncurrent node in a breadth-first manner, and coherency is extracted implicitly during\ntree traversal. While this algorithm potentially allows high SIMD utilization, it re-\nquires expensive gather-and-scatter operatio",
			  "stead of testing n rays against one node, a single\nray is intersected with n nodes of a n-ary BVH simultaneously, where branching fac-\ntors of 416 have been investigated. Aside from efficient SIMD utilization, higher\nbranching factors reduce the depth and memory footprint of a BVH. On the negative\nside, early culling opportunities are reduced because some nodes are intersected that\nwould not have been visited in a tree with a lower branching factor. This leads to\nwasted operations and bandwidth.",
			  ", 2015\nhttp://jcgt.org\nrithms, a major drawback is a common traversal order that is enforced for every ray\nin the batch. This restriction was relaxed by the introduction of dynamic ray stream\ntraversal (DRST) [ Barringer and Akenine-Moller 2014 ], where, in the case of BVH4,\neach ray can follow almost the same traversal order that would result from single ray\ntraversal. Despite its high performance, two major drawbacks are apparent:\n ... \nA",
			  "Ray Stream Traversal**\nRay streams have emerged as an efficient approach to extract hidden coherence from\nincoherent ray batches, reducing memory bandwidth and increasing SIMD utilization\ncompared to single ray traversal. The dynamic ray stream traversal (DRST) [ Bar-\nringer and Akenine-Moller 2014 ] appears to be the fastest algorithm so far, deriving\nits performance from the ability to grant each ray its individual traversal order to in-\ncrease culling efficiency. In the BVH4 case, however, the possible permutations of\nthe traversal order are limited to eight out of 24 (compare balanced type, Section 2.2 ).\nIn addition, during every traversal step, rays are mapped to nine different bins, pro-\nducing considerable maintenance overhead and leading to batch fragmentation. In the\nfollowing, we propose a ray stream implementation, which like the DRST allows each\nray in the batch to traverse the tree in its preferred order. Unlike the DRST however,\na ray may follow any of the 24 possible permutations. In addition, no fragmentation\nof the ray batch happens during traversal (only once initially) and the number of bin",
			  "r contribution is twofold: For coherent ray sets, we introduce\na large packet traversal tailored to the BVH4 that is faster than the original BVH2 variant, and\nfor incoherent ray batches we propose a novel implementation of ray streams which reduces\nthe bookkeeping cost while strictly maintaining the preferred traversal order of individual\nrays. B",
			  "An entirely different approach has\nbeen proposed by several papers concurrently [ Wald et al. 2008 ; Ernst and Greiner\n2008 ; Dammertz et al. 2008 ]. Instead of testing n rays against one node, a single\nray is intersected with n nodes of a n-ary BVH simultaneously, where branching fac-\ntors of 416 have been investigated.",
			  "e solve challenge (1) with our efficient look-up mechanism introduced in Sec-\ntion 2.2 and challenge (2) with a combination of a deferred packet test of last re-\n ... ",
			  "\nFor coherent rays, SIMD packet traversal was first introduced for the BSP acceler-\nation structure [ Wald et al. 2001 ] and later extended to bounding volume hierarchies\n(BVH). Packet traversal traces n rays in parallel, where n is the number of SIMD\nelements. As long as all rays follow the same path through the acceleration structure,\nfull SIMD utilization is achieved. Otherwise, divergent rays need to be masked and\nthe calculations on the corresponding vector elements are wasted. For tightly bun-\ndled coherent rays, identical paths are the common case, and a significant speed-up is\nobserved for packet tracing.",
			  "\nFor coherent rays, SIMD packet traversal was first introduced for the BSP acceler-\nation structure [ Wald et al. 2001 ] and later extended to bounding volume hierarchies\n(BVH). Packet traversal traces n rays in parallel, where n is the number of SIMD\nelements. As long as all rays follow the same path through the acceleration structure,\nfull SIMD utilization is achieved. Otherwise, divergent rays need to be masked and\nthe calculations on the corresponding vector elements are wasted. For tightly bun-\ndled coherent rays, identical paths are the common case, and a significant speed-up is\nobserved for packet tracing.",
			  "The coherence of a ray set can be defined as the tendency of the individual rays to\nfollow the same traversal path through an acceleration structure and to intersect the\nsame primitives."
			]
		  },
		  {
			"title": "15.1 Mapping Path Tracing to the GPU",
			"url": "https://www.pbr-book.org/4ed/Wavefront_Rendering_on_GPUs/Mapping_Path_Tracing_to_the_GPU",
			"excerpts": [
			  "Another advantage of the wavefront approach is that different numbers of registers can be allocated to different kernels. Thus, simple kernels can use fewer ...Read more"
			]
		  },
		  {
			"title": "Algorithms and Data Structures for Interactive Ray Tracing ... - SciDok",
			"url": "https://scidok.sulb.uni-saarland.de/bitstream/20.500.11880/26447/1/Stefan_Popov_PhD_Thesis.pdf",
			"excerpts": [
			  "The key observation that enables our algorithm to work is that BVH traversal does\nnot need to store the entry or exit distances or points in the traversal stack. They\ncan be computed on the fly, since the AABB is always stored with the node. Thus,\nthe per-ray traversal stack only holds pointers to the far nodes that still need to be\ntraversed. Applied to packet traversal, this means again that the stack needs to hold\nonly the far nodes that have to be visited by the packet in the future.",
			  "Our algorithm (see Algorithm 9.1) maps one ray to one thread and thus one packet\nto one SIMD width warp. It traverses the tree synchronously with the whole packet,\nworking on one node at a time and processing the whole packet against it. If the node\nis a leaf, it intersects the rays in the packet with the contained geometry. Each thread\nkeeps the distance to the closest intersected primitive found so far in a variable. If a\ncloser primitive is found in the leaf, this variable is updated.",
			  "If the currently visited node is not a leaf, the algorithm intersects all rays of the\npacket with both children to determine the entry/exit distances. Each ray determines\nfor itself which of the two children it intersects and in case it intersects both, in which\norder it wants to visit them (line 18).",
			  "Applied to packet traversal, this means again that the stack needs to hold\nonly the far nodes that have to be visited by the packet in the future.",
			  "Thus, if we use\n32 bit pointers to the nodes (which is the case, because of the limited GPU memory),\nthe amortized storage requirement of a ray becomes exactly 1 bit per node.",
			  "lgorithm 9.1) maps one ray to one thread and thus one packet\nto one SIMD width warp."
			]
		  },
		  {
			"title": "Branch divergence - OptiX",
			"url": "https://forums.developer.nvidia.com/t/branch-divergence/176258",
			"excerpts": [
			  "There are different sources of divergence, so it depends on the scene & renderer whether divergence will be a problem. One source of divergence is ray traversal, intersection, and any-hit shaders. When rays in a warp are traversing different depths of an acceleration structure, divergence happens. If the rays are going through parts of the scene with very different geometric density, divergence appears.",
			  "Yes there are studies on this topic, and yes there are things you can do to improve situations that have significant divergence. For traversal, using the RTX hardware traversal and intersection for all of the scene geometry is a major way to both improve performance and cut down divergence. Avoiding any-hit programs and custom intersection programs when possible is another way to improve performance and potentially improve divergence. (Its not always possible to avoid any-hit programs or custom intersectors, and I dont want to stigmatize the use of valid necessary features, but any-hit and intersection programs do interrupt hardware traversal to execute your code on the SMs, and so theres significant overhead.)",
			  "A wavefront architecture also allows you consolidate ray tracing work at every step of path depth, so the kernels get smaller as you go, and warps stay compacted with active work on all threads."
			]
		  },
		  {
			"title": "Wavefront Path Tracing - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2019/07/18/wavefront-path-tracing/",
			"excerpts": [
			  "There is a problem with the algorithm. A primary ray may find a light right away, or after a single random bounce, or after fifty bounces. A CPU programmer may see a potential stack overflow; a GPU programmer should see *low hardware utilization* . The problem is caused by the (conditional) tail recursion: a path may get terminated at a light source, or it may continue if it hit something else. Translated to many threads: a portion of the threads will get terminated, and a portion continues.",
			  "The hardware utilization problem is amplified by the SIMT execution model of GPUs. Threads are organized in groups, e.g. 32 threads go together in a *warp* on a Pascal GPU (10xx class NVidia hardware). The threads in a warp share a single program counter: they execute in lock-step, so every program instruction is executed by the 32 threads simultaneously. SIMT stands for: *single instruction multiple thread* , which describes this concept well.",
			  "The streaming path tracing algorithm is designed to combat the root of the occupancy problem. Streaming path tracing splits the path tracing algorithm in four phases:",
			  "**Generate**",
			  "**Extend**",
			  "**Shade**",
			  "**Connect**",
			  "Each phase is implemented as a separate program. So instead of running the full path tracer as a single GPU program (kernel), we now have *four* kernels. And on top of that, they execute in a loop, as we will see shortly.",
			  "The problem is caused by the (conditional) tail recursion: a path may get terminated at a light source, or it may continue if it hit something else. Translated to many threads: a portion of the threads will get terminated, and a portion continues. After a few bounces, we have a few threads that have work left to do, while most threads are waiting for the final threads to finish."
			]
		  },
		  {
			"title": "Raytracing on AMDs RDNA 2/3, and Nvidias Turing and Pascal",
			"url": "https://chipsandcheese.com/p/raytracing-on-amds-rdna-2-3-and-nvidias-turing-and-pascal",
			"excerpts": [
			  "It defines BVH-es in two structures  a top level acceleration structures (TLAS), and a bottom level acceleration structure (BLAS).Read more"
			]
		  },
		  {
			"title": "How to build a BVH  part 5: TLAS & BLAS - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/05/07/how-to-build-a-bvh-part-5-tlas-blas/",
			"excerpts": [
			  "The nodes that we use to combine a set of BVHs into a single BVH are referred to as the top level acceleration structure, or TLAS. It ...Read more"
			]
		  },
		  {
			"title": "an-adaptive-heterogeneous-runtime-for-irregular- ...",
			"url": "https://scispace.com/pdf/an-adaptive-heterogeneous-runtime-for-irregular-applications-4ybnel0b2m.pdf",
			"excerpts": [
			  "The process\nstarts with a 16-wide packet traversal which performs 16-wide box tests. At any\npoint in time, the bit in an active mask will be counted to indicate how many\nof the packets rays are still active for a subtree. If this number falls below a\ngiven threshold, which is set to 7, the process leaves the packet traversal mode\nand sequentially traces all active rays in the single-ray mode[12].",
			  "The drawback\nof this method is that the threshold may need to change and the program will\nrequire recompilation if the system is moved to a machine with a shorter SIMD\nlane.",
			  "the hybrid packet/single-ray tracing algorithm is\nimplemented by utilizing a specific type of BVH tree.",
			  "his method does not prevent any possible divergent execution that\nlowers the effectiveness of the SIMD engine."
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "Hybrid traversal**\nAs mentioned above this method is a combination of the\npacket and if-if traversal ones.\nThe idea is that packet\ntraversal performs best near the tree root where rays are\ncoherent whereas if-if traversal is better suited for travers-\ning nodes near the leaves. It is, however, unclear when and\nhow to switch between the two of the method",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Intel Resource Director Technology (Intel RDT)",
			"url": "https://eci.intel.com/docs/3.0/development/intel-pqos.html",
			"excerpts": [
			  "Intel Cache Allocation Technology (CAT) provides a method to partition processor caches and assign these partitions to a Class-of-Service (COS)."
			]
		  },
		  {
			"title": "Adaptive Ray Packet Reordering",
			"url": "https://graphics.stanford.edu/~boulos/papers/reorder_rt08.pdf",
			"excerpts": [
			  "Empirically, our threshold of 50% works rather well (for more detailed comparisons see Section 5), but a more complicated heuristic might reorder less often for ...Read more"
			]
		  },
		  {
			"title": "Combining Single and Packet-Ray Tracing for Arbitrary Ray Distributions on the Intel MIC Architecture - PubMed",
			"url": "https://pubmed.ncbi.nlm.nih.gov/22084142/",
			"excerpts": [
			  "In this paper, we introduce a single-ray tracing scheme for incoherent rays that uses just one traversal stack on 16-wide SIMD hardware. It uses a bounding ...Read more"
			]
		  },
		  {
			"title": "Introduction to Cache Allocation Technology in the Intel Xeon...",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-to-cache-allocation-technology.html",
			"excerpts": [
			  "Intels Cache Allocation Technology (CAT) helps address shared resource concerns by providing software control of where data is allocated into the last-level cache (LLC), enabling isolation and prioritization of key applications.",
			  "CAT) provides software-programmable control over the amount of cache space that can be consumed by a given thread, app, VM, or container",
			  "cation Technology (CAT) enables privileged software such as an OS or VMM to control data placement in the last-level cache (LLC), enabling isolation and prioritization of important threads, apps, containers, or VMs, given [software support](/content/www/us/en/develop/articles/software-enabling-for-cache-allocation-technology.html) . While an initial version of CAT was introduced on a limited set of [communications processors in the Intel Xeon processor E5-2600 v3 family](http://www.intel.com/content/www/us/en/communications/cache-monitoring-cache-allocation-technologies.html) , the CAT feature is significantly enhanced and now available on all SKUs starting with the Intel Xeon processor E5 v4 famil"
			]
		  },
		  {
			"title": "An Evaluation of Intel Cache Allocation Technology for Data",
			"url": "https://www.diva-portal.org/smash/get/diva2:1622362/FULLTEXT01.pdf",
			"excerpts": [
			  "Intel CAT is a tool from Intel that can be used\nto control cache allocation for some of their Xeon CPU models.",
			  "In particular,\nIntel CAT can allocate shared cache resources to specific CPU cores and in turn\nto specific applications running on those cores.",
			  "Applications over-utilizing the\nL3 cache are commonly called L3 cache noisy neighbors [ 3 ] [ 4 ] and in Figure\n2.5 an example of a noisy neighbor application and a prioritized application\nis illustrated.",
			  "The L3 cache in the Intel Xeon Scalable CPUs is N-way set-associative and\nthe particular model used in this thesis has 11 cache ways (N=11). [ 2 , p. 482]",
			  "Intel CAT uses the notion of Classes of Service (CLOS) to group CPU\ncores into classes which can then be allocated a certain number of cache ways\navailable to its disposal. More concretely, CAT defines Capacity Bitmasks\n(CBMs) for each CLOS to specify how much cache is allocated to a CLOS.",
			  "The bits set to\n1 in the CBM must be a continuous string of bits. The cache allocations defined\nby the CBMs can overlap for multiple CLOSs, resulting in that they share that\nportion of the cache. An example of this is shown in Figure 2.7 . Intel CAT\nis supported by hardware in the CPU and configured through Model-Specific\nRegisters (MSRs) . [ 1 ]",
			  " was successfully used to increase the performance of the GET\n ... \n--disable - libunwind - exceptions\n--\nenable -gnu -unique - object\n--enable -linker -build -\nid\n--with -gcc -major - version - only\n--with -linker -\nhash - style =gnu\n--enable - plugin\n--enable - initfini"
			]
		  },
		  {
			"title": "Cache Allocation Technology - Real-time Ubuntu documentation",
			"url": "https://documentation.ubuntu.com/real-time/latest/tutorial/intel-tcc/intel-cat/",
			"excerpts": [
			  "\nLets take a look at how Intel Cache Allocation Technology (CAT) can help mitigate these sources of contention.\nCA",
			  "CAT provides the ability to partition caches at various levels in the caching hierarchy.",
			  "CAT provides the ability to partition caches at various levels in the caching hierarchy.",
			  "Initially, the default cache configuration is used, where all cache ways are shared.",
			  "In the second step, the statistics are compared with the Last Level Cache (LLC) partitioned to provide an exclusive portion of the cache to the real-time test application."
			]
		  },
		  {
			"title": "Why Intel added cache partitioning",
			"url": "https://danluu.com/intel-cat/",
			"excerpts": [
			  "needs.\nIntel's  [Cache Allocation Technology](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.pdf)  (CAT) allows the LLC to limit which cores can can access different parts of the cache. Since we often want to pin performance sensitive tasks to cores anyway, this allows us to divide up the cache on a per-task basis.\nIn",
			  "Intel's [April 2015 whitepaper on what they call Cache Allocation Technology (CAT)](http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/cache-allocation-technology-white-paper.pdf) has some simple benchmarks comparing CAT vs. no-CAT. In this example, they measure the latency to respond to PCIe interrupts while another application has heavy CPU-to-memory traffic, with CAT on and off.",
			  "order to address this problem, Intel introduced what they call Code and Data Prioritization Technology (CDP). This is an extension of CAT that allows cores to separately limit which subsets of the LLC instructions and data can occupy. Since it's targeted at the last-level cache, it doesn't directly address the graph above, which shows L2 cache miss rates. However, the cost of an L2 cache miss that hits in the LLC is something like [26ns on Broadwell vs. 86ns for an L2 miss that also misses the LLC and has to go to main memory](http://users.atw.hu/instlatx64/GenuineIntel00306D4_Broadwell2_NewMemLat.txt) , which is a substantial difference"
			]
		  },
		  {
			"title": "Embree: A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://cseweb.ucsd.edu/~ravir/274/15/papers/a143-wald.pdf",
			"excerpts": [
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "Yet\nthe memory storage order for a BVH optimized for packets may be\nsuboptimal for single-ray methods (the converse may also be true).",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "**Hybrid Traversal and Intersection**"
			]
		  },
		  {
			"title": "A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree.pdf",
			"excerpts": [
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "Yet\nthe memory storage order for a BVH optimized for packets may be\nsuboptimal for single-ray methods (the converse may also be true).",
			  "This\nmode begins traversal using packets, and dynamically switches to\nsingle-ray traversal when the number of active rays in a packet falls\nbelow a threshold [Benthin et al. 2012].",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes.",
			  "**5.1.4**",
			  "**Hybrid Traversal and Intersection**"
			]
		  },
		  {
			"title": "benchmarking - How to write a pointer-chasing benchmark using 64-bit pointers in CUDA? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/36416843/how-to-write-a-pointer-chasing-benchmark-using-64-bit-pointers-in-cuda",
			"excerpts": [
			  "This research paper runs a series of several CUDA microbenchmarks on a GPU to obtain statistics like global memory latency, instruction throughput, etc.Read more"
			]
		  },
		  {
			"title": "Solved: Thank you both for your",
			"url": "https://community.intel.com/t5/Intel-Moderncode-for-Parallel/Topology-Bandwidth-and-Latency-of-the-Cache-Coherence-Network/m-p/1116922",
			"excerpts": [
			  "Given these lists of addresses mapping to a single L3 slice, one can then perform standard pointer-chasing latency tests with full control over ...Read more"
			]
		  },
		  {
			"title": "Accelerating Pointer Chasing in 3D-Stacked Memory",
			"url": "http://pdl.cmu.edu/PDL-FTP/associated/16iccd_impica.pdf",
			"excerpts": [
			  "\nPointer chasing is currently performed by the CPU cores, as part\nof an application thread. While this approach eases the integration of\npointer chasing into larger programs, pointer chasing can be inefficient\nwithin the CPU, as it introduces several sources of performance degra-\ndation: (1) dependencies exist between memory requests to the linked\nnodes, resulting in serialized memory accesses and limiting the avail-\nable instruction-level and memory-level parallelism [33,61,62,67,75];\n(2",
			  "we profile two popular applications that heavily de-\npend on linked data structures, using a state-of-art Intel Xeon system: 1\n(1) *Memcached* [25], using a real Twitter dataset [23] as its input; and\n(2) *DBx1000* [94], an in-memory database system, using the TPC-\nC benchmark [87] as its input.",
			  "We propose to improve the performance of pointer chasing by lever-\naging processing-in-memory (PIM) to alleviate the memory bottle-\nneck. Instead of sequentially fetching *each node* from memory and\nsending it to the CPU when an application is looking for a particular\nnode, PIM-based pointer chasing consists of (1) traversing the linked\ndata structures *in memory* , and (2) returning only the final node found\nto the CPU.",
			  "Linked list** . We use the linked list traversal microbenchmark [98]\nderived from the *health* workload in the Olden benchmark suite [73].\nThe parameters are configured to approximate the performance of\nthe *health* workload. We measure the performance of the linked list\ntraversal after 30,000 iterations.",
			  "**Hash table** . We create a microbenchmark from the hash table\nimplementation of *Memcached* [25]. The hash table in Memcached\nresolves hash collisions using chaining via linked lists. When there are\nmore than 1.5 *n* items in a table of *n* buckets, it doubles the number of\n2 We sweep the size of the IMPICA cache from 32KB to 128KB, and find that\nit has negligible effect on our results [35].\n5\nbuckets. We f",
			  "building blocks in a wide range of workloads, to evaluate the native\nperformance of performance chasing operations: linked lists, hash\ntables, and B-trees. We also evaluate the performance improvement\nin a real data-intensive workload, measuring the transaction latency\nand throughout of DBx1000 [94], an in-memory OLTP database. We\nmodify all four workloads to offload each pointer chasing request to\nIMPICA. To minimize communication overhead, we map the IMPICA\nregisters to user mode address space, thereby avoiding the need for\ncostly kernel code intervention.",
			  "IMPICA effectively utilizes the in-\nternal memory bandwidth in 3D-stacked memory, which is cheap\nand abundant."
			]
		  },
		  {
			"title": "c - what is ChaseNS in this pointer-chasing benchmark - Stack Overflow",
			"url": "https://stackoverflow.com/questions/72620752/what-is-chasens-in-this-pointer-chasing-benchmark",
			"excerpts": [
			  "212 ns is a *long* time to wait for a cache-miss load, but with contention from multiple cores it's maybe plausible?",
			  "This is a [pointer-chasing](https://en.wikichip.org/wiki/pointer_chasing#:%7E:text=Pointer%20chasing%20refers%20to%20a,serially%2Ddependent%20chain%20of%20loads.) microbenchmark, like `p = p->next` , so you're measuring load latency by making each load-address dependent on the previous load's result.",
			  "This is a [pointer-chasing](https://en.wikichip.org/wiki/pointer_chasing#:%7E:text=Pointer%20chasing%20refers%20to%20a,serially%2Ddependent%20chain%20of%20loads.) microbenchmark, like `p = p->next` , so you're measuring load latency by making each load-address dependent on the previous load's result.",
			  "So hopefully the access pattern is *not* regular, otherwise hardware prefetching would defeat it, by having the next thing to load already in local L1d cache before the load-address is known.",
			  "e.g. make an array of pointers to pointers (like `struct foo { struct foo *next; };` ) with each one pointing to the next, then shuffle it, so iterating over that linked list touches cache lines in a random order within that 512 MiB working set.",
			  "e.g. make an array of pointers to pointers (like `struct foo { struct foo *next; };` ) with each one pointing to the next, then shuffle it, so iterating over that linked list touches cache lines in a random order within that 512 MiB working set."
			]
		  },
		  {
			"title": "[PDF] A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/A-Study-of-Pointer-Chasing-Performance-on-Systems-Weisz-Melber/e43bf3e494a945744c84a7eb64f2bb08e9862802",
			"excerpts": [
			  "Section Title: A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems > Topics",
			  "Content:\nAI-Generated\n[Field Programmable Gate Array (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/502114721?corpusId=17857032) [Traversal Performance (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/7293980890?corpusId=17857032) [In-memory (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/16217196227?corpusId=17857032) [Pointer Chasing (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/42373953410?corpusId=17857032)"
			]
		  },
		  {
			"title": "356477-Optimization-Reference-Manual-V2-002.pdf",
			"url": "https://cdrdv2-public.intel.com/814199/356477-Optimization-Reference-Manual-V2-002.pdf",
			"excerpts": [
			  "MOV rax, [rax]\n4\nMOV rax, disp32[rax] , disp32 < 2048\n4\nMOV rax, [rcx+rax]\n5\nMOV rax, disp32[rcx+rax] , disp32 < 2048\n5"
			]
		  },
		  {
			"title": "Characterizing and Modeling Non-Volatile Memory Systems",
			"url": "https://www.microarch.org/micro53/papers/738300a496.pdf",
			"excerpts": [
			  "to bypass CPU caches, while still generating\ncacheline-sized memory accesses, all pointer chasing tests are\nimplemented using non-temporal AVX512 load/store instruc-\ntions. ",
			  "Buffer Prober** detects the on-DIMM buffer capacity, entry\n ..",
			  "*Pointer chasing* is a random memory access benchmark:\nit divides a contiguous memory region  referred to as a\n498\npointer chasing region (PC-Region)  into equal-sized blocks\n(PC-Blocks); it reads/writes all PC-Blocks in a PC-Region in\nrandom order, and sequentially accesses data within each PC-\nBlock.",
			  "*Pointer chasing* is a random memory access benchmark:\nit divides a contiguous memory region  referred to as a\n498\npointer chasing region (PC-Region)  into equal-sized blocks\n(PC-Blocks); it reads/writes all PC-Blocks in a PC-Region in\nrandom order, and sequentially accesses data within each PC-\nBlock.",
			  "Pointer chasing has three variants to detect various buffer\narchitecture characteristics: (1) collecting average latency per\ncache line with a fixed PC-Block size across various PC-\nRegion sizes, (2) quantifying read and write amplification by\nusing a fixed PC-Region size, while varying the size of PC-\nBlock, (3) issuing read-after-write requests, which issue writes\nin a pointer chasing order, followed by reads in the same order."
			]
		  },
		  {
			"title": "Characterizing and Modeling Non-Volatile Memory Systems",
			"url": "https://swanson.ucsd.edu/data/bib/pdfs/MICRO20-LensVans.pdf",
			"excerpts": [
			  "cacheline-sized memory accesses, all pointer chasing tests are\nimplemented using non-temporal AVX512 load/store instruc-\ntions.",
			  "Pointer chasing has three variants to detect various buffer\narchitecture characteristics: (1) collecting average latency per\ncache line with a fixed PC-Block size across various PC-\nRegion sizes, (2) quantifying read and write amplification by\nusing a fixed PC-Region size, while varying the size of PC-\nBlock, (3) issuing read-after-write requests, which issue writes\nin a pointer chasing order, followed by reads in the same order.",
			  "*Overwrite* repeatedly generates sequential writes to a fixed\nmemory region, and then measures the execution time of each\niteration.",
			  "Figure 9a shows the load and store latency of the pointer\nchasing microbenchmark across different access region sizes\nwith single-DIMM configuration.",
			  "The store latency curves of\nVANS match the curves of real-system profiling well with\na difference lower than 10% across all the access region\nsizes.",
			  "Figure 9b shows the pointer chasing latency of six DIMMs",
			  "Overall, our evaluation shows that\nVANS achieves an average 86.5% accuracy across four metrics\n(Figure 9e)."
			]
		  },
		  {
			"title": "assembly - Not sure about the AMD Zen 3 Architecture (functional units, issue time / latency of instructions) - Stack Overflow",
			"url": "https://stackoverflow.com/questions/75543818/not-sure-about-the-amd-zen-3-architecture-functional-units-issue-time-latenc",
			"excerpts": [
			  "The minimum time (in cycles) between execution of dependent operations is called latency. uops.info measures that for every instruction, or ..."
			]
		  },
		  {
			"title": "A Study of Pointer-Chasing Performance on Shared- ...",
			"url": "https://users.ece.cmu.edu/~jhoe/distribution/2016/fpga16.pdf",
			"excerpts": [
			  "Pointer-Chasing.** While the class of irregular parallel ap-\nplications is broad and varied, a fundamental behavior is\npointer-chasing. In pointer chasing, the computation is re-\nquired to dereference a pointer to retrieve each node from\nmemory, which contains both a data payload to be processed\nand a pointer (or pointers) to subsequent nodes. The exact\ncomputation on the payload and the determination of the\nnext pointer to follow depend on the specific data structure\nand algorithm in use. In this paper, we ignore these differ-\nences and focus on only the basic effects of memory access\nlatency and bandwidth on pointer chasing. It is our con-\ntention that the optimization of basic pointer-chasing per-\nformance ultimately determines the opportunities for FPGA\nacceleration of irregular parallel application",
			  "For this purpose, we fixed a simple reference behavior,\nnamely a linked-list traversal. This reference behavior is pa-\nrameterized by (1) node layout in memory (best- vs. worst-\ncase in our experiments); (2) per node data payload size;\n(3) payload dependence (an artificial constraint that payload\nmust be retrieved before following the next pointer); and (4)\nconcurrent traversals (availability of multiple independent\ntraversals).",
			  "Taken together, these parameters abstractly\ncapture the execution differences of different pointer-based\nalgorithms and data-structures. The two execution require-"
			]
		  },
		  {
			"title": "uops.info",
			"url": "https://www.uops.info/cache.html",
			"excerpts": [
			  "L1 data cache\nSize: 48 kB\nAssociativity: 12\nNumber of sets: 64\nWay size: 4 kB\nLatency: 5 cycles [Link](cache/lat_ICL.html)",
			  "L2 cache\nSize: 512 kB\nAssociativity: 8\nNumber of sets: 1024\nWay size: 64 kB\nLatency: 13 cycles [Link](cache/lat_ICL.html)",
			  "L3 cache\nSize: 6 MB\nAssociativity: 12\nNumber of CBoxes: 4\nNumber of slices: 8\nNumber of sets (per slice): 1024\nWay size (per slice): 64 kB\nLatency: 41 cycles [Link](cache/lat_ICL.html)",
			  "Core i3-8121U (Cannon Lake)**\nL1 data cache\nSize: 32 kB\nAssociativity: 8\nNumber of sets: 64\nWay size: 4 kB\nLatency: 4 cycles [Link](cache/lat_CNL.htm",
			  "Section Title: Caches\nContent:\n4\nNumber of sets: 1024\nWay size: 64 kB\nLatency: 12 cycles [Link](cache/lat_CFL.html)",
			  "L3 cache\nSize: 12 MB\nAssociativity: 16\nNumber of CBoxes: 6\nNumber of slices: 12\nNumber of sets (per slice): 1024\nWay size (per slice): 64 kB\nLatency: 41 cycles [Link](cache/lat_CFL.html)"
			]
		  },
		  {
			"title": "Popping the Hood on Golden Cove - by Chester Lam",
			"url": "https://chipsandcheese.com/p/popping-the-hood-on-golden-cove",
			"excerpts": [
			  "For address generation, Golden Cove can handle three loads and two stores per cycle. The load and store AGUs are separate on Intels diagram, unlike AMD where three AGUs handle both loads and stores.",
			  "Ideal (core width limited) instruction bandwidth for GLC/Zen 3 is 48 bytes/cycle. For Ice Lake, its 40 bytes/cycle, and for Skylake, its 32 bytes/cycle.",
			  "Golden Cove puts its floating point units behind three ports, an improvement over previous Intel architectures that only had two ports for floating point loads.",
			  "AMD uses split scheduling queues, which might be easier to implement in silicon, but requires careful tuning to make instructions for most loads are well distributed among the queues.",
			  "Golden Cove can do floating point additions with two cycle latency. Weve seen that before in low frequency designs like VIAs Nano. But Golden Cove is doing this at over 5 GHz. Thats incredible, and Intels engineers should be proud.",
			  "AMD and Intel can both do vector integer addition with 1 cycle latency. However, Golden Coves vector integer multiplier has 10 cycle latency (with packed 32-bit integers), just like older Intel CPUs. Thats far worse than Zen 3s 3 cycle latency, or Zen 2s 4 cycle latency.",
			  "Past L1, all CPUs here can read 16 bytes/cycle from L2. The average x86 instruction is 3-4 bytes long in integer code (our test with 8 byte NOPs is more applicable to very AVX-heavy code). 16 bytes/cycle is therefore enough instruction bandwidth to feed the core with 4 to 5 instructions per cycle.",
			  "For integer loads, Golden Coves extra ALU and tweaked renamer should still boost performance. But I feel Intel left some integer performance on table to stretch Golden Coves design across Alder Lake and Sapphire Rapids."
			]
		  },
		  {
			"title": "Memory-level parallelism",
			"url": "https://grokipedia.com/page/memory_level_parallelism",
			"excerpts": [
			  "The Cimple runtime system extends this by implementing an IMLP task model using coroutines that yield at long-latency memory points, multiplexing up to 50 independent requests per core to saturate hardware prefetch buffers and achieve up to 6.4 single-thread speedup in pointer-chasing tasks like binary tree lookups.",
			  "The Cimple runtime system extends this by implementing an IMLP task model using coroutines that yield at long-latency memory points, multiplexing up to 50 independent requests per core to saturate hardware prefetch buffers and achieve up to 6.4 single-thread speedup in pointer-chasing tasks like binary tree lookups.",
			  "In out-of-order (OoO) execution, memory-level parallelism (MLP) is exploited by allowing the processor to issue and track multiple independent memory operations concurrently, thereby tolerating latency from cache misses without stalling the entire pipeline. The reorder buffer (ROB) plays a central role in this mechanism, maintaining the speculative execution state of instructions while load/store queues handle the tracking of outstanding memory accesses; this setup enables non-dependent instructions to proceed even as memory requests are pending, effectively overlapping their latencies to uncover and utilize MLP. A key enabler of this parallelism is the miss status holding register (MSHR), which manages multiple unresolved cache misses by queuing details such as the requested address and associated instructions, preventing the pipeline from blocking on a single miss and allowing subsequent independent loads to be dispatched in parallel. In modern OoO cores, this capability typically sustains 8-16 outstanding misses, which can reduce stall cycles by up to 50% in memory-intensive workloads by exploiting the inherent independence among memory references.",
			  "To realize high MLP, processors require robust miss status holding registers (MSHRs) to track multiple outstanding memory requests. Conventional MSHRs support 816 entries, but workloads with sustained high MLP demand 3264 entries to avoid stalls. Banked or set-associative MSHR organizations with subentries for secondary misses (dependent requests to the same cache line) are essential to sustain bandwidth without excessive area overhead. [2] Additional techniques, such as out-of-order execution with large reorder buffers and runahead execution, further enhance MLP by allowing continued progress during memory stalls, potentially doubling performance in latency-bound scenarios.",
			  " per queue entry) and yields performance gains of up to 24% in pointer-chasing benchmarks like mst, with average IPC improvements of 11% across memory-bound workloads by increasing effective MLP from 2 to over 6 overlapping misses.",
			  "For instance, in a simulated MIPS R10000-like processor, this setup adds minimal hardware (e.g., two flag bits per queue entry) and yields performance gains of up to 24% in pointer-chasing benchmarks like mst, with average IPC improvements of 11% across memory-bound workloads by increasing effective MLP from 2 to over 6 overlapping misses.",
			  "In modern CPU architectures such as Intel and AMD x86 processors, exploiting high memory-level parallelism (MLP) significantly enhances performance by overlapping memory accesses, leading to instructions per cycle (IPC) uplifts of 20-60% in memory-bound server workloads like databases.",
			  "each core supports up to 10 outstanding memory requests, enabling effective tolerance of long latencies in server environments.",
			  "Memory-level parallelism > Applications in Modern Computing > Impact on CPU and GPU Performance",
			  "Intel and AMD x86 processors, exploiting high memory-level parallelism (MLP) significantly enhances performance by overlapping memory accesses, leading to instructions per cycle (IPC) uplifts of 20-60% in memory-bound server workloads like databases"
			]
		  },
		  {
			"title": "Measuring the memory-level parallelism of a system using a small C++ program?  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2018/11/05/measuring-the-memory-level-parallelism-of-a-system-using-a-small-c-program/",
			"excerpts": [
			  "The idea is simple: we visit N random locations in a big array.",
			  "We make sure that the processor cannot tell which location we will visit next before the previous location has been visited.",
			  "There is a data dependency between memory accesses.",
			  "Section Title: Measuring the memory-level parallelism of a system using a small C++ program?",
			  "| Intel Haswell | 7 |",
			  "| Intel Skylake | 9 |",
			  "| ARM Cortex A57 | 5 |",
			  "The pointer chasing test has the downside that it starts to produce ugly code once you run out of registers.",
			  "Our processors can issue several memory requests at the same time. In a multicore processor, each core has an upper limit on the number of outstanding memory requests, which is reported to be 10 on recent Intel processors.",
			  "Our processors can issue several memory requests at the same time. In a multicore processor, each core has an upper limit on the number of outstanding memory requests, which is reported to be 10 on recent Intel processors."
			]
		  },
		  {
			"title": "A Framework for Managing Heterogeneous Memory for ...",
			"url": "https://escholarship.org/content/qt7k32s3tv/qt7k32s3tv_noSplash_c93e1141dd2e856492f34784e85a0f7e.pdf",
			"excerpts": [
			  "In 2LM, all benchmarks were run on two NUMA nodes and assigned all 96 threads. ... extra level of pointer chasing causes a slight slowdown when embedding tables ...Read more"
			]
		  },
		  {
			"title": "What are the differences in HBM memory latency between ...",
			"url": "https://massedcompute.com/faq-answers/?question=What%20are%20the%20differences%20in%20HBM%20memory%20latency%20between%20NVIDIA%20A100%20and%20H100%20GPUs%20in%20large%20language%20model%20training?",
			"excerpts": [
			  "HBM3 vs. HBM2e: The H100 (when equipped with HBM3) offers lower latency compared to the A100's HBM2e, thanks to higher memory clock speeds and improved ...Read more"
			]
		  },
		  {
			"title": "NVIDIA A100 Tensor Core GPU Architecture",
			"url": "https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf",
			"excerpts": [
			  "latency benefits of A100's new L2 cache. Each L2 cache partition is divided into 40 L2 cache slices. Eight 512 KB L2 slices are associated with each memory ...Read more",
			  "New asynchronous copy instruction loads data directly from global memory into shared\nmemory, optionally bypassing L1 cache, and eliminating the need for intermediate\nregister file (RF) usage",
			  "the NVIDIA A100 GPU has 40 GB of high-speed\nHBM2 memory with a class-leading 1555 GB/sec of memory bandwidth",
			  "The A100 L2 cache is a shared resource for the GPCs and SMs and lies outside of the GPCs.",
			  "The global and local memory areas accessed by CUDA programs reside in HBM2 memory\nspace, and is referred to as device memory in CUDA parlance. Constant memory space resides\nin device memory and is cached in the constant cache.",
			  "A100 L2 cache residency controls help applications reduce DRAM bandwidth. This example shows\ndifferent data buffers highlighted with colors to indicate data that has been marked for persistent caching\nin L2.",
			  "A100 Compute Data Compression improves DRAM bandwidth, L2 bandwidth, and L2 capacity."
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Optimization Reference ...",
			"url": "https://cdrdv2-public.intel.com/779559/355308-Software-Optimization-Manual-048-Changes-Doc-2.pdf",
			"excerpts": [
			  "This document is an update to the optimization recommendations contained in the Intel 64 and IA-32. Architectures Optimization Reference Manual, also known as ...Read more"
			]
		  },
		  {
			"title": "Iterating over linked list in C++ is slower than in Go - Stack Overflow",
			"url": "https://stackoverflow.com/questions/50274433/iterating-over-linked-list-in-c-is-slower-than-in-go",
			"excerpts": [
			  "Section Title: [Iterating over linked list in C++ is slower than in Go](/questions/50274433/iterating-over-linked-list-in-c-is-slower-than-in-go) > 3 Comments",
			  "Hey, thanks for the reply! Unfortunately I'm not able to replicate your results; gcc runs as slow for me as clang++. Are you running something like `gcc -std=c++11 -stdlib=libc++ -lstdc++ minimal.cpp -O3; ./a.out` ?",
			  "I have compiled with: g++ --std=c++11 -O3 -lstdc++"
			]
		  },
		  {
			"title": "Understanding CPU port contention. | Easyperf ",
			"url": "https://easyperf.net/blog/2018/03/21/port-contention",
			"excerpts": [
			  "In my IvyBridge CPU I have 2 ports for executing loads, meaning that we can schedule 2 loads at the same time.",
			  "We can see that our 16 loads were scheduled equally between PORT2 and PORT3, each port takes 8 uops.",
			  "On modern x86 processors load instruction takes at least 4 cycles to execute even the data is in the L1-cache.",
			  "So, we will have 16 reads of 4 bytes.",
			  "the throughput is what matters (as Travis mentioned in his comment).",
			  "There will be still 2 retired load instructions per cycle.",
			  "There will be still 2 retired load instructions per cycle.",
			  "We can see that all `bswap` instructions nicely fit into the pipeline causing no hazards."
			]
		  },
		  {
			"title": "Skylake: Intel's Longest Serving Architecture",
			"url": "https://chipsandcheese.com/p/skylake-intels-longest-serving-architecture",
			"excerpts": [
			  "Client Skylakes port layout is similar to Haswells. There are four integer execution ports, two general purpose AGU ports, and a store-only AGU port. Three of the integer execution ports, numbered 0, 1, and 5, handle floating point and vector execution as well.",
			  "Even though port layout wasnt changed, Intel did beef up floating point and vector execution capabilities.",
			  "On the floating point side, latency for floating point adds, multiplies, and fused multiply adds are all standardized at four cycles and two per cycle throughput, suggesting the fused multiply add (FMA) units are handling all of those operations.",
			  "On both architectures, stores crossing a 64 byte cacheline take two cycles to complete.",
			  "Loads that cross a cacheline boundary can often complete in a single cycle, likely because the L1 data cache has two load ports."
			]
		  },
		  {
			"title": "\n\tHaswell L2 cache bandwidth to L1 (64 bytes/cycle)? - Intel Community\n",
			"url": "https://community.intel.com/t5/Software-Tuning-Performance/Haswell-L2-cache-bandwidth-to-L1-64-bytes-cycle/m-p/1014866",
			"excerpts": [
			  "The Haswell core can only issue two loads per cycle, so they have to be 256-bit AVX loads to have any chance of achieving a rate of 64 Bytes/cycle.",
			  "There are very clearly only 2 load units (ports 2 and 3 shown in Figure 2-1 of the Intel Architectures Optimization Manual, document 248966-030, September 2014), so only two load uops can be executed per cycle.",
			  "The biggest load payload is 256 bits == 32 Bytes, and 64 Bytes / 2 loads = 32 Bytes per load.",
			  "There is absolutely no doubt that you can only move 64 Bytes per cycle into registers by using AVX instructions.  There are very clearly only 2 load units (ports 2 and 3 shown in Figure 2-1 of the Intel Architectures Optimization Manual, document 248966-030, September 2014), so only two load uops can be executed per cycle.   The biggest load payload is 256 bits == 32 Bytes, and 64 Bytes / 2 loads = 32 Bytes per load.",
			  "Then the next interrupt occurs and the cycle starts over again. The new version of the Optimization Reference manual (248966-037, Table 2-1) says that the Skylake Server processor has twice the maximum L2 bandwidth of the Broadwell processor -- 64 Bytes/cycle \"max\" instead of 32, with 52 Bytes/cycle \"sustained\" instead of 25.",
			  "Haswell L2 cache bandwidth to L1 (64 bytes/cycle)?",
			  "If you add a second read to the above loop, to the same cache line, even to the same location, I can never get better than 2 cycles per iteration (I get close, about 2.05 cycles). So it seems the L1D can satisfy a load from L2, and a load from the core, but not a *second* load from the core in the same cycle."
			]
		  },
		  {
			"title": "Basics on NVIDIA GPU Hardware Architecture - HECC Knowledge Base",
			"url": "https://www.nas.nasa.gov/hecc/support/kb/basics-on-nvidia-gpu-hardware-architecture_704.html",
			"excerpts": [
			  "\nThese differences speak to the fact that GPUs are designed to maximize throughput instead of minimizing latency. The high throughput is realized via a large number of registers and the use of high bandwidth memory. Instead of minimizing latency, GPUs rely on hiding latency through asychronous SIMT operations.",
			  "There tend to be more layers of caches in CPUs than GPUs. For example, the Intel Skylake, Intel Cascade Lake, AMD Rome, AMD Milan, and NVIDIA Grace CPU processors all have three levels of caches: L1, L2, and L3.On the other hand, the NVIDIA V100, A100, and H100 only have L1 and L2 caches.",
			  "The latency of L2 cache in GPU (shared by SMs) is approximately 200 cycles as reported by independent benchmarking works for V100 and A100 listed in the Reference section.",
			  "A warp is a group of 32 threads within a thread block that executes the same instruction simultaneously. These threads are scheduled in batches by a warp scheduler to execute on an SM.",
			  "Latency of device memory is between about 200 to 1000 cycles, as reported by independent benchmarking work for the V100 and A100, listed in the Reference section.",
			  "The size of a thread block is set by an application. However, it should be a multiple of 32 due to the warp scheduling. A common choice of a thread block size is 256.",
			  "Note: In concept, executing a warp (32 threads) on multiple cores of an SM is similar to executing vector instructions (using xmm/ymm/zmm registers on 4/8/16 data elements in a vector) on a CPU core.",
			  "The GPU memory hierarchy does not look like a pyramid. Specifically, as shown below, the L1 cache size can be smaller than the register size.",
			  "The latency of L1/shared memory is approximately 20 - 30 cycles as reported by independent benchmarking works listed in the Reference section. In comparison, L1 latency in CPUs is roughly five cycles.",
			  "L2 cache keeps data read/written from/to device memory to service requests from SMs in the GPU."
			]
		  },
		  {
			"title": "Nvidia's H100: Funny L2, and Tons of Bandwidth",
			"url": "https://chipsandcheese.com/p/nvidias-h100-funny-l2-and-tons-of-bandwidth",
			"excerpts": [
			  "H100s L2 cache feels like a two-level setup rather than a single level of cache. A thread running on H100 can access the far L2 cache a bit faster than on A100, so Nvidia has improved compared to the prior generation",
			  "With five stacks of HBM2e, the H100 can pull just short of 2 TB/s from VRAM.",
			  "H100s VRAM bandwidth will be very useful for massive working sets without cache-friendly access patterns.",
			  "Consumer GPUs in recent years have moved towards maintaining good performance when faced with small workloads."
			]
		  },
		  {
			"title": "Comparing NVIDIA H100 vs A100 GPUs for AI Workloads | OpenMetal IaaS",
			"url": "https://openmetal.io/resources/blog/nvidia-h100-vs-a100-gpu-comparison/",
			"excerpts": [
			  "The H100s combination of FP8 support and HBM3 memory allows it to handle more concurrent inference requests with reduced latency. This is particularly important for real-time applications like chat assistants, code generation tools, [fraud detection](/resources/blog/big-data-for-fraud-detection-a-guide-for-financial-services-and-e-commerce/) systems, and other latency-sensitive inference pipelines.",
			  "A100** : Equipped with 40 GB or 80 GB of HBM2e memory, it offers up to 2 TB/s memory bandwidth.",
			  "H100** : Comes with 80 GB of HBM3 memory, delivering up to 3.35 TB/s memory bandwidth.",
			  "This memory improvement in the H100 supports higher batch sizes, larger model inference, and more concurrent user sessions.",
			  "| Feature | A100 (Ampere) | H100 (Hopper) |\n| Release Year | 2020 | 2022 |\n| Memory | 40 GB or 80 GB HBM2e | 80 GB HBM3 |\n| Memory Bandwidth | Up to 2 TB/s | Up to 3.35 TB/s |\n| MIG Support | Yes (up to 7 instances) | Yes (2nd Gen MIG, 7 instances) |\n| Transformer Engine | No | Yes (FP8 support) |\n| Training Performance | Baseline | Up to 2.4x faster |\n| Inference Throughput | ~130 tokens/second | 250300 tokens/second |\n| Latency | Moderate | Lower |\n| Power Efficiency | Good | Higher |"
			]
		  },
		  {
			"title": "NVIDIA Ampere Architecture In-Depth | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/",
			"excerpts": [
			  "The combined capacity of the L1 data cache and shared memory is 192 KB/SM in A100 vs. 128 KB/SM in V100. Simultaneous execution of FP32 and ...Read more"
			]
		  },
		  {
			"title": "assembly - How to interpret uops.info? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/71418230/how-to-interpret-uops-info",
			"excerpts": [
			  "So 0.5 means it can run once per 0.5 cycles, i.e. 2/clock, as expected for CPUs that we know have 2 load ports.Read more"
			]
		  },
		  {
			"title": "Pipeline of Intel Core CPUs",
			"url": "https://www.uops.info/background.html",
			"excerpts": [
			  "Each port can accept at most one op in every cycle. However, as most functional units are fully pipelined, a port can typically accept a new op in every cycle ...Read more"
			]
		  },
		  {
			"title": "Question about Skylake Execution Unit Ports : r/simd",
			"url": "https://www.reddit.com/r/simd/comments/9ztifk/question_about_skylake_execution_unit_ports/",
			"excerpts": [
			  "My understanding is that Port 7 is an AGU like Port 2/3 has (Port 2/3 also performs loads themselves), but can only be used for for store operations.Read more"
			]
		  },
		  {
			"title": "The penalty is quite small, 1-3 cycles each direction. RAM latency is 1-2 orders... | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24062677",
			"excerpts": [
			  "L1D is not many cycles away: it is 4 or 5 for scalar loads, 6 or 7 for xmm or ymm loads. If the load misses, it doesn't much matter if it's a ..."
			]
		  },
		  {
			"title": "Going Armchair Quarterback on Golden Cove's Caches",
			"url": "https://chipsandcheese.com/p/going-armchair-quarterback-on-golden-coves-caches",
			"excerpts": [
			  "On top of that, M1's L1D has 3 cycle latency. It's like a better version of Phenom's L1D. We saw earlier how much potential that had ..."
			]
		  },
		  {
			"title": "performance - Load/stores per cycle for recent CPU architecture generations - Stack Overflow",
			"url": "https://stackoverflow.com/questions/45106951/load-stores-per-cycle-for-recent-cpu-architecture-generations",
			"excerpts": [
			  "**Sandy/Ivy** : per cycle, 2 loads, or 1 load and 1 store. 256bit loads and stores count double, but only with respect to the load or store itself - it still only has one address so the AGU becomes available again the next cycle. By mixing in some 256b operations you can still get 2x 128b loads and 1x 128b store per cycle.",
			  "**Haswell/Broadwell** : 2 loads *and* a store, and 256bit loads/stores *don't* count double. Port 7 (store AGU) can only handle *simple* address calculations (base+const, no index), complex cases will go to p2/p3 and compete with loads, simple cases may compete anyway but at least don't *have* to.",
			  "**Sky/Kaby** : the same as Broadwell",
			  "**Ice/Tiger Lake** : 2 loads and 2 stores per clock, with fully separate execution units for each (store-address uops don't run on load ports.) 2/clock stores can only be sustained if stores are to the same cache line. i.e. 1/clock write to L1d cache, but a write can commit two store-buffer entries if they're to the same line. For memory-ordering reasons, the two store-buffer entries have to be the two oldest, so alternating stores to two separate arrays couldn't benefit from this unless you can unroll.",
			  "**Alder Lake / Sapphire Rapids** : 3 loads and 1 store, or 2 loads and 2 stores. Agner Fog reports those throughputs for sizes up to 128-bit, but \"somewhat less\" for 256 and 512-bit loads/stores. Commit to L1d may be limited like Ice Lake for more than 1 store per clock.",
			  "**Bulldozer** : 2 loads, or 1 load and 1 store. 256bit loads and stores count double.",
			  "e.\n**Jaguar** : 1 load or 1 store, and 256bit loads and stores count double. By far the worst one in this list, because it's the only low-power arch in the list.",
			  "**Zen 1** (first-gen Ryzen): 2 loads, or 1 load and 1 store. 256bit loads and stores count double.",
			  "[**Zen 2**](https://en.wikichip.org/wiki/amd/microarchitectures/zen_2) (Most Ryzen 3xxx and 4xxx, but there are some 3xxx models that are only Zen+ not Zen 2).",
			  "3 AGUs (2 load/store, 1 store-only). Up to two 256-bit load operations and one 256-bit store per cycle.",
			  "[**Zen 3**](https://en.wikichip.org/wiki/amd/microarchitectures/zen_3) : Load throughput increased from 2 to 3 for scalar integer GPRs.",
			  "Store throughput increased from 1 to 2 for scalar integer GPRs. (Wikichip incorrectly states this as \"if not 256-bit\", but https://uops.info/ testing [confirms only 1/clock](https://uops.info/html-tp/ZEN3/VMOVAPS_M128_XMM-Measurements.html) vector stores even with 128-bit `vmovaps [mem], xmm` .",
			  "zen_4)** : no change from Zen 3. AVX-512 512-bit ops are single-uop, but occupy load and store-data units for 2 cycles each, like how Sandy/Ivy Bridge handled 256-bit load/store. (Same for 512-bit ALU uops, single uop unlike how Zen 1 handled 256-bit.)\n"
			]
		  },
		  {
			"title": "3. The microarchitecture of Intel, AMD, and VIA CPUs",
			"url": "https://www.agner.org/optimize/microarchitecture.pdf",
			"excerpts": [
			  "The width of the memory ports has been doubled relative to previous processors. The\nmaximum throughput to the level-1 cache is now two 32-byte reads and one 32-byte write\nper clock. This makes it possible to copy a block of memory at a speed of 32 bytes per clock\ncycle.",
			  "**Cache**\n**Skylake**\nop cache\n1536 ops, 8 way, 6 op line size, per core\nLevel 1 code\n32 kB, 8 way, 64 sets, 64 B line size,\nlatency 4, per core\nLevel 1 data\n32 kB, 8 way, 64 sets, 64 B line size,\nlatency 4, per core\nLevel 2\n256 kB - 1MB, 4 - 16 ways, 1024 sets, 64 B line\nsize, latency 14, per core.\nLevel 3\n3-24 MB, 64 B line size, latency 34-85, shared\n**Table 11.2. Cache sizes on Skylake**",
			  "The 256-bit or 512-bit read and write bandwidth (see p. 144) makes it advantageous to use\nYMM or ZMM registers for copying or zeroing large blocks of memory. The REP MOVS\n ..."
			]
		  },
		  {
			"title": "NVIDIA ADA GPU ARCHITECTURE",
			"url": "https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf",
			"excerpts": [
			  "The Ada SM contains 128 KB of Level 1 cache. This cache features a unified architecture that can\nbe configured to function as an L1 data cache or shared memory depending on the workload.",
			  "AD102 has been\noutfitted with 98304 KB of L2 cache, an improvement of 16x over the 6144 KB that shipped in\nGA102. All applications will benefit from having such a large pool of fast cache memory available,\nand complex operations such as ray tracing (particularly path tracing) will yield the greatest\nbenefit."
			]
		  },
		  {
			"title": "1. NVIDIA Ada GPU Architecture Tuning Guide  Ada Tuning Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/ada-tuning-guide/index.html",
			"excerpts": [
			  " the portion of the L1 cache dedicated to shared memory (known as the *carveout* ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using `cudaFuncSetAttribute()` with the attribute `cudaFuncAttributePreferredSharedMemoryCarveout` . T",
			  ".\nCUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. T",
			  "nt:\nLike the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp."
			]
		  },
		  {
			"title": "Decomposing BVH to accelerate traversal - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/decomposing-bvh-to-accelerate-traversal/283911",
			"excerpts": [
			  "If I have a large BVH, can I speed up the BVH traversal through decomposing this BVH into multiple smaller BVHs and accessing these GAS handles at the same ...Read more"
			]
		  },
		  {
			"title": "algorithm - Optimizing BVH Traversal with GPU - Stack Overflow",
			"url": "https://stackoverflow.com/questions/25703174/optimizing-bvh-traversal-with-gpu",
			"excerpts": [
			  "Traversal is the single most expensive computation for my program right now and it prevents large scenes (>2k triangles) from running at acceptable frame rates.",
			  "I am at a loss as to how it can be performed faster.",
			  "I created a bounding volume hierarchy that is generated every frame. Due to it's use, each node must have two children, no more, no less.",
			  "To traverse it I used the concepts provided by the paper at this [link](https://graphics.cg.uni-saarland.de/fileadmin/cguds/papers/2011/hapala_sccg2011/hapala_sccg2011.pdf) , where each thread traverses the following code.",
			  "I tried to minimize the memory accesses within the context of the algorithm, however the divergence, which I assume to be the problem, seems an impossible hurdle to overcome."
			]
		  },
		  {
			"title": "c++ - Optimizing BVH traversal in ray tracer - Stack Overflow",
			"url": "https://stackoverflow.com/questions/61222620/optimizing-bvh-traversal-in-ray-tracer",
			"excerpts": [
			  "Sticking all the nodes in a contiguous array gave around a 2% speedup on average over a bunch of runs, thanks!",
			  "You should definitively keep your node in an array or use some kind of memory pool, this will indeed improve your performance w.r.t. the cache lines.",
			  "You should definitively keep your node in an array or use some kind of memory pool, this will indeed improve your performance w.r.t. the cache lines. Regarding the speed of float versus double, it's really a tricky question and really depends on what you are doing. If you are missing a lot of L1 cache then double will cost you more than float as the hardware has to load more data into the register. If you decide to use SSE then float is a real advantage because the CPU can crunch 4 operations for float whereas you only have two for double.",
			  "Also this makes me think: are you using any mathematical function from the std into your intersection code? If so, you should really consider changing them to less robust but faster version of them (I'm mostly thinking of exp and sqrt).",
			  "By far the most common intersection is the BBox intersection which only uses really uses *,-,+,<,>. The other intersection functions (triangle and sphere) do use more complicated functions (division, sqrt) but only make up 7% of execution time combined, so I haven't bothered to optimize them yet.",
			  "VX! With a bit of refactoring in your intersection code, you could intersect four bounding boxes at once and therefore have nice boost in your application. You could have a look at what [embree](https://github.com/embree/embree) does (the intel tracer) especially in the math "
			]
		  },
		  {
			"title": "How to speed up the traversal speed of BVH? - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/how-to-speed-up-the-traversal-speed-of-bvh/256481",
			"excerpts": [
			  "Are there any factors that can affect the traversal speed of BVH? For example, the size of the triangles, the distance between triangles, the density of triangles or the length of rays. Thank you!",
			  "Yes, please read this thread and take note of the AS compaction and different AS build flags. Follow the links to them inside the OptiX Programming Guide.",
			  "Then read this thread again because it touches on the acceleration structures and geometry flags as well:",
			  "Try to avoid nesting AABBs of things inside each other.",
			  "If you just have independent triangles in a single GAS, that might not affect you. If the geometric primitives penetrate each other or if there are huge differences in their AABB extents so that AABB overlap a lot, that would affect traversal performance,",
			  "You would need to benchmark that yourself for your specific use case.",
			  "The ray [tmin, tmax] interval length could affect the traversal performance greatly.",
			  "Obviously the shorter the rays are, compared to your scene size, the fewer AABB can be intersected.",
			  "This might also be interesting when considering how to implement the individual device programs.",
			  "https://forums.developer.nvidia.com/t/relationship-of-nvidia-optix-7-programs/251690"
			]
		  },
		  {
			"title": "How is the triangle vertex data of BVH arranged in memory? - Visualization / OptiX - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/how-is-the-triangle-vertex-data-of-bvh-arranged-in-memory/289903",
			"excerpts": [
			  "OptiX has some flags which influence how the acceleration structure is built.",
			  "Specifically [OPTIX_BUILD_FLAG_PREFER_FAST_TRACE](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html) and [OPTIX_BUILD_FLAG_PREFER_FAST_BUILD](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html)",
			  "and the [OPTIX_GEOMETRY_FLAG_REQUIRE_SINGLE_ANYHIT_CALL](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html)",
			  "Described here: [https://raytracing-docs.nvidia.com/optix8/guide/index.html_structures](https://raytracing-docs.nvidia.com/optix8/guide/index.html)\nThen there",
			  "Described here: [https://raytracing-docs.nvidia.com/optix8/guide/index.html_structures](https://raytracing-docs.nvidia.com/optix8/guide/index.html)\nThen there",
			  "Then there is acceleration structure compaction which will improve memory locality of the final AS data.",
			  "Also there is the [OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS](https://raytracing-docs.nvidia.com/optix8/api/group__optix__types.html) flag which allows to read vertex positions from the AS which can affect performance negatively.",
			  "Is it arranged according to spatial location or just according to the BVH structure?",
			  "The BVH structure depends on the spatial location of the primitives and their sizes."
			]
		  },
		  {
			"title": "How to build a BVH  Part 1: Basics - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/04/13/how-to-build-a-bvh-part-1-basics/",
			"excerpts": [
			  "In this first article we discuss the bare basics: what the BVH is used for, and how a basic BVH is constructed and traversed with a ray.Read more"
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/2601097.2601222",
			"excerpts": [
			  "an efficient algorithm for traversing large packets of rays against a bounding volume hierarchy in a way that groups coherent rays during traversal.",
			  "In contrast to previous large packet traversal methods, our algorithm allows for individual traversal order for each ray, which is essential for efficient ray tracing.",
			  "our new technique increases traversal performance by 36--53%, and is applicable to most ray tracers."
			]
		  },
		  {
			"title": "A State-of-the-Art Performance Withstanding the Test-of-Time - ACM SIGGRAPH Blog",
			"url": "https://blog.siggraph.org/2025/12/a-state-of-the-art-performance-withstanding-the-test-of-time.html/",
			"excerpts": [
			  "y Tracing](https://dl.acm.org/doi/10.1145/2601097.2601199) was awarded a SIGGRAPH 2025 Test-of-Time Award, given to Technical Papers that have had a significant and lasting impact on the computer graphics and interactive techniques over the last decade. This paper introduces the Embree system and describes in detail what it takes to build a professional-grade modular ray tracing framework. It achieved state-of-the-art performance with a simple and elegant architecture and came with an open-source implementation that has been adopted by many since then."
			]
		  },
		  {
			"title": "DUAL STREAMING FOR HARDWARE-ACCELERATED RAY ...",
			"url": "https://radiantflux.net/data/PHDCG_19/KShkurko_PhdThesis.pdf",
			"excerpts": [
			  "The scene stream consists of multiple *scene segments* that collectively represent the entire\nscene geometry data and that can be processed independently.",
			  "Each scene segment is\norganized in memory so that all of its nodes and geometry can be accessed sequentially to\nimprove the spatial locality of the data.",
			  "rior nodes use\n64 bytes, while the leaf nodes use 8 bytes.",
			  "The ray stream is the collection of all rays that are in flight, split into multiple queues,\none per scene segment.",
			  "The ray bucket header stores the number of rays contained within and a\npointer (a memory address) to the next ray bucket in the list.",
			  "Because each ray uses the\nsame number of words for storage, we can treat each ray bucket as an array rather than a",
			  "The interior nodes use\n64 bytes, while the leaf nodes use 8 bytes. The interior nodes are larger because they use\n48 bytes extra to store the AABBs for their child",
			  "\nStoring scene segments changes more significantly, Figure 5.18d. The compressed rep-\nresentation adds a 7 word header in the beginning, which stores the uncompressed AABB\nfor the scene segment (6 words) and an address for the first triangle in the scene segment (1\n ",
			  ". The compressed rep-\nresentation adds a 7 word header in the beginning, which stores the uncompressed AABB\nfor the scene segment (6 words) and an address for the first triangle in the scene segment (1\n ."
			]
		  },
		  {
			"title": "GitHub - jbikker/tinybvh: Single-header dependency-free BVH construction and traversal library.",
			"url": "https://github.com/jbikker/tinybvh",
			"excerpts": [
			  "`BVH::Build` : Efficient plain-C/C+ binned SAH BVH builder which should run on any platform.",
			  "`BVH_GPU` : This format uses 64 bytes per node and stores the AABBs of the two child nodes. This is the format presented in the [2009 Aila & Laine paper](https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf) . It can be traversed with a simple GPU kernel.",
			  "`BVH4_GPU` : A compact version of the `BVH4` format, which will be faster for GPU ray tracing.",
			  "`BVH4_CPU` : SSE-optimzied wide BVH traversal. The fastest option for CPUs that do not support AVX.",
			  "GPU-friendly layouts, including 'Compressed Wide BVH' (CWBVH) for state-of-the-art GPU performance",
			  "Wide BVHs (any width) using collapsing",
			  "A constructed BVH can be used to quickly intersect a ray with the geometry, using `BVH::Intersect` or `BVH::IsOccluded` , for shadow rays.",
			  "The 32-byte size allows for cache-line alignment."
			]
		  },
		  {
			"title": "tinybvh  Manual  Advanced - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2025/01/25/tinybvh-manual-advanced/",
			"excerpts": [
			  "`BVH4_GPU` : This layout uses a wide (and thus shallow) BVH to speedup traversal of divergent rays. The reduced number of traversal steps improves memory coherency at the expense of more calculations per step, which is a good trade-off in most cases. Conversion is however more expensive than for the `BVH_GPU` layout.",
			  "`CWBVH` : This is the final layout proposed by NVIDIA before introducing their RTX hardware. CWBVH uses an 8-wide BVH with heavily compressed nodes. Additionally, triangle data is stored without any indirection interleaved with the nodes for optimal access. This is a fast layout, but obtaining it is relatively costly. This is mostly useful for static geometry. Also note that the traversal kernel is complex; it requires specialized bit manipulation instructions to run at full speed. Porting this code will be challenging.",
			  "`BVH_GPU` : This layout follows the recommendations in a 2009 paper by Aila & Laine. This BVH layout is quickly obtained from the default BVH layout and offers reasonable ray tracing performance. The traversal kernel is short and simple and can easily be implemented in any shader or GPGPU language.",
			  "`BVH_GPU` : This layout follows the recommendations in a 2009 paper by Aila & Laine. This BVH layout is quickly obtained from the default BVH layout and offers reasonable ray tracing performance. The traversal kernel is short and simple and can easily be implemented in any shader or GPGPU language.",
			  "The manual is split up in multiple compact documents:\n[Basic Use. Building a BVH, tracing a ray, alternative layouts, TLAS/BLAS](https://jacco.ompf2.com/2025/01/24/tinybvh-manual-basic-use/) .\n**Advanced Topics (this document).** GPU ray tracing, custom math libraries, tinybvh settings."
			]
		  },
		  {
			"title": "How to build a BVH  part 2: Faster Rays - Jacco's Blog",
			"url": "https://jacco.ompf2.com/2022/04/18/how-to-build-a-bvh-part-2-faster-rays/",
			"excerpts": [
			  "To make better use of the caches, we should ensure that data that we use is similar to data we have recently seen. This is known as *temporal data locality* . In a ray tracer this can be achieved by rendering the image in tiles.",
			  "The only thing that we can do is shorten the ray: Any hit closer than ray.t reduces ray.t, and if that means that the entry point to another AABB is now out of reach, we can cull that AABB."
			]
		  },
		  {
			"title": "Ray Tracing Acceleration with Near-memory Computing",
			"url": "https://dl.acm.org/doi/full/10.1145/3725843.3756067",
			"excerpts": [
			  "Ray tracing generates realistic images, but even with specialized hardware support for memory traversal ... Ray Tracing Gems II. Apress ...Read more"
			]
		  },
		  {
			"title": "SIGGRAPH 2019: Ray Tracing Gems 1.1 - Cinematic ...",
			"url": "https://developer.nvidia.com/siggraph/2019/video/sig935-vid",
			"excerpts": [
			  "The new book \"Ray Tracing Gems\" (http://raytracinggems.com, free electronically) is a collection of 32 articles by experts in the field. Authors of selected ..."
			]
		  },
		  {
			"title": "HLBVH: Hierarchical LBVH Construction for Real-Time ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2010-06_HLBVH-Hierarchical-LBVH/HLBVH-final.pdf",
			"excerpts": [
			  "As future work, the authors suggest the possibility of exploiting this scheme in the context of BVH construction by sorting the primitives along a Morton curve,.Read more"
			]
		  },
		  {
			"title": "Ray Tracing Gems - II (Preview) | NVIDIA Developer",
			"url": "https://developer.nvidia.com/ray-tracing-gems-ii/preview",
			"excerpts": [
			  "Section Title: Ray Tracing Gems II - Chapter Previews\nContent:\nRendering experts come together to unearth true gems for developers of games, architectural applications, visualization, and more in this exciting era of real-time rendering.\nEvery Wednesday in July, NVIDIA Developer's can access pre-print PDFs of full chapters from the upcoming book Ray Tracing Gems II available free for download."
			]
		  },
		  {
			"title": "3rd Installment of Ray Tracing Gems Now Available For Free",
			"url": "https://developer.nvidia.com/blog/3rd-installment-of-ray-tracing-gems-now-available-for-free/",
			"excerpts": [
			  "Ray tracing provides developers with an organic, photoreal solution to crafting reflections, refractions, and shadows."
			]
		  },
		  {
			"title": "Geometry Query Optimizations in CAD-based Tessellations ...",
			"url": "https://asset.library.wisc.edu/1711.dl/NNODVI4XHMSOV8F/R/file-4a60b.pdf",
			"excerpts": [
			  "Nodes in Embrees BVH are stored com-\npactly using an encoding scheme which contains the memory address of\nbox information as an integer value. Because nodes are byte-aligned, mem-\nory addresses will be offset by the same amount as the node alignment. For\nexample, if nodes are memory aligned to 16 bytes, the four least significan",
			  "Embree applies Pointer-Based prefetching while traversing its BVH.\nAs nodes intersected by the ray are added to the stack, the bounding box\ninformation for the node at the top of the stack is prefetched for intersection\nin the next step of the BVH traversal."
			]
		  },
		  {
			"title": "Thinking Parallel, Part II: Tree Traversal on the GPU | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/thinking-parallel-part-ii-tree-traversal-gpu/",
			"excerpts": [
			  "Section Title: Thinking Parallel, Part II: Tree Traversal on the GPU > Bounding Volume Hierarchy []()",
			  "We will build our approach around a [bounding volume hierarchy](http://en.wikipedia.org/wiki/Bounding_volume_hierarchy) (BVH), which is a commonly used acceleration structure in [ray tracing](https://developer.nvidia.com/discover/ray-tracing) (for example).",
			  "A bounding volume hierarchy is essentially a [hierarchical grouping](https://developer.nvidia.com/discover/cluster-analysis) of 3D objects, where each group is associated with a conservative bounding box.",
			  "Suppose we have eight objects, O 1 -O 8 , the green triangles in the figure above. In a BVH, individual objects are represented by leaf nodes (green spheres in the figure), groups of objects by internal nodes (N 1 -N 7 , orange spheres), and the entire scene by the root node (N 1 ).",
			  "Each internal node (e.g. N 2 ) has two children (N 4 and N 5 ), and is associated with a bounding volume (orange rectangle) that fully contains all the underlying objects (O 1 -O 4 ).",
			  "The bounding volumes can basically be any 3D shapes, but we will use axis-aligned bounding boxes (AABBs) for simplicity.",
			  "Our overall approach is to first construct a BVH over the given set of 3D objects, and then use it to accelerate the search for potentially colliding pairs. We will postpone the discussion of efficient hierarchy construction to the third part of this series. For now, lets just assume that we already have the BVH in place"
			]
		  },
		  {
			"title": "Minimizing Ray Tracing Memory Traffic through Quantized ...",
			"url": "https://jo.dreggn.org/home/2025_quant.pdf",
			"excerpts": [
			  "Traversing a BVH8 with ray streams entails additional algorithmic complexity, with the promise to amortize quantization data over more child ...Read more"
			]
		  },
		  {
			"title": "Classifying Memory Access Patterns for Prefetching",
			"url": "https://hlitz.github.io/papers/asplos2020.pdf",
			"excerpts": [
			  "Linearizing irregular memory accesses for improved correlated prefetching. ... Prefetching with helper threads for loosely coupled multiprocessor systems.Read more"
			]
		  },
		  {
			"title": "DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations",
			"url": "https://arxiv.org/html/2506.22849v1",
			"excerpts": [
			  "Our BVH traversal is based on a SIMD stack-based traversal loop executed on the GPU.",
			  "DOBB, a BVH that leverages discrete rotations and shared OBB transforms at the hierarchy node level to improve traversal performance in ray tracing.",
			  "The ray is then tested for intersection against all child nodes.",
			  "The input ray is transformed into the rotated space prior to intersection with all internal node children.",
			  "Additionally, we leverage hardware-accelerated ray-matrix transformations to enable ray-OBB intersection."
			]
		  },
		  {
			"title": "A Study of Persistent Threads Style GPU Programming for ...",
			"url": "https://www.classes.cs.uchicago.edu/archive/2016/winter/32001-1/papers/AStudyofPersistentThreadsStyleGPUProgrammingforGPGPUWorkloads.pdf",
			"excerpts": [
			  "Aila\net al. present a thorough study of improving the e ffi ciency\nof processing the irregular the *trace()* phase in ray tracing\non vector processors through PT [1].",
			  "warp-wide blocks for avoiding a single ray holding several\nwarps within a block hostage 2 ,and bypassing the hardware\nscheduler by the use of PT."
			]
		  },
		  {
			"title": "Software Prefetches - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/software-prefetches",
			"excerpts": [
			  "Software prefetching faces challenges with irregular pointer-based structures due to the pointer-chasing problem, which limits early prefetch initiation because the address of future nodes cannot be determined without traversing intermediate nodes.",
			  "Greedy prefetching is a technique for [linked data](../computer-science/linked-data) structures, inserting prefetch instructions for successor nodes, but its effectiveness is limited by the inability to overlap prefetch latency with more than a [single iteration](../computer-science/single-iteration) .",
			  "Jump pointer prefetching and prefetch arrays are used to address early node prefetching in linked lists.",
			  "Natural Pointer Techniques The simplest software prefetching technique for LDS traversal is greedy prefetching, proposed by Luk and Mowry .",
			  "Greedy prefetching is attractive due to its simplicity. However, its ability to properly time prefetch initiation is limited. For the linked-list traversal in Figure 3.25 (a) , each prefetch overlaps with a single-loop iteration only."
			]
		  },
		  {
			"title": "The pros and cons of explicit software prefetching - Johnny's Software Lab",
			"url": "https://johnnysswlab.com/the-pros-and-cons-of-explicit-software-prefetching/",
			"excerpts": [
			  "Pointer chasing codes (linked lists or binary trees) need to be rewritten in order to benefit from prefetching. Here is the example binary tree lookup:",
			  "This code has a instruction dependency on memory loads. In order to prefetch, we need to know a few memory addresses in advance. However, with these type of codes this is not possible: we dont know the address of the next node until we have loaded the current node and performed the comparison.",
			  "Issuing a prefetch request immediately before accessing a variable doesnt work.",
			  "Luckily, there is a solution. We already covered the techniques which are aimed at braking dependency chains or interleaving several dependency chains in our post about [instruction level parallelism](https://johnnysswlab.com/instruction-level-parallelism-in-practice-speeding-up-memory-bound-programs-with-low-ilp/) . Techniques presented there combine nicely with explicit software prefetching. Lets take our binary tree example and perform 16 parallel searches:",
			  "This code performs 16 parallel searches. We can easily add two prefetches like this:",
			  "This will cause the CPU to prefetch the correct child. If we were running one search in parallel, the access to this child would be immediately after the prefetch and prefetching wouldnt make sense.",
			  "With 16 parallel searches, there is enough time for the hardware to prefetch the child before it is being accessed."
			]
		  },
		  {
			"title": "Wide BVH Traversal with a Short Stack",
			"url": "https://www.embree.org/papers/2019-HPG-ShortStack.pdf",
			"excerpts": [
			  "Applying our algorithm to wide BVHs, we demonstrate that the number of traversal steps with just five stack entries is close to that of a full traversal stack."
			]
		  },
		  {
			"title": "Bf-Tree: A Modern Read-Write-Optimized Concurrent Range ...",
			"url": "https://vldb.org/pvldb/vol17/p3442-hao.pdf",
			"excerpts": [
			  "**Figure 2: High level architecture of** **Bf-Tree** **.** Like conventional\nB-Tree, but pages in the buffer pool are variable lengths.",
			  "The key insight of this paper is that we can *separate cache pages*\n*from disk pages* , i.e., the cache pages are no longer a mirror of their\ndisk content. Instead, they contain a judiciously chosen subset of\nthe disk page that is worth caching. Called *mini-pages* , these cached\npages are a native part of the trees memory component and can\n ... ",
			  "Caching hot records.** Before reading the leaf page from disk, a\nread operation first (binary) searches the mini-page for the desired\nrecord and terminates early if the record is found. Searching the\nmini-page is efficient as the records are sorted and in-memory. If\nthe record is not found in the mini-page, we load the corresponding\nleaf page from disk.",
			  "**Figure 3: Mini-page (var len) / leaf node (4096 bytes) layout**\nthe search early. Note that the mini-page will cache the individual\nrecords, not the entire page, avoiding the inefficient page-level\ncaching of conventional B-Trees. To avoid flooding the mini-page\nwith cold records, we only cache the records from the leaf page at a\nlow probability, e.g., 1%. Caching hot records is implemented as an\nin-memory insert operation to the mini-page, which may trigger\nthe mini-page to grow as needed.",
			  "**Caching range gaps.** Record caching does not help with range\nscans because the range query has to look at the leaf page for the\nfull set of records; in other words, record caching breaks the spatial\nlocality needed for range scans. On the other hand, a page-level\ncache is ideal for range scans, as it preserves the spatial locality of\nrecords. Mini-pages support caching range gaps: when a mini-page\ngrows to the full size, we merge (if necessary) and convert it into\na full leaf page mirroring disk.",
			  "**3.2**\n**Mini and leaf page layout**\nMini-pages and leaf pages share the same layout, storing key-value\npairs in sorted order and allowing efficient lookups. In Bf-Tree , they\nshare the same implementation, except that mini-pages can have\nvarying lengths.",
			  "Tree still has the best caching ratio because its mini-page design can\nbetter handle the internal fragmentation of the leaf pages. Practical\nB-Tree leaf pages only have about 70% of the space used due to the\nnature of page splitting. When caching the entire page in memory,\nthis leads to a 30% waste of memory. Bf-Tree s mini-pages will\ndynamically grow and shrink to fit the actual number of records in\nthe page, leading to a better caching ratio.",
			  "*Figure 11: Impact of read and write workloads.**\nFigure 11 shows the impact of read and write workloads on Bf-\nTree and baseline systems. For systems that implement buffered\nwrite, e.g., Bf-Tree , B ** -Tree, and RocksDB, the throughput is higher\nwith more write operations. This is because most write operations\nare buffered in memory and only flushed to disk when the buffer\nis full. But for read operations, each cache miss incurs a random\ndisk IO to read the data from the disk.",
			  "Figure 2: High level architecture of** **Bf-Tree** **.** Like conventional\nB-Tree, but pages in the buffer pool are variable lengt"
			]
		  },
		  {
			"title": "[2506.22849] DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations",
			"url": "https://arxiv.org/abs/2506.22849",
			"excerpts": [
			  "Oriented bounding box (OBB) bounding volume hierarchies offer a more precise fit than axis-aligned bounding box hierarchies in scenarios with thin elongated and arbitrarily rotated geometry, enhancing intersection test performance in ray tracing.",
			  "we introduce a novel OBB construction technique where all internal node children share a consistent OBB transform, chosen from a fixed set of discrete quantized rotations.",
			  "This allows for efficient encoding and reduces the computational complexity of OBB transformations.",
			  "Despite a 12.6% increase in build time, our experimental results demonstrate an average improvement of 18.5% in primary, 32.4% in secondary rays, and maximum gain of 65% in ray intersection performance, highlighting its potential for advancing real-time applications."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://www.researchgate.net/publication/281895915_Array_Layouts_for_Comparison-Based_Searching",
			"excerpts": [
			  "In addition to the obvious sorted order/binary search combination we consider the Eytzinger breadth-first-search (BFS) layout normally used ...Read more"
			]
		  },
		  {
			"title": "performance - Using SIMD/AVX/SSE for tree traversal - Stack Overflow",
			"url": "https://stackoverflow.com/questions/20616605/using-simd-avx-sse-for-tree-traversal",
			"excerpts": [
			  "I've used SSE2/AVX2 to help perform a B+tree search. Here's code to perform a \"binary search\" on a full cache line of 16 DWORDs in AVX2: Copy.Read more"
			]
		  },
		  {
			"title": "What role do branch mispredictions play in hash table ...",
			"url": "https://stackoverflow.com/questions/62997105/what-role-do-branch-mispredictions-play-in-hash-table-lookup-performance",
			"excerpts": [
			  "High performance hash tables would optimise with tricks like checking batches of four keys at once between branches to reduce mispredictions.Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing improves SIMD hash tables",
			"url": "https://reiner.org/cuckoo-hashing",
			"excerpts": [
			  "Baseline SIMD hash tables such as Swiss Tables use SIMD quadratic probing. This starts searching from a position determined by the hash function, searches 4 ...Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Cuckoo_hashing",
			"excerpts": [
			  "Cuckoo hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table, with worst-case constant lookup time.Read more"
			]
		  },
		  {
			"title": "SIMD Vectorized Hashing for Grouped Aggregation",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Gurumurthy:ADBIS18.pdf",
			"excerpts": [
			  "Cuckoo hashing resolves collision by using multiple hash tables [9]. These tables resolve collision by swapping collided keys. The collided key, during ...Read more"
			]
		  },
		  {
			"title": "SIMD Acceleration for Main-Memory Index Structures",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Wallewein-EisingBS:BDAS18.pdf",
			"excerpts": [
			  "processor architectures [1]. Several index structures have already shown that the\nbottleneck from RAM to CPU can be overcome using Single Instruction Multiple\nData (SIMD) [24] operations. These index structures include: the k-ary Search\nTree (Seg-Tree) [5], Adapted Radix Tree (ART) [6], Fast Architecture Sensitive\nTree (FAST) [7], and Vector-Advanced and Compressed Structure Tree (VAST)\n[8]. All approaches use SIMD only for key comparison within tree traversal and\ntry to decrease the key size to fit more keys into one SIMD register. There-\nfore FAST and Seg-Tree only provide implementations for search algorithms",
			  "In this work we make the following contributions:\n**** We compare different optimizations of index structures to fulfill requirements\nof modern database systems\n2\nSIMD Acceleration for Index Structures\n**** We highlight the usage of SIMD and the cache-line adaptations in all ap-\nproaches\n**** We state the performance impact of optimizations on the index structures\n**** We discuss the usefulness of optimizations for our own index structure Elf [9].",
			  "We organized the rest of the paper as follows. In Section 2, we give the prelim-"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "the scatter instruction is only\navailable in limited types of processors hence the vertically-vectorized insertions can only be\nimplemented on specific hardware.",
			  "In each\nvectorized lookup, the vertical approach will pass a vector (of register size) of inputs through\nthe lookup process and by using mask registers and advanced SIMD features (like SIMD\npermutations) probe those keys at the same time.",
			  "rizontal vectorization, each cell of the hash-table entries array is bucketed into *N*\ninner cells. Then, while doing a lookup on the hash table and after computing the hash\nvalue, using the SIMD logical operations, the lookup algorithm can concurrently check the\nvalue of *N* bucket key",
			  "vertical vectorization yields\nhigher performance than the horizontal approach."
			]
		  }
		],
		"reasoning": "The provided value for 'quantitative_performance_model' describes a hybrid model that considers both CPU and GPU performance, with a focus on memory latency, Memory-Level Parallelism (MLP), cache hierarchies, and optimization strategies. The excerpts that are relevant discuss these aspects in the context of parallel processing, particularly on GPUs and CPUs, and address challenges related to memory access patterns and latency.\n\nSeveral excerpts discuss the limitations of CPU execution, such as the constraint of 2 load/store units per cycle and the limited number of MSHRs that restrict MLP. They also detail cache latencies across different levels (L1, L2, L3, DRAM) and how working set size affects hit/miss rates, which are crucial parameters for the CPU model. Excerpts discussing GPU performance highlight massive multithreading for latency hiding, warp execution, L1 cache/shared memory, L2 cache, HBM memory bandwidth, and the number of in-flight memory requests, all of which are directly relevant to the GPU model parameters.\n\nThe key insight from the provided value emphasizes that the traversal problem is memory-latency bound due to serialized, data-dependent pointer-chasing. This aligns with excerpts that discuss the limitations of load/store units, the core challenge of software parallelism exceeding hardware MLP, and the need for optimizations that reduce memory access costs (e.g., cache-friendly layouts, compression) or restructure algorithms to hide latency (e.g., batching, DAE, asynchronous operations). Excerpts discussing these specific optimization strategies are highly relevant.\n\nExcerpts that detail specific performance bottlenecks like memory bandwidth, cache misses, and latency are relevant. For instance, discussions on the impact of divergent execution on SIMD efficiency and the challenges of irregular memory access patterns directly support the model's focus on these issues. Techniques like batching rays, using wide BVHs, optimizing tree layouts (e.g., cache-oblivious, van Emde Boas), and software prefetching are also directly related to the optimization strategies mentioned in the model's key insight.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models",
		"citations": [
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "Idea: Decouple operand\naccess and execution via\ntwo separate instruction\nstreams that communicate\nvia ISA-visible queues .",
			  "Advantages:\n+ Execute stream can run ahead of the access stream and vice\nversa\n+ If A is waiting for memory, E can perform useful work\n+ If A hits in cache, it supplies data to lagging E\n+ Queues reduce the number of required registers\n+ Limited out-of-order execution without wakeup/select complexi",
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)",
			  "Decoupled Access/Execute (DAE)",
			  "Smith,  Decoupled Access/Execute\nComputer Architectures ,  ISCA 1982,\nACM TOCS 1984.",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream.",
			  "Control flow synchronization (like branches) is handled via a separate branch queue.",
			  "The compiler analyzes the program, identifies operations belonging to each stream, and generates two distinct instruction sequences that explicitly communicate via queue operations inserted by the compiler.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "Leveraging Warp Specialization for High Performance on GPUs",
			"url": "https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf",
			"excerpts": [
			  "e crucial insight for warp specialization is that while con-\ntrol divergence within a warp results in performance degradation,\ndivergence between warps does not.",
			  "med barriers pro-\nvide two operations: *arrive* and *sync* . Arrive is a non-blocking op-\neration which registers that a warp has arrived at a barrier and then\ncontinues execution. Sync is a blocking operation that waits until\nall the necessary warps have arrived or synced on the barrie",
			  "Warp-specialized partitioning provides a useful mechanism for\nDSL compilers when grappling with computations that exhibit\nboth irregularity and large working sets.",
			  "The mapping compilation stage is responsible for taking in an\narbitrary dataflow graph of operations and mapping it onto the\nspecified number of warps and available GPU memories.",
			  "It is important to\nnote that named barriers support synchronization between arbitrary\nsubsets of warps within a CTA, including allowing synchronization\nbetween a single pair of warps as in this example.",
			  "rtitioning computations us-\ning warp specialization allows Singe to deal efficiently with the\nirregularity in both data access patterns and computation.",
			  "Consumer Warp",
			  "Producer Warp",
			  "(signal begin)",
			  "(wait until ready)",
			  "bar.sync",
			  "bar.sync",
			  "(wait until begin)",
			  "bar.arrive",
			  "bar.arrive",
			  "(signal ready)",
			  "producer-consumer named barrier is used\nto indicate to the QSSA warps when the needed values from the\nnon-QSSA warps have been written into shared memory.",
			  "arp-specialized programs also require more expressive syn-\nchronization mechanisms",
			  "Warp specialization exploits the division\nof a thread block into warps to partition computations into sub-\ncomputations such that each sub-computation is executed by a\ndifferent warp within a thread block.",
			  "However, by using inline PTX statements, a CUDA program\nhas access to a more expressive set of intra-CTA synchronization\nprimitives referred to as *named barriers*",
			  "Using arrive and sync operations, programmers can encode\nproducer-consumer relationships in warp-specialized programs.",
			  "gure 2 illustrates using two named barriers to coordinate move-\nment of data from a producer warp (red) to a consumer warp (blue)\nthrough a buffer in shared memory.",
			  "he\nconsumer warp signals the buffer is ready by performing a non-\nblocking arrive operation.",
			  "Since the arrive is non-blocking, the\nconsumer warp is free to perform additional work while waiting\nfor the buffer to be filled.",
			  "At some point the consumer warp blocks\non the second named barrier waiting for the buffer to be full.",
			  "The\nproducer warp signals when the buffer is full using a non-blocking\narrive operation on the second named barrier.",
			  "Named Barrier 0",
			  "Named Barrier 1"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  " root cause of this entanglement is the require-\nment encouraged by the CUDA programming model that\nthreads of a CTA perform both memory accesses and com-\nputation. By creating specialized warps that perform inde-\npendent compute and memory operations we can tease apart\nthe issues that affect memory performance from those that\naffect compute performan",
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps.",
			  "arp specialization allows subsets of threads within\na CTA to have their behavior tuned for a particular purpose\nwhich enables more efficient consumption of constrained re-\nsources",
			  "If single buffering is not exploiting enough MLP to keep\nthe memory system busy, an alternative is to create two\nbuffers with two sets of DMA warps for transferring data.\nWe call this two-buffer technique double buffering ",
			  "DMA warps also improve programmer productivity\nby decoupling the need for thread array shapes to match\ndata layout.",
			  "Named Barrier 0",
			  "Named Barrier 1",
			  "compute warps",
			  "DMA warps",
			  "start_async_dma()",
			  "wait_for_dma_fnish()",
			  "wait_for_dma_start()",
			  "(bar.sync)",
			  "(bar.sync)",
			  "fnish_async_dma()",
			  "(bar.arrive)",
			  "(bar.arrive)",
			  "The\nproducer/consumer nature of our synchronization mecha-\nnisms allow the programmer to employ a variety of tech-\nniques to overlap communication and computation",
			  "e accomplish fine-grained synchronization by using in-\nlined PTX assembly to express named barriers .",
			  "med\nbarriers are hardware resources that support a barrier oper-\nation for a subset of warps in a CTA and can be identified\nby a unique name (e.g. immediate value in PTX).",
			  "There are\ntwo named barriers associated with every cudaDMA object.\nTwo barriers are required to track whether the data buffer\nin shared memory is full or empty.",
			  "e use the PTX in-\nstruction bar.arrive , which allows a thread to signal its ar-\nrival at a named barrier without blocking the threads execu-\ntion",
			  "This functionality is useful for producer-consumer\nsynchronization by allowing a producer to indicate that a\ndata transfer has finished filling a buffer while permitting\nthe producer thread to continue to perform work.",
			  "Similarly,\na consuming thread can use the same instruction to indicate\nthat a buffer has been read and is now empty.",
			  "For blocking\noperations we use the PTX instruction bar.sync to block\non a named barrier.",
			  "CudaDMA has shown the benefits of emulating an asyn-\nchronous DMA engine in software on a GP",
			  "The CudaDMA approach to GPU programming is therefore\ngeneral enough to be applied to any CUDA program.",
			  "CudaDMA encapsulates this technique in order to make it\nmore generally available to a range of application workloads.",
			  "By decoupling the compute and\nDMA warps, CudaDMA enables this approach without\nsacrificing memory system performance.",
			  "CudaDMA enables the\nprogrammer to decouple the shape of data from how the\ndata is transferred by creating specialized DMA warps for"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more",
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
			"url": "https://arxiv.org/html/2510.14719v2",
			"excerpts": [
			  "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT ...Read more"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "t.\nThere is also a \"Branch From\nQueue\" (BFQ) instruction that is conditional on\nthe branch outcome at the head of the branch queue\ncoming from the\nopposite processor.\n",
			  ".\nThus conditional branches\nappear in the two processors as complementary\npairs.",
			  " deadlock can occur if\nboth the AEQ and EAQ are full and both processors\nare blocked by the full queues, or if both queues\nare empty and both processors are blocked by the\nempty queues.\n"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. ",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Little's Law as Viewed on Its 50th Anniversary",
			"url": "https://people.cs.umass.edu/~emery/classes/cmpsci691st/readings/OS/Littles-Law-50-Years-Later.pdf",
			"excerpts": [
			  ".\nLittles Law says that the average number of items in a\nqueuing system, denoted *L* , equals the average arrival rate\nof items to the system, ** , multiplied by the average waiting\ntime of an item in the system, *W* . Thus,\n*L* = ** *W* *0*\nF",
			  "\nThe two biggest fields in which Littles Law regularly\ncomes into play are operations management (OM) and\ncomputer architecture (computers).",
			  "Latency is a key performance measure for computers\n(low is good) and has an unspoken average implicitly\nattached, but *response time* seems more meaningful for a\nlayperson.",
			  "e\ncomputer architect wants to understand what is going on so\nas best to design the system.",
			  "Computer memory comes in various *tiers* with different\ninherent response times. The tiers range from CPU cache\n(fast), to DRAM (dynamic random access memory) (not\nquite as fast), to RAM (random access memory) (a lit-\ntle slower), all the way to solid-state drives and finally to\nnetwork attached storage (banks of hard disk drives).",
			  "The server CPU contains microprocessors, each of which\ncontains cores, perhaps 8 or 16 of them. Each is a von\nNeumann computer. Now we are coming to the queues. The\npiece of code the computer is executing is aptly called a\nthread (of instructions). The author visualizes it as a list of\ninstructions running down his penciled code sheet. Oh, oh,\nthe instruction calls for a piece of data that is somewhere\nelse in memory, electronically far away from the thread.\nThe thread STOPS and sends out a request for that data."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://dl.acm.org/doi/abs/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982. ... 1984. ISSN ...Read more",
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings ... James E Smith profile image James E. Smith. Department of Electrical and ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Overview and Code Examples",
			"url": "https://www.nvidia.com/content/PDF/GDC2011/Brucek_KhailanySC11.pdf",
			"excerpts": [
			  "Warp Specialization. CudaDMA enables warp specialization: DMA warps. Maximize MLP. Compute warps. No stalls due to memory. CudaDMA objects manage warp ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://alastairreid.github.io/RelatedWork/papers/smith:tocs:1984/",
			"excerpts": [
			  "Decoupled access/execute computer architectures. James E. Smith [doi] [Google Scholar] [DBLP] [Citeseer]. ACM Transactions on Computer Systems 2(4)Read more"
			]
		  },
		  {
			"title": "(PDF) A decoupled access-execute architecture for ...",
			"url": "https://www.researchgate.net/publication/326637256_A_decoupled_access-execute_architecture_for_reconfigurable_accelerators",
			"excerpts": [
			  "PDF | Mapping computational intensive applications on reconfigurable technology for acceleration requires two main implementation parts: (a) ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114% ( ...Read more"
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "This paper demonstrates significant performance gains on a commodity 8-core multicore machine running a variety of codes transformed with DSWP+. Formats ...Read more"
			]
		  },
		  {
			"title": "Scalable-Grain Pipeline Parallelization Method for Multi- ...",
			"url": "https://inria.hal.science/hal-01513778/file/978-3-642-40820-5_23_Chapter.pdf",
			"excerpts": [
			  "Some approaches partition individual instructions across pro- cessors, such as the decoupled software pipeline (DSWP) method [10], while.Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "1.1 Problem. Decoupled software pipelining [5] presents an easy way to automatically ex- tract thread-level parallelism for general loops in any program.Read more"
			]
		  },
		  {
			"title": "decoupled-software-pipelining-creates-parallelization- ...",
			"url": "https://scispace.com/pdf/decoupled-software-pipelining-creates-parallelization-tyrvhzup00.pdf",
			"excerpts": [
			  "An automatic parallelization technique, DSWP splits the loop body into several stages distributed across multiple threads, and executes them in a pipeline."
			]
		  },
		  {
			"title": "\nParallel Techniques of the Sequential Codes Based on Multi-core",
			"url": "https://scialert.net/fulltext/?doi=itj.2013.1673.1684",
			"excerpts": [
			  "DSWP parallelizes a loop by partitioning the body of the loop into a sequence of pipeline stages. Each stage is then executed by a separate thread. The threads ...Read more"
			]
		  },
		  {
			"title": "PIPELINED MULTITHREADING TRANSFORMATIONS AND ...",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_ram.pdf",
			"excerpts": [
			  "tolerate variable latency and to overcome scope restrictions imposed by single PC ar-\nchitectures, a program transformation called *decoupled software pipelining* (DSWP) is pre-\nsented in this chapter",
			  "SWP avoids heavy hardware usage by attacking the fundamental\nproblem of working within a single-threaded execution model, and moving to a multi-\nthreaded execution model.",
			  "Only useful, about-to-execute instructions are even brought into\nthe core. The rest are conveniently left in the instruction cache or their results committed\nand retired from the processor core.",
			  "Unlike the single-threaded multi-core techniques pre-\nsented in Table 2.1, DSWP is an entirely *non-speculative* techniqu",
			  "Each DSWP thread\nperforms useful work towards program completion.",
			  "The concurrent multithreading model of DSWP means that each participating thread\ncommits its register and memory state concurrently and independently of other threads.",
			  "The current thread model for DSWP is as follows. Execution begins as a single thread,\ncalled *primary thread* . It spawns all necessary *auxiliary threads* at the beginning of a\nprogram",
			  "When the primary thread reaches a DSWPed loop, auxiliary threads are set up\nwith necessary loop live-in values.",
			  "Similarly, upon loop termination, loop live-outs from\nauxiliary threads have to be communicated back to the primary thread."
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization. They also promise much wider flexi-\nbility than data-parallelism-centric methods for processors, aiming\nfor the effective parallelization of general-purpose application",
			  "In this paper, we provide another method to decouple control-\nflow regions of serial programs into concurrent tasks, exposing\npipeline and data parallelism",
			  "The power and simplicity of the\nmethod rely on the restriction that all streams should retain a\nsynchronous semantics [8].",
			  "ics [8]. It amounts to checking the sufficient\ncondition that the source and target of any decoupled dependence\nare control-dependent on the same node in the control dependence\ntree (this assumes structured control flow).",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "For example, when\nthere are no dependences between loop iterations of a DSWP stage,\nthe incoming data can be distributed over multiple data-parallel\nworker threads dedicated to this stage, while the outgoing data can\nbe merged to proceed with downstream pipeline stages."
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformati",
			  "WP operates by partitioning the instructions of a\nloop among a sequence of loops. The new loops are concurrently\nexecuted on different threads, with dependences among them flow-\ning in a single direction, thus forming a pipeline of threads.",
			  "The performance results indicate\nthe potential of this technique to exploit iteration-level parallelism\nin loops that cannot be parallelized as DOALL.",
			  "e evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining with the Synchronization Array",
			"url": "https://liberty.princeton.edu/Publications/pact04_dswp.pdf",
			"excerpts": [
			  " software pipelining (DSWP), a technique that stati-*\n*cally splits a single-threaded sequential loop into multi-*\n*ple non-speculative threads, each of which performs use-*\n*ful computation essential for overall program correctne",
			  "g threads execute on thread-parallel architec-*\n*tures such as simultaneous multithreaded (SMT) cores or*\n*chip multiprocessors (CMP), expose additional instruction*\n*level parallelism, and tolerate latency better than the orig-*\n*inal single-threaded RDS lo",
			  "accomplish this,\nthe paper presents the *synchronization array* which ap-\npears to the ISA as a set of queues capable of supporting\nboth out-of-order execution and speculative issue",
			  "e traversal and computation threads behave as a tra-\nditional decoupled producer-consumer pair.",
			  "DSWP threads these loops\nfor parallel execution on SMT or CMP processors.",
			  "ever, for this threading technique to be effective, a very\nlow overhead communication and synchronization mecha-\nnism between the threads is required. "
			]
		  }
		],
		"reasoning": "The 'Decoupled Access/Execute (DAE)' model is described as an architecture that separates the instruction stream into two distinct units: an 'Access' processor for memory operations and an 'Execute' processor for computations. These units communicate via explicit FIFO queues, allowing them to operate asynchronously. This decoupling enables the Access processor to run ahead of the Execute processor, effectively prefetching data and hiding memory latency. This concept is realized on CPUs as Decoupled Software Pipelining (DSWP), where different cores take on Access and Execute roles, and on GPUs as 'Warp Specialization', where specific warps handle memory transfers (often using asynchronous copies) and others perform computations, synchronizing through shared memory and hardware barriers. The primary benefit is maximizing memory latency hiding by overlapping computation with memory access, which is crucial for tolerating long, variable memory latencies and keeping many memory requests in flight. This model is particularly well-suited for scenarios with limited load slots and a large number of independent walkers, as it allows for continuous processing by keeping compute units busy while memory operations are pending.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "e modified the persistent threads approach to periodi-\ncally replace the terminated rays with new ones that start from the\nroot",
			  "We implemented persistent variants of the packet traversal and\nwhile-while.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "Persistent trace()",
			  "s shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray typ",
			  "e could reasonably believe\nthat packet traversal would be close to the simulated optimal per-\nformance with at least coherent primary rays where few redundant\nnodes are visited.",
			  "e packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "Primary\n149.2\n122.1\n82\nAO\n100.7\n86.1\n86\nDiffuse\n36.7\n32.3\n88\n~2X performance from persistent threads\n~85% of simulated, also in other scenes\nHard to get much closer\nOptimal dual issue, no resource conflicts, infinitely\nfast memory, 20K threads",
			  "Primary\n166.7\n135.6\n81\nAO\n160.7\n130.7\n81\nDiffuse\n81.4\n62.4\n77\n~1.5X performance from persistent threads\n~80% of simulated, other scenes ~85%\nAlways faster than packet traversal\n2X with incoherent rays",
			  "rsistent threads**\nLaunch only enough threads to fill the machine\nonce\nWarps fetch work from global pool using atomic\ncounter until the pool is empty\nBypasses hardware work distribution\nSimple and generic solution\nPseudocode in the paper",
			  "Persistent threads**\nLaunch only enough threads to fill the machine\nonce\nWarps fetch work from global pool using atomic\ncounter until the pool is empty\nBypasses hardware work distribution\nSimple and generic solution\nPseudocode in the pa",
			  "Persistent while-while",
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.researchgate.net/publication/221249082_Understanding_the_Efficiency_of_Ray_Traversal_on_GPUs",
			"excerpts": [
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal. Aila and Laine [2009] proposed to increase SIMD efficiency by replacing already finished rays with new ones from a global queue.",
			  "The algorithm spawns an optimal number of warps that fetch rays from the global queue.",
			  " and Laine [2009] proposed an efficient stack-based traversal algorithm of bounding volume hierarchies on the GPU with persistent warps and dynamic"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://users.aalto.fi/~laines9/publications/aila2012tr1_paper.pdf",
			"excerpts": [
			  "Aila and Laine [AL09] hypothesize that it might be ben- eficial to replace terminated rays once the SIMD utilization drops below a threshold. In practice this ...Read more"
			]
		  },
		  {
			"title": "Embree: A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://cseweb.ucsd.edu/~ravir/274/15/papers/a143-wald.pdf",
			"excerpts": [
			  "Single-Ray Vectorization**\nOther work has focused on using vectorization to accelerate spa-\ntial data structure traversal for individual rays. For example, multi-\nbranching BVH structures enable multiple nodes or primitives to\nbe tested for intersection against the same ray in parallel. Quad-\nbranching BVH (BVH4) data structures perform well with 4-wide\nand 16-wide vector units [Ernst and Greiner 2008; Dammertz et al.\n2008; Benthin et al. 2012], but higher branching factors offer di-\nminishing returns [Wald et al. 2008].\nGenerally speaking, single ray vectorization is faster than packet\ntracing for incoherent ray distributions (and can be used within a\nscalar renderer), but is slower for coherent rays. For this reason, hy-\nbrid techniques have been developed which can dynamically switch\nbetween packet tracing where rays are coherent, and single-ray vec-\ntorization where not [Benthin et al. 2012",
			  "For this reason, we allow a given BVH type in Embree to be used\nwith both single-ray and packet traversal and intersection kernels,\nenabling the implementation of a hybrid ray traversal mode. This\nmode begins traversal using packets, and dynamically switches to\nsingle-ray traversal when the number of active rays in a packet falls",
			  "mode begins traversal using packets, and dynamically switches to",
			  "single-ray traversal when the number of active rays in a packet falls",
			  "below a threshold [Benthin et al. 2012].",
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "cket tracing is conceptually simple compared to single-ray SIMD\ntraversal and intersection. In classical packet tracing, rays in a given\npacket are intersected with the same BVH node or triangle at each\nstep of traversal. However, it is also possible to implement packet\ntracing following a single program multiple data (SPMD) program-\nming model. Here, the rays of a packet are independently traversed\nthrough the BVH, and each ray is potentially tested against different\nBVH nodes or triangles [Aila and Laine 2009].",
			  "For this reason, we allow a given BVH type in Embree to be used",
			  "with both single-ray and packet traversal and intersection kernels,",
			  "enabling the implementation of a hybrid ray traversal mode. This",
			  "ing between packet and single-ray kernels [Benthin et al. 2012].",
			  "Yet\nthe memory storage order for a BVH optimized for packets may be",
			  "suboptimal for single-ray methods (the converse may also be true).",
			  "For example, the Embree packet intersection kernels test multiple",
			  "rays against a single triangle while the single-ray kernels typically",
			  "intersect a single ray with multiple triangles. The storage order",
			  "of the triangles is different in both cases (e.g. triangle1n versus",
			  "triangle4n ), but we have found the difference in packet tracing",
			  "performance to be minimal.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes."
			]
		  },
		  {
			"title": "(PDF) Combining Single and Packet-Ray Tracing for Arbitrary Ray Distributions on the Intel MIC Architecture",
			"url": "https://www.researchgate.net/publication/51798993_Combining_Single_and_Packet-Ray_Tracing_for_Arbitrary_Ray_Distributions_on_the_Intel_MIC_Architecture",
			"excerpts": [
			  "The idea is to start traversal with packets, and switch to single-ray traversal when too many rays in the packet are inactive.",
			  "This hybrid algorithm outperforms conventional packet tracers by up to 2X.",
			  "The solution to this problem is to combine packet traversal with single-ray traversal [Ben+12] to form a hybrid algorithm.",
			  "The two most commonly used schemes are either packet tracing, or relying on a separate traversal stack for each SIMD lane.",
			  "This method maps well\nto current CPU designs, but for incoherent rays suffers from\nlow SIMD utilization when too many rays get inactive.",
			  "A hybrid approach that uses compaction\nof large ray sets and uses a similar idea of a fall back to single\nray tracing in case no coherence is detected was presented\nby Tsakok et al. [20].",
			  "Unfortunately, single-ray traversal is not as e cient as packet traversal for coherent rays: It fails to take advantage of the fact that similar rays traverse similar parts of the tree."
			]
		  },
		  {
			"title": "Batching of divergent rays on GPU architectures",
			"url": "https://studenttheses.uu.nl/bitstream/handle/20.500.12932/41250/thesis_final.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "Benthin et al.[ Ben+12 ] devised a hybrid traversal model that combines ray packets and single ray\ntraversal."
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Hybrid traversal**\nAs mentioned above this method is a combination of the\npacket and if-if traversal ones.\nThe idea is that packet\ntraversal performs best near the tree root where rays are\ncoherent whereas if-if traversal is better suited for travers-\ning nodes near the leaves. It is, however, unclear when and\nhow to switch between the two of the method",
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Combining Single and Packet-Ray Tracing for Arbitrary Ray Distributions on the Intel MIC Architecture - PubMed",
			"url": "https://pubmed.ncbi.nlm.nih.gov/22084142/",
			"excerpts": [
			  "In this paper, we introduce a single-ray tracing scheme for incoherent rays that uses just one traversal stack on 16-wide SIMD hardware. It uses a bounding ...Read more"
			]
		  },
		  {
			"title": "A Kernel Framework for Efficient CPU Ray Tracing",
			"url": "https://www.embree.org/papers/2014-Siggraph-Embree.pdf",
			"excerpts": [
			  "In scenes with a mix of coherent and incoherent rays, BVH traversal\nand intersection performance can benefit from dynamically switch-\ning between packet and single-ray kernels [Benthin et al. 2012].",
			  "This\nmode begins traversal using packets, and dynamically switches to\nsingle-ray traversal when the number of active rays in a packet falls\nbelow a threshold [Benthin et al. 2012].",
			  "This mode can improve\ntraversal performance by 50% compared to packets alone.",
			  "The Embree packet kernels implement classical rather than SPMD\npacket tracing. This approach simplifies the control flow, enables\nthe use of load-and-broadcast memory operations in place of costly\ngathers, and amortizes scalar computation across SIMD lanes."
			]
		  },
		  {
			"title": "an-adaptive-heterogeneous-runtime-for-irregular- ...",
			"url": "https://scispace.com/pdf/an-adaptive-heterogeneous-runtime-for-irregular-applications-4ybnel0b2m.pdf",
			"excerpts": [
			  "The process\nstarts with a 16-wide packet traversal which performs 16-wide box tests. At any\npoint in time, the bit in an active mask will be counted to indicate how many\nof the packets rays are still active for a subtree. If this number falls below a\ngiven threshold, which is set to 7, the process leaves the packet traversal mode\nand sequentially traces all active rays in the single-ray mode[12].",
			  "The drawback\nof this method is that the threshold may need to change and the program will\nrequire recompilation if the system is moved to a machine with a shorter SIMD\nlane.",
			  "the hybrid packet/single-ray tracing algorithm is\nimplemented by utilizing a specific type of BVH tree.",
			  "his method does not prevent any possible divergent execution that\nlowers the effectiveness of the SIMD engine."
			]
		  },
		  {
			"title": "Adaptive Ray Packet Reordering",
			"url": "https://graphics.stanford.edu/~boulos/papers/reorder_rt08.pdf",
			"excerpts": [
			  "Empirically, our threshold of 50% works rather well (for more detailed comparisons see Section 5), but a more complicated heuristic might reorder less often for ...Read more"
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "Section Title: Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware > Abstract\nContent:\nRecent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o",
			  "Dynamic warp formation improves performance by cre-\nating new thread warps out of diverged warps as the shader\nprogram execute"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  " [14] W. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, Dynamic warp formation and scheduling for efficient gpu control flow, in *International Symposium on Microarchitecture (MICRO)* , 2007. ",
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow | Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2007.12",
			"excerpts": [
			  "We show that a realistic hardware im- plementation that dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes improves performance by an average of 20.7% for an estimated area increase of 4.7%."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Thread block compaction for efficient SIMT control flow",
			"url": "https://ieeexplore.ieee.org/document/5749714/",
			"excerpts": [
			  "Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data cores to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22% over a baseline per-warp, stack-based reconvergence mechanism, and 17% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "A survey of architectural approaches for improving GPGPU ...",
			"url": "https://mkhairy.github.io/Docs/jpdc-survey.pdf",
			"excerpts": [
			  "Fung and Aamodt [46] proposed thread block compaction (TBC) that allows a group of warps, that belong to the same thread block, to share the same PDOM stack.Read more"
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages",
			  "Evaluated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path",
			  "SIMT execution.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "valuated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path\nSIMT execution"
			]
		  },
		  {
			"title": "[PDF] Thread block compaction for efficient SIMT control flow",
			"url": "https://www.semanticscholar.org/paper/Thread-block-compaction-for-efficient-SIMT-control-Fung-Aamodt/8bd6f67ef03b3c138c52f3e9b1716aebe937d244",
			"excerpts": [
			  "This paper proposes and evaluates the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, ..."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "http://www.danielwong.org/classes/_media/ee260_w17/threadblockcompaction.pdf",
			"excerpts": [
			  ", we propose and*\n*evaluate the benefits of extending the sharing of resources*\n*in a block of warps, already used for scratchpad mem-*\n*ory, to exploit control flow locality among threads (where*\n*such sharing may at first seem detrimental). In our pro-*\n*posal, warps within a thread block share a common block-*\n*wide stack for divergence handling. At a divergent branch,*\n*threads are compacted into new warps in har",
			  "he modifications consist of three major parts: a modi-\nfied branch unit ( 1 ), a new hardware unit called the thread\ncompactor ( 2 ), and a modified instruction buffer called the\nwarp buffer ( 3 ). The branch unit ( 1 ) has a block-wide re-\nconvergence stack for each block. Each entry in the stack\nconsists of the starting PC (PC) of the basic block that cor-\nresponds to the entry, the reconvergence PC (RPC) that in-\ndicates when this entry will be popped from the stack, a\nwarp counter (WCnt) that stores the number of compacted\nwarps this entry contains, and a block-wide activemask that\nrecords which thread is executing the curre",
			  " comparison to DWF [9], thread block compaction ac-\ncomplishes the lookup-and-merge operation of the warp\nLUT and the warp pool [9] with simpler hardware. In\nDWF, an incoming warp is broken down every cycle and\n .."
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  },
		  {
			"title": "Generalizing Ray Tracing Accelerators for Tree Traversals ...",
			"url": "https://intra.engr.ucr.edu/~htseng/files/2024MICRO-TTA.pdf",
			"excerpts": [
			  "Figure 1 shows the average SIMT efficiency (percent of\nactive threads per warp due to control flow divergence) and\nDRAM bandwidth utilization of several tree traversal applica-\ntions on GPUs, profiled on an NVIDIA RTX 2080 Ti GPU and\nalso measured using Vulkan-Sim [ 83 ] with configurations list"
			]
		  },
		  {
			"title": "Treelet Accelerated Ray Tracing on GPUs",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/chou.asplos2025.pdf",
			"excerpts": [
			  "We\napply warp repacking to regroup active rays together into a\nnew warp, boosting SIMT efficiency and performance.",
			  "How-\never, this leads to a sharp decrease in SIMT efficiency due\nto varying BVH node access counts from different ray",
			  "This work explores how to efficiently implement treelet\nqueues on modern ray tracing capable GPUs. We propose\nVirtualized Treelet Queues, an architecture that increases\nthe number of concurrent rays in flight and treelet queues\nthat dynamically switch between treelet and ray stationary\ntraversal modes to increase efficiency.",
			  "o\ntake advantage of the increase in concurrent rays, we imple-\nment dynamic treelet queues in the GPUs RT unit to group\nup rays that access the same treelet together, achieving better\ncache locality and reduced miss rates",
			  "During later phases of\nray traversal when rays diverge more, treelet queues end up\nunderpopulated and it becomes inefficient to process rays\nin treelets. We propose to group up these underpopulated\ntreelet queues and traverse them regularly instead.",
			  "We propose to group up these underpopulated\ntreelet queues and traverse them regularly instead.",
			  "With\nall optimizations, virtualized treelet queues achieve up to\n2.55  better path tracing performance under usage scenarios\ncomparable to video games on ray tracing capable GPUs.",
			  "a et al. [ 5 ]\nproposed to subdivide the BVH tree into smaller subtrees, or\ntreelets, that fit in the processors cache to reduce memory\ntraff",
			  "Treelet queues essentially achieve a similar goal by grouping\nup rays based on their accessed treelet, but without the high\noverhead.",
			  "Ray virtualization increases the number of concurrent rays in flight to cre- ate more cache reuse opportunities by terminating raygen shaders ...Read more"
			]
		  },
		  {
			"title": "Enabling advanced GPU features in PyTorch  Warp Specialization  PyTorch",
			"url": "https://pytorch.org/blog/warp-specialization/",
			"excerpts": [
			  "Warp specialization (WS) is a GPU programming technique where warps (a group of 32 threads on NVIDIA GPUs) within a threadblock are assigned distinct roles or tasks. This approach optimizes performance by enabling efficient execution of workloads that require task differentiation or cooperative processing. It enhances kernel performance by leveraging an asynchronous execution model, where different parts of the kernel are managed by separate hardware units. Data communication between these units, facilitated via shared memory on the NVIDIA H100, is highly efficient. Compared to a uniform warp approach, warp specialization allows the hardware multitasking warp scheduler to operate more effectively, maximizing resource utilization and overall performance.",
			  "Using GEMM as an example, a typical uniform warp approach on the H100 GPU involves 8 warps per thread block collectively computing a tile of the output tensor. These 8 warps are divided into two warp groups (WG), with each group cooperatively computing half of the tile using efficient warp-group-level MMA (WGMMA) instructions, as illustrated in Figure 1.",
			  "ask Partitioning** : The entire kernel is automatically divided into asynchronous tasks based on predefined heuristics. The compiler determines how to utilize one producer warp group and a user-specified number of consumer warp groups to execute the kernel. It assigns task IDs to specific anchor operations, which then influence the task assignments for remaining operations through asynchronous task ID propagation and dependency analysi",
			  "ta Partitioning for Multiple Consumer Groups** : Efficiently partitioning data among multiple consumer groups is key to optimizing workload distribution. On the H100 GPU, the compiler, by default, attempts to partition the input tensor `A` along the `M` dimension, allowing each consumer group to compute half of the output tensor independently. This strategy, known as [cooperative partitioning](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md) , maximizes efficiency under most conditions. However, if this split leads to inefficienciessuch as producing a workload smaller than the native WGMMA instruction sizethe compiler dynamically adjusts and partitions along the `N` dimension instead"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://meistdan.github.io/publications/bvh_star/paper.pdf",
			"excerpts": [
			  " BVH is divided into *treelets* (small subtrees within the to-\ntal BVH) which are set to the size of either the L1 or L2 cache. The\narchitecture maintains a set of ray *queues* , with queues assigned\nto treelets at runtime. Rays begin tracing at the root treelet, and\nas rays cross treelet boundaries, they are placed in the ray queues\nto be processed later when their required treelet is present in the\ncache. The architecture thus attempts to maximize the number of\nrays which are processed each time a treelet is loaded on-chip, re-\nducing memory bandwidth"
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "With treelet prefetching, as rays traverse the BVH tree and visit the root node of treelets, corresponding treelets can be prefetched to load deeper levels of the tree before they are needed.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulations show treelet based traversal reduces performance slightly by 3.7% over a DFS baseline. However, when combined with treelet prefetching, the overall speedup reaches 32.1% while maintaining the same power consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache. Ray tracing is a pointer-chasing application and memory accesses are divergent and hard to predict. With the treelet based traversal algorithm introduced previously, memory accesses are now clustered as individual treelets, making it possible to prefetch easily.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "... BVH tree statistics for each scene are outlined in Table 2. The scene ... We use the concept of treelets which are connected subpartitions of a BVH tree.Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "We form treelets by grouping connected BVH nodes to maximize the size of each treelet. It is a greedy algorithm that starts from the BVH root node and greedily adds nodes to the current treelet until the maximum treelet size is reached.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "We propose to add a treelet prefetcher that prefetches treelets into the L1 cache of the GPU based on the rays in the warp buffer.",
			  "The threshold comparator generates the prefetch enable signal if the treelet popularity is greater than a manually set threshold which ranges from 0 to the maximum number of rays in the warp buffer ().",
			  "The prefetch enable is ANDed with the upper bits of the treelet root node address to generate the treelet prefetch address and sent to the prefetch queue to be processed ().",
			  "We only require the upper bits of the treelet root node address because the treelets have a fixed maximum size and nodes within a treelet are organized to be packed together in memory.",
			  "The treelet prefetcher also records the address of the last treelet it prefetches to avoid pushing duplicate treelet addresses to the prefetch queue and prefetching the same treelet multiple times in a row.",
			  "We combine treelet prefetching with a treelet based traversal algorithm in the ray tracing accelerator to further reduce ray traversal latenc",
			  "This section describes our proposed treelet prefetching technique for ray tracing.",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "As a ray visits a treelet root node, its subsequent memory accesses will also be to the nodes in the treelet since accesses to nodes from different treelets are deferred to the *otherTreeletStack",
			  "Thus, we can prefetch the entire treelet to the GPU's cache and reduce the latency of accessing nodes in the current treelet.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "d\nTreelet Prefetching For Ray Tracing\nMICRO 23, October 28November 01, 2023, Toronto, ON, Canada\nmore frequently than the lower levels. In the next sections, we pro-\npose a treelet based ray traversal algorithm with an accompanying\nprefetcher.",
			  "s a ray visits a treelet root node, its subse-\nquent memory accesses will also be to the nodes in the treelet since\naccesses to nodes from different treelets are deferred to the *oth-*\n*erTreelet",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "a treelet prefetcher to the RT unit to speed\nup ray traversal along with a prefetch queue to hold the issued\nprefetch addresses, both of which are highlighted in red in Figur",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "In Vulkan-Sim [ 41 ], the simulator we use\nfor evaluation, the ray tracing accelerator is referred to as the RT\nunit. When a warp issues a trace ray instruction, it enters the RT\nunit during the pipelines execute stage and is queued in the warp\nbuffer, which holds ray metadata for all 32 threads of the warp.",
			  "This work presents a treelet prefetching scheme to improve ray\ntraversal performance.",
			  "**7.4**\n**Treelet Based Ray Tracing Techniques**\nNavratil et al. [ 33 ] tackled incoherent rays by collecting rays into\n ... \nstaging buffer which might require non-trivial shader modifications\nto realize on a GPU and are not discussed in their paper.",
			  "**7.5**\n**Ray Traversal Acceleration Techniques**\n**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "We propose to add\na treelet prefetcher that prefetches treelets into the L1 cache of\nthe GPU based on the rays in the warp buffer.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict.",
			  "With 512B treelets, the treelet\nbased memory layout performs best with a 31.9% speedup over\nthe baseline.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "It is a greedy algorithm that starts from the BVH\nroot node and greedily adds nodes to the current treelet until the\nmaximum treelet size is reached. T",
			  "Figure 3 shows\nan example of a two-wide BVH tree partitioned into treelets with a\nmaximum size of 4 nodes each.",
			  "reelet formation initializes the *remainingBytes* to the maximum\ntreelet size and adds the BVH root address to the *pendingTreelets*\nqueue and traversal sta"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nDaniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, and Jir Bittner. 2021. A Survey on Bounding Volume Hierarchies for Ray Tracing. Computer Graphics Forum (2021).\n[Go",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Meister%2C+Shinji+Ogaki%2C+Carsten+Benthin%2C+Michael%C2%A0J.+Doyle%2C+Michael+Guthe%2C+and+Jir%C3%AD+Bittner.+2021.+A+Survey+on+Bounding+Volume+Hierarchies+for+Ray+Tracing.+Computer+Graphics+Forum+%282021%29.)",
			  "[32]",
			  "Bochang Moon, Yongyoung Byun, Tae-Joon Kim, Pio Claudio, Hye-Sun Kim, Yun-Ji Ban, Seung Woo Nam, and Sung-Eui Yoon. 2010. Cache-Oblivious Ray Reordering. ACM Transactions on Graphics (TOG) (2010).",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Bochang+Moon%2C+Yongyoung+Byun%2C+Tae-Joon+Kim%2C+Pio+Claudio%2C+Hye-Sun+Kim%2C+Yun-Ji+Ban%2C+Seung%C2%A0Woo+Nam%2C+and+Sung-Eui+Yoon.+2010.+Cache-Oblivious+Ray+Reordering.+ACM+Transactions+on+Graphics+%28TOG%29+%282010%29.)",
			  "[33]",
			  "Paul Arthur Navratil, Donald S. Fussell, Calvin Lin, and William R. Mark. 2007. Dynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In IEEE Symposium on Interactive Ray Tracing. 95104.\n[Dig",
			  "[Digital Library](/doi/10.1109/RT.2007.4342596)",
			  "[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FRT.2007.4342596)",
			  "[34]"
			]
		  },
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads."
			]
		  }
		],
		"reasoning": "The `finegrained_field_value` describes five GPU optimization techniques. I will go through each one and identify relevant excerpts. \n\n**1. Persistent Threads and Work Queues:**\nThis technique is well-supported by multiple excerpts. Several sources describe \"persistent threads\" as a solution where a large grid of threads is launched once to continuously fetch new work from a global pool using an atomic counter, bypassing the hardware work distribution. This is explicitly mentioned as a way to improve efficiency for ray traversal on GPUs. The concept of replacing terminated rays with new ones from a global queue to increase SIMD efficiency is also part of this, as is the use of local work queues to assign rays to warps and keep extra rays in shared memory/registers to maintain high SIMD utilization. Performance gains of 1.5x-2.2x for packet traversal and 1.5x for per-ray traversal are reported, and \"atomic coalescing\" within work queue management is noted to provide a 32% performance improvement.\n\n**2. Hybrid Packet/Single-Walker Traversal:**\nThis technique is strongly supported. Multiple excerpts describe starting traversal with packets and dynamically switching to single-ray traversal when the number of active rays in a packet falls below a threshold. This approach is noted to adapt to ray coherence, being efficient for coherent rays in packets and for incoherent rays in single-ray mode. Performance improvements of up to 2x over conventional packet tracers and up to 50% compared to packets alone in frameworks like Embree are explicitly stated.\n\n**3. Work Compaction / Warp Repacking:**\nThis technique is well-supported. Excerpts discuss dynamic warp formation and scheduling, where threads are regrouped into new warps on the fly following divergent branch outcomes to improve SIMD efficiency. Thread block compaction is presented as a mechanism where warps within a thread block share a reconvergence stack, and threads are compacted into new, fully utilized warps at divergent branches. This is shown to provide an average speedup of 22% over a baseline and 17% versus dynamic warp formation. The concept of warp repacking to regroup active rays and boost SIMT efficiency is also mentioned in the context of treelet accelerated ray tracing, contributing to overall speedups.\n\n**4. Treelet Scheduling:**\nThis technique is extensively supported. Numerous excerpts describe \"treelets\" as small subtrees of a BVH, sized to fit in cache. The core idea is to group rays (or walkers) that visit the same treelet into queues and process them together once the treelet is loaded into cache, thereby increasing memory reuse and reducing memory traffic. Treelet prefetching is proposed to reduce latency by prefetching corresponding treelets when a ray visits alet root node. Performance gains of up to 2.55x in path tracing scenarios when combined with virtualized queues and warp repacking, and an average speedup of 32.1% over a baseline RT Unit when combined with treelet prefetching, are reported.\n\n**5. Cooperative BVH Traversal (CoopRT):**\nThis technique is directly supported. Excerpts describe CoopRT as a scheme where idle threads within a warp assist busy threads with BVH traversal. It involves coalescing node addresses from the top-of-stack of these threads to remove redundant cache or memory accesses. A specialized Load Balancing Unit (LBU) helps identify threads that can offer or need help. This approach is shown to achieve up to 5.11x speedup, with an average of 2.15x, over a baseline RT unit in simulation, with a moderate area overhead.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Avx2/branch-optimized binary search in .NET  GitHub",
			"url": "https://gist.github.com/buybackoff/f403be01486220baba8a9d4fe22c3cf6",
			"excerpts": [
			  "Binary search is theoretically optimal, but it's possible to speed it up substantially using AVX2 and branchless code even in .NET Core.",
			  "Memory access is the limiting factor for binary search. When we access each element for comparison a cache line is loaded, so we could load a 32-byte vector almost free, check if it contains the target value, and if not - reduce the search space by `32/sizeof(T)` elements instead of 1 element. This gives quite good performance improvement (code in `BinarySearch1.cs` and results in the table 1 below).",
			  "The linear search was not using AVX2, and for linear AVX2 should definitely work, shouldn't it!? With vectorized linear search and some additional branching optimization the performance is improved by *additional* 30-50% for the most relevant N (code in `BinarySearch2.cs` and results in the table 2 below)."
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "ta* .\nModern hardware can do [lots of stuff](https://software.intel.com/sites/landingpage/IntrinsicsGuide) under this paradigm, leveraging *data-level parallelism* . For example, the simplest thing you can do on modern Intel CPUs is to:\nload 256-bit block of ints (which is \\(\\frac{256}{32} = 8\\) ints),\nload another 256-bit block of ints,\nadd them together,\nwrite the result somewhere else\nand this whole transaction costs the same as loading and adding just two intswhich means we can do 8 times more work. Magic!\n",
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD",
			  "e assume throughout\nthat the registers of the processor of interest are vectors of\n*k* ** 1 scalars",
			  "Table 1 lists the worst-case number of iterations performed\nby binary search (Bin), SIMDized binary search (Bin[ *k* ** 1])\nand *k* -ary search for various dataset sizes and values of *k",
			  "learly, *k* -ary search is the more attractive the larger the\ndataset and the larger the value of *k* .",
			  "uture gen-\nerations of processors will support much larger values of *k*\nand thus provide further efficiency gains",
			  "r example, Intel\nrecently announced that its upcoming processors will sup-\nport the AVX instruction set [6] with 256-bit vector registers\n( *k* = 9 for 32-bit keys) for the 2010 processor generation and\nup to 512-bit vector registers ( *k* = 17) for later generations.",
			  "Similarly, the upcoming Larrabee GPGPU [9]a hybrid be-\ntween a GPU and a multi-core CPUprovides 16-way vec-\ntor registers ( *k* = 17) for integer, single-precision float, and\ndouble-precision float instructi"
			]
		  },
		  {
			"title": "k-ary search on modern processors | Proceedings of the Fifth International Workshop on Data Management on New Hardware",
			"url": "https://dl.acm.org/doi/10.1145/1565694.1565705",
			"excerpts": [
			  "This paper presents novel tree-based search algorithms that exploit the SIMD instructions found in virtually all modern processors. The algorithms are a natural extension of binary search: While binary search performs one comparison at each iteration, thereby cutting the search space in two halves, our algorithms perform *k* comparisons at a time and thus cut the search space into *k* pieces. On traditional processors, this so-called *k* -ary search procedure is not beneficial because the cost increase per iteration offsets the cost reduction due to the reduced number of iterations. On modern processors, however, multiple scalar operations can be executed simultaneously, which makes *k* -ary search attractive. In this paper, we provide two different search algorithms that differ in terms of efficiency and memory access patterns. Both algorithms are first described in a platform independent way and then evaluated on various state-of-the-art processors. Our experiments suggest that *k* -ary search provides significant performance improvements (factor two and more) on most platforms."
			]
		  },
		  {
			"title": "DEX: Scalable Range Indexing on Disaggregated Memory",
			"url": "https://www.vldb.org/pvldb/vol17/p2603-lu.pdf%3C/ee%3E",
			"excerpts": [
			  "The adaptive radix tree:\nARTful indexing for main-memory databases."
			]
		  },
		  {
			"title": "[PDF] Improving index performance through prefetching | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/99d4b6749d93726e007da067c7b36cb06321d1a3",
			"excerpts": [
			  "### Prefetching J+-Tree: A Cache-Optimized Main Memory Database Index Structure",
			  "The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans and can provide better performance on both time (search, scan, update) and space aspects.",
			  "### Fractal prefetching B+-Trees: optimizing both cache and disk performance",
			  "Fractal prefetching B+-Trees are proposed, which embed \"cache-optimized\" trees within \"disk-optimization\" trees, in order to optimize both cache and I/O performance.",
			  "### BS-tree: A gapped data-parallel B-tree",
			  "BS-tree is proposed, an in-memory implementation of the B+-tree that adopts the structure of the disk-based index, setting the node size to a memory block that can be processed fast and in parallel using SIMD instructions.",
			  "### Making B+- trees cache conscious in main memory",
			  "CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node, and introduces two variants of CSB+, which can reduce the copying cost when there is a split and preallocate space for the full node group to reduce the split cost."
			]
		  },
		  {
			"title": "M.Sc. Thesis",
			"url": "https://skemman.is/bitstream/1946/7489/1/MSc_Arni-Mar-Jonsson.pdf",
			"excerpts": [
			  "Prefetching B + -tree (pB + -tree) [9] is another main-memory indexing method, which\nis almost identical to the B + -tree. It uses prefetching (see Section 2.3.3), which allows it\nto have nodes wider than a cache-line, without having to wait an entire cache-miss latency\nfor each cache-line accessed. Wider nodes result in shallower trees, and the benefits of\nhaving a shallow tree usually outweigh the prefetching overhead.",
			  "any modern CPUs have parallel memory systems. They can fetch multiple cache-lines\nfrom main-memory at the same time. Programs can instruct the CPU to fetch a given\ncache-line by issuing so-called prefetch instructions. Multiple prefetch instructions can\nbe executed at the same time. For example, the Alpha 21264 CPU has a 150 cycle cache-\nmiss latency. It can, however, fetch 15 cache-lines at the same time, meaning that a\ncache-line can be delivered to cache every 10 cycles. If a program knows what cache-line\nit will need 150 cycles later, it can prefetch it and 150 cycles later it is available in c",
			  "This way the perceived cache-miss latency is 0 cycles, even though the actual cache-miss\nlatency is unchanged",
			  "**Node Layout and Prefetching**",
			  "Pointer elimination and prefetching as described in this chapter are complementary tech-\nniques [9]. It is possible to increase the node width of the CSB + -tree and prefetching\nnodes like the pB + -tree does, resulting in the Prefetching CSB + -Tree (pCSB + -tree).\nThat way, you get better performance than the pB + -tree, since the branch factor is slightly\nhigher.",
			  "The cache-performance of the pCSB + -tree can be found using the same method as pB + -\ntree and doubling the branch factor.",
			  "Indicespredominantly B + -treesare a key performance component of DBMSs. Un-\nfortunately, however, B + -trees have been shown to utilize cache memory poorly [9], trig-\ngering the development of many cache-conscious indices. The CSS-tree [26] and CSB + -\ntree [25] improve cache performance by not storing pointers to all the children of a node,\neffectively compacting the index structure and improving locality.",
			  "In their seminal paper, Ailamaki et al. [2] showed that less than half of the CPU\ntime for commercial DBMSs is spent on computations.",
			  "he Prefetching CSB + -Tree",
			  "he Prefetching CSB + -Tree",
			  "**General Description**",
			  "**General Description**",
			  "Chapter 7",
			  "**Conclusions**",
			  "In this thesis we have studied the performance of the pB + -tree on the Itanium 2 processor."
			]
		  },
		  {
			"title": "(PDF) Improving Index Performance through Prefetching",
			"url": "https://www.researchgate.net/publication/2528098_Improving_Index_Performance_through_Prefetching",
			"excerpts": [
			  "This paper proposes and evaluates Prefetching B -Trees#, which use prefetching to accelerate two important operations on B -Tree indices: searches and range scans.",
			  "To accelerate searches, pB -Trees use prefetching to e create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B -Tree, thereby decreasing the number of expensive misses when going from parenttochild without signi increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.2.5 for main-memory B -Trees.",
			  "In addition, it outperforms and is complementary to #Cache-SensitiveB -Trees.",
			  "To accelerate range scans, pB -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys.",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "However, CSS-Trees are designed for decision support workloads with relatively static data.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Nevertheless, for applications that require incremental updates, traditional B + -Trees perform well.",
			  "Our goal is to make B + -Trees as cache conscious as CSS-Trees without increasing their update cost too much.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "The rest of the children can be found by adding an offset to that address.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "CSB + -Trees support incremental updates in a way similar to B + -Trees.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Content:",
			  "Content:",
			  "We also introduce two variants of CSB + -Trees. Segmented CSB + -Trees divide the child nodes into segments.",
			  "Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node.",
			  "Segmented CSB + -Trees can reduce the copying cost when there is a split since only one segment needs to be moved.",
			  "Full CSB + -Trees preallocate space for the full node group and thus reduce the split cost.",
			  "Our performance studies show that CSB + -Trees are useful for a wide range of applications."
			]
		  },
		  {
			"title": "Understanding the efficiency of ray traversal on GPUs | Proceedings of the Conference on High Performance Graphics 2009",
			"url": "https://dl.acm.org/doi/10.1145/1572769.1572792",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Deep-Learning-Driven Prefetching for Far Memory",
			"url": "https://arxiv.org/html/2506.00384v1",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications | Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2010.44",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.semanticscholar.org/paper/The-adaptive-radix-tree%3A-ARTful-indexing-for-Leis-Kemper/6abf5107efc723c655956f027b4a67565b048799",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This works well for simple access patterns, like iterating over array in increasing or decreasing order, but for something complex like what we have here its not going to perform well.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree .",
			  "This blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.",
			  "The goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!",
			  "The source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Speeding up independent binary searches by interleaving them  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/",
			"excerpts": [
			  "Each data access is done using fewer than 10 instructions in my implementation, which is far below the number of cycles and small compared to the size of the instruction buffers, so finding ways to reduce the instruction count should not help.",
			  "redit: This work is the result of a collaboration with Travis Downs and Nathan Kurz, though all of the mistakes are mine.\nD",
			  "Daniel Lemire, \"Speeding up independent binary searches by interleaving them,\" in *Daniel Lemire's blog* , September 14, 2019, https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/ .",
			  "The condition move instructions are pretty much standard and old at this point.",
			  "On Cannon Lake, however, you should be able to do better."
			]
		  },
		  {
			"title": "abseil / Swiss Tables Design Notes",
			"url": "https://abseil.io/about/design/swisstables",
			"excerpts": [
			  "Within Swiss tables, the result of the hash function produces a 64-bit hash\nvalue. We split this value up into two parts:\nH1, a 57 bit hash value, used to identify the element index within the table\nitself, which is truncated and modulated as any normal hash value would be for\nlookup and insertion purposes.\nH2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "Swiss tables hold a densely packed array of metadata, containing presence\ninformation for entries in the table. This presence information allows us to\noptimize both lookup and insertion operations. This metadata adds one byte of\noverhead for every entry in the table.",
			  "The metadata of a Swiss table stores presence information (whether the element\nis empty, deleted, or full). Each metadata entry consists of one byte, which\nconsists of a single control bit and the 7 bit H2 hash. The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "When searching for items in the table we use [SSE instructions](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) to scan for\ncandidate matches. The process of finding an element can be roughly summarized\nas follows:\nUse the H1 hash to find the start of the bucket chain for that hash.\nUse the H2 hash to construct a mask.\nUse SSE instructions and the mask to produce a set of candidate matches.\nPerform an equality check on each candidate.\nIf no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates. Note that a deleted element does not cease\nprobing, though an empty element would.\nSteps 2+3 can be summarized visually as:\nEquivalent code for this lookup appears below:\nThis process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "Use the H2 hash to construct a mask.",
			  "Perform an equality check on each candidate.",
			  "This process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "This metadata adds one byte of\noverhead for every entry in the table.",
			  "H2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "Use the H1 hash to find the start of the bucket chain for that hash.",
			  "Use SSE instructions and the mask to produce a set of candidate matches.",
			  "If no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates.",
			  "The H2 hash bits are stored separately within the metadata section of\nthe table."
			]
		  },
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "For a keys hash, split into:\n`H1` : upper 57 bits to compute the group index\n`H2` : lower 7 bits as the fingerprint stored in the control byte",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "It uses an **open addressing** hash table structure.",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "Linear Probing ( `i + j` ): Move one slot ( `j` ) at a time from the original hash position ( `i` )",
			  "Quadratic Probing( `i + j` ): Move `j` slots from the original hash position ( `i` )",
			  "Swiss Table has the following key characteristics:",
			  "It combines **linear probing** with the **Robin Hood hashing algorithm** to resolve collisions.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split)",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split) > Lookup flow (high level):",
			  "Use `H1` to pick a starting group.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > Open Addressing Hash Table"
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Swiss Tables boast improvements to efficiency and provide C++11 codebases early\naccess to APIs from C++17 and C++20.",
			  "\nThese hash tables live within the [Abseil `container` library](",
			  "We are extremely pleased to announce the availability of the new Swiss Table family of hashtables in Abseil and the `absl::Hash` hashing framework that allows easy extensibility for user defined types.",
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "The magic of SwissTables** is in the implementation detail of how it determines if the group contains the key or not. Instead of iterating over all the slots in the group, **SwissTables** leverage the Control Word!",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Swiss Tables, popularised by Google's Abseil C++ library, takes a different approach, primarily using **open addressing** (instead of chaining) with a clever variation of **linear probing** leveraging a dedicated metadata array."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "The vector search is coded using SIMD intrinsics, SSE2 on x86_64 and\nNEON on aarch64. These instructions are a non-optional part of those\nplatforms (unlike later SIMD instruction sets like AVX2 or SVE), so no\nspecial compilation flags are required. The exact vector operations\nperformed differs between x86_64 and aarch64 because aarch64 lacks a\nmovemask instruction, but the F14 algorithm is the same.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate).",
			  "F14 performs collision resolution if a chunk overflows or if two keys both pass the filtering step. The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "The lower bits of the full hash code determine the chunk. The upper bits are used to filter which slots in a chunk might hold the search key.",
			  "Chunking is an effective strategy because the chance that 15 of the tables keys will map to a chunk with 14 slots is much lower than the chance that two keys will map to one slot. For instance, imagine you are in a room with 180 people. The chance that one other person has the same birthday as you is about 50 percent, but the chance that there are 14 people who were born in the same fortnight as you is much lower than 1 percent.",
			  "Below is a plot of the likelihood that an algorithm wont find a search key in the very first place it looks. The happiest place on the graph is the bottom right, where the high load factor saves memory and the lack of collisions means that keys are found quickly with predictable control flow. Youll notice that the plot includes lines for both F14 ideal and F14 with 7-bit tag. The former includes only chunk overflow, while the latter reflects the actual algorithm. Theres a 1/128 chance that two keys have the same 7-bit tag even with a high-quality hash function.",
			  "The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "Collisions are the bane of a hash table: Resolving them creates unpredictable control flow and requires extra memory accesses. Modern processors are fast largely because of pipelining  each core has many execution units that allow the actual work of instructions to overlap."
			]
		  },
		  {
			"title": "Database Processing-in-Memory: An Experimental Study",
			"url": "https://pages.cs.wisc.edu/~yxy/cs839-s20/papers/p334-kepe.pdf",
			"excerpts": [
			  "ns. The hash join and\naggregation require the *gather* and *scatter* SIMD memory\ninstructions to load and store multiple entries of hash tables.",
			  "n par-\nticular, we present a new SIMD sorting algorithm that re-\nquires fewer memory instructions compared to the state of\nthe art [21]. For each operator, we gauge the latency and en-\nergy spend to process TPC-H and Zipf distribution dataset",
			  "Finally, the sorting operation and sort-merge join require the\n*min/max* and *shuffle* SIMD instructions.",
			  "**SIMD units** Unified func. units (integer + floating-point) @1 GHz;",
			  "**4.**",
			  "**IMPLEMENTATION DETAILS OF THE**",
			  "**SIMD QUERY OPERATORS**",
			  "In a nutshell, the implementations of the selection and\nprojection operators require SIMD *load* and *store* memory\ninstructions."
			]
		  },
		  {
			"title": "Efficient SIMD and MIMD parallelization of hash-based aggregation by conflict mitigation | Proceedings of the International Conference on Supercomputing",
			"url": "https://dl.acm.org/doi/10.1145/3079079.3079080",
			"excerpts": [
			  "To address this problem, we design a variant of basic bucket hashing and a *bucketized* aggregation procedure that can utilize both SIMD and MIMD parallelism efficiently. Our approach first adds distinct offsets to input rows on different SIMD lanes, which reduces the possibility of different lanes accessing identical slot in the hash table.",
			  "For parallelization across cores, we adopt separate hash tables and optimize with parallel reduction and a hybrid approach.",
			  "### SIMD Vectorized Hashing for Grouped Aggregation"
			]
		  },
		  {
			"title": "40x faster hash joiner with vectorized execution",
			"url": "https://www.cockroachlabs.com/blog/vectorized-hash-joiner/",
			"excerpts": [
			  "\nAlso, as you might have guessed from the word \"vectorized\", organizing data in this batched, columnar fashion is the primary prerequisite for using SIMD CPU instructions, which operate on a vector of data at a time. ",
			  "Now that we have a taste of what vectorized execution and hash joiners are, let's take it one step further and combine the two concepts. The challenge is to break down the hash join algorithm into a series of simple loops over a single column, with as few run-time decisions, if statements and jumps as possible. Marcin Zukowski described one such algorithm to implement a many-to-one inner hash join in his paper \" [**Balancing Vectorized Query Execution with Bandwidth-Optimized Storage**](https://dare.uva.nl/search?identifier=5ccbb60a-38b8-4eeb-858a-e7735dd37487) \". This paper laid invaluable groundwork for our vectorized hash join operator."
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best.",
			  ".\n**Hash Tables.** Richter et al . [ 66 ] conduct an analysis of scalar\nhashing schemes and derive a decision guide that focuses on the\nworkload at hand. They implement a variant of VLP on AVX2. Poly-\nchroniou et al . [ 65 ] differentiate horizontal and vertical vectoriza-\ntion. Vertical vectorization looks up multiple keys in parallel, which\nrequires scatter/gather operations and bulk inserts/lookups. Hori-\nzontal vectorization serves as a drop-in replacement for scalar hash\ntables. Pietrzyk et al . [ 63 ] implement a conflict detection-aware ver-\nsion of vertical VLP. Behrens et al . [ 13 ] use OpenCL to implement\nvertical VFP. Metas F14 [ 20 , 21 ] and Googles Abseil containers [ 28 ]\nare industry implementations of BBC using SSE/NEON.",
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better."
			]
		  },
		  {
			"title": "Faster Go maps with Swiss Tables",
			"url": "https://go.dev/blog/swisstable",
			"excerpts": [
			  "This improvement to probing behavior allowed both the Abseil and Go implementations to increase the maximum load factor of Swiss Table maps ...Read more",
			  "Compute hash(key) and break the hash into two parts: the upper 57-bits (called h1 ) and the lower 7 bits (called h2 ).  The upper bits ( h1 ) ...Read more"
			]
		  },
		  {
			"title": "Deep Dive Into GO 1.24 Swiss Table-Part1 | by Rajesh Samala | Medium",
			"url": "https://medium.com/@samal.rajesh/deep-dive-into-go-1-24-swiss-table-part1-0d96e49630ee",
			"excerpts": [
			  "One of its key optimizations is the use of SIMD hardware to perform parallel operations on multiple control bytes simultaneously.",
			  "Slots where `h2` does match are potential matches, but we must still check the entire key, as there is potential for collisions (1/128 probability of collision with a 7-bit hash, so still quite low).",
			  "It is high performance hash-table, it replace earlier hashmap which is based on open addressing with linear probing. It enhanced map type with open addressing, quadratic probing, SIMD optimizations and cache aligned memory layouts.",
			  "Swiss table achieves the faster lookups, lower memory usage and better scalability.",
			  "Each group contains 8-slots and an 8-byte control words(64bit, 1byte per slots)",
			  "The control word encodes:",
			  "State: Empty(0xff), occupied or deleted",
			  "H2: A 7 bit fragment of 64-bit hash, it used for quick slot filtering.",
			  "from abseil.io",
			  "For example, a 64-bit control word(8-bytes, each byte represent a slot) comparison can be completed in a single SIMD operation, allowing the table to process multiple slots in parallel, significantly improving lookup and insertion performance."
			]
		  },
		  {
			"title": "SwissTable: A High-Performance Hash Table Implementation - DEV Community",
			"url": "https://dev.to/huizhou92/swisstable-a-high-performance-hash-table-implementation-1knc",
			"excerpts": [
			  "SwissTable uses a new metadata control mechanism to significantly reduce unnecessary `key` comparisons and leverages SIMD instructions to boost throughput.",
			  "In `swisstable` , `ctrl` is an array of `metadata` , corresponding to the `group[K, V]` array. Each `group` has 8 `slots` .",
			  "The hash is divided into `57 bits` for H1 to determine the starting `groups` , and the remaining `7 bits` called H2, stored in `metadata` as the hash signature of the current key for subsequent search and filtering.",
			  "The key advantage of `swisstable` over traditional hash tables lies in the metadata called `ctrl` . Control information includes:",
			  "Whether a slot is empty: `0b10000000`",
			  "Whether a slot has been deleted: `0b11111110`",
			  "The key's hash signature (H2) in a slot: `0bh2`",
			  "Multiplying `h2` by `0x0101010101010101` to get a uint64, allowing simultaneous comparison with 8 `ctrl` values.",
			  "The process of adding data in `swisstable` involves several steps:",
			  "Calculate the hash value and split it into `h1` and `h2` . Using `h1` , determine the starting groups.",
			  "Use `metaMatchH2` to check the current group's `metadata` for a matching `h2` . If found, further check for the matching key and update the value if they match.",
			  "If no matching key is found, use `metaMatchEmpty` to check for empty `slots` in the current group. Insert the new key-value pair if an empty slot is found and update the `metadata` and `resident` count.",
			  "If no empty slots are available in the current group, perform linear probing to check the next `groups` ."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases",
			"url": "https://15721.courses.cs.cmu.edu/spring2016/papers/p1493-polychroniou.pdf",
			"excerpts": [
			  "Our\nvectorization principle is to process a different key per SIMD\nlane using gathers to access the hash table.",
			  "Hash tables are used in database systems to execute joins\nand aggregations since they allow constant time key lookups.",
			  "The vectorized implementation of probing a hash table\nusing a linear probing scheme is shown in Algorithm 5."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases | Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/10.1145/2723372.2747645",
			"excerpts": [
			  " Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[D",
			  " Section Title: Rethinking SIMD Vectorization for In-Memory Databases > References",
			  "Content:\n[Digital Library](/doi/10.14778/2002938.2002940)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.14778%2F2002938.2002940)\n[23]\nR. Pagh et al. Cuckoo hashing. J. Algorithms, 51(2):122--144, May 2004.\n[Digital Library](/doi/10.1016/j.jalgor.2003.12.002)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016%2Fj.jalgor.2003.12.002)\n[24]\nH. Pirk et al. Accelerating foreign-key joins using asymmetric memory channels. In ADMS, 2011.\n[Google Scholar](https://scholar.google.com/scholar?q=H.+Pirk+et+al.+Accelerating+foreign-key+joins+using+asymmetric+memory+channels.+In+ADMS%2C+2011.)\n[25]\nO. Polychroniou et al. High throughput heavy hitter aggregation for modern SIMD processors. In DaMoN, 2013.\n[Digital Library](/doi/10.1145/2485278.2485284)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2485278.2485284)\n[26]\nO. Polychroniou et al. A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort. In SIGMOD, pages 755--766, 2014.\n[Digital Library](/doi/10.1145/2588555.2610522)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2588555.2610522)\n[27]\nO. Polychroniou et al. Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[Digital Library](/doi/10.1145/2619228.2619234)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2619228.2619234)\n[28]"
			]
		  },
		  {
			"title": "Library-based Prefetching for Pointer-intensive Applications",
			"url": "http://csl.stanford.edu/~christos/publications/2006.library_prefetch.manuscript.pdf",
			"excerpts": [
			  "In general, compiler-based prefetching has to handle the difficulty of data- structure layout analysis and pointer disambiguation."
			]
		  },
		  {
			"title": "A prefetching indexing scheme for in-memory database ...",
			"url": "https://www.sciencedirect.com/science/article/pii/S0167739X24000840",
			"excerpts": [
			  "There are two typical cache prefetching strategies: reducing the number of cache misses and reducing the impact of cache misses. Generally, a database workload ..."
			]
		  },
		  {
			"title": "Optimal Prefetching in Random Trees",
			"url": "https://inria.hal.science/hal-03361953v2/document",
			"excerpts": [
			  "Prefetching is a basic technique underlying many computer science applications. Its main purpose is to reduce the time needed to access some ..."
			]
		  },
		  {
			"title": "A speculation-friendly binary search tree",
			"url": "https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/cpe.4883",
			"excerpts": [
			  "We introduce a speculation-friendly tree (s-tree for short) as a tree that transiently breaks its balance structural invariant without hampering the abstraction ...Read more"
			]
		  },
		  {
			"title": "Fractal Prefetching B+-Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/CMU-CS-02-115.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effectively optimizes both CPU cache and disk performance on modern processors, for ...Read more"
			]
		  },
		  {
			"title": "Improving index performance through prefetching | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/abs/10.1145/376284.375688",
			"excerpts": [
			  "This paper proposes and evaluate *Prefetching B* + - *Trees* (pB + -Trees), which use prefetching to accelerate two important operations on B + -Tree indices: searches and range scans.",
			  "To accelerate searches, pB + -Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B + -Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B + -Trees.",
			  "In addition, it outperforms and is complementary to Cache-Sensitive B + -Trees.",
			  "To accelerate range scans, pB + -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB + -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "ur results show that this technique yields over a *sixfold* speedup on range scans of 1000+ keys",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | Proceedings of the seventh international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/237090.237190",
			"excerpts": [
			  "Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors.",
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures.",
			  "Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme ( *greedy prefetching* ) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000.",
			  "Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45% for the applications we study."
			]
		  },
		  {
			"title": "Lecture 22 Prefetching Recursive Data Structures",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s16/www/lectures/L22-Prefetching-Pointer-Structures.pdf",
			"excerpts": [
			  "Summary of Prefetching Algorithms\n\nGreedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "**Carnegie Mellon**\n**Lecture 22**\n**Prefetching Recursive Data Structures**\nMaterial from: C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data\nStructures. In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "Eliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45%",
			  "Add new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n\nTrade space & time for better control over prefetching distances",
			  " History-Pointer Prefetching",
			  "\nCreation order equals major traversal order in treeadd & perimeter",
			  " and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in tr",
			  " Data-Linearization Prefetching",
			  "No pointer dereferences are required",
			  "Map nodes close in the traversal to contiguous memory",
			  "hree schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetch",
			  " improves performance significantly for half of Olden",
			  "Greedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "call: Example Code with Prefetching Arrays\n15-745: Prefetching Pointer Structures\n3\n**for (i = 0; i < 3; i++)**\n**for (j = 0; j < 100; j++)**\n**A[i][j] = B[j][0] + B[j+1][0];**\nOriginal Code\n**prefetch(&B[0][0]);**",
			  "Applicable because a list structure does not change over time",
			  "Improved accuracy outweighs increased overhead in this case",
			  "**H** = history-pointer prefetching",
			  "Health",
			  "Health",
			  "Performance of Data-Linearization Prefetching",
			  " hence data linearization is done without data restructuring",
			  "9% and 18% speedups over greedy prefetching through:",
			  " 94%->78% in perimeter, 87%->81% in treeadd\n",
			  " while maintaining good coverage factors:",
			  " 100%->80% in perimeter, 100%->93% in treeadd",
			  "**G** = greedy prefetching",
			  "**G** = greedy prefetching",
			  "**D** = data-linearization prefetching",
			  "15-745: Prefetching Pointer Structures",
			  "15-745: Prefetching Pointer Structures",
			  "load stall",
			  "load stall",
			  "load stall",
			  "store stall",
			  "store stall",
			  "store stall",
			  "inst. stall",
			  "inst. stall",
			  "inst. stall",
			  "busy",
			  "busy",
			  "busy",
			  "Three schemes to overcome the pointer-chasing problem:",
			  "Carnegie Mellon",
			  "\n**preorder(treeNode * t){**\n**if (t != NULL){**\n**pf(t->left);**\n**pf(t->right);**\n**process(t->data);**\n**preorder(t->left);**\n**preorder(t->right);**\n**}**\n*",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "**O** = original",
			  "Conclusions",
			  "Automated greedy prefetching in SUIF",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**"
			]
		  },
		  {
			"title": "Automatic compiler-inserted prefetching for pointer-based ...",
			"url": "https://ieeexplore.ieee.org/document/752654/",
			"excerpts": [
			  "As the disparity between processor and memory speeds continues to grow, memory latency is becoming an increasingly important performance bottleneck.",
			  "While software-controlled prefetching is an attractive technique for tolerating this latency, its success has been limited thus far to array-based numeric codes.",
			  "In this paper, we expand the scope of automatic compiler-inserted prefetching to also include the recursive data structures commonly found in pointer-based applications.",
			  "We propose three compiler-based prefetching schemes, and automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler.",
			  "Our experimental results demonstrate that compiler-inserted prefetching can offer significant performance gains on both uniprocessors and large-scale shared-memory multiprocessors."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  " We claim that we can achieve $O(\\log_B N)$ page accesses, but without having to know $B$ ahead of time.",
			  "The data structure were using is a good old balanced Binary Search Tree.",
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page.",
			  "One application of the recursive van Embde Boas layout",
			  "Suppose the page size is $B$. Every time step, the height of atoms halves.",
			  "Were interested in the height of the atoms at the first time step where an entire atom can fit in a page.",
			  "Since the number of vertices in a complete binary tree grows exponentially with height, that happens when atoms have height $\\Theta(\\log B)$. T",
			  ". Then, analysing the layout at this resolution, we can fit any atom (which all have the same height of $\\Theta(\\log B)$) into the cache with 1 page load*.",
			  "Then, now consider what happens in a search. A search basically consists of a path from the root of the BST to some leaf*. This path will spend some time in the first atom, until it reaches the leaf of the atom and goes into the next atom, and so forth.",
			  ". Since the path always start at the root vertex of an atom and ends on a leaf, it will spend $\\Theta(\\log B)$ steps in that atom.",
			  "Since the overall search path is $\\log N$ steps long, well need $O(\\frac{\\log N}{\\log B}) = O(\\log_B N)$ atoms, and thats the number of page accesses we need."
			]
		  },
		  {
			"title": "Lab note #044 Sailing by cache-oblivious data structures",
			"url": "https://interjectedfuture.com/lab-notes-044-sailing-by-cache-oblivious-data-structures/",
			"excerpts": [
			  "Cache-oblivious data structures base their Big-O on the number of cache line transfers between different levels of the memory hierarchy to minimize its growth as the data set gets bigger.",
			  "Through it, I learned about the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "If cache-oblivious works as advertised, why haven't I heard of any modern databases using it as an index?",
			  "the idea is 25 years old."
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "http://pdl.cmu.edu/PDL-FTP/Database/CMU-CS-00-177.pdf",
			"excerpts": [
			  "Luk and Mowry proposed three solutions to the pointer-chasing problem 13, 14 ... We then prefetch the next chunk ahead in the jump-pointer array.Read more"
			]
		  },
		  {
			"title": "[PDF] Automatic Compiler-Inserted Prefetching for Pointer-Based Applications | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Automatic-Compiler-Inserted-Prefetching-for-Luk-Mowry/be36f6c5b363116faf91e6eb62f5585b226656a5",
			"excerpts": [
			  "The scope of automatic compiler-inserted prefetching is expanded to also include the recursive data structures commonly found in pointer-based applications, ..."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Our goal: In this paper, we aim to provide an effective, bandwidth- efficient, and low-cost solution to prefetching linked data structures by 1) overcoming the ...Read more",
			  "is paper proposes a low-cost hardware/software cooperative*\n*technique that enables bandwidth-efficient prefetching of linked data*\n*structure",
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,",
			  "The prefetcher brings cache blocks into the L2 (last-\nlevel) cache, since we use an out-of-order execution machine that can\ntolerate short L1-miss latencies",
			  "far ahead of the demand miss\nstream the prefetcher can send requests is determined by the *Prefetch*\n*Distance* parameter.",
			  "ch relies on the observation that most virtual addresses share com-\n ... \ning and object metadata. In *PLDI* , 2004."
			]
		  },
		  {
			"title": "Lecture 21 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s11/public/lectures/L21-Data-Prefetching.pdf",
			"excerpts": [
			  "\nI.\nPrefetching for Arrays\nII. Prefetching for Recursive Data Structures\nReading: ALSU 11 11 4\n**Carnegie Mellon**\nReading: ALSU 11.11.4\nAdvanced readings (optional):\nT.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm for\nPrefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.\nC.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures. In\nProceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.\nTodd C. Mowry\n15-745: Data Prefetching\n1\nThe Memory Latency Problem\n**Carnegie Mellon**\n\n processor speed >>  memory speed\n\ncaches are not a panacea\nTodd C. Mowry\n15-745: Data Prefetching\n2\nUniprocessor Cache Performance on Scientific Code\n\nApplications from SPEC, SPLASH, and NAS Parallel.\nM\nb\nt\nt\ni\nl f MIPS R4000 (100 MH )\n**Carnegie Mellon**\n\nMemory subsystem typical of MIPS R4000 (100 MHz):\n 8K / 256K direct-mapped caches, 32 byte lines\n miss penalties: 12 / 75 cycles\n\n8 of 13 spend > 50% of time stalled for memory\nTodd C. Mowry\n15-745: Data Prefetching\n3\nPrefetching for Arrays: Overview\n\nTolerating Memory Latency\n\nPrefetching Compiler Algorithm and Results\n\nImplications of These Results\n**Carnegie Mellon**\nTodd C. Mowry\n15-745: Data",
			  "Performance of History-Pointer Prefetching\n**O** = original\n\nApplicable because a list structure does not change over time\n**O**\noriginal\n**G** = greedy prefetching\n**H** = history-pointer prefetching\nHealth\n**Carnegie Mellon**\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)\n\nImproved accuracy outweighs increased overhead in this case",
			  "Performance of Data-Linearization Prefetching\n**O** = original\n**G**\nd\nf t hi\n\nCreation order equals major traversal order in treeadd & perimeter\nhence data linearization is done without data restructuring\n**G** = greedy prefetching\n**D** = data-linearization prefetching\n**Carnegie Mellon**\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in treeadd",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "**Lecture 21**",
			  "**Compiler Algorithms for Prefetching Data**",
			  "**Compiler Algorithms for Prefetching Data**"
			]
		  },
		  {
			"title": "Lectures 26-27 Compiler Algorithms for Prefetching Data",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s15/public/lectures/L26-27-Data-Prefetching.pdf",
			"excerpts": [
			  "Propose 3 schemes to overcome the pointer-chasing problem:",
			  "**Compiler Algorithms for Prefetching Data**",
			  "II. Prefetching for Recursive Data Structures",
			  "f Data-Linearization Prefetching",
			  "59",
			  "Conclusions",
			  "Conclusions",
			  "Propose 3 schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetching",
			  "Automated greedy prefetching in SUIF",
			  "Automated greedy prefetching in SUIF",
			  "T.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm fo",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures.",
			  "fewer unnecessary prefetches:",
			  "94%->78% in perimeter, 87%->81% in treeadd",
			  "while maintaining good coverage factors:",
			  "100%->80% in perimeter, 100%->93% in treeadd",
			  "**Lectures 26-27**",
			  "Reading: ALSU 11.11.4",
			  "Advanced readings (optional):",
			  "Prefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.",
			  "In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "1",
			  "\ncaches are not a panacea",
			  "Uniprocessor Cache Performance on Scientific Code",
			  "Applications from SPEC, SPLASH, and NAS Parallel.\n",
			  "\nMemory subsystem typical of MIPS R4000 (100 MHz):",
			  " 8K / 256K direct-mapped caches, 32 byte lines\n",
			  " miss penalties: 12 / 75 cycles",
			  "8 of 13 spend > 50% of time stalled for memory",
			  "3",
			  "Prefetching for Arrays: Overview",
			  "Tolerating Memory Latency",
			  "Prefetching Compiler Algorithm and Results",
			  "Implications of These Results",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "4",
			  "2",
			  "2",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "Coping with Memory Latency",
			  "**Reduce Latency:**",
			  " Locality Optimizations\n",
			  " reorder iterations to improve cache reuse\n",
			  "**Tolerate Latency:**",
			  " move data close to the processor before it is needed\n",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2017/Papers-Session2/HPG2017_AcceleratedSingleRayTracing.pdf",
			"excerpts": [
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2"
			]
		  },
		  {
			"title": "Lecture 27 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s12/public/lectures/L27-Data-Prefetching-1up.pdf",
			"excerpts": [
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n\n1\n\n",
			  "reedy Prefetching\n\nOur proposals:\n\n\nuse existing pointer(s) in ni to approximate &ni+d\n\na dd new pointer(s) to ni to a pproximate &ni+d\n\nni\nni+d\n\nn\n\na new p oin",
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n",
			  "History-Pointer Prefetching\n\n\nAdd new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n",
			  "Data-Linearization Prefetching\n\n\nNo pointer dereferences are required\n\nMap nodes close in the traversal to contiguous memory\n",
			  "Performance of Compiler-Inserted Greedy Prefetching\n\nload stall\nO = Original\n\nG = Compiler-Inserted Greedy Prefetching\n\nload stall\nstore stall\ninst. stall\nbusy\n\n\nEliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45",
			  "Performance of History-Pointer Prefetching\n\nO = original\n\nG = greedy prefetching\n\nH = history-pointer prefetching\n\n\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n\nHealth\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "Performance of Data-Linearization Prefetching\n\nO = original\nG = greedy prefetching\n\nD = data-linearization prefetching\n\n\nCreation order equals major traversal order in treeadd & perimeter\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n\n 94%->78% in perimeter, 87%->81% in treeadd\n\n while maintaining good coverage factors:\n\n 100% >80% in perimeter  100% >93% in treeadd\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance.",
			  "In the evaluation, the proposed solution achieves an average speedup of 1.37  over a set of memory-intensive benchmarks.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/635508.605427",
			"excerpts": [
			  "C.-K. Luk and T. Mowry. Compiler-based prefetching for recursive data structures. In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 222-233, Cambridge, Massachusetts, October 1996. ACM.",
			  "D. Joseph and D. Grunwald. Prefetching using markov predictors. In Proceedings of the 24th Annual International Symposium on Computer Architecture, pages 252-263, Denver, Colorado, June 1997. ACM."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[30] A. Roth, A. Moshovos, and G. S. Sohi. Dependence based prefetching\nfor linked data structures. In *ASPLOS-8* , 1998.",
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown",
			  "Dependence based\nprefetching for linked data structures, *",
			  "indirections through memory as seen in sparse tensor algebra\nand graphs [ 13  15 ], and more generally applications with\npointer chasing [ 10 , 11 ]. The target application influences the\ndata access pattern that the prefetcher tries to identify and\nprefetch for. For example, a common access pattern in sparse\ntensor algebra and graph computations is An [ *...* A1 [ A0 [ i ]] *...* ] .\nCorr",
			  "Yu et al. [ 13 ] (a.k.a. IMP) tries to detect\naccess patterns given by Y [ Z [ i ]] (2-level IMP) and X [ Y [ Z [ i ]]]\n(3-level IMP), for striding loop variable i , and prefetch\ndata by assuming that Y [ Z [ i + ** ]] and X [ Y [ Z [ i + ** ]]] will\nbe needed in the future. Ainsw"
			]
		  },
		  {
			"title": "Dependence Based Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1998/asplos-prefetch-lds.pdf",
			"excerpts": [
			  "k and Mowry [12] proposed and evaluated a greedy compiler\nalgorithm for scheduling software prefetches for linked data struc-\ntures. They showed this scheme to be effective for certain pro-\ngrams, citing instruction overhead and the generation of useless\nprefetches as performance degradation factors for other",
			  "ry [12] presented a case for history-pointer prefetch-\ning, which augments linked structure nodes with prefetching\npointer fields, and data-linearization, in which LDS are program-\nmatically laid out at runtime to allow sequential prefetch machin-\nery to capture their traversal.",
			  "eir\nalgorithm uses type information to identify recurrent pointer\naccesses, including those accessed via arrays, and may have advan-\ntages in tailoring a prefetch schedule to a particular traversal.",
			  "It collects these loads along with the\ndependence relationships that connect them and constructs a\ndescription of the steps the program has followed to traverse the\nstructure.",
			  "Predicting that the program will continue to follow these\nsame steps, a small prefetch engine takes this description and spec-\nulatively executes it in parallel with the original progra",
			  "Linked data structures (LDS) such as lists and trees are used in\nmany important applications."
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Later Luk and\nMowry proposed a compiler-based prefetching scheme for\nrecursive data structures [22].",
			  "This requires extra storage at\nruntime.",
			  "Jump Pointer [29], which has\nbeen used widely to break the serial LDS (linked data struc-\nture) traversal, stores pointers several iterations ahead in the\nnode currently visite",
			  "In a recent work, Gau-\ntam Doshi et al. [14] discussed the downside of software\nprefetching and exploited the use of rotating registers and\npredication to reduce the instruction overhead",
			  "Profile Guided Software Prefetching",
			  "Software prefetching is ineffective in pointer-based pro-\ngrams. To address this problem, Chi K. Luk et al. presented\na Profile Guided Post-Link Stride Prefetching [23] using a\nstride profile to obtain prefetching guidance for the com-\npil",
			  "[22] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching\nFor Recursive Data Structures. In *ASPLOS-7* , pages 222\n233. ACM Press, 1996",
			  "[23] C.-K. Luk, R. Muth, H. Patil, R. Weiss, P. G. Lowney, and\nR. Cohn. Profile-Guided Post-link Stride Prefetching. In\n*ICS-16* , pages 167178. ACM Press, 2002."
			]
		  },
		  {
			"title": "Dynamic Hot Data Stream Prefetching for General-Purpose ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s09/www/papers/prefetch_hds.pdf",
			"excerpts": [
			  "Jump pointers are a software technique for prefetching linked data\nstructures, overcoming the array-and-loop limitation.",
			  "Artificial\njump pointers are extra pointers stored into an object that point to\nan object some distance ahead in the traversal order.",
			  "d. Natural jump pointers are existing pointers in the\ndata\nstructure\nused\nfor\nprefetching.\n",
			  "ng.\nFor\nexample,\ngreedy\nprefetching makes the assumption that when a program uses an\nobject o, it will use the objects that o points to, in the near future,\nand hence prefetches the targets of all pointer fields.",
			  " These\ntechniques were introduced by Luk and Mowry in [22] and refined\nin [5, 18].",
			  "n\ndependence-based prefetching, producer-consumer pairs of loads\nare identified, and a prefetch engine speculatively traverses and\nprefetches them [26].",
			  "\nThe hardware technique that best corresponds to history-pointers is\ncorrelation-based prefetching. As originally proposed, it learns\ndigrams of a key and prefetch addresses: when the key is observed,\nthe prefetch is issued [6]. J"
			]
		  },
		  {
			"title": "2003 Workshop on Duplicating, Deconstructing and ... - PHARM",
			"url": "https://pharm.ece.wisc.edu/wddd/2003/wddd2003_proceedings.pdf",
			"excerpts": [
			  " ware prefetch when bandwidth is limited; with sufficient\nbandwidth software prefetch is the most successful strategy.\nHowever, their research also shows that the combination of\ncache-conscious allocation and software prefetch might not\nlead to further performance improvements, instead it coun-\nteracts changes in bandwidth or latency. Their results are\nsimilar to ours, although we have implemented a different\nsoftware prefetch that does not require any extra memory.\nSeveral researchers have studied hardware prefetch,\nor hybrid schemes, and successfully adapted hardware\nprefetch to pointer-based data structures with irregular ac-\ncess behavior. However, they generally require more hard-\nware than those evaluated in this study. Hardware support\nhas been investigated by the use of lock-up free prefetching,\n[13], and prefetch buffers, [10], and general prefetching in\nhardware is described in [20, 21] together with other cache\nmemory aspects. Karlsson et al., [11], propose a technique\nfor prefetching pointer-based data structures, either in soft-\nware combined with hardware or in software alone, by im-\nplementing prefetch arrays, making it possible to prefetch\nboth short data structures and longer data structures without\nknowing the traversal path. Roth et al. have investigated\nmore adaptable strategies for hybrid prefetch schemes, us-\ning dependence graphs, [18], and jump pointer prefetching,\n[19]. In [19], Roth et al. evaluate a framework for jump-\n ... \n[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999.\n[20] Alan J. Smith. Cache memories. *ACM Computing*\n*Surveys* , 14:3:473530, September 1982.\n[21] Steven P. VanderWiel and David Lilja. Data prefetch\nmechanisms.\n*ACM Computing Surveys* , 32:2:174\n199, June 2000.\n[22] Chengqiang Zhang and Sally A. McKee. Hardware-\nonly stream prefetching and dynamic access order-\ning. In *International Conference on Supercomputing* ,\npages 167175, 2000.\n[23] L. Zhang, S. McKee, W. Hsieh, and J. Carter. Pointer-\nbased prefetching within the impulse adaptable mem-\nory controller: Initial results. In *Proceedings of the*\n*Workshop on Solving the Memory Wall Problem* , June\n2000.\n[24] Craig B. Zilles. Benchmark health considered harm-\nful. *Computer Architecture News* , 29:3, 2001.\n13\n**Comparison of State-Preserving vs. Non-State-Preserving Leakage Control**\n**in Caches**\nDharmesh Parikh\n\n, Yan Zhang\n\n, Karthik Sankaranarayanan\n\n, Kevin Skadron\n\n, Mircea Stan\n\n Dept. ",
			  "[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999."
			]
		  },
		  {
			"title": "APT-GET | Proceedings of the Seventeenth European Conference on Computer Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3492321.3519583",
			"excerpts": [
			  "imin Chen, Anastassia Ailamaki, Phillip B Gibbons, and Todd C Mowry. 2004. Improving Hash Join Performance through Prefetching. In *Proceedings of the 20th International Conference on Data Engineering.* 116.",
			  "mir Roth, Andreas Moshovos, and Gurindar S Sohi. 1998. Dependence based prefetching for linked data structures. *ACM SIGOPS Operating Systems Review* 32, 5 (1998), 115--126",
			  "mison D Collins, Hong Wang, Dean M Tullsen, Christopher Hughes, Yong-Fong Lee, Dan Lavery, and John P Shen. 2001. Speculative pre-computation: Long-range prefetching of delinquent loads. In *Proceedings 28th Annual International Symposium on Computer Architecture.* IEEE, 14--25."
			]
		  },
		  {
			"title": "MetaSys-open-source-cross-layer-metadata- ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/MetaSys-open-source-cross-layer-metadata-management_taco22-arxiv22.pdf",
			"excerpts": [
			  "[30] Chi-Keung Luk and T. C. Mowry. Cooperative Prefetching: Compiler and Hardware Support for Effective Instruction Prefetching in Modern\nProcessors. In *MICRO* , 1998.",
			  "[31] Trishul M Chilimbi and Martin Hirzel. Dynamic Hot Data Stream Prefetching for General-Purpose Programs. In *PLDI* , 2002."
			]
		  },
		  {
			"title": "References - Engineering Information Technology",
			"url": "https://user.eng.umd.edu/~blj/memory/Book-References.pdf",
			"excerpts": [
			  "A. Roth, A. Moshovos, and G. Sohi. 1998. Dependence\nbased prefetching for linked data structures. In\nThe 8th Int. Conf. on Architectural Support for\nProgramming Languages and Operating Systems\n(ASPLOS), pp. 115126, October 1998.",
			  "A. Roth and G. S. Sohi. 1999. Effective jump-pointer\nprefetching for linked data structures. In Proc. 26th Int.\nSymp. on Computer Architecture (ISCA), Atlanta, GA,\nMay 1999.",
			  "E. Rotenberg, S. Bennett, and J. E. Smith. 1996.\nTrace cache: A low latency approach to high\nbandwidth instruction fetching. In Proc. 29th Ann.\nACM/IEEE Int. Symp. on Microarchitecture (MICRO-\n29), pp. 2435, Paris, France, December 1996"
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/248209.237190",
			"excerpts": [
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the ...Read more"
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/384265.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "Dependence based prefetching for linked data structures > Abstract",
			  "Content:\nWe introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Why is SOA (Structures of Arrays) faster than AOS?",
			"url": "https://www.reddit.com/r/C_Programming/comments/9jg7hy/why_is_soa_structures_of_arrays_faster_than_aos/",
			"excerpts": [
			  "I have heard that SOA (Structures of Arrays) is faster due to CPU caching then AOS (Array of Structures). But that doesn't really make sense to me.Read more"
			]
		  },
		  {
			"title": "c - cache locality for a binary tree - Stack Overflow",
			"url": "https://stackoverflow.com/questions/31281905/cache-locality-for-a-binary-tree",
			"excerpts": [
			  "If you want data to be collocated to take advantage of cache locality a simple alternative is to allocate an array of the struct and then allocate from that ...Read more"
			]
		  },
		  {
			"title": "Help with BVH memory access optimizations",
			"url": "https://www.reddit.com/r/GraphicsProgramming/comments/1f90xbi/help_with_bvh_memory_access_optimizations/",
			"excerpts": [
			  "pointer chasing and frequent cache misses.",
			  "flatten the tree.",
			  "Note that `sizeof(BVHNode)` is exactly 32, so a node and one of its children should be pulled together to a cache line as far as I understand. This has made the program twice as much slower!",
			  "Surprisingly this did not improve performance. It is actually comparable to the pointer-based implementation.",
			  "My understanding is that this makes traversal slow due to pointer chasing and frequent cache misses.",
			  "So I quickly moved to flatten the tree."
			]
		  },
		  {
			"title": "Binary search is a pathological case for caches - Paul Khuong: some Lisp",
			"url": "https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/",
			"excerpts": [
			  "Binary search is a pathological case for caches",
			  "\nBinary search suffers from a related ailment when executed on medium\nor large vectors of almost power-of-two size (in bytes)",
			  "Two vectors of 64KB each, allocated at 0x10000 and 0x20000, have the unfortunate property that, for each index, the data at that index in both vectors will map to the same cache lines in a direct-mapped cache with 1024 lines of 64 bytes each.",
			  "the L1D cache has 512 lines of 64 bytes each, L2 4096\nlines, and L3 196608 lines (12 MB). Crucially, the L1 and L2 caches are\n8-way set-associative, and the L3 16-way.",
			  "There are workarounds, on both the hardware and software sides.",
			  "cache lines are power-of-two sizes, as are L1 and L2 set (buckets) counts."
			]
		  },
		  {
			"title": "Locality, B-trees, and splay trees",
			"url": "https://www.cs.cornell.edu/courses/cs312/2004sp/lectures/lec24.html",
			"excerpts": [
			  "Having caches only helps if when the processor needs to get some data, it is\nalready in the cache. Thus, the first time the processor access the memory, it\nmust wait for the data to arrive. On subsequent reads from the same location,\nthere is a good chance that the cache will be able to serve the memory request\nwithout involving main memory. Of course, since the cache is much smaller than\nthe main memory, it can't store all of main memory. The cache is constantly\nthrowing out information about memory locations in order to make space for new\ndata. The processor only gets speedup from the cache if the data fetched from\nmemory is still in the cache when it is needed again. When the cache has the\ndata that is needed by the processor,  it is called a **cache hit** . If\nnot, it is a **cache miss** . The ratio of the number of hits to misses is\ncalled the **cache hit ratio** .",
			  "Caches improve performance when memory accesses exhibit: reads from memory\ntends to request the same locations repeatedly, or at least memory locations\nnear previous requests. A tendency to revisit the same or nearby locations is\nknown as **locality** . Computations that exhibit locality will have a relatively high cache hit\nratio.",
			  "at caches actually store chunks of memory rather than individual words of\nmemory. So a series of memory reads to nearby memory locations are likely to\nmostly hit in the cache. When there is a cache miss, a whole sequence of memory\nwords is requested from main memory at once, because it is cheaper to read\nmemory that way. The cache records cached memory locations in units of **cache\nlines** whose size depends on the size of the cache (typically 4 - 32 words).",
			  "The same idea can be applied to trees. Binary trees are not good for locality\nbecause a given node of the binary tree probably occupies only a fraction of a\ncache line. **B-trees** are a way to get better locality. As in the hash\ntable trick above, we store several elements in a single node -- as many as will\nfit in a cache line."
			]
		  },
		  {
			"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
			"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
			"excerpts": [
			  "is seemingly simple decision can result in **10-100x performance differences** in modern computing system",
			  "The choice between AoS and SoA affects everything from CPU cache efficiency to SIMD vectorization capabilities to GPU memory coalescing patterns.",
			  "Memory Access Pattern Animation"
			]
		  },
		  {
			"title": "Array of Structs and Struct of Arrays | *^^*  HWisnu's blog   ^^",
			"url": "https://hwisnu.bearblog.dev/array-of-structs-and-struct-of-arrays/",
			"excerpts": [
			  "s (AoS) can be less cache-friendly because accessing multiple fields of a single element requires loading multiple cache lines. ",
			  "oA is almost 30% faster based on this simple example, the difference will get significantly magnified with more complex scenario, sometimes SoA (optimized) is 10x faster compared to AoS as shown in this [great article"
			]
		  },
		  {
			"title": "AoS and SoA - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
			"excerpts": [
			  "**Structure of arrays** ( **SoA** ) is a layout separating elements of a [record](/wiki/Record_(computer_science) \"Record (computer science)\") (or 'struct' in the [C programming language](/wiki/C_(programming_language) \"C (programming language)\") ) into one parallel array per [field](/wiki/Field_(computer_science) \"Field (computer science)\") . [[ 1 ]]() The motivation is easier manipulation with packed [SIMD instructions](/wiki/SIMD_instruction \"SIMD instruction\") in most [instruction set architectures](/wiki/Instruction_set_architecture \"Instruction set architecture\") , since a single [SIMD register](/wiki/SIMD_register \"SIMD register\") can load [homogeneous data](/w/index.php?title=Homogeneous_data&action=edit&redlink=1 \"Homogeneous data (page does not exist)\") , possibly transferred by a wide [internal datapath](/wiki/Internal_datapath \"Internal datapath\") (e.g. [128-bit](/wiki/128-bit \"128-bit\") ). If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line. The downside is requiring more [cache ways](/wiki/Cache_way \"Cache way\") when traversing data, and inefficient [indexed addressing](/wiki/Indexed_addressing \"Indexed addressing\") .",
			  "For example, to store *N* points in 3D space using a structure of arrays:",
			  "array of structures",
			  " ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved. This is often more intuitive, and supported directly by most [programming languages](/wiki/Programming_languages \"Programming languages\") .",
			  "AoS vs. SoA presents a choice when considering 3D or [4D vector](/wiki/4D_vector \"4D vector\") data on machines with four-lane SIMD hardware. SIMD ISAs are usually designed for homogeneous data, however some provide a [dot product](/wiki/Dot_product \"Dot product\") instruction [[ 5 ]]() and additional permutes, making the AoS case easier to handle.",
			  "Although most [GPU](/wiki/Graphics_processing_unit \"Graphics processing unit\") hardware has moved away from 4D instructions to scalar [SIMT](/wiki/Single_instruction,_multiple_threads \"Single instruction, multiple threads\") pipelines, [[ 6 ]]() modern [compute kernels](/wiki/Compute_kernel \"Compute kernel\") using SoA instead of AoS can still give better performance due to memory coalescing. [[ 7 ]]()",
			  "SoA is mostly found in languages, libraries, or [metaprogramming](/wiki/Metaprogramming \"Metaprogramming\") tools used to support a [data-oriented design](/wiki/Data-oriented_design \"Data-oriented design\") . Examples include:\n\"Data frames\", as implemented in [R](/wiki/R_(programming_language) \"R (programming language)\") , [Python](/wiki/Python_(programming_language) \"Python (programming language)\") 's Pandas package, and [Julia](/wiki/Julia_(programming_language) \"Julia (programming language)\") 's DataFrames.jl package, are interfaces to access SoA like AoS.\nThe Julia package StructArrays.jl allows for accessing SoA as AoS to combine the performance of SoA with the intuitiveness of AoS."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "It is shown that this type of hash table can benefit from the\ncache locality and multi-threading more than ordinary hash tables.",
			  "Moreover, the batch design\nfor hash tables is amenable to using advanced features of modern processors such as prefetching\nand SIMD vectorization.",
			  "While state-of-the-art research and open-source projects on batch hash\ntables made efforts to propose improved designs by better usage of mentioned hardware features,\ntheir approaches still do not fully exploit the existing opportunities for performance improvements.",
			  "Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing.",
			  "To allow developers to fully take\nadvantage of its performance, we recommend a high-level batch API design.",
			  "Our experimental\nresults show the superiority and competitiveness of this approach in comparison with the alternative\nimplementations and state-of-the-art for the data-intensive workloads of relational join processing,\nset operations, and sparse vector processing.",
			  "The SIMD is a hardware\nfeature that allows the simultaneous execution of an operation on a vector of values.",
			  "On\nthe other hand, prefetching is a hardware feature that allows the program to request future\nmemory accesses in advance and asynchronous to the other computations.",
			  "We will cover the\nmore-detailed definitions of these two concepts later in this section.",
			  " 1 ], Horton [ 9 ] and Cuckoo++[ 23 ] have focused on improving the\nperformance of batch hash tables by applying SIMD and prefetching techniques to a specific\ntype of SIMD-aware batch hash table designs called Bucketized Cuckoo Hash Tables (BCHTs)",
			  " Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing. ",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "Modern CPUs support hardware and software prefetch-\ning. Prefetching improves the performance of a program by amortizing the costs of memory\naccess over tim",
			  "In hash tables, regardless of the hashing scheme, accessing entries is based on the value\nof the computed hash for each provided key.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable.",
			  " \nFigure 6 depicts a generic and high-level algorithm for combining prefetching with vertical\nvectorization (based on the assumption that we take the group-prefetching approach instead\nof standard prefetching",
			  "By having a group of keys as input, before starting the vertical\nvectorization, we define a loop over the group keys (prefetching loop).",
			  "foreach\ngroup in array by GROUP_SIZE {",
			  "// prefetching\nstage",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "performance - Using SIMD/AVX/SSE for tree traversal - Stack Overflow",
			"url": "https://stackoverflow.com/questions/20616605/using-simd-avx-sse-for-tree-traversal",
			"excerpts": [
			  "I've used SSE2/AVX2 to help perform a B+tree search. Here's code to perform a \"binary search\" on a full cache line of 16 DWORDs in AVX2: Copy.Read more"
			]
		  },
		  {
			"title": "What role do branch mispredictions play in hash table ...",
			"url": "https://stackoverflow.com/questions/62997105/what-role-do-branch-mispredictions-play-in-hash-table-lookup-performance",
			"excerpts": [
			  "High performance hash tables would optimise with tricks like checking batches of four keys at once between branches to reduce mispredictions.Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing improves SIMD hash tables",
			"url": "https://reiner.org/cuckoo-hashing",
			"excerpts": [
			  "Baseline SIMD hash tables such as Swiss Tables use SIMD quadratic probing. This starts searching from a position determined by the hash function, searches 4 ...Read more"
			]
		  },
		  {
			"title": "Cuckoo hashing - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Cuckoo_hashing",
			"excerpts": [
			  "Cuckoo hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table, with worst-case constant lookup time.Read more"
			]
		  },
		  {
			"title": "SIMD Vectorized Hashing for Grouped Aggregation",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Gurumurthy:ADBIS18.pdf",
			"excerpts": [
			  "Cuckoo hashing resolves collision by using multiple hash tables [9]. These tables resolve collision by swapping collided keys. The collided key, during ...Read more"
			]
		  },
		  {
			"title": "SIMD Acceleration for Main-Memory Index Structures",
			"url": "https://wwwiti.cs.uni-magdeburg.de/iti_db/publikationen/ps/auto/Wallewein-EisingBS:BDAS18.pdf",
			"excerpts": [
			  "processor architectures [1]. Several index structures have already shown that the\nbottleneck from RAM to CPU can be overcome using Single Instruction Multiple\nData (SIMD) [24] operations. These index structures include: the k-ary Search\nTree (Seg-Tree) [5], Adapted Radix Tree (ART) [6], Fast Architecture Sensitive\nTree (FAST) [7], and Vector-Advanced and Compressed Structure Tree (VAST)\n[8]. All approaches use SIMD only for key comparison within tree traversal and\ntry to decrease the key size to fit more keys into one SIMD register. There-\nfore FAST and Seg-Tree only provide implementations for search algorithms",
			  "In this work we make the following contributions:\n**** We compare different optimizations of index structures to fulfill requirements\nof modern database systems\n2\nSIMD Acceleration for Index Structures\n**** We highlight the usage of SIMD and the cache-line adaptations in all ap-\nproaches\n**** We state the performance impact of optimizations on the index structures\n**** We discuss the usefulness of optimizations for our own index structure Elf [9].",
			  "We organized the rest of the paper as follows. In Section 2, we give the prelim-"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "the scatter instruction is only\navailable in limited types of processors hence the vertically-vectorized insertions can only be\nimplemented on specific hardware.",
			  "In each\nvectorized lookup, the vertical approach will pass a vector (of register size) of inputs through\nthe lookup process and by using mask registers and advanced SIMD features (like SIMD\npermutations) probe those keys at the same time.",
			  "rizontal vectorization, each cell of the hash-table entries array is bucketed into *N*\ninner cells. Then, while doing a lookup on the hash table and after computing the hash\nvalue, using the SIMD logical operations, the lookup algorithm can concurrently check the\nvalue of *N* bucket key",
			  "vertical vectorization yields\nhigher performance than the horizontal approach."
			]
		  }
		],
		"reasoning": "The user is looking for techniques to optimize parallel tree traversal, specifically addressing scattered memory access and maintaining SIMD efficiency. The provided `finegrained_field_value` outlines several database and hashing optimization techniques that are relevant to this problem. \n\n**Branchless SIMD Node Search:**\nSeveral excerpts discuss the benefits of SIMD for tree traversal and the issues with conditional branches. For instance, it is noted that SIMD instructions can be used to perform multiple comparisons at once, and that eliminating conditional branches reduces branch mispredictions, which are expensive on modern CPUs. The idea of using SIMD to compare against all elements regardless of the outcome, computing a bitmask, and then using instructions like `ffs` to find the relevant bit is directly mentioned as a way to avoid branching. This approach is explicitly applied to B-tree search, where 8 elements can be loaded and compared in parallel using AVX2, and the resulting vector mask can be used to determine the next step without explicit branches. The concept of permuting nodes during preprocessing to correct for SIMD instruction output is also discussed, further supporting the idea of optimizing node search for SIMD.\n\n**Pointer Elimination & Implicit Layouts (Eytzinger):**\nExcerpts highlight that binary search over a sorted array suffers from poor cache locality due to its memory access pattern. The Eytzinger layout is presented as a cache-friendly way to store a binary search tree in a contiguous array, where child locations can be implicitly calculated (e.g., left child at `2*i + 1`, right child at `2*i + 2`). This eliminates explicit pointers, which are noted to consume significant memory and lead to scattered accesses. The Eytzinger layout is described as theoretically optimal for temporal locality, as elements closer to the root are stored closer to the beginning of the array, making them more likely to be in cache. This directly addresses the problem of scattered memory access and improves cache utilization.\n\n**SIMD Group Probing with H1/H2 Hash Splitting:**\nHigh-performance hash tables like Google's SwissTable and Facebook's F14 are discussed, which employ techniques directly applicable to the user's problem. These hash tables split a key's hash into two parts: H1 (upper bits) to determine a group of slots, and H2 (lower 7 bits) as a fingerprint. SIMD instructions are then used to compare the H2 fingerprint against multiple control bytes (fingerprints) within a group simultaneously. This allows for quickly filtering a large number of candidates (e.g., 16 slots) in a single SIMD operation, drastically reducing the need for expensive full key comparisons. This technique is described as a crucial performance trick for searching deep probe chains inexpensively, which is highly relevant to optimizing the `hash(current_value ^ node_value)` step in the user's traversal problem.\n\n**Vectorized Hashing (Vertical Vectorization):**\nSeveral sources discuss vectorized hash tables and the concept of ",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.0",
		"citations": [
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  }
		],
		"reasoning": "The fine-grained field value describes a Decoupled Access/Execute (DAE) model where memory access is separated from computation, using two cooperating units with explicit queues or communication channels to overlap memory latency with computation. It also notes platform realizations: on CPUs as Decoupled Software Pipelining (DSWP) with distinct access and execute roles, and on GPUs as warp specialization where some warps handle memory access while others compute, with synchronization via queues or barriers. Direct supporting content from the excerpts includes: the simplest form of DAE being separated into an Access processor and an Execute processor, each with its own instruction stream; memory fetched and placed into a queue for the other unit to consume; and the Execute unit using the data produced by the Access unit as needed. Additional excerpts reinforce the same concept by describing two separate streams (access and execute) that communicate via queues, the walls of memory latency being hidden by decoupling and overlapping work, and the role of named barriers/queues to synchronize producer-consumer like access between streams. The cited discussions of warp specialization and named barriers on GPUs further support the idea that DAE can be realized as warp-partitioned memory-access (DMA-like) and compute-warp activities, communicating through hardware/documented queues or barriers. Finally, other excerpts emphasize the architectural benefits (latency tolerance, memory requests in flight) and the historical/academic grounding of DAE in Smiths work, which corroborates the field values claim about the models core benefits and realizations across CPU and GPU platforms.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.1",
		"citations": [
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ.",
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor."
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  }
		],
		"reasoning": "The described field value advocates breaking a large, monolithic traversal into multiple smaller, specialized kernels, each performing a single task (node intersection, hash computation, etc.), with workers organized into queues that move between tasks. Excerpts that detail the Access-to-Execute Queue model show how memory accesses are decoupled from computation, with an A-processor handling address calculations and memory operations and an E-processor performing computation, communicating through dedicated queues. This directly supports the notion of decomposing traversal work into a sequence of kernel-like stages that pass state via queues rather than a single mega-kernel. Additional excerpts describe how the Execute stream can run ahead or behind the Access stream, enabling overlap and reduced stalls, which aligns with the aim of avoiding a single, serial traversal and instead maintaining a pipeline of specialized kernels. The Warp Specialization and related papers provide concrete mechanisms for partitioning work into sub-computations and warps, which helps in achieving SIMD efficiency when traversals would otherwise diverge. In particular, insights about producer-consumer named barriers and queue-based communication demonstrate practical synchronization patterns that map neatly onto a multi-kernel, staged approach, while discussions of warp specialization show how to maintain high occupancy and reduce divergence by assigning sub-computations to different warps. Taken together, these excerpts collectively corroborate the central claims of the fine-grained field value: moving away from megakernel traversal toward a wavefront-like, segmented, queue-coordinated execution model with warp-specialized stages improves SIMT efficiency and memory/compute overlap.\n",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding",
		"citations": [
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "We duplicate the backward slice code\nand assign new registers to it. By analogy, this code is the carrot\nand the main computation is the horse.",
			  "Prior to the entry into\nthe loop, the carrot is first extended *k* iterations ahead of the horse.\nWe call this phase in the dynamic execution the *head start* phase.",
			  "After the entry into the loop, the carrot locks steps with the horse\nand stays a constant *k* iterations ahead. We call this phase in the\ndynamic execution the *stay ahead* phase.",
			  "During the last *k* iterations\nof the loop, the carrot ceases to stay ahead and merges with the\nhorse. We call this phase of dynamic execution the *join* phase.",
			  "To see why, let us consider the example of the\nbinary tree where both the paths are equally likely. If we want to\nprefetch *k* iterations ahead, then there are 2 *k* possible addresses\nto prefetch.",
			  "We have the option of either prefetching all of those\naddresses or implementing a software-based branch predictor to\nselect one of the addresses to prefetch."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/html/2505.21669v1",
			"excerpts": [
			  "These two benchmarks ( bintree_dfs and bintree_bfs ) perform common traversals of binary tree data structures to compute a sum.",
			  "The DFS benchmark is implemented recursively, while the BFS benchmark uses a std::queue from the C++ STL.",
			  "This tree is staticonly lookups are performed.",
			  "The researchers also state the technique does not perform well on trees.",
			  "Most importantly, prefetch requests for an LDS are issued sequentially, and thus performance degrades with increased effective memory access times."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads ...",
			"url": "https://www.bohrium.com/paper-details/helper-without-threads-customized-prefetching-for-delinquent-irregular-loads/867745821129441724-108609",
			"excerpts": [
			  "Download the full PDF of Helper Without Threads: Customized Prefetching for Delinquent. Includes comprehensive summary, implementation ..."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~tmj32/papers/docs/ainsworth19-tocs.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automatically generate software prefetches for indirect\nmemory accesses, a special class of irregular memory accesses often seen in high-performance workloads.",
			  "Across a set of memory-bound benchmarks, our automated pass achieves average\nspeedups of 1.3  for an Intel Haswell processor, 1.1  for both an ARM Cortex-A57 and Qualcomm Kryo,\n1.2  for a Cortex-72 and an Intel Kaby Lake, and 1.35  for an Intel Xeon Phi Knights Landing, each of which\nis an out-of-order core, and performance improvements of 2.1  and 2.7  for the in-order ARM Cortex-A53\nand first generation Intel Xeon Phi."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses | Department of Computer Science and Technology",
			"url": "https://www.cst.cam.ac.uk/blog/tmj32/software-prefetching-indirect-memory-accesses",
			"excerpts": [
			  "In the paper we create a compiler pass that will automatically identify opportunities to insert prefetches where we find these memory-indirect accesses.",
			  "One of these is that there must be an induction variable within the transitive closure of the source operand (which, for a load, is the operand that calculates the address to load from).",
			  "This means we search backwards through the data dependence graph, starting at the load, until we find this induction variable.",
			  "When we have identified loads that need to be prefetched then we duplicate all of the necessary computation to calculate the address and insert the software prefetch instructions.",
			  "We also have to add code around any loads that are part of this address computation to prevent them from causing errors at runtime if they calculate an invalid address, for example running beyond the end of an array.",
			  "There are more details in the paper including information about how we schedule the prefetches so that data is available immediately before being used."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automat-\nically generate software prefetches for indirect memory\naccesses, a special class of irregular memory accesses of-\nten seen in high-performance workloads. We evaluate this\nacross a wide set of systems, all of which gain benefit from\nthe technique. We then evaluate the extent to which good\nprefetch instructions are architecture dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Categ",
			  " dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Cat"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective: ACM Transactions on Computer Systems: Vol 36, No 3",
			"url": "https://dl.acm.org/doi/10.1145/3319393",
			"excerpts": [
			  "CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being required.",
			  ")Indirect memory accesses have irregular access patterns that limit the performance\nof conventional software and hardware-based prefetchers. To address this problem,\nwe propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect\nmemory ...",
			  "Software prefetching for indirect memory accesses\")CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being require"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://llvm.org/devmtg/2017-03/assets/slides/software_prefetching_for_indirect_memory_accesses.pdf",
			"excerpts": [
			  "Software Prefetching for Indirect. Memory Accesses. Sam Ainsworth and Timothy M. Jones. Computer Laboratory. Page 2. What should we software prefetch? Stride ...Read more"
			]
		  },
		  {
			"title": "Long-range Prefetching of Delinquent Loads",
			"url": "http://cseweb.ucsd.edu/~tullsen/isca2001.pdf",
			"excerpts": [
			  "Speculative Precomputation, a tech-*\n*nique that uses idle thread contexts in a multithreaded ar-*\n*chitecture to improve performance of single-threaded appli",
			  "ecula-\ntive threads are spawned under one of two conditions: when\nencounteringa basic trigger, which occurs when a designated\ninstruction in the main thread reaches a particular pipeline\nstage (such as the commit stage), or a chaining trigger, when\none speculative thread explicitly spawns another.",
			  "A speculative thread is spawned by allocating a hardware\nthread context, copying necessary live-in values into its reg-\nister file, and providing the thread context with the address of\nthe first instruction of the threa",
			  "If a free hardware context\nis not available the spawn request is ignored.",
			  "Necessary live-in values are always copied into the thread\ncontext when a speculative thread is spawned.",
			  "peculative threads execute precomputation slices (p-\nslices), which are sequences of dependent instructions which\nhave been extracted from the non-speculative thread and\ncompute the address accessed by delinquent loads.",
			  "When\na speculative thread is spawned, it precomputes the address\nexpected to be accessed by a future delinquent load, and\nprefetches the data.",
			  "wo primary forms of Speculative Precom-*\n*putation are evaluat",
			  "Delinquent Loads",
			  "We find that in most programs the set of delinquent\nloads is quite small; commonly 10 or fewer static loads cause\nmore than 80% of L1 data cache misses.",
			  " precomputa-\ntion slices used by our work are constructed within an in-\n ...",
			  "ulative precom-\nputation could be thought of as a special prefetch mech-\nanism that effectively targets load instructions that tradi-\ntionally have been difficult to handle via prefetching, such\nas loads that do not exhibit predictable access patterns and\nchains of dependent load",
			  "Speculative threads can be spawned",
			  "hardware structure is called the Outstanding Slice\nCounter (OSC). This structure tracks, for a subset of delin-\nquent loads, the number of instances of delinquent loads\nfor which a speculative thread has been spawned but for\nwhich the main thread has not yet committed the corre-\nsponding load."
			]
		  },
		  {
			"title": "Pointer Cache Assisted Prefetching - Computer Science",
			"url": "https://cseweb.ucsd.edu/~calder/papers/MICRO-02-PCache.pdf",
			"excerpts": [
			  ".\nSpeculative precomputation [6] works by identifying the\nsmall number of static loads, known as delinquent loads, that\nare responsible for the vast majority of memory stall cycles.",
			  "\nPrecomputation slices (p-slices), sequences of dependent in-\nstructions which, when executed, produce the address of a\nfuture delinquent load, are extracted from the program be-\ning accelerated. ",
			  " When an instruction in the non-speculative\nthread that has been identified as a trigger instruction reaches\nsome point in the pipeline (typically commit or rename),\nthe corresponding p-slice is spawned into an available SMT\nthread context.\n",
			  "Speculative slices [28] focus largely on the use of precom-\nputation to predict future branch outcomes and to correlate\npredictions to future branch instances in the non-speculative\nthread, but they also support load prefetching.",
			  "\nSoftware controlled pre-execution [13] focuses on the use\nof specialized, compiler inserted code that is executed in\n",
			  "e trace to extract data reference sequences that fre-\nquently repeat in the same order. At this point, the system\ninserts prefetch instructions to detect and prefetch these fre-\nquent data references.",
			  "The sampling and optimization are\ndone dynamically at runtime with very low overhead.\n*",
			  "The\nprograms static instructions are analyzed in the reverse order\nof execution from a delinquent load, building up a slice of in-\nstructions the load is directly and indirectly dependent upon.",
			  "Slice construction terminates when an-\nalyzing an instruction far enough from the delinquent load\nthat a spawned thread can provide a timely prefetch, or when\nfurther analysis will add additional instructions to the slice\nwithout providing further performance benefits.",
			  ". In this form,\na slice consists of a sequence of instructions in the order they\nwere analyzed.",
			  "The single path slices constructed in this work are trig-",
			  "Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer ...Read more",
			  "However, traditional prefetching techniques have diffi- culty with sequences of irregular accesses. A common ex- ample of this type of access is pointer chains, ..."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Speculative precomputation: long-range prefetching of delinquentloads",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "[PDF] Speculative precomputation: long-range prefetching ...",
			"url": "https://www.semanticscholar.org/paper/Speculative-precomputation%3A-long-range-prefetching-Collins-Wang/cd42d31aa8f4d07a41556ee4640cb47d3401b9ef",
			"excerpts": [
			  "This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of ..."
			]
		  },
		  {
			"title": "Copyright by Milad Olia Hashemi 2016",
			"url": "https://repositories.lib.utexas.edu/bitstreams/4d988cbc-f809-418f-972d-8202b4c72bf4/download",
			"excerpts": [
			  "Jeffery A. Brown, Hong Wang, George Chrysos, Perry H. Wang, and John P.\nShen.\nSpeculative precomputation on chip multiprocessors.\nIn *Workshop on*\n*Multithreaded Execution, Architecture, and Compilation* , 2001.",
			  "Luis Ceze, James Tuck, Josep Torrellas, and Calin Cascaval. Bulk disambiguation\nof speculative threads in multiprocessors. In *ISCA* , 2006.",
			  "Murali Annavaram, Jignesh M. Patel, and Edward S. Davidson. Data prefetching\nby dependence graph precomputation. In *ISCA* , 2001."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "putation*\n*enables*\n*effective*\n*cache*\n*prefetching for even irregular memory access behavior, by*\n*using an alternate thread on a multithreaded or multi-core*\n*architecture. This paper describes a system that constructs*\n*and runs precomputation based prefetching threads via*\n*event-driven dynamic optimization. Precomputation threads*\n*are dynamically constructed by a runtime compiler from the*\n*programs frequently executed hot traces, and are adapted*\n*to the memory behavior automatically. Both construction*\n*and execution of the prefetching threads happen in another*\n*thread, imposing little overhead on the main thread. This*\n*paper also presents several techniques to accelerate the pre-*\n*computation threads, including colocation of p-threads with*\n*hot traces, dynamic stride prediction, and automatic adap-*\n*tation of runahead and jumpstart distan",
			  "While in-\nlined prefetches are typically effective for simple addressing\npatterns (e.g., strided addresses), p-thread based prefetching\nhas the potential to handle more complex address patterns\n(e.g. pointer chasing), or accesses embedded in more com-\nplex control flow. This is because the prefetching address is\ncomputed via actual code extracted from the main thread.",
			  "ead.\nA successful precomputation-based prefetcher must ad-\ndress several challenges. It must be able to determine the\nproper distance by which the prefetching thread should lead\nthe main thread, and it should have the ability to control that\ndistance. It must create lightweight threads that can actually\nproceed faster than the main thread, so that they stay out in\nfront. It must prevent p-threads from diverging from the ad-\ndress stream of the main thread, or at least detect when it\nhas happened. This divergence may be the result of control\nflow or address value speculation in the p-thread. Runaway\nprefetching may unnecessarily displace useful data, resulting\nin more data cache m",
			  " sophistication of slice creation.\nMore recent work by Lu, et al. [12] dynamically con-\nstructs p-slices via a runtime optimizer running on an idle\ncore. A single user-level thread is multiplexed to detect the\nprograms phases, construct the p-thread code, and perform\nprecomputation prefetching.",
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "**Loop Re-rolling**  A hot trace may contain multiple\ncopies of the same code due to loop unrolling done during\nstatic compilation. We perform loop *re-rolling* for the p-\nslice (i.e. removing the redundant loop copies) to reduce\nduplicated computation inside a p-slice. This optimization\nincreases the granularity at which we can set the prefetch\nrun-ahead distance, since the prefetch distance is always an\nintegral number of iterations.",
			  "**Object-Based Prefetching**  We perform same-object\nbased prefetching, as in our prior work on inline prefetch-\ning [26]. Same-object prefetching clusters prefetches falling",
			  "**P-Thread Jump Starting**\nSometimes, the only way to get the prefetch thread ahead\nof the main thread is to give it a head start. Existing dy-\nnamic precomputation schemes (e.g.\n[12]) typically start\np-threads from the same starting point (same iteration) as the\nmain thread.",
			  "**6**\n**Results**\nThis section evaluates the cost and performance of our dy-\nnamically generated precomputation based prefetching tech-\nnique.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches).",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  },
		  {
			"title": "Understanding the Backward Slices of Performance ...",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/2000/slice.isca.pdf",
			"excerpts": [
			  "n general, pre-execution amounts\nto guessing the existence of a future performance degrading\ninstruction and executing it (or what we think it will be) some time\nprior to its actual encounter in the machine, thereby at least par-\ntially hiding its latency"
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "In this section requirements for\neffective direct pre-execution are examined and hardware\nmechanisms supporting these requirements are described.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Tree traversal is an intensive pointer-chasing operation, requiring traversing to a node in the tree and finding the child pointers, before being able to find the child node addresses and issue loads.",
			  "With treelet prefetching, as rays traverse the BVH tree and visit the root node of treelets, corresponding treelets can be prefetched to load deeper levels of the tree before they are needed.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulations show treelet based traversal reduces performance slightly by 3.7% over a DFS baseline. However, when combined with treelet prefetching, the overall speedup reaches 32.1% while maintaining the same power consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache. Ray tracing is a pointer-chasing application and memory accesses are divergent and hard to predict. With the treelet based traversal algorithm introduced previously, memory accesses are now clustered as individual treelets, making it possible to prefetch easily.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "... BVH tree statistics for each scene are outlined in Table 2. The scene ... We use the concept of treelets which are connected subpartitions of a BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "We form treelets by grouping connected BVH nodes to maximize the size of each treelet. It is a greedy algorithm that starts from the BVH root node and greedily adds nodes to the current treelet until the maximum treelet size is reached.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "We propose to add a treelet prefetcher that prefetches treelets into the L1 cache of the GPU based on the rays in the warp buffer.",
			  "The threshold comparator generates the prefetch enable signal if the treelet popularity is greater than a manually set threshold which ranges from 0 to the maximum number of rays in the warp buffer ().",
			  "The prefetch enable is ANDed with the upper bits of the treelet root node address to generate the treelet prefetch address and sent to the prefetch queue to be processed ().",
			  "We only require the upper bits of the treelet root node address because the treelets have a fixed maximum size and nodes within a treelet are organized to be packed together in memory.",
			  "The treelet prefetcher also records the address of the last treelet it prefetches to avoid pushing duplicate treelet addresses to the prefetch queue and prefetching the same treelet multiple times in a row.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "We combine treelet prefetching with a treelet based traversal algorithm in the ray tracing accelerator to further reduce ray traversal latenc",
			  "This section describes our proposed treelet prefetching technique for ray tracing.",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "As a ray visits a treelet root node, its subsequent memory accesses will also be to the nodes in the treelet since accesses to nodes from different treelets are deferred to the *otherTreeletStack",
			  "Thus, we can prefetch the entire treelet to the GPU's cache and reduce the latency of accessing nodes in the current treelet."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "d\nTreelet Prefetching For Ray Tracing\nMICRO 23, October 28November 01, 2023, Toronto, ON, Canada\nmore frequently than the lower levels. In the next sections, we pro-\npose a treelet based ray traversal algorithm with an accompanying\nprefetcher.",
			  "s a ray visits a treelet root node, its subse-\nquent memory accesses will also be to the nodes in the treelet since\naccesses to nodes from different treelets are deferred to the *oth-*\n*erTreelet",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "a treelet prefetcher to the RT unit to speed\nup ray traversal along with a prefetch queue to hold the issued\nprefetch addresses, both of which are highlighted in red in Figur",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "In Vulkan-Sim [ 41 ], the simulator we use\nfor evaluation, the ray tracing accelerator is referred to as the RT\nunit. When a warp issues a trace ray instruction, it enters the RT\nunit during the pipelines execute stage and is queued in the warp\nbuffer, which holds ray metadata for all 32 threads of the warp.",
			  "This work presents a treelet prefetching scheme to improve ray\ntraversal performance.",
			  "**7.4**\n**Treelet Based Ray Tracing Techniques**\nNavratil et al. [ 33 ] tackled incoherent rays by collecting rays into\n ... \nstaging buffer which might require non-trivial shader modifications\nto realize on a GPU and are not discussed in their paper.",
			  "**7.5**\n**Ray Traversal Acceleration Techniques**\n**Ray Sorting.** Ray sorting improves ray coherency by grouping\nrays that traverse similar parts of the AS.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "We propose to add\na treelet prefetcher that prefetches treelets into the L1 cache of\nthe GPU based on the rays in the warp buffer.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict.",
			  "With 512B treelets, the treelet\nbased memory layout performs best with a 31.9% speedup over\nthe baseline.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The original BVH node\nlayout is allocated to be 64 bytes and stores the bounding box,\npointers, and other metadata for the child nodes of a 6-wide BVH\ntree.",
			  "It is a greedy algorithm that starts from the BVH\nroot node and greedily adds nodes to the current treelet until the\nmaximum treelet size is reached. T",
			  "Figure 3 shows\nan example of a two-wide BVH tree partitioned into treelets with a\nmaximum size of 4 nodes each.",
			  "reelet formation initializes the *remainingBytes* to the maximum\ntreelet size and adds the BVH root address to the *pendingTreelets*\nqueue and traversal sta"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1999/jmp-ptr.isca.pdf",
			"excerpts": [
			  "bstract**\n*Current techniques for prefetching linked data structures*\n*(LDS) exploit the work available in one loop iteration or*\n*recursive call to overlap pointer chasing latency. Jump-*\n*pointers, which provide direct access to non-adjacent*\n*nodes, can be used for prefetching when loop and recur-*\n*sive procedure bodies are small and do not have sufficient*\n*work to overlap a long latency. This paper describes a*\n*framework for jump-pointer prefetching (JPP) that sup-*\n*ports four prefetching idioms: queue, full, chain, and root*\n*jumping and three implementations: software-only, hard-*\n*ware-only, and a cooperative software/hardware tech-*\n*nique.",
			  "n a suite of pointer intensive programs, jump-*\n*pointer prefetching reduces memory stall time by 72% for*\n*software, 83% for cooperative and 55% for hardware, pro-*\n*ducing speedups of 15%, 20% and 22% respecti",
			  "general purpose technique for tol-\nerating serialized latencies that result from LDS tra-\nversal.\nBy storing explicit jump-pointers to nodes\nseveral hops away, JPP overcomes the pointer-chasing\nproblem.\nIt is able to generate prefetch addresses\ndirectly, rather than in a serial fashion, and is effective\neven in situations where not enough work is available\nto hide latencies by scheduling.\n**",
			  "P implementations: software-only,\nhardware-only, and cooperative.\nFor those programs\nwith appreciable memory latency components, these\nimplementations reduce overall observed memory\nlatency by 72%, 55%, and 83%, respectively and\nachieve speedups of 15%, 22%, and 20%.\n",
			  "**Abstract**",
			  "*Current techniques for prefetching linked data structures*",
			  "\n*(LDS) exploit the work available in one loop iteration or*",
			  "*recursive call to overlap pointer chasing latency. Jump-*",
			  "*pointers, which provide direct access to non-adjacent*",
			  "*nodes, can be used for prefetching when loop and recur-*",
			  "*sive procedure bodies are small and do not have sufficient*",
			  "*work to overlap a long latency. This paper describes a*",
			  "*framework for jump-pointer prefetching (JPP) that sup-*",
			  "*ports four prefetching idioms: queue, full, chain, and root*",
			  "*jumping and three implementations: software-only, hard-*",
			  "*ware-only, and a cooperative software/hardware tech-*",
			  "*nique.*",
			  "*On a suite of pointer intensive programs, jump-*",
			  "*pointer prefetching reduces memory stall time by 72% for*",
			  "*software, 83% for cooperative and 55% for hardware, pro-*",
			  "*ducing speedups of 15%, 20% and 22% respectively.*",
			  "*\n**1 Introduction**",
			  "Linked data structures (LDS) are common in many appli-",
			  "cations, and their importance is growing with the spread of",
			  "object-oriented programming.",
			  "The popularity of LDS",
			  "stems from their flexibility, not their performance. LDS",
			  "access, often referred to as *pointer-chasing* , entails chains",
			  "of data dependent loads that serialize address generation",
			  "and memory access.",
			  "In traversing an LDS, these loads",
			  "often form the programs critical path.\nC",
			  "Consequently,",
			  "when they miss in the cache, they can severely limit paral-",
			  "lelism and degrade performance.",
			  "Prefetching is one way to hide LDS load latency and",
			  " ... \nefficiently [6] and to parallelize searches and reductions on",
			  "lists [9].",
			  "Discussions of maintaining recursion-avoiding",
			  "traversal *threads* in non-linear data structures can be found",
			  "in data structures literature [18]. As noted earlier, Luk and",
			  "Mowry [11] suggested the use of programmer controlled",
			  "jump-pointers for prefetching. We are not aware of any",
			  "implementations, actual or proposed,\nof hardware or",
			  "cooperative jump-pointer prefetching.",
			  "\n**6 Summary and Future Directions**\n",
			  "In this paper, we describe the general technique of jump-",
			  "pointer prefetching (JPP) for tolerating linked structure",
			  "(LDS) access latency. JPP is effective when limited work",
			  "is available between successive dependent accesses (e.g., a",
			  "***Figure***",
			  "***7.***",
			  "***Tolerating***",
			  "***longer***",
			  "***memory***",
			  "***latencies.***",
			  "*Execution times for health: the first group of bars uses*",
			  "*the base configuration (70 cycle memory latency), the*",
			  "*second and third simulate long memory latency (280*",
			  "cycles).*",
			  "*In terms of prefetching, the first two*",
			  "*configurations use a jump interval (the distance*",
			  "*\n*between a jump-pointers home and target nodes) of 8,*",
			  "the third uses an interval of 16.*",
			  "0.0",
			  "0.5",
			  "1.0",
			  "1.5",
			  "2.0",
			  "2.5",
			  "3.0",
			  "MemLat=70",
			  "Interval=8",
			  "Interval=8",
			  "BDSCH",
			  "BDSCH",
			  "BDSCH",
			  "MemLat=280",
			  "MemLat=280",
			  "Interval=16",
			  "Normalized Execution Time",
			  "memory latency",
			  "Compute time",
			  "**Legend: B:** Base",
			  "**D:** DBP",
			  "**S:** Software JPP",
			  "**C:** Cooperative JPP",
			  "**H:** Hardware JPP",
			  "tight pointer chasing loop) to enable aggressive scheduling"
			]
		  },
		  {
			"title": "The Efficacy of Software Prefetching and Locality ...",
			"url": "https://jilp.org/vol6/v6paper7.pdf",
			"excerpts": [
			  "In jump pointer prefetching, additional pointers are inserted into a dynamic data structure\nto connect non-consecutive link elements.",
			  "These jump pointers allow prefetch instructions to\nname link elements further down the pointer chain ( i.e. a prefetch distance, PD , away which\nis computed as in Sections 3.1 and 3.2) without sequentially traversing the intermediate links.",
			  "Consequently, prefetch instructions can overlap the fetch of multiple link elements simultaneously\nby issuing prefetches through the memory addresses stored in the jump pointers.",
			  "Jump pointer prefetching, however, cannot prefetch the first PD link nodes in a linked list\nbecause there are no jump pointers that point to these early nodes.",
			  "To enable prefetching of early\nnodes, jump pointer prefetching can be extended with prefetch arrays [7]. In this technique, an\narray of prefetch pointers is added to every linked list to point to the first PD link nodes.",
			  "Hence,\nprefetches can be issued through the memory addresses in the prefetch arrays before traversing\neach linked list to cover the early nodes, much like the prologue loops in affine array and indexed\narray prefetching prefetch the first PD array elements.",
			  "Figure 5 Part(A) illustrates the addition\nof a prologue loop that performs prefetching through a prefetch array.",
			  "Software prefetching for pointer-chasing codes suffers high overhead to create and manage jump\npointers, as described in Section 5.2.",
			  "owever, jump pointers may not be necessary when prefetch-\ning is combined with ccmalloc memory allocation",
			  "Since intelligent allocation places link nodes\ncontiguously in memory, prefetch instructions can access future link nodes by simple indexing, just\nas for affine array accesses.",
			  "Figure 9 shows the effect of ccmalloc on nodes linked together by\npointers.",
			  "From the right-hand part of the figure, it is intuitive that a compiler can insert prefetches\nfor list nodes further down the list using the size of a node and the location of the first node.",
			  "This approach, which we call index prefetching [1, 37], was originally proposed in [8].",
			  "With in-\ndex prefetching, the jump pointers can be removed, thus eliminating all the overhead associated\nwith jump pointer prefetchin",
			  "To quantify this benefit, we created index prefetching versions for\nHealth and MST , and show the results for these benchmarks in Figure 18.",
			  "(We did not create an\nindex prefetching version for EM3D , our third pointer-chasing benchmark, since it already achieves\nhigh performance with normal software prefetching as shown in Figures 10 and 13)."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching For Linked Data ...",
			"url": "https://www.scribd.com/document/861770035/9",
			"excerpts": [
			  "This paper presents a framework for jump-pointer prefetching (JPP) aimed at improving the performance of linked data structures (LDS) by ...Read more"
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "Jump pointers, which provide direct access to non-adjacent nodes, can be used for prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures",
			  "New technique: Jump Pointer Prefetching",
			  "Creates parallelism",
			  "Hides arbitrary latency",
			  "Choice of implementation: software, hardware, cooperative",
			  "Problem: Pointer chasing latency",
			  "pointer loads",
			  "Jump pointer prefetching:\nOverlap pointer loads with each other anyway!"
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "http://pdl.cmu.edu/PDL-FTP/Database/CMU-CS-00-177.pdf",
			"excerpts": [
			  "Luk and Mowry proposed three solutions to the pointer-chasing problem 13, 14 ... We then prefetch the next chunk ahead in the jump-pointer array.Read more"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Our goal: In this paper, we aim to provide an effective, bandwidth- efficient, and low-cost solution to prefetching linked data structures by 1) overcoming the ...Read more",
			  "is paper proposes a low-cost hardware/software cooperative*\n*technique that enables bandwidth-efficient prefetching of linked data*\n*structure",
			  "The prefetcher brings cache blocks into the L2 (last-\nlevel) cache, since we use an out-of-order execution machine that can\ntolerate short L1-miss latencies",
			  "far ahead of the demand miss\nstream the prefetcher can send requests is determined by the *Prefetch*\n*Distance* parameter.",
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,",
			  "ch relies on the observation that most virtual addresses share com-\n ... \ning and object metadata. In *PLDI* , 2004."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[30] A. Roth, A. Moshovos, and G. S. Sohi. Dependence based prefetching\nfor linked data structures. In *ASPLOS-8* , 1998.",
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown",
			  "Dependence based\nprefetching for linked data structures, *",
			  "indirections through memory as seen in sparse tensor algebra\nand graphs [ 13  15 ], and more generally applications with\npointer chasing [ 10 , 11 ]. The target application influences the\ndata access pattern that the prefetcher tries to identify and\nprefetch for. For example, a common access pattern in sparse\ntensor algebra and graph computations is An [ *...* A1 [ A0 [ i ]] *...* ] .\nCorr",
			  "Yu et al. [ 13 ] (a.k.a. IMP) tries to detect\naccess patterns given by Y [ Z [ i ]] (2-level IMP) and X [ Y [ Z [ i ]]]\n(3-level IMP), for striding loop variable i , and prefetch\ndata by assuming that Y [ Z [ i + ** ]] and X [ Y [ Z [ i + ** ]]] will\nbe needed in the future. Ainsw"
			]
		  },
		  {
			"title": "Dependence Based Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1998/asplos-prefetch-lds.pdf",
			"excerpts": [
			  "k and Mowry [12] proposed and evaluated a greedy compiler\nalgorithm for scheduling software prefetches for linked data struc-\ntures. They showed this scheme to be effective for certain pro-\ngrams, citing instruction overhead and the generation of useless\nprefetches as performance degradation factors for other",
			  "ry [12] presented a case for history-pointer prefetch-\ning, which augments linked structure nodes with prefetching\npointer fields, and data-linearization, in which LDS are program-\nmatically laid out at runtime to allow sequential prefetch machin-\nery to capture their traversal.",
			  "eir\nalgorithm uses type information to identify recurrent pointer\naccesses, including those accessed via arrays, and may have advan-\ntages in tailoring a prefetch schedule to a particular traversal.",
			  "It collects these loads along with the\ndependence relationships that connect them and constructs a\ndescription of the steps the program has followed to traverse the\nstructure.",
			  "Predicting that the program will continue to follow these\nsame steps, a small prefetch engine takes this description and spec-\nulatively executes it in parallel with the original progra",
			  "Linked data structures (LDS) such as lists and trees are used in\nmany important applications."
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Later Luk and\nMowry proposed a compiler-based prefetching scheme for\nrecursive data structures [22].",
			  "This requires extra storage at\nruntime.",
			  "Jump Pointer [29], which has\nbeen used widely to break the serial LDS (linked data struc-\nture) traversal, stores pointers several iterations ahead in the\nnode currently visite",
			  "In a recent work, Gau-\ntam Doshi et al. [14] discussed the downside of software\nprefetching and exploited the use of rotating registers and\npredication to reduce the instruction overhead",
			  "Profile Guided Software Prefetching",
			  "Software prefetching is ineffective in pointer-based pro-\ngrams. To address this problem, Chi K. Luk et al. presented\na Profile Guided Post-Link Stride Prefetching [23] using a\nstride profile to obtain prefetching guidance for the com-\npil",
			  "[22] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching\nFor Recursive Data Structures. In *ASPLOS-7* , pages 222\n233. ACM Press, 1996",
			  "[23] C.-K. Luk, R. Muth, H. Patil, R. Weiss, P. G. Lowney, and\nR. Cohn. Profile-Guided Post-link Stride Prefetching. In\n*ICS-16* , pages 167178. ACM Press, 2002.",
			  "24] T. C. Mowry, M. S. Lam, and A. Gupta. Design and Evalua-\ntion of A Compiler Algorithm for Prefetching. In *ASPLOS-*\n*5* , pages 6273. ACM Press, ",
			  "[25] T. C. Mowry and C.-K. Luk. Predicting Data Cache Misses\nin Non-Numeric Applications Through Correlation Profil-\ning. In *Micro-30* , pages 314320. IEEE Computer Society\nPress, 199"
			]
		  },
		  {
			"title": "Dynamic Hot Data Stream Prefetching for General-Purpose ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s09/www/papers/prefetch_hds.pdf",
			"excerpts": [
			  "Jump pointers are a software technique for prefetching linked data\nstructures, overcoming the array-and-loop limitation.",
			  "Artificial\njump pointers are extra pointers stored into an object that point to\nan object some distance ahead in the traversal order.",
			  "d. Natural jump pointers are existing pointers in the\ndata\nstructure\nused\nfor\nprefetching.\n",
			  "ng.\nFor\nexample,\ngreedy\nprefetching makes the assumption that when a program uses an\nobject o, it will use the objects that o points to, in the near future,\nand hence prefetches the targets of all pointer fields.",
			  " These\ntechniques were introduced by Luk and Mowry in [22] and refined\nin [5, 18].",
			  "n\ndependence-based prefetching, producer-consumer pairs of loads\nare identified, and a prefetch engine speculatively traverses and\nprefetches them [26].",
			  "\nThe hardware technique that best corresponds to history-pointers is\ncorrelation-based prefetching. As originally proposed, it learns\ndigrams of a key and prefetch addresses: when the key is observed,\nthe prefetch is issued [6]. J"
			]
		  },
		  {
			"title": "2003 Workshop on Duplicating, Deconstructing and ... - PHARM",
			"url": "https://pharm.ece.wisc.edu/wddd/2003/wddd2003_proceedings.pdf",
			"excerpts": [
			  " ware prefetch when bandwidth is limited; with sufficient\nbandwidth software prefetch is the most successful strategy.\nHowever, their research also shows that the combination of\ncache-conscious allocation and software prefetch might not\nlead to further performance improvements, instead it coun-\nteracts changes in bandwidth or latency. Their results are\nsimilar to ours, although we have implemented a different\nsoftware prefetch that does not require any extra memory.\nSeveral researchers have studied hardware prefetch,\nor hybrid schemes, and successfully adapted hardware\nprefetch to pointer-based data structures with irregular ac-\ncess behavior. However, they generally require more hard-\nware than those evaluated in this study. Hardware support\nhas been investigated by the use of lock-up free prefetching,\n[13], and prefetch buffers, [10], and general prefetching in\nhardware is described in [20, 21] together with other cache\nmemory aspects. Karlsson et al., [11], propose a technique\nfor prefetching pointer-based data structures, either in soft-\nware combined with hardware or in software alone, by im-\nplementing prefetch arrays, making it possible to prefetch\nboth short data structures and longer data structures without\nknowing the traversal path. Roth et al. have investigated\nmore adaptable strategies for hybrid prefetch schemes, us-\ning dependence graphs, [18], and jump pointer prefetching,\n[19]. In [19], Roth et al. evaluate a framework for jump-\n ... \n[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999.\n[20] Alan J. Smith. Cache memories. *ACM Computing*\n*Surveys* , 14:3:473530, September 1982.\n[21] Steven P. VanderWiel and David Lilja. Data prefetch\nmechanisms.\n*ACM Computing Surveys* , 32:2:174\n199, June 2000.\n[22] Chengqiang Zhang and Sally A. McKee. Hardware-\nonly stream prefetching and dynamic access order-\ning. In *International Conference on Supercomputing* ,\npages 167175, 2000.\n[23] L. Zhang, S. McKee, W. Hsieh, and J. Carter. Pointer-\nbased prefetching within the impulse adaptable mem-\nory controller: Initial results. In *Proceedings of the*\n*Workshop on Solving the Memory Wall Problem* , June\n2000.\n[24] Craig B. Zilles. Benchmark health considered harm-\nful. *Computer Architecture News* , 29:3, 2001.\n13\n**Comparison of State-Preserving vs. Non-State-Preserving Leakage Control**\n**in Caches**\nDharmesh Parikh\n\n, Yan Zhang\n\n, Karthik Sankaranarayanan\n\n, Kevin Skadron\n\n, Mircea Stan\n\n Dept. ",
			  "[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999."
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "the\ndependence relationships between loads and stores, and prefetch\nthe subsequent nodes on their basis [ 4 ].",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system.",
			  "oosely-coupled processors have an advantage*\n*in that fine-grain resources, such as processor and L1 cache re",
			  "sources, are not contended by the application and helper threads,",
			  "*present techniques to alleviate this. O",
			  "r approach exploits large*\n*loop-based code regions and is based on a new synchronization",
			  "mechanism between the application and helper threads.",
			  "This*\n*mechanism precisely controls how far ahead the execution of t"
			]
		  },
		  {
			"title": "Software Prefetches - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/software-prefetches",
			"excerpts": [
			  "Software prefetching faces challenges with irregular pointer-based structures due to the pointer-chasing problem, which limits early prefetch initiation because the address of future nodes cannot be determined without traversing intermediate nodes.",
			  "Greedy prefetching is a technique for [linked data](../computer-science/linked-data) structures, inserting prefetch instructions for successor nodes, but its effectiveness is limited by the inability to overlap prefetch latency with more than a [single iteration](../computer-science/single-iteration) .",
			  "Jump pointer prefetching and prefetch arrays are used to address early node prefetching in linked lists.",
			  "Natural Pointer Techniques The simplest software prefetching technique for LDS traversal is greedy prefetching, proposed by Luk and Mowry .",
			  "Greedy prefetching is attractive due to its simplicity. However, its ability to properly time prefetch initiation is limited. For the linked-list traversal in Figure 3.25 (a) , each prefetch overlaps with a single-loop iteration only."
			]
		  },
		  {
			"title": "The pros and cons of explicit software prefetching - Johnny's Software Lab",
			"url": "https://johnnysswlab.com/the-pros-and-cons-of-explicit-software-prefetching/",
			"excerpts": [
			  "Pointer chasing codes (linked lists or binary trees) need to be rewritten in order to benefit from prefetching. Here is the example binary tree lookup:",
			  "This code has a instruction dependency on memory loads. In order to prefetch, we need to know a few memory addresses in advance. However, with these type of codes this is not possible: we dont know the address of the next node until we have loaded the current node and performed the comparison.",
			  "Issuing a prefetch request immediately before accessing a variable doesnt work.",
			  "Luckily, there is a solution. We already covered the techniques which are aimed at braking dependency chains or interleaving several dependency chains in our post about [instruction level parallelism](https://johnnysswlab.com/instruction-level-parallelism-in-practice-speeding-up-memory-bound-programs-with-low-ilp/) . Techniques presented there combine nicely with explicit software prefetching. Lets take our binary tree example and perform 16 parallel searches:",
			  "This code performs 16 parallel searches. We can easily add two prefetches like this:",
			  "This will cause the CPU to prefetch the correct child. If we were running one search in parallel, the access to this child would be immediately after the prefetch and prefetching wouldnt make sense.",
			  "With 16 parallel searches, there is enough time for the hardware to prefetch the child before it is being accessed."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta- ...",
			"url": "https://jilp.org/vol13/v13paper2.pdf",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " Correlating Prediction\nTables (DCPT). DCPT builds upon two previously proposed prefetcher techniques, com-\nbining them and refining their ideas to achieve better perf"
			]
		  },
		  {
			"title": "Prefetching",
			"url": "https://ece752.ece.wisc.edu/lect15-prefetching.pdf",
			"excerpts": [
			  "Markov prefetching forms address correlations",
			  "Joseph and Grunwald (ISCA 97)",
			  "Uses global memory addresses as states in the Markov graph",
			  "Correlation Table *approximates* Markov graph",
			  "obal History Buffer (GHB)\n Holds miss address\nhistory in FIFO order",
			  "Global History Buffer",
			  "Delta-based prefetching leads to much smaller table than\nclassical Markov Prefetching",
			  "Delta-based prefetching can remove compulsory misses"
			]
		  },
		  {
			"title": "Data prefetching on in-order processors",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/2268c547-2c42-49d5-9e73-7578ebe3758e/content",
			"excerpts": [
			  "[14] K. J. Nesbit and J. E. Smith, Data cache prefetching using a global history buffer, Software, IEE Proceedings-, 2004. [15] S. Srinath, O. Mutlu, H ...Read more"
			]
		  },
		  {
			"title": "Feedback Mechanisms for Improving Probabilistic Memory ...",
			"url": "https://www.cs.utexas.edu/~lin/papers/hpca09.pdf",
			"excerpts": [
			  "The efficiency of stream prefetching has been improved by Nesbit and. Smith [18], who introduce the Global History Buffer to im- prove prefetch effectiveness ...Read more"
			]
		  },
		  {
			"title": "Data Access History Cache and Associated Data Prefetching ...",
			"url": "http://www.cs.iit.edu/~scs/assets/files/SC07_DAHC.pdf",
			"excerpts": [
			  "Nesbit and Smith proposed a global history buffer for data prefetching in [14] and. [15]. The similarity between their work and our work is that both attempt ..."
			]
		  },
		  {
			"title": "TDT4260 Computer Architecture Mini-Project",
			"url": "https://www.nichele.eu/files/nichele_tdt4260.pdf",
			"excerpts": [
			  "[14] M. Grannaes, M. Jahre and L. Natvig. Multi-level Hardware Prefetching. Using Low Complexity Delta Correlating Prediction Tables with Partial. Matching.Read more"
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer",
			"url": "https://www.researchgate.net/publication/3215463_Data_Cache_Prefetching_Using_a_Global_History_Buffer",
			"excerpts": [
			  "This research is to design a history table-based linear analysis ... This paper studies hardware prefetching for second-level (L2) caches."
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | Proceedings of the 10th International Symposium on High Performance Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1109/HPCA.2004.10030",
			"excerpts": [
			  "A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction.",
			  "The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.",
			  "Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods.",
			  "Global History Buffer prefetching can increase correlation prefetching performance by 20% and cut its memory traffic by 90%. Furthermore, the Global History Buffer can make correlations within a loads address stream, which can increase stride prefetching performance by 6%. "
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | IEEE Micro",
			"url": "https://dl.acm.org/doi/abs/10.1109/MM.2005.6",
			"excerpts": [
			  "By organizing data cache prefetch information in a new way, a GHB supports existing prefetch algorithms more effectively than conventional prefetch tables. It reduces stale table data, improving accuracy and reducing memory traffic. It contains a more complete picture of cache miss history and is smaller than conventional tables",
			  "The structure is based on a Global History Buffer that holds the most\nrecent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer",
			  "HPCA '04: Proceedings of the 10th International Symposium on High Performance Computer ArchitectureA new structure for implementing data cache prefetching is proposed and analyzed via",
			  "A new structure for implementing data cache prefetching is proposed and analyzed via"
			]
		  },
		  {
			"title": "DATA CACHE PREFETCHING USING A GLOBAL ...",
			"url": "https://minds.wisconsin.edu/bitstream/1793/11158/1/file_1.pdf",
			"excerpts": [
			  "As a circular buffer, the GHB prefetching\nstructure eliminates many problems associat-\ned with conventional tables. First, the GHB\nFIFO naturally gives table space priority to\nthe most recent history, thus eliminating the\nstale-data problem.",
			  "dex table entries contain point-\ners into the GHB.",
			  "The GHB is larger, with a size chosen to hold\na representative portion of the miss address\nstream. Last, and perhaps most important, a\ndesigner can use the ordered global history to\ncreate more-sophisticated prefetching meth-\nods than conventional stride and correlation\nprefetchin"
			]
		  },
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%.",
			  "In\nfigure 7 we show the average portion of deltas that can be\nrepresented with a given amount of bits across all SPEC2006\nbenchmarks.",
			  "Although the coverage steadily increases with the amount\nof bits used, speedup has a distinct knee at around 7 bits.",
			  "In figure 8 we show the geometric mean of speedups as\na function of the number of deltas per table entry.",
			  "One of the main differences between DCPT and PC/DC is\nthat DCPT stores deltas, while PC/DC stores entire addresses\nin its GHB.",
			  "the deltas are usually quite small, fewer\nbits are needed to represent a delta than a full address."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov predictors | Proceedings of the 24th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/264107.264207",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the *Markov prefetcher.* This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching *multiple reference predictions* from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "Prefetching using Markov predictors for ISCA 1997 - IBM Research",
			"url": "https://research.ibm.com/publications/prefetching-using-markov-predictors--1",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems.",
			  "In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs.",
			  "The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://people.ucsc.edu/~hlitz/papers/crisp.pdf",
			"excerpts": [
			  " Prefetching using Markov predictors.\n",
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using Markov predictors.",
			  "Tempo-\nral prefetchers [ 8 , 49 , 54 , 119 , 122 ] track the temporal order of\ncache line accesses based on Markov prefetching [ 55 ] introducing\nsignificant storage overheads in the order of megabytes in con-\ntrast to CRISP",
			  "CRISP can be\ncombined with these prior approaches to increase coverage by\nreducing the miss penalty of irregular memory accesses. T"
			]
		  },
		  {
			"title": "Perceptron-Based Prefetch Filtering - Engineering People Site",
			"url": "https://people.engr.tamu.edu/djimenez/pdfs/ppf_isca2019.pdf",
			"excerpts": [
			  "7.2\nLookahead Prefetchers\nUnlike spatial prefetchers, lookahead prefetchers take program order\ninto account when they make predictions. Shevgoor et al. propose\nthe Variable Length Delta Prefetcher (VLDP) [ 35 ], which correlates\nhistories of deltas between cache line accesses within memory pages\nwith the next delta within that page. SPP [ 2 ] and KPCs prefetching\ncomponent [ 36 ] are more recent examples of lookahead prefetchers.\n",
			  "Ishii et al. propose the Access Map Pattern\nMatching prefetcher (AMPM) [ 11 ], which creates a map of all ac-\ncessed lines within a region of memory, and then analyzes this map\non every access to see if any fixed-stride pattern can be identified\nand prefetched that is centered on the current access.",
			  "In a single core configuration, PPF increases performance by\n3.78% compared to the underlying prefetcher, SPP.",
			  "In a multi-core\nsystem running a mixes of memory intensive SPEC CPU 2017 traces,\nPPF saw an improvement of 11.4% over SPP for a 4-core system,\nand 9.65% for an 8-core system.",
			  "Michaud proposes the\nBest-Offset Prefetcher [ 34 ], which determines the optimal offset to\nprefetch while considering memory latency and prefetch timeliness.",
			  "DRAM-Aware\nAMPM (DA-AMPM) [ 32 ] is a variant of AMPM that delays some\nprefetches so they can be issued together with others in the same\nDRAM row, increasing bandwidth utilization.",
			  "Pugsley et al. pro-\npose the Sandbox Prefetcher [ 33 ], which analyzes candidate fixed-\noffset prefetchers in a sandboxed environment to determine which is\nmost suitable for the current program phase."
			]
		  },
		  {
			"title": "Building Efficient Neural Prefetcher",
			"url": "https://www.memsys.io/wp-content/uploads/2023/09/3.pdf",
			"excerpts": [
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using markov predictors. In\n*Proceedings of the 24th annual international symposium on Computer architecture* .\n252",
			  "chun Kim, Seth H Pugsley, Paul V Gratz, AL Narasimha Reddy, Chris Wilker-\nson, and Zeshan Chishti. 2016. Path confidence based lookahead prefetching.\nIn *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture*\n*(MICRO)* . IEEE, 112."
			]
		  },
		  {
			"title": "Arsenal of Hardware Prefetchers",
			"url": "https://www.arxiv.org/pdf/1911.10349v1",
			"excerpts": [
			  "ature Path Prefetcher** (SPP) [ 5 ] stores the stride pat-\nterns in a compressed form in the signature table (ST). Each\nentry in the ST is used to index into the pattern table (PT),\nwhich is used to predict the next stride and also contains the\nconfidence for the current prefetch. The signature is then up-\ndated with the latest stride and is used to recursively lookup\nthe PT to predict more strides. This goes on until the confi-\ndence, which is multiplied with the last prefetch confidenc"
			]
		  },
		  {
			"title": "L5 - Advanced Memory Prefetching Techniques",
			"url": "https://coconote.app/notes/5551d401-c94c-4d86-bb70-c67c9e8bb912",
			"excerpts": [
			  "Delta correlation prefetchers track repeating delta patterns between accessed addresses to predict future accesses. Correlation-based ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov Predictors",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=joseph_isca97.pdf",
			"excerpts": [
			  "The Markov prefetcher provides the best performance across nll\nthe applications, particularly when applied to both the instruction\nand data caches."
			]
		  },
		  {
			"title": "A Survey on Recent Hardware Data Prefetching ...",
			"url": "https://arxiv.org/pdf/2009.00715",
			"excerpts": [
			  "99] D. Joseph and D. Grunwald, Prefetching Using Markov Predictors, in *Proceedings of the International Symposium on*\n*Computer Architecture (ISCA)* , pp. 252263, 1997"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/635508.605427",
			"excerpts": [
			  "D. Joseph and D. Grunwald. Prefetching using markov predictors. In Proceedings of the 24th Annual International Symposium on Computer Architecture, pages 252-263, Denver, Colorado, June 1997. ACM."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/384265.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "Dependence based prefetching for linked data structures > Abstract",
			  "Content:\nWe introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  " uses the underlying data of the application, and provides an 11.3% speedup using no additional processor state. By adding less than 1/2% space 2 overhead to the second level cache, performance can be further increased to 12.6% across a range of \"real world\" applications.\n*",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems.",
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "There are a number of ways to\nidentify \"likely\" addresses.",
			  "Roth\net al.\nintroduced dependence-based techniques for capturing\nproducer-consumer load pairs [ 12].",
			  "his paper investigates a technique that predicts addresses in\npointer-intensive applications using a hardware only technique with\nno built-in biases toward the layout of the recursive data struc-\nSection Title: A state",
			  "ability to \"run ahead\" of an application has been shown to be a re-\nquirement for pointer-intensive applications [12], which tradition-\nally do not provide sufficient computational work for masking the\nprefetch latency.",
			  "me hybrid prefetch engines [13] do have the\nability to run several instances ahead of the processor, but require\napriori\nknowledge of the layout of the data structure, and in some\ncases, the traversal order of the structu",
			  "es. Content-based prefetching works by examining the content of data as it is moved from memory to the caches. Data values that are likely to be addresses are then translated and pushed to a prefetch buffer. Con",
			  "N\nIn early processor designs, the performance of the processor and\nmemory were comparable, but in the last 20 years their relative\nperformances have steadily diverged [4], with the performance im-\nprovemen"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  "Content-Directed Data Prefetching, a data*\nprefetching architecture that exploits the memory allocation used\nby operating systems and runtime systems to improve the perfor-\nmance of pointer-intensive applications constructed using modem\nlanguage systems. This technique is modeled after conservative\ngarbage collection, and prefetches \"likely\" virtual addresses ob-\nserved in memory references",
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance.",
			  "In the evaluation, the proposed solution achieves an average speedup of 1.37  over a set of memory-intensive benchmarks."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "(PDF) On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor",
			"url": "https://www.academia.edu/126501330/On_Reusing_the_Results_of_Pre_Executed_Instructions_in_a_Runahead_Execution_Processor",
			"excerpts": [
			  "Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs.",
			  "runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient.",
			  "In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance."
			]
		  },
		  {
			"title": "Filtered runahead execution with a runahead buffer | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830812",
			"excerpts": [
			  "Section Title: Filtered runahead execution with a runahead buffer",
			  "Section Title: Filtered runahead execution with a runahead buffer > References",
			  "Authors : [Milad Hashemi](# \"Milad Hashemi\") Milad Hashemi",
			  "Milad Hashemi",
			  "The University of Texas at Austin",
			  "Pages 358 - 369",
			  "https://doi.org/10.1145/2830772.2830812"
			]
		  }
		],
		"reasoning": "The user is looking for advanced and obscure techniques for speculative prefetching and latency hiding for parallel tree traversal with scattered memory access. The provided excerpts offer several relevant techniques. \n\n**Inline Software Prefetching ('Helper Without Threads')** is highly suitable. This technique involves inserting prefetching logic directly into the application's main thread, running ahead to issue prefetches for future data without the overhead of separate OS threads. This directly addresses the 'Delinquent Irregular Loads' (DILs) that characterize the user's problem, which are loads with no constant or striding address patterns. It has been shown to achieve up to 2x single-thread performance improvement and up to 83% faster than helper-thread implementations for memory-bound irregular workloads. A caveat is that the prefetch distance is statically controlled, which simplifies tuning but is less adaptive. This method is particularly relevant because it avoids the complexities of thread management that would be problematic for 256 independent walkers.\n\n**Helper-Thread Prefetching (Speculative Precomputation)** is conceptually suitable for pointer-chasing. It uses idle SMT contexts to run a helper thread that executes a 'slice' of the main code to compute future delinquent load addresses and issue prefetches. While effective for pointer-chasing, it is likely not scalable for 256 walkers due to the limited number of SMT contexts and the high overhead of managing hundreds of helper threads. It can reduce memory stall time by up to 72% but requires spare SMT contexts and suffers from significant thread management and synchronization overhead.\n\n**Treelet Prefetching (GPU)** is extremely suitable for a GPU implementation, which is relevant given the user's interest in GPU/graphics research. This technique partitions the BVH into small subtrees called 'treelets'. When a walker enters a treelet's root, the hardware prefetches all nodes within that treelet. This directly attacks the pointer-chasing dependency chain by prefetching a coarse-grained, spatially local chunk of the tree, turning many dependent loads into a single, larger prefetch operation. It has been shown to reduce BVH memory access latency by 54% on average, leading to a 32.1% IPC improvement in simulation. A key trade-off is that a significant percentage of prefetched nodes may go unused, wasting some bandwidth.\n\n**Jump-Pointer Prefetching (JPP)** is very suitable as it is designed specifically for linked data structures. It augments the tree with additional 'jump pointers' that skip intermediate nodes to point to distant descendants. A prefetch engine can follow these to prefetch nodes far ahead, directly breaking long dependency chains. The cooperative software/hardware model can reduce memory stall time by 83%, yielding a a 20% overall speedup, while the software-only version reduced stall time by 72% for a 15% speedup. The main caveat is the overhead of creating and storing the extra pointers.\n\n**Correlation Prefetching (GHB, DCPT)** is moderately suitable. These hardware-based prefetchers learn patterns in the memory access stream, tracking sequences of deltas (strides) between consecutive miss addresses to predict the next. While powerful for irregular patterns, their effectiveness on pure pointer-chasing can be limited if there are no repeating delta patterns, making them more effective for patterns like `A[B[i]]` than `A->next`. DCPT showed a 42% speedup over a baseline on SPEC2006 benchmarks with only 4KB of storage overhead, and GHB can improve correlation prefetching performance by 20%. Their effectiveness for this specific tree traversal must be benchmarked.\n\n**Content-Directed Prefetching (CDP)** is conceptually a perfect fit for pointer-chasing. It's a stateless hardware mechanism that scans fetched cache lines, identifies values that look like valid memory addresses (pointers), and speculatively prefetches them, recursively traversing pointer chains. However, it can suffer from a high rate of mispredictions, leading to significant cache pollution and wasted bandwidth. It achieved an 11.3% speedup with less than 0.5% space overhead in the L2 cache. Hybrid versions that use compiler hints to identify pointer-rich regions are more efficient.\n\n**Runahead Execution** is a general-purpose latency-hiding hardware technique. Upon a long-latency cache miss, the processor checkpoints its state and speculatively executes instructions down the miss path. The results are discarded, but the side effect is that it triggers the hardware prefetcher for subsequent data misses. While it can increase memory latency tolerance, it is known to be highly inefficient in terms of energy and wasted work, as many speculative instructions are executed and thrown away.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.3",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. ",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  }
		],
		"reasoning": "The core field value describes a software layer that collects and groups memory requests from many parallel walkers, then issues them as coalesced transactions. It also envisions a scheduling mechanism (a software layer) that groups requests destined for the same node or cache line, effectively converting scattered accesses into fewer, more regular memory transactions. The most directly relevant excerpts describe memory-access scheduling via queues that decouple access from execution, and explicitly frame a two-stream (or queued) workflow where memory requests are gathered and fed to an executor, which supports more regularized access patterns. This aligns with the concept of software-driven coalescing and scheduling to reduce memory traffic and improve effective bandwidth. Additional highly relevant material discusses warp specialization and DMA-like memory-transfer abstractions that enable overlapping or coalesced memory operations by partitioning work into memory-focused and compute-focused streams, thereby achieving memory bandwidth efficiency in practice. Finally, other excerpts describe batched or partitioned traversal ideas and two-queue coordination that provide architectural patterns consistent with batching and coordinating access in a data-parallel setting. Taken together, these excerpts substantiate the feasibility and design space for software-request coalescing / access scheduling as described in the field value, including both conceptual models and example realizations on GPUs (queues, AEQ/EAQ, DMA-like buffering, and warp-specialization.)",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.4",
		"citations": [
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads."
			]
		  }
		],
		"reasoning": "The finegrained field value defines CoopRT as a hardware-level optimization that reuses idle threads within a divergent warp to help with tree traversal, including a memory scheduler that coalesces node fetches from multiple threads. The most directly supportive content appears in excerpts that introduce CoopRT and describe its core ideas: cooperative BVH traversal, where threads within a warp assist each other, and per-cycle warp scheduling and compaction concepts that align with reclaiming idle compute power to accelerate busy threads. Several excerpts explicitly state or imply that the RT unit uses cooperative threads, thread compaction, and shared warp-level mechanisms to improve SIMD/SIMT efficiency and achieve substantial speedups. Other excerpts discuss related mechanisms (e.g., active-thread compaction, dynamic warp formation, thread block compaction) and corroborate the general benefits of coordinating multiple threads to reduce divergence effects and improve memory access patterns during traversal. The strongest support comes from entries that name CoopRT or describe a two-stack or cooperative traversal approach, how threads are compacted or reassigned, and the observed performance gains in Vulkan-Sim or RT units, which match the reported performance improvements in the field value.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.2",
		"citations": [
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)"
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  }
		],
		"reasoning": "The field value describes a pattern where a large grid of threads operates as persistent workers that continually pull work from a shared global queue, using atomics to claim tasks and maintain high throughput despite irregular workloads. Direct matches are found in discussions of decoupled access/execute architectures that separate memory access from compute, and in descriptions of the Access to Execute Queue (AEQ) and the Execute to Access Queue (EAQ) that coordinate memory operations and computation. The idea that memory requests and results flow through explicit queues aligns with the work-queue / producer-consumer model embedded in these excerpts, where separate streams or queues coordinate data movement and computation. Further support comes from discussions of warp specialization and named barriers, which describe fine-grained synchronization and partitioning of work among warps to exploit irregular access patterns and maintain high throughput, relevant to how a work queue can be serviced efficiently. Additional excerpts point to the hardware and compiler support for such queue-based designs (e.g., CUDA DMA style architectures and barrier-based synchronization), which underpins the practical realization of a persistent-work-queue model on GPUs. Taken together, these excerpts substantiate the concept of persistent worker threads pulling work from a global queue, coordinated by queues and lightweight synchronization primitives, to maximize throughput for irregular workloads with limited per-thread memory bandwidth.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.1",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o"
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The target field value describes a traversal technique that starts with a cohort of walkers operating in lockstep (a packet) to exploit SIMD efficiency when the traversal paths are coherent, and later switches to independent, single-walker traversal as activity dwindles and divergence increases. Several excerpts discuss packet-based traversal and its benefits, including evidence that grouping rays or threads into packets can yield substantial speedups and better SIMD utilization, particularly at the root of BVH-like structures where coherence is higher. For example, discussions of persistent threads forming a pool and processing work from a global queue show how work can be distributed to maintain high throughput when coherence is present, while also permitting dynamic reassignment of work as load and divergence change. This directly supports the core idea of a hybrid approach that starts with packet traversal and transitions to single-walker traversal when the active walker count falls below a threshold. Additional excerpts describe dynamic warp formation and thread-block compaction as concrete mechanisms to maintain SIMD efficiency under divergence, by regrouping or compacting threads into new warps after divergent branches. These mechanisms map well to the proposed switching behavior and the use of warp-level primitives to monitor activity (e.g., ballot/popcount). Other sources discuss hybrid traversal modes that switch between packet and single-ray (or single-walker) traversal and quantify performance gains (e.g., up to 50% improvements in certain Embree configurations when using hybrid packet/single-ray strategies). They also discuss the architectural implications of coalesced versus scattered accesses and the need to adapt traversal strategy as coherence changes along the tree traversal, aligning with the described strategy of starting coherent and amortizing into single-walker phases. Taken together, these excerpts corroborate the feasibility and effectiveness of a hybrid packet/single-walker traversal technique, the use of warp-level synchronization and reformation to manage divergence, and the potential performance benefits observed in related GPU ray-tracing and BVH traversal research.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.4",
		"citations": [
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  }
		],
		"reasoning": "Warp specialization is presented as partitioning computations into sub-computations assigned to different warps within a thread block, which directly aligns with a hardware-level mechanism that groups threads dynamically to maintain SIMD efficiency in the face of divergence. The idea that computations can be partitioned into warp-specific subcomputations and that this approach improves handling of irregular access patterns matches the notion of dynamically regrouping threads after divergence to keep execution coherent. Additionally, the material describes efficient fine-grained synchronization between warps via producer-consumer named barriers, which is a concrete hardware-supported mechanism enabling coordinated regrouping and data exchange without incurring heavy synchronization costs. The discussion of warp specialization enabling dealing with irregular access patterns and large working sets further supports the core benefit described in the field value, namely restoring SIMD efficiency after divergence. Related materials extend this concept to hardware-supported WASP-style approaches where warp-level queues, address generation, and memory/compute overlap are explicitly designed to enable dynamic grouping and coordination, which reinforces the same underlying hardware-driven principle. Some sources also illustrate how partitioning computations into warp-specialized sub-computations improves performance on irregular workloads and supports large working sets within limited memory bandwidth, which is conceptually consistent with the proposed Dynamic Warp Formation / Thread Block Compaction model. Collectively, the excerpts converge on the same theme: hardware-supported warp specialization and dynamic regrouping of threads to mitigate divergence and maintain high memory/computation overlap in GPUs. The most directly supportive pieces explicitly describe partitioning computations across warps, dynamic regrouping, and efficient warp-to-warp synchronization, while additional sources elaborate on the broader hardware-software ecosystem (e.g., WASP, CudaDMA) that embodies the same architectural principle.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.3",
		"citations": [
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet."
			]
		  }
		],
		"reasoning": "The most directly supportive content describes treelets as small subtrees that form the basis of per-treelet traversal, enabling prefetching and clustering of memory accesses. When a traversal groups rays (or walkers) by their accessed treelets, memory loads become more coherent within a treelets scope, which reduces pointer-chasing latency and exploits cache locality. In particular, treelets are introduced as subtrees sized to fit cache, and the traversal is organized around treelet queues where a single treelets data is loaded and all walkers targeting that treelet are processed together. This aligns with the finegrained field values emphasis on partitioning the BVH into treelets to fit cache levels and grouping walkers into queues per treelet, with a scheduler loading the treelet into cache before processing its queue. Additional excerpts describe a treelet prefetcher and prefetching strategy that brings the entire treelet into cache ahead of traversal, further supporting the notion that clustering memory accesses within a treelet reduces bandwidth bottlenecks. Performance results cited in these sources indicate significant speedups when combining treelet-based traversal with prefetching and related queueing/scheduling optimizations, including improvements up to 2.55x in path tracing scenarios, which directly corroborate the claimed reported_performance_gain in the field value. The presence of discussions about a two-stack hardware prefetcher and queueing strategies reinforces the mechanism by which the scheduler can group walkers by treelet and load data ahead of use, as described in the field value. Taken together, these excerpts collectively substantiate the core idea of splitting the BVH into treelets, routing walkers through treelet-based queues, and using a scheduler/prefetcher to maximize cache locality and memory efficiency, with empirical performance benefits reported when these techniques are combined with additional optimizations like warp repacking and virtualized queues.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.0",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A."
			]
		  }
		],
		"reasoning": "The strongest support comes from passages that explicitly describe long-running persistent threads that fetch work from a global pool using an atomic counter until the pool is empty. This directly maps to the idea of persistent threads maintaining occupancy by continuously pulling new tasks from a shared queue, bypassing standard scheduling to handle divergent or irregular workloads. The related statements about launching enough threads to fill the machine once and then using a global pool further corroborate the same mechanism and its intent to sustain throughput under irregular control flow. Additional excerpts reinforce the persisted-thread concept by discussing persistent variants of traversal, which aligns with the notion of keeping a steady pool of work active across rounds. Finally, excerpts that report performance gains associated with such strategies (e.g., 1.5x-2.2x improvements for packet traversal, and a notable 32% improvement under coalesced work-queue scenarios) provide empirical backing for the claimed benefits of persistent threads and work-queue scheduling in the described context.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.2",
		"citations": [
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The most directly relevant material describes a concrete mechanism for improving SIMD efficiency by regrouping divergent threads into new warps, i.e., thread block compaction and dynamic warp formation. These excerpts explicitly discuss the concept of sharing or reorganizing warps to reduce divergence, the presence of a per-block reconvergence stack, and hardware/software mechanisms (like a thread compactor) to perform this regrouping. They also quantify gains in SIMD efficiency, with measured speedups when using compaction alone and larger gains when combined with related scheduling techniques, which aligns with the field values claim of a 22% average improvement from hardware-based thread block compaction and potential multipliers when combined with other techniques (e.g., treelet scheduling). For instance, sections describe: - sharing a block-wide reconvergence stack and compacting divergent warps into new warps to improve SIMD efficiency; - a hardware unit or modified branch/path handling that compacts threads into fewer, more coherent warps; - empirical results showing approximately 22% speedup over a per-warp baseline for divergent applications, and higher gains when integrated with complementary approaches. These points directly support the field values mechanism (compaction identified as active regrouping of active walkers/warps) and its primary benefit (nearly full SIMT efficiency) as well as the reported performance figures. Additional excerpts discuss dynamic warp formation and its role in improving SIMD branch execution, which reinforces the same underlying idea of regrouping work to combat divergence and improve throughput. While related papers on treelets, prefetching, and hybrid traversal provide broader context on memory access and traversal efficiency, they are less central to the specific compaction mechanism described and thus considered less directly supportive of the exact field value, though they offer complementary insights on overall performance gains when multiple techniques are layered together.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.0.model",
		"citations": [
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one."
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  }
		],
		"reasoning": "The target field value corresponds to the architectural concept of Decoupled Access/Execute (DAE). The most directly supportive information is that DAE architecture is explicitly described as separated into two major functional units, the Access Processor and the Execute Processor, i.e., two independent streams handling memory access and computation. It is also described as having two separate instruction streams that communicate via queues, i.e., an explicit decoupling mechanism with inter-stream communication. Additional supportive detail notes that the A-processor handles memory address calculation and memory read/write requests, and that data is moved between the A and E streams via a FIFO-like queue structure, which reinforces the core DAE model of decoupled operation and buffer-based coordination. These elements together precisely map to the field value of Decoupled Access/Execute, establishing the architectural pattern the research is interested in optimizing for (divergent traversal, memory access scheduling, and prefetching considerations in a two-stream design). Other excerpts reinforce the same concept by describing the same architecture in various sources (SIGARCH note, course notes, and lecture material), including the notion of ISA-visible queues and the general idea of decoupling operand access from execution. These reinforce the interpretation of DAE as the model in question and provide corroborating language that strengthens the mapping between the field value and the described architecture.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.0.primary_benefit",
		"citations": [
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  }
		],
		"reasoning": "The central claim is that latency hiding is achieved by decoupling memory access from computation and by using buffering/queuing to keep multiple memory requests in flight. Excerpts describing decoupled access/execute architectures present a clear mechanism: data is fetched by the memory-facing A-processor and then consumed by the execution-focused E-processor, with data transfers routed through an access-to-execute queue. This separation enables overlapping work with memory operations and allows keeping the memory subsystem busy even when computation depends on memory. The concept of two separate instruction streams communicating via queues reinforces how latency can be hidden by scheduling and decoupling, which matches the target field values emphasis on overlapping computations with memory access and maintaining many outstanding memory requests. Additional excerpts discuss the specifics of how memory requests enter and exit the pipelines, which supports the idea of orchestrating many concurrent memory operations to tolerate latency and fit scenarios with tight load-bandwidth constraints across many walkers. Further notes on synchronization and potential disadvantages provide nuance about the practical limits and engineering trade-offs, which contextualize how such models might be adapted for irregular or data-dependent traversal patterns. A final excerpt referencing GPU memory bandwidth optimization via warp specialization offers a related but broader perspective on maintaining high throughput in memory-bound workloads, reinforcing the general principle of maximizing effective memory utilization in parallel, batched computations.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.3",
		"citations": [
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  }
		],
		"reasoning": "The target fine-grained value discusses using vectorized, batched hashing in a SIMD-friendly manner to accelerate the hash computation and subsequent probing during traversal. Excerpts describing batch and vectorized hash tables directly support this idea: they present concepts of Vec-HT style designs that perform hashing and lookups in batches using SIMD, and explain vertical vectorization where a group of keys (or walkers) are processed in parallel. They also discuss partitioning work into batches that feed SIMD-friendly probes, which aligns with the proposed approach of computing hash(current_value ^ node_value) for many walkers in parallel and then driving the next traversal step from those batched hashes. Related excerpts emphasize that batching, SIMD-wide hashing, and prefetching are used to improve throughput in memory-bound hash-table and traversal contexts, which matches the described need to transform the hashing step into a vectorized, pipeline-friendly operation and to feed a high-throughput, batched traversal. Specific points cited include: (a) the concept of a batch-enabled, SIMD-accelerated hash table design where operations are organized in groups to leverage cache locality and vector units, (b) explicit discussion of vertical/vectorized hashing where a vector of inputs is processed through the lookup, and (c) the role of prefetching and memory access coalescing in such SIMD-enabled hash structures to sustain throughput during batched traversals. The combination of batch hashing with SIMD probing and vectorized memory access matches the described application of computing and using hashes for a large number of walkers and then using those results to decide traversal directions in a data-parallel manner.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.0.core_concept",
		"citations": [
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream."
			]
		  }
		],
		"reasoning": "The key idea in the finegrained field value is that the instruction stream is split into two cooperating units: an Access unit dedicated to memory operations and an Execute unit dedicated to computation. These two units communicate via explicit queues, enabling the Access unit to advance ahead of the Execute unit and prefetch data, thereby hiding memory latency. This directly maps to the field values description of a two-unit, decoupled pipeline with FIFO queues that coordinate work and allow prefetching. Multiple excerpts explicitly describe this two-unit separation and the queue-based coordination: one excerpt defines a two-processor (A-processor and E-processor) decoupled architecture with separate instruction streams communicating via queues; another excerpt notes that the Access stream loads data into a queue visible to the Execute stream, enabling data and control flow to be decoupled. Additional excerpts reinforce that the two streams can run asynchronously, with one progressing (or prefetching) while the other performs computation, which is the mechanism by which latency can be hidden in memory-bound phases of traversal. Collectively, these excerpts support the claim that structuring work as memory-focused and computation-focused units with explicit queues is a recognized approach to improving throughput in environments with irregular or divergent memory access, such as a batched, SIMD-friendly traversal of a binary tree by multiple walkers starting at a common root and diverging later. The content also mentions the concept of the Execute stream productively running ahead or alongside the Access stream, which aligns with the motivation to maximize CPU/GPU utilization under memory bandwidth constraints in the given scenario.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.0.platform_realization",
		"citations": [
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one."
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor."
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream."
			]
		  }
		],
		"reasoning": "The field value states that on CPUs this technique is known as Decoupled Software Pipelining (DSWP), and on GPUs it appears as Warp Specialization, where distinct warps handle memory access while others perform computation, with inter-stream communication via shared memory or queues. The most directly supportive content describes decoupled access/execute architectures as a core approach to separating operand access from execution, including the use of two separate processor streams and queues for communication between memory and compute stages. This aligns with the notion of DS Pipelining and makes explicit the CPU realization as a decoupled architecture with distinct A-processor and E-processor units and a memory-access dominated vs compute-dominated workflow. The GPU-specific warp specialization references the technique of dedicating warps to memory access and others to computation, which mirrors the field values GPU realization. The excerpts also discuss the mechanism by which data flows between the two streams, via queues or shared memory, supporting the claim of inter-stage communication critical to DSPl/warp-style arrangements. Additional notes on asynchronous operation, and the potential for one stream to advance while the other waits, reinforce the practical implications of DS-Pipelining and warp specialization in real hardware. Together, these pieces corroborate the mapped CPU and GPU realizations, the structural separation of concerns, and the inter-stream communication mechanisms that realize the described scheduling/execution model.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.4",
		"citations": [
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  }
		],
		"reasoning": "The most compelling evidence directly supporting the finegrained field value is a passage that explicitly introduces CSB+-Trees and describes the core idea: storing all child nodes contiguously and keeping only the address of the first child, with the remainder located via simple offsets. This aligns precisely with the described technique of contiguous child storage and reduced pointer overhead. Additional passages that discuss CSB+-Trees as cache-conscious structures reinforce the claimed data-density and cache-line benefits, noting that contiguity improves locality and allows larger portions of the tree to fit in cache, thus reducing memory traffic. Other excerpts that discuss CSS-Trees or cache-conscious tree designs provide corroborating context about cache-awareness in tree layouts and the goal of improving locality, which strengthens the broader rationale for the CSB+-Tree approach, though they are less specific about the exact contiguous layout mechanism. Some excerpts touch on related ideas such as wider node considerations or general prefetching strategies; these are supportive but not central to the exact mechanism of contiguous child storage. In sum, the strongest support comes from explicit CSB+-Tree descriptions that advocate contiguous child storage and pointer elimination to improve cache utilization, followed by supportive discussions on cache-conscious tree layouts and related structures.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.1",
		"citations": [
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page."
			]
		  }
		],
		"reasoning": "The most directly relevant material discusses Eytzinger layout as a cache-friendly, pointer-free arrangement for binary trees. It explains that Eytzinger organizes nodes in an array and uses arithmetic relationships to locate children, which eliminates pointer-based memory indirection and improves data locality. This directly supports the finegrained field values claim that a fully implicit, arithmetic-based layout increases data density, reduces scattered accesses, and enhances cache locality, which in turn benefits vectorized or SIMD-based traversal under constrained memory bandwidth. Additional excerpts describe how such layouts improve temporal/spatial locality and how prefetching can be more effective when traversal follows predictable index-based patterns rather than scattered pointer chasing. The presence of sections highlighting prefetching strategies and the practical performance benefits of cache-conscious layouts further reinforces the applicability of Eytzinger-style organization to the described 256-walker traversal scenario, especially given the initial root convergence and subsequent divergence. The cited materials collectively corroborate that moving to an implicit array layout (as in Eytzinger) reduces pointer overhead and enables more cache-friendly and prefetcher-friendly traversal, aligning with the finegrained field values core claims about memory traffic reduction, cache utilization, and predictable access patterns.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.1.model",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  }
		],
		"reasoning": "The most relevant material describes warp specialization as a mechanism to maintain high SIMD efficiency when dealing with irregular control flow and memory access, which directly speaks to the wavefront/warp-level execution concept. It discusses overlapping memory access and compute via specialized warps, enabling more coherent and batched progression even when individual threads diverge, which is precisely the tension in the user scenario where 256 walkers start from a common root but diverge as traversal proceeds. This set of excerpts also emphasizes hardware- and compiler-assisted techniques to realize fine-grained parallelism with constrained memory bandwidth, aligning with the goal of achieving high throughput without resorting to coarse-grained megakernels. Additional mentions of warp specialization and pipeline parallelism reinforce the notion that structured, warp-aware scheduling and memory coalescing can mitigate irregular access penalties, which is central to the users optimization focus for a parallel tree traversal workload. The remaining excerpts, while about related architectural concepts (decoupled access/execute models), provide context about historical and architectural underpinnings but are less directly connected to warp-level execution strategies and memory coalescing for irregular traversal patterns. Overall, the strongest support comes from discussions of warp specialization and memory-overlap strategies; the rest provide supplementary context about GPU architectures and parallel programming models.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.2",
		"citations": [
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "Use `H1` to pick a starting group.",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "Swiss Table has the following key characteristics:",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed."
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions."
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  }
		],
		"reasoning": "The field value envisions splitting a 64-bit hash into H1 (high bits) used to select a starting group and H2 (low bits) used as a fingerprint that is matched against a SIMD-wide fingerprint in the group. This approach is exactly what is described in excerpts that state: - Use H1 to pick a starting group, which determines the candidate cluster for the lookup. - Scan a group of slots in parallel by comparing H2 fingerprints (or control-byte fingerprints) using SIMD, generating a mask of potential matches. - Only for matching positions, check the actual keys; if a slot is marked EMPTY, stop early. These excerpts collectively demonstrate the core workflow: partition the hash into a group-index via H1, use SIMD-capable control structures (control bytes or fingerprints) to filter the group-wide candidates with H2, and then proceed to full-key verification only on the reduced set. Additional excerpts discuss the use of control bytes and group-wide SIMD filtering, which underpins the efficiency of this approach by enabling parallel probing and early discard of non-matches. The material also notes that modern SIMD techniques can handle scanning multiple control bytes (or fingerprints) per instruction, and that the group size in practical designs is chosen to align with SIMD width (e.g., 8 or 16 slots per group), which directly supports the proposed grouping strategy for 256 parallel walkers. Taken together, these sources corroborate the idea of a SIMD-friendly, H1/H2-splitting, group-based probing strategy to accelerate traversal with irregular memory access patterns. ",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.1.primary_benefit",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  }
		],
		"reasoning": "The field value asserts a dramatic reduction in control flow divergence within a warp by using specialized kernels where all threads execute a single instruction path. Excerpts describing warp specialization as a technique to handle irregular data access and computation patterns on GPUs directly support this claim, since specialization aligns the execution paths of threads within a warp, minimizing divergent branches. Specifically, the discussion of warp specialization enabling high performance code for GPUs and dealing efficiently with irregular access patterns provides concrete evidence that specialization can reduce divergence and improve SIMT efficiency. Additional notes about overlapping memory access and compute via warp-level mechanisms reinforce the same theme: by structuring work so that threads in a warp follow a cohesive path, throughput and occupancy can be improved, and pressure on registers reduced. The combination of these points directly connects to the idea that specialized, warp-focused implementations yield lower divergence and better utilization of SIMD/SIMT hardware, which matches the finegrained field value.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.0",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD"
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best.",
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate)."
			]
		  }
		],
		"reasoning": "The most directly relevant material discusses using SIMD to avoid branches by computing and applying a bitmask derived from comparisons, then selecting outcomes with a blend or movemask-based approach. For example, a SIMD strategy is described where comparisons across multiple elements produce a vector mask, which is then collapsed into a scalar mask to locate the desired element, or where a bitmask is formed from many comparisons and used to pick the next indices without branching. This directly maps to the idea of using the LSB-driven mask to route 256 walkers through the binary tree via a single instruction sequence that chooses between left and right children without conditional jumps.\n\nFurther, there are examples that show loading multiple elements, generating a vector mask from those comparisons, and then extracting or using that mask to drive decisions, which aligns with forming an LSB-based bitmask across the batch of walkers and feeding that mask into a blend or select operation to pick the correct child indices for each walker without a branch.\n\nAdditional excerpts expand on SIMD-based traversal in related structures (e.g., vectors of keys in B-trees or k-ary searches) and on using SIMD masks to constrain which paths are active, all of which reinforces the core idea of replacing per-walker branches with vectorized selection logic. There is also relevant discussion on complex tree layouts and SIMD-friendly traversal patterns that support faster, batched decisions when many walkers diverge from a common root, which underpins the batching aspect of the proposed approach.\n\nExcerpts that discuss using 16-wide or wider SIMD operations to perform multiple comparisons and then combine results through masking, bit-twiddling, and permutation/blend operations provide concrete mechanisms that would implement the proposed branchless traversal. The material about implicit/static B-trees and related SIMD routines demonstrates that doing many comparisons in parallel and then resolving the outcomes with masks and blends is a practical pathway for improving throughput in tree traversals, which supports the field values claim about replacing conditional branches with SIMD-based selection.\n\nIn sum, the most relevant content directly describes the core mechanism: forming a bitmask from per-walker comparisons and using SIMD blend/mask operations to choose the next node indices without branches. The related excerpts extend this by showing how to manage multiple comparisons in parallel, extract and apply masks, and use bitwise or permute-based selection to keep the pipeline full while traversing a tree. Overall, these sources collectively corroborate the branchless, SIMD-based node selection approach for batched traversal.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.1.platform_realization",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ."
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more"
			]
		  }
		],
		"reasoning": "The fine-grained field value describes a GPU-centric execution model where a large pool of walkers resides in global memory, each task completing in a kernel and then the walker being enqueued for subsequent specialized kernels, with synchronization required between kernel launches. This points to concerns about coordinating work across multiple kernels, exploiting warp-level or pipeline parallelism to mask memory accesses, and handling irregular or divergent work within a batched GPU execution setting. Excerpts that discuss warp specialization and GPU pipeline parallelism directly address techniques for coordinating and overlapping compute with memory access in GPUs, which aligns with the stated need to manage many parallel walkers across kernel boundaries and to maintain high throughput despite irregular traversal. For example, discussions of warp specialization enabling high performance on GPUs by partitioning work or tailoring execution paths to divergences are highly relevant to structuring multiple specialized kernels that handle different phases of the walkers work. Additionally, material on overlapping memory access and compute through hardware and compiler support for warp-level mechanisms demonstrates concrete approaches to reducing global memory bandwidth pressure while preserving throughput across kernel launches. Related discussions on data-flow architectures (such as decoupled access/execute concepts) provide broader context on separating memory access from compute and on queueing or buffering data between stages, which conceptually supports the idea of enqueuing walker states for the next kernel, even if not describing exactly the same mechanism. Finally, references that address irregular access patterns and techniques to cope with divergence in a GPU setting offer a theoretical basis for batching or predicting accesses in a structured traversal like a binary tree when many walkers start from a common root but diverge thereafter.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.0",
		"citations": [
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). "
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  }
		],
		"reasoning": "The target field value describes an inline software prefetching technique that inserts prefetching logic directly into the main thread, running ahead of the primary computation to prefetch data for delinquent irregular loads without spawning helper threads. Excerpts that explicitly mention inline software prefetching and embedding prefetch logic in the main thread provide the strongest support. For example, one excerpt states that the inline software prefetching technique overcomes restrictions by inserting the helper code into the main thread itself, which aligns with the concept of an inline, software-only prefetcher operating within the primary execution thread. Another excerpt describes generating and inserting a customized software prefetcher extracted from the applications dataflow, all without access to the source code, which reinforces the idea of software-embedded prefetch logic rather than a separate helper thread. A further excerpt explicitly calls out an inline software prefetching approach that runs ahead of the main computation, which is a direct match to the described technique. Additional excerpts discuss the broader category of prefetching for delinquent irregular loads, including the notion of prefetch slices and inline prefetching strategies, reinforcing relevance to the overall concept of inline, software-driven prefetching rather than hardware or helper-thread approaches. While some excerpts describe related ideas (e.g., helper-thread prefetching, delinquent load targeting, or generic software prefetch concepts), they are slightly less specific to the exact inline, single-thread, main-thread-internal prefetching mechanism described in the field value. Taken together, these excerpts corroborate the core idea of injecting prefetch logic into the main thread to prefetch data for irregular accesses without creating extra threads, and they illustrate the practical benefits and caveats of such an approach.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.2",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The fine-grained field describes a speculative prefetching technique built around a specialized hardware prefetcher that partitions a BVH tree into treelets and prefetches an entire treelet when a walker reaches a treelet root. This approach is explicitly aimed at reducing pointer-chasing latency in GPU-style BVH traversal by clustering memory accesses and prefetching ahead of the main path. The most directly supportive excerpts describe the core concept and its reported impact: a treelet-based prefetching scheme that prefetches BVH treelets to the GPU cache and thereby accelerates ray traversal, with concrete performance results indicating substantial speedups in a realistic ray-tracing context. In more detail, the excerpts explain that treelets are smaller subtrees created by splitting the BVH, that prefetching treelets can happen as rays traverse treelets, and that this clustering enables prefetching of deeper levels preemptively, reducing pointer-chasing latency. The strongest, explicit performance datapoint notes an average 32.1% speedup in a GPU ray-tracing setting when treelet prefetching is employed in combination with a treelet-based traversal, which aligns with the stated reported gains in the finegrained field. Additional excerpts reinforce the architectural rationale: treelets cluster memory accesses to maximize reuse and simplify prefetching, and there are references to hardware-oriented designs where a treelet prefetcher is integrated with the traversal logic. Taken together, these excerpts robustly support the idea that a GPU-oriented treelet prefetcher operates by partitioning BVH into treelets, triggering prefetches at treelet roots, and achieving substantial latency reduction and throughput gains, with caveats noted about potential unused prefetched data in some cases. These pieces coherently map onto the path specifier and the reported gains, including the 54% reduction in BVH memory access latency and the 32.1% IPC improvement reported in simulations, as described in the cited study.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.3.model",
		"citations": [
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. "
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  }
		],
		"reasoning": "The finegrained field value describes a concept of coordinating and improving how memory requests are issued and scheduled in a highly parallel, partially divergent traversal. Excerpts that discuss synchronization primitives and producer-consumer barriers in GPU kernels are directly related to scheduling questions: they describe how to synchronize work across warps and manage when memory requests are issued relative to computation. Excerpts describing warp specialization and the use of hardware or software structures to overlap memory accesses with computation provide concrete mechanisms to coalesce or hide memory latency, which aligns with software-directed scheduling and access batching in a multi-walker traversal. Excerpts detailing warp-level queues, circular buffers mapped in register space, and techniques for overlapping memory access with compute address the core idea of coalescing and scheduling requests to improve throughput. Excerpts that frame memory bandwidth improvements via specialization, or that discuss addressing irregular access patterns with structured scheduling, further support the notion of coordinating access requests to reduce stalls. Taken together, these sources collectively support concepts of software- or hardware-assisted access scheduling and coalescing in a batched, parallel traversal context.",
		"confidence": "medium"
	  },
	  {
		"field": "execution_and_scheduling_models.3.primary_benefit",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. "
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps."
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  }
		],
		"reasoning": "The core claim is that memory access efficiency and bandwidth are improved by techniques that overlap or coordinate memory access with computation, and by structuring or scheduling work to reduce the number of memory transactions. The most directly relevant excerpt presents warp specialization as a powerful technique for overlapping memory access and compute operations, enabling better GPU performance when traversals encounter irregular access patterns. This directly supports the idea that reorganizing computation at the warp level can hide latency and make memory access more regular, which aligns with reducing transactions and making better use of limited load slots. Related excerpts describe hardware and compiler support for warp-oriented queues that allow streaming and gather patterns, effectively enabling more coherent memory access across threads and reducing latency overhead. Further, an example of optimizing memory bandwidth via warp specialization demonstrates tangible speedups, reinforcing the notion that these strategies translate into actual efficiency gains in memory-bound workloads. Additional material on warp specialization and producer-consumer synchronization highlights synchronization primitives that enable coordinated access across warps, which can help maintain temporal locality and reduce random scattering in memory access. Semantic links between these excerpts and the finegrained field value include: (a) overlapping memory access with computation to hide latency, (b) using warp-level constructs (queues, specialized warps) to coordinate and coalesce access, (c) transforming irregular access patterns into more regular, batched-like accesses through scheduling and hardware support, and (d) evidence of practical performance gains when applying warp-focused optimizations to memory-bound kernels. The surrounding discussions on decoupled architectures and general memory access planning provide context but are secondary to the specific warp-specialization and memory-coalescing approaches central to the field value.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.2.primary_benefit",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  }
		],
		"reasoning": "The finegrained field value highlights maximizing GPU throughput and utilization for irregular workloads by ensuring that completed work immediately yields new tasks, thereby preventing compute resources from idling and efficiently managing a large set of parallel walkers. Excerpts describing warp-level specialization and hardware-assisted address generation address the core needs of overlapping memory access with computation and enabling fine-grained scheduling to hide latency under irregular access patterns. In particular, the discussion of warp specialization and warp-level queues demonstrates how to overlap memory access and compute across threads within a warp, which directly supports keeping 256 independent walkers busy when their traversal paths diverge. Further, material on extracting throughput through specialized pipelines and memory bandwidth optimization provides concrete mechanisms to sustain high utilization when memory access is irregular and data-dependent. The decoupled access/execute related notes describe an architectural approach where memory access and computation are decoupled to execute asynchronously, which is a natural fit for guiding how to schedule work for irregular traversals and prevent stalls. Related discussions about memory bandwidth optimization techniques, such as hardware-assisted address generation and coalesced/overlapped memory access patterns, reinforce the core idea of maintaining throughput under irregular workloads. Collectively, these excerpts support the notion that targeted scheduling, overlap, and specialized memory access strategies are central to achieving high throughput for a large, irregular, walker-based traversal scenario. ",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.4",
		"citations": [
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  }
		],
		"reasoning": "The chosen excerpts collectively describe correlation-based and delta-based prefetching techniques that are designed to predict future memory addresses in the presence of irregular pointer-based access patterns. In particular, the DCPT family describes delta-correlating prediction tables that track deltas between miss addresses to forecast subsequent accesses, which directly aligns with the fields emphasis on learning and prefetching memory addresses in irregular traversal scenarios. The Global History Buffer (GHB) context is also discussed in conjunction with DCPT in these excerpts, illustrating a mechanism to store recent miss histories and use that history to drive prefetching decisions. These passages directly support the notion of using hardware-supported, pattern-learning prefetchers to hide memory latency in pointer-chasing traversal. Some excerpts also acknowledge caveats, such as DCPTs varying effectiveness depending on the regularity of delta patterns, which is relevant to evaluating suitability for a complete binary-tree traversal where dereferenced pointers and child selections may not exhibit strong repeatable delta sequences. Taken together, the strongest evidence comes from discussions that explicitly define DCPT and GHB-based correlation prefetching, their potential gains, and their deployment considerations in irregular memory-access workloads. The later excerpts broaden the claim with performance figures and hardware/software considerations, reinforcing the practical relevance while also signaling that benefits are workload-dependent.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.1",
		"citations": [
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load."
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  }
		],
		"reasoning": "The most relevant information directly describes the technique in question: using helper threads to perform prefetching by executing a precomputed slice of the main program (pre-execution) so that future delinquent loads are prefetched. This aligns with the field values description of a dedicated helper thread running ahead to compute load addresses and trigger prefetches, effectively hiding latency for irregular/pointer-chasing memory accesses. The cited excerpts explicitly state inline software prefetching with a dedicated helper thread, and the concept of pre-execution slices that produce addresses for delinquent loads. They also discuss that these slices can be backward slices or pre-execution code extracted to run on spare contexts, which matches the fields notion of speculative precomputation and runahead style techniques. Furthermore, background discussions cover the limitations and potential gains (e.g., memory stall reductions, need for spare contexts, and synchronization overhead), which support the fields suitability assessment and caveats. Additional excerpts address backward-slice construction, program slicing as a basis for pre-execution, and general prefetching strategies (jointly supporting the core idea of facing delinquent loads with speculative precomputation). Collectively, these excerpts provide direct, theory-backed linkage between the helper-thread speculative prefetching technique and its practical considerations, execution model, and reported efficacy.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.4.mechanism_summary",
		"citations": [
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp."
			]
		  }
		],
		"reasoning": "The most directly relevant material describes a per-thread traversal stack that stores node addresses rather than data, which aligns with the concept of helper threads accessing a shared traversal structure to assist the busy threads. This indicates an architectural pattern where auxiliary threads participate in the traversal, enabling shared work and reducing individual divergence. Instances where the warp scheduler compacts active threads to form fewer, more efficient warps illustrate how divergence and scheduling can be optimized at the hardware/warp level, which complements the idea of helper threads being used to assist busy threads within a warp. The discussion of combining CoopRT with a prefetcher highlights the feasibility and trade-offs of adding memory-system assistance to sustain memory throughput under cooperative execution. Mentions of coalescing addresses to remove redundant cache/memory accesses demonstrate an explicit mechanism for coalescing memory requests across multiple threads in a warp, which directly supports the proposed memory-scheduling coalescence of node fetches from several threads. References to active thread compaction and handling of divergence further support the notion that coordinating threads within a warp can improve throughput when many walkers diverge early, providing a concrete hardware-oriented approach to the target optimization. In sum, these excerpts collectively validate the feasibility of using helper threads and warp-level coordination, along with memory-coalescing strategies, to optimize irregular, data-dependent tree traversal.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.5",
		"citations": [
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems."
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  }
		],
		"reasoning": "The most directly relevant material explicitly discusses content-directed prefetching and its variants. Excerpt describing content-directed prefetching as a technique that monitors cache blocks and identifies candidate addresses to prefetch within those blocks, using a virtual address predictor and potentially applying to pointer-based traversals, aligns closely with the fine-grained field value about scanning cache-line contents for pointers and prefetching along chains. Excerpts that detail a stateless variant of content-directed prefetching, including its lack of extra state and its targeting of likely addresses within cache lines, map precisely to the requested mechanism. Additional excerpts describe related content-aware and compiler-assisted approaches, which, while not identical to CDP, provide important context about how content information can drive prefetching in dynamic or pointer-intensive workloads, and they discuss performance outcomes and design considerations that inform the caveats and effectiveness of such approaches. Collectively, these excerpts establish that content-directed or content-aware prefetching can be stateless or lightly stateful, operates by inspecting data being fetched to discover addresses, and can yield measurable speedups (with caveats such as mispredictions or pollution). The strongest support comes from direct CDP-focused descriptions, followed by explicit mentions of content-directed prefetching mechanisms and their performance implications, with compiler-directed variants offering complementary insight into practical implementations and potential benefits.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.3",
		"citations": [
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown"
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system."
			]
		  }
		],
		"reasoning": "Jump-Pointer Prefetching is central to improving traversal of linked structures by prefetching far-ahead addresses via dedicated jump pointers. The most directly relevant passages describe the core idea: prefetching addresses by using jump pointers that skip intermediate nodes, enabling prefetch engines to reach distant descendants ahead of the main thread. This directly addresses the primary bottleneck in the users scenariothe irregular, data-dependent memory accesses caused by pointer-chasing in a binary tree with many walkers diverging from the root. The excerpts explicitly state that jump pointers provide direct access to nodes further ahead and that prefetching using these pointers can be implemented in software, hardware, or in a cooperative fashion. They also quantify benefits observed in prior work (e.g., reductions in memory stall and speedups with different implementations) and discuss caveats such as overhead for maintaining jump pointers. Additional excerpts connect these ideas to related prefetching concepts (e.g., backward slices and pre-execution) to illustrate how JPP can integrate with broader memory-hiding strategies, though they are secondary to the core JPP mechanism for this question. The cited content collectively supports: (a) the existence and mechanism of Jump-Pointer Prefetching; (b) its applicability to pointer-chasing traversals in trees; (c) the feasibility of software/hardware/cooperative implementations; and (d) empirical gains and caveats, consistent with the field value description.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.4.technique",
		"citations": [
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads.",
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline."
			]
		  }
		],
		"reasoning": "The target field value corresponds to a specific technique: Cooperative BVH Traversal (CoopRT). Excerpts that explicitly describe CoopRT and its core ideascooperative threads traversing a BVH, warp-level scheduling decisions, and the associated data structures for per-thread traversaldirectly validate the existence and characteristics of CoopRT as discussed in the literature. The strongest connections are found in passages that state: (a) the traversal is performed cooperatively by threads, aiming to remove redundant memory accesses through coalescing addresses and sharing work within a warp; (b) the RT unit operates at warp granularity with a warp scheduler selecting non-stalling warps, implying coordinated traversal across threads; (c) changes to the per-thread data path, such as storing traversal stacks of addresses rather than data, support the cooperative model; (d) CoopRT can be combined with prefetchers, balancing increased parallelism with bandwidth considerations; (e) performance evaluations show speedups in comparison to baseline ray tracing units, illustrating practical benefits of the cooperative approach. Additional excerpts discuss auxiliary mechanisms like a Load Balancing Unit, active thread compaction to reduce divergence, and integration with policy choices at the warp levelall of which reinforce the CoopRT framework and its implementation details. These elements collectively confirm the application of CoopRT-style cooperative BVH traversal as the targeted technique and illustrate its architectural and performance implications.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.3.platform_realization",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. "
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  }
		],
		"reasoning": "The most relevant excerpts discuss practical GPU-based techniques that address irregular memory access and divergent traversal through architecture and compiler support for warp specialization, which aligns with the described need to coordinate many walkers by locality before traversal. In particular, there are explicit mentions of warp specialization enabling overlapping memory access and compute, which directly supports the idea of organizing work to improve coalescing and locality. There is also discussion of hardware queueing and register-file level buffering that can map well to batching and scheduling traversal work, providing mechanisms to realize batched memory accesses and reduce scattered reads. Additional excerpts describe capturing and exploiting gather or streaming access patterns, which echoes the concept of sorting or grouping walkers by destination or locality prior to kernel launch. Finally, related material on batched or partitioned computations with warp-oriented strategies showcases concrete approaches to handle divergent paths and irregular memory patterns, paralleling the requested singleflight-like or batched lookups concept in the field value. Direct alignment is seen in the emphasis on warp specialization to overlap memory access with computation, and in the portrayal of hardware/compiler support to enable such scheduling and batching without increasing bandwidth demands beyond the processors capabilities. The cited materials collectively illustrate how to realize the described batching/scheduling techniques at the platform level, particularly on GPU architectures with warps and shared memory resources.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.4.primary_benefit",
		"citations": [
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself."
			]
		  }
		],
		"reasoning": "The most relevant passages describe explicitly compacting active threads across warps to form fewer, more efficient execution groups, which directly addresses reclaiming compute from idle threads to accelerate active ones. One excerpt states that at the beginning of each ray bounce, active threads across multiple warps are compacted to form fewer but more efficient warps, which maps to reclaiming idle compute to boost progress of active threads. Another excerpt notes that active thread compaction is used to mitigate divergence in path-tracing, again aligning with reclaiming idle compute power to speed up work. Additional passages discuss warp-level scheduling that selects non-stalling warps, which complements the idea of maintaining high SIMD/SIMT efficiency by avoiding stalls caused by divergence. Supporting context about avoiding redundant memory accesses through coalescing and related per-thread structures also reinforces the theme of coordinating threads to maintain higher throughput when traversal paths diverge. Together, these excerpts collectively support the notion that deliberate thread coordination/compaction strategies are used to counteract low SIMT efficiency due to divergence by reallocating or concentrating active work.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.2.model",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  }
		],
		"reasoning": "The most relevant material discusses the use of warp-level queues and the overlap of memory access with computation to achieve higher throughput. Specifically, discussions about warp-level register file queues enabling fine-grained streaming and hardware-assisted address generation describe a mechanism to keep many threads busy by reusing a structured, queue-driven flow of work and data, which aligns with the notion of persistent threads or work queues that continuously feed units of work to be processed. The emphasis on warp specialization and overlapping memory access with compute reinforces the concept of maintaining a pool of work units (threads) that can be scheduled and re-used to mitigate memory bandwidth bottlenecks. Additional material on producer-consumer synchronization and barriers provides concrete synchronization primitives that would support a persistent-work-queue model, where workers consume tasks from a queue and synchronize as needed. Other excerpts detailing decoupled access/execute architectures emphasize separation of memory access and compute streams, which can inspire batching and long-lived worker threads that switch between tasks via a shared scheduling mechanism. Overall, the strongest support comes from the excerpts that explicitly describe queue-based, warp-level orchestration and overlapping execution with memory access, which map directly to the idea of persistent threads and work queues as a scheduling and execution model. The remaining excerpts provide complementary context about broader execution/architecture models and synchronization that underpin such a design.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.2.core_concept",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  }
		],
		"reasoning": "The core value describes a GPU-focused execution model where a large, fixed grid of threads (persistent threads) repeatedly pulls tasks from a shared work queue using atomic operations, thereby bypassing the hardware scheduler to better accommodate workloads with variable task durations. Directly supporting this idea, the notion of warp specialization emphasizes overlapping memory access with compute by organizing execution along warp boundaries, enabling finer-grained scheduling and reduced divergence when multiple threads contend for memory. This aligns with the goal of sustaining throughput under highly variable workloads by coordinating how memory access patterns are issued and how many warps can advance in tandem. Additional material on warp-based memory bandwidth optimization (including specialized memory-access coordination via warp-level constructs) reinforces the possibility of achieving higher effective bandwidth and better latency hiding for irregular access patterns. The cited discussions of synchronization primitives and warp-specialized kernels further illustrate architectural mechanisms to coordinate many lightweight threads (warps) while maintaining efficient progress, which is essential when 256 or more workers share a global work queue and rely on atomic task fetches. Finally, references to hardware-assisted address generation and the broader concept of warp specialization for overlapping compute and memory operations provide concrete avenues to implement a persistent-thread, task-fetching model on GPUs with limited memory bandwidth. Collectively, these excerpts support the idea that a persistent, batched, warp-aware execution strategy can improve throughput and masking of irregular memory accesses in a GPU setting.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.1.mechanism_summary",
		"citations": [
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The fine-grained field value specifies a mechanism where walkers are initially grouped into a packet (such as a 32-thread warp) and traverse the tree in lockstep, with active walker count tracked via warp-level primitives. This directly corresponds to the idea of packet traversal where a group follows the same path and SIMD efficiency is maximized by maintaining coherence within the packet, with a switch to independent traversal if the active count falls below a threshold. Supporting content describes: packet traversal as a way to share traversal state and keep walkers aligned, and the notion that divergent traversal paths reduce SIMD efficiency unless mechanisms such as dynamic regrouping or compaction are employed to restore coherence. Several excerpts discuss the concept of performing traversal on groups of rays/warps with shared stacks or shared control flow, illustrating how packet-based approaches improve throughput and how dereferencing or divergence can be mitigated by regrouping threads or compacting warps. These sources also cover related techniques for handling divergence and reconvergence in SIMT/SIMD environments, which align with the need to switch to single-walker traversal when parallelism within a packet becomes unproductive. In sum, the most directly relevant content describes packet-based traversal, retention of SIMD coherence within groups, and strategies for handling divergence or reduced activity, all of which map to the described mechanism of monitoring active walkers and transitioning to independent traversal when needed. The remaining sources provide broader context on SIMD traversal, warp formation, and reconvergence strategies that support the rationale for such a mechanism but are less directly tied to the exact packet-to-single-walker switch behavior.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.6",
		"citations": [
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18]."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches)."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68].",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache."
			]
		  }
		],
		"reasoning": "The most directly relevant information describes prescient/direct pre-execution where a spare thread executes code ahead of the main thread to prefetch data, which is a form of runahead-like latency hiding. This explains how pre-execution slices are constructed (backward slices) to reproduce necessary computations for next accesses, enabling prefetches to be issued ahead of the actual demand. The concept of backward slices shows how a pre-execution thread can derive the memory addresses for delinquent loads by re-executing the relevant portion of the computational dependency chain, which directly maps to generating prefetch addresses for irregular memory accesses in a batched tree-walker scenario. The backward-slice mechanism is described as extracting the necessary set of preceding instructions that determine a future loads address, which is exactly the kind of calculation required to seed prefetches for subsequent tree nodes in a divergent traversal. The runahead/head-start ideas illustrate dynamic techniques to push prefetching forward in time by starting the auxiliary thread early and keeping it ahead, which can substantially improve throughput when memory latency dominates, albeit with energy and wasted-work costs. The discussion of prefetching architectures that combine inline pre-execution with runahead slices further clarifies practical implementation considerations, including how to time the prefetches and how to balance main-thread progress with the auxiliary threads computation. Additional excerpts describe the broader family of speculative precomputation approaches (long-range prefetching, backward slices for branches and loads, and the use of p-slices) and how they can be coordinated with hardware features (e.g., jump-start distances, runahead buffers) to hide latency. Taken together, these excerpts support the field values core claim that runahead-style speculative pre-execution is a viable latency-hiding mechanism for irregular pointer-chasing workloads, albeit with caveats about energy efficiency and potential waste, and with practical guidance on slice construction and scheduling. The strongest connections are to the concrete description of direct pre-execution and backward slices, followed by descriptions of p-slice construction and runahead/jump-start concepts, and then by broader notes on runahead-related architectures and performance outcomes.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.2.platform_realization",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Synchronizes the two upon control flow instructions (using branch queues)"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  }
		],
		"reasoning": "The most relevant content directly discusses warp-level strategies and coalescing or overlapping memory access to improve throughput in GPU pipelines. One excerpt describes warp specialization as a powerful technique for overlapping memory access and compute to achieve better GPU performance, which directly maps to the idea of using warp-level primitives to coalesce or coordinate atomic work-claim operations across many threads. Another excerpt highlights warp-level register file queues and hardware-accelerated address generation that enable fine-grained streaming and patterned memory access, which can support efficient batching and scheduling of work items in a parallel walker scenario. Additional content references CudaDMA-style memory bandwidth improvements via warp specialization, reinforcing the notion that warp-aware layouts and scheduling can reduce contention and improve throughput. There is also material on synchronization mechanisms and producer-consumer barriers in GPUs, which aligns with the concept of coordinating atomic work-claim operations across a warp family. Other excerpts discuss decoupled access/execute and related scheduling concepts, which, while not about atomics per se, provide broader context on overlapping memory and compute streams and can inform how a work-claim counter could be implemented or optimized in a GPU setting. Together, these excerpts support the field value by illustrating concrete warp-centric approaches to coalescing work items, coordinating atomic work-claim patterns, and leveraging memory scheduling to minimize contention in highly parallel, data-dependent traversals.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.4.reported_performance_gain",
		"citations": [
		  {
			"title": "CoopRT: Accelerating BVH Traversal for Ray Tracing via Cooperative Threads | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731118",
			"excerpts": [
			  "We evaluated CoopRT in Vulkan-sim, a cycle-level simulator, and observed up to 5.11x speedup over the baseline, with a geometric mean of 2.15x speedup at the cost of a moderate area overhead of 3.0% of the warp buffer in the RT unit. Using the energy-delay product, our CoopRT achieves an average of 2.29x improvement over the baseline.",
			  "The node addresses from the TOSes of these threads are coalesced to remove redundant cache or memory accesses.",
			  "Aila et al. [ [9]() ] implement a GPU ray tracer to assess the performance and bottlenecks of ray traversal on GPUs. They explore replacing early terminated rays with new ones, wider BVH trees, and work queues to improve SIMD efficiency.",
			  "the Load Balancing Unit (LBU) looks for a thread that needs help, and another thread that can offer help within the scheduled warp.",
			  "To support our cooperative BVH traversal, we modify the warp buffer and the accompanying logic in the RT unit. Fig. [7]() shows the high level block diagram of our proposed implementation, with the added per-thread structures highlighted using red and added per-RT unit structures highlighted using purple. At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "CoopRT can be combined with a prefetcher, such as the Treelet prefetcher, although the benefits would need more careful consideration. The reason is that CoopRT increases parallelism and may saturate the memory bandwidth. In this case, the bandwidth left for prefetching would be limited.",
			  "At the beginning of each ray bounce, active threads across multiple warps are compacted together to form fewer but more efficient warps, which is similar to the idea in [ [21]() ].",
			  "Due to its parallel nature, ray tracing has been implemented and studied on GPUs.",
			  "At every cycle, the warp scheduler in the RT unit picks a non-stalling warp from the warp buffers.",
			  "Therefore, it may address the inactive thread problem to some degree (as it needs to compact different numbers of TBs or warps for each bounce), but not early finishing threads.",
			  "imulate CoopRT across 13 scenes in Lumibench [ [35]() ], and show that CoopRT achieves up to 5.11x speedup, with an average of 2.15x compared to the baseline RT unit",
			  "The RT unit can be viewed as a specialized execution lane operating at warp granularity.",
			  "The per-thread traversal stack stores the addresses of the nodes instead of the node data itself.",
			  "Wald [ [42]() ] proposes active thread compaction to mitigate divergence in PT."
			]
		  }
		],
		"reasoning": "The most directly relevant excerpt explicitly states the claimed performance results: it reports achieving up to a 5.11x speedup over the baseline traversal, with a geometric mean of 2.15x, and notes a moderate hardware area overhead of 3.0% of the warp buffer. This directly corroborates the target field value. Additional excerpts provide context on how cooperative threading and architectural adjustments affect performance and memory behavior, which helps explain where those gains may come from: cooperative BVH traversal and per-thread scheduling can improve SIMD efficiency and reduce divergence; supporting infrastructure like warp buffers and per-thread structures are described as part of the implementation. Other excerpts discuss potential integration with prefetchers, memory coalescing, and thread compaction, which relate to the same optimization problem (scattered access, divergence, and memory bandwidth) and help situate the performance results within a broader optimization strategy. Finally, some excerpts mention the general context of GPU ray tracing traversal, which reinforces the relevance of memory access patterns and warp-level execution models to achieving speedups in practice.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.1.primary_benefit",
		"citations": [
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot"
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o"
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  }
		],
		"reasoning": "The target field value describes an adaptive traversal strategy that leverages coherence when many walkers share the same path and gracefully handles divergence as the workload becomes irregular, effectively blending packet-like efficiency with per-walker traversal. The most directly relevant excerpts discuss megakernel concepts and coherence-aware scheduling on GPUs, which explicitly address active thread compaction and dynamic regrouping of work to maintain SIMD efficiency amid divergent execution. For example, discussions on megakernels analyze how aggregating computation into larger, more coherent execution units can significantly boost SIMD utilization in ray/path tracing scenarios where coherence varies over time. This directly supports the idea of dynamically adapting traversal strategy to preserve throughput as coherence changes across rounds. Additional excerpts describe packet traversal where a group of rays share a traversal path, highlighting the coherence advantage in early rounds, and the use of shared data structures or re-grouping (warps) to reduce divergence later. Together, these sources provide concrete backing for an approach that starts with coherent, packet-like traversal and transitions to more flexible, single-walker or regrouped-warp traversal as divergence grows. The cited works on dynamic warp formation and thread block compaction further illustrate concrete mechanisms to achieve the same end: maintain high SIMD efficiency by batching or reorganizing work to exploit temporal locality and minimize divergence, which aligns with the proposed finegrained field value.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.4.model",
		"citations": [
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  }
		],
		"reasoning": "The fine-grained field value describes Dynamic Warp Formation and Thread Block Compaction, i.e., techniques that group or reorganize execution units (warps or threads) dynamically to improve throughput on irregular or divergent traversal patterns. Excerpts discussing warp specialization demonstrate the core idea of partitioning computations into sub-computations assigned to different warps, which directly supports dynamic grouping of work and coherent scheduling to mitigate divergence and irregular memory access. The excerpts also describe how warp-specialized code can be emitted by a compiler and how such specialization improves performance by aligning computation with memory access patterns, which underpins the concept of forming warps dynamically to handle irregular traversals. Further, references to hardware and compiler support for warp specialization, including streaming patterns, gather-like memory access, and address generation, illustrate concrete mechanisms enabling dynamic warp formation and potential thread block compaction. Mention of producer-consumer synchronization across warps adds a realistic scheduling primitive to compact execution blocks and coordinate their progress, aligning with the notion of thread block compaction in a dynamic, warp-aware execution model. Collectively, these excerpts map closely to a research focus on warp-aware scheduling strategies that form or compact execution units to sustain high throughput under irregular, data-dependent traversal paths, especially when many parallel workers start from a common root and diverge over time, which is central to dynamic warp formation and thread block compaction.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.1.core_concept",
		"citations": [
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ."
			]
		  }
		],
		"reasoning": "The proposed finegrained field value advocates breaking a monolithic traversal kernel into multiple specialized kernels, and organizing work via queues so each kernel handles a single task (e.g., node intersection, hash computation). It also emphasizes exploiting warp specialization and scheduling to cope with irregular traversal paths. Content describing partitioning computations using warp specialization explains how heterogeneous, task-focused components can be used to address irregular access patterns and divergent control flow, which directly supports the concept of decomposing work into specialized kernels and using scheduling to manage progress. Similarly, highlights about warp specialization enabling efficient handling of irregularity reinforce the strategy of distributing tasks across kernels to maintain throughput even when paths diverge. Additional references to GPU pipelines with warp-level organization and hardware-assisted address generation illustrate concrete mechanisms by which specialized kernels and scheduling can overlap memory access with computation, aligning with the goal of reducing monolithic, divergent traversal into coordinated, small units of work. Discussions of decoupled access/execute architectures and the separation of computing into distinct functional units further corroborate the architectural principle of decomposing the traversal into time- or space-partitioned stages, each potentially running in its own kernel or stage, which matches the described approach of task-specific kernels and work queues.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.4.primary_benefit",
		"citations": [
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware.",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.",
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  }
		],
		"reasoning": "The most relevant passages directly address warp specialization and its ability to handle irregular data access and divergence, which aligns with restoring SIMD efficiency after branches. For example, a passage states that the warp-specialized code emitted is significantly faster than prior data-parallel GPU kernels, illustrating tangible performance gains when divergence is present. Additional passages describe partitioning computations using warp specialization to deal with irregularity in both data access patterns and computation, underscoring how this technique can maintain SIMD efficiency amid divergent paths. Other excerpts describe enabling fine-grained streaming, gather memory access patterns, and fitting large working sets into on-chip memory through warp-level mechanisms, all of which provide mechanisms by which divergence can be mitigated and SIMD utilization improved. The remaining excerpts extend these ideas to related hardware/software implementations (WASP, CUDA/warp specialization literature) and quantify improvements in specific contexts, reinforcing the general claim that warp-specialization and related techniques can restore or improve SIMD efficiency in the presence of irregular traversal or divergence. Taken together, these excerpts support the claim that analogous techniques could automatically restore SIMD efficiency after divergent branches, contributing to higher hardware utilization, with evidence of substantial performance benefits in practice. The reasoning rests on connecting warp specialization to improved handling of irregular accesses and divergent control flow, which is the core mechanism behind restoring SIMD efficiency and boosting throughput in the described traversal scenarios.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.0.technique",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  }
		],
		"reasoning": "The most relevant passages explicitly discuss running long-lived, persistent threads that pull work from a global pool or queue, which is the core idea behind persistent threads and work queues for GPU traversal workloads. One excerpt states that long-running persistent threads can fetch work from a global pool using an atomic counter until the pool is empty, directly illustrating the work-queue concept in a GPU context. Another excerpt reiterates the use of persistent threads and notes that this design aims to keep the GPU busy by distributing work through a central pool. Additional excerpts describe variants labeled as persistent trace or persistent while-while traversals, highlighting that persistence and batched/queued work are central to achieving higher throughput, which further reinforces the identification of the technique. Supporting context about performance benefits and implementation details (e.g., persistence variants and the close relationship to tracing/ray traversal) corroborate that the discussed technique corresponds to persistent threads and work queues as a strategy for irregular traversal workloads. Overall, these excerpts together provide direct evidence for the technique name and its GPU-oriented application, with no conflicting information present.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.3.core_concept",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. "
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  }
		],
		"reasoning": "The field value describes a software layer that collects and groups memory requests from many walkers that would otherwise access different nodes, and then schedules them to coalesce into fewer physical memory transactions. This maps to the idea of exploiting warp specialization to overlap memory access with computation, enabling batched or coalesced access patterns even in irregular traversal. Excerpts that discuss warp specialization as a mechanism to overlap memory access and compute, and to reorganize how memory requests are issued (for example by mapping queues or using specialized hardware support for streaming) directly support the concept of grouping requests to improve bandwidth efficiency. Related content that mentions handling irregular access patterns through specialization or circular buffers further reinforces the notion of batching access to shared structures (like a tree) to reduce memory traffic. Additional excerpts touching on synchronization or producer-consumer barriers indicate practical requirements when coordinating grouped requests across warps, which is a natural consequence of a software layer that coalesces memory accesses. In this sense, the strongest support comes from discussions of warp specialization enabling overlapped and batched memory operations, followed by papers that articulate hardware/software techniques for streaming, grouped or coalesced accesses, and the broader context of coordinating such activity across multiple threads/wars for higher throughput. The remaining excerpts provide supporting context about memory access patterns and scheduling but do not address coalescing/grouping of requests as directly.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.3.primary_benefit",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The targeted fine-grained field value claims that using treelets and treelet-based batching improves cache locality and reduces memory bandwidth by amortizing treelet loads across many walkers, which is particularly beneficial in memory-bandwidth-bound scenarios. Excerpts that explicitly discuss prefetching BVH/treelets in treelet granularity to the GPU cache show direct mechanisms for bringing multiple nodes into cache together, enabling memory reuse across walkers. Excerpts describing batching rays that visit the same treelet demonstrate grouping traversal work to reduce divergent or scattered memory access, which aligns with amortizing loads and improving locality. Several excerpts discuss the treelet concept as a way to queue or batch work for rays or traversals, and to prefetch or group connected BVH nodes to maximize cache locality and memory reuse. Taken together, these excerpts provide direct support for: (a) organizing traversal into treelets to enable coarser-grained memory access, (b) prefetching or loading treelet data to the cache ahead of time, and (c) using batching to reduce scattered access and improve bandwidth efficiency in a parallel walker scenario.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.1.reported_performance_gain",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program"
			]
		  },
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot",
			  "l stack-max traversal\nmethod. In this method packet traversal ends when the\nshared stack size is bigger than a predefined threshold. In\nthis moment if-if traversal starts from the last visited node\nand later on visits each of the nodes on the shared stack."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  },
		  {
			"title": "Dynamic Warp Formation and Scheduling for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.micro2007.pdf",
			"excerpts": [
			  "branch outcomes for different processing*\n*elements signi* *fi* *cantly degrades performance. In this paper,*\n*we explore mechanisms for more ef* *fi* *cient SIMD branch ex-*\n*ecution on GPUs. We show that a realistic hardware im-*\n*plementation that dynamically regroups threads into new*\n*warps on the* *fl* *y following the occurrence of diverging*\n*branch outcomes improves performance by an average of*\n*20.7% for an estimated area increase o"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed"
			]
		  }
		],
		"reasoning": "The strongest support for the stated performance claim comes from passages that quantify speedups when using traversal optimizations. For example, one source reports that a packet traversal approach yielded 1.5 to 2.2 times faster performance, approaching the theoretical upper bound for multiple ray types; another passage notes that a persistent variant of the traversal can be faster than packet traversal in all cases, with notable gains in certain ray types. These details directly align with the idea that combining packet-based traversal or optimized, batched traversal strategies can deliver up to a 2x speedup compared to purely per-ray approaches. Additional excerpts describe how aligning 256 diverse work items through coherence- and scheduling-aware techniques (dynamic warp formation, SIMT reconvergence strategies, and thread compaction) improves utilization and reduces divergence penalties; these mechanisms explain why the potential speedups materialize in practice when memory access patterns are batched or coerced into coherent paths. There are also explicit mentions of coherence improvements yielding substantial throughputs (e.g., measurable speedups over baseline per-warp implementations), which reinforces the underlying mechanism that supports large performance gains when transitioning from highly divergent, irregular access to more coherent, batched traversal. While not all excerpts quantify an Embree-specific improvement (the field value mentions Embree as a framework that could see gains), the reported 32%speedup ranges and the documented 2x-like improvements in packet- vs. per-ray traversal provide a coherent narrative for why the target field value can hold under realistic workloads and GPU architectures. In sum, the excerpts collectively support a best-case order-of-magnitude improvement through packetized or batched traversal strategies, aided by coherence and warp-management techniques, and also describe the conditions under which the gains are most pronounced. The combination of direct speedup numbers and the described structural techniques forms a plausible bridge to the fine-grained field value, while noting that Embree-specific 50% gains are not directly evidenced in these excerpts. ",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.0.primary_benefit",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  }
		],
		"reasoning": "The target fine-grained field value describes a mechanism where, once a thread or warp becomes idle, it immediately pulls new work to sustain occupancy and mitigate divergence-related slowdowns. Excerpts describing persistent thread strategies explicitly articulate launching a limited set of workers that continually fetch new tasks from a global pool using an atomic counter until exhausted. This directly maps to the idea of maintaining high compute occupancy by keeping the work pipeline full even when traversal paths diverge. Additional excerpts discuss persistent variants of traversal that achieve significant throughput improvements and note that memory bandwidth does not become the primary bottleneck under such a scheme, reinforcing that active work scheduling and immediate re-dispatch are effective for sustaining throughput. Collectively, these passages support the concept that maintaining occupancy through continuous work-stealing or reallocation is a viable approach to address irregular traversal and control-flow divergence on GPUs. The discussion of speedups and bandwidth considerations further corroborates the practical benefits and limitations of such a scheduling strategy in real GPU workloads, aligning with the described technique of preventing idle cycles by promptly sourcing new work.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.4.core_concept",
		"citations": [
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  }
		],
		"reasoning": "The most relevant excerpts describe warp specialization as a mechanism to partition work and deal with irregular data access by assigning sub-computations to different warps, which directly supports the notion of dynamically regrouping threads into coherent execution units after divergence. They also discuss architecture and compiler techniques necessary to construct warp-specializing execution models, which aligns with a hardware-level execution model that manages thread grouping. Additional excerpts mention that warp-specialized partitioning can fit large working sets in on-chip memories and use synchronization constructs like producer-consumer barriers, which are integral to maintaining coherence when threads are regrouped. Together, these excerpts support the idea of dynamically managing thread groups to recover coherence post-divergence and enabling robust hardware-level execution models such as thread block compaction, even though the latter term may be described in related literature. The presence of hardware-oriented papers (WASP) and compiler/architecture discussions further reinforces the feasibility of such models in practice and their relevance to high-throughput, irregular-access workloads.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.1.technique",
		"citations": [
		  {
			"title": "Traversal methods for GPU ray tracing - cescg",
			"url": "https://old.cescg.org/CESCG-2010/papers/BrnoMU-Vinkler-Marek.pdf",
			"excerpts": [
			  "Packet traversal**\nThe basic characteristic of packet traversal is that a group\nof rays follows exactly the same path in the BVH tree. This\nis achieved by sharing the traversal stack among the rays\nin the packet. Each time the rays want to decide which\nnode to traverse next they have to vot"
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  },
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://www.highperformancegraphics.org/previous/www_2009/presentations/aila-understanding.pdf",
			"excerpts": [
			  "**1. Replacing terminated rays**\nThreads with terminated rays are idle until warp\nterminates\nReplace terminated rays with new ones\nLess coherent execution & memory accesses\nRemember: per-ray kernels beat packets\nCurrently helps in some cases, usually not\nWith ENUM & POPC, +20% possible in ambient\nocclusion and diffuse, simulator says\nIff not limited by memory speed"
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels considered harmful: Wavefront path tracing on GPUs",
			"url": "https://www.researchgate.net/publication/262312981_Megakernels_considered_harmful_Wavefront_path_tracing_on_GPUs",
			"excerpts": [
			  "We study this problem and define a new measure called heterogeneous efficiency to measure SIMD divergence among multiple shaders of different complexities in a ray tracing application. We devise seven different algorithms for scheduling shaders onto SIMD processors to avoid divergence.",
			  "The GPU leverages SIMD efficiency when shading because it ras- terizes a triangle at a time, running the same shader on all of its fragments. Ray tracing sacrifices this shader coherence, and the result is that SIMD units often must run different shaders simul- taneously resulting in serialization."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The fine-grained field value describes a traversal technique that blends packet-based traversal with single-walker progression, aiming to keep SIMD efficiency while accommodating divergent paths. Excerpts that explicitly discuss packet traversal as a method to share work among multiple rays and reduce divergence are highly relevant, since a hybrid approach would naturally borrow the idea of grouping work to exploit spatial or temporal locality in the early, similar-path stages and then transition to more granular, per-walker progress as divergence increases. The material that shows packet traversals benefits, including shared traversal stacks and the trade-offs between packet-based and per-ray approaches, directly supports the feasibility and rationale for a hybrid approach. Additional context on memory bandwidth considerations, persistence of traversal variants, and reconvergence strategies further supports how a hybrid method could be designed to maintain high throughput under irregular access patterns. Related work on dynamic warp formation and thread compaction provides additional mechanisms to manage divergence and maintain efficiency when walkers branch, which complements a hybrid traversal design. Overall, the most relevant material presents concrete evidence for the benefits of packet-based sharing in traversal and how divergence and memory access patterns impact performance, which is the core motivation for a Hybrid Packet/Single-Walker Traversal. Partial evidence about how different traversal strategies perform under memory-bound conditions and how to reuse work across threads lends additional, though indirect, support for a hybrid approach.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.3.technique",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The strongest support for Treelet Scheduling comes from passages that explicitly mention scheduling-related mechanisms in the context of treelets. One excerpt describes a scheduler being used (PMR scheduler) in conjunction with treelet traversal, highlighting how scheduling decisions are used to coordinate work and improve performance, which directly aligns with the concept of scheduling treelets to manage divergent traversal efficiently. Related excerpts discuss the core idea of treelets, small subtrees formed to increase memory locality and batch work, and how prefetching and queuing treelets can enable processing rays or walkers that visit the same treelet together, thereby reducing irregular memory accesses. Additional passages describe forming treelets by grouping connected nodes to maximize their size and linking the treelet concept to cache-friendly, batched traversal, which provides further justification for scheduling those treelets to optimize throughput. Some excerpts also note that treelet prefetching can decouple traversal from immediate child visits and prefetch in treelet granularity, reinforcing how a scheduling strategy can batch work across multiple walkers starting at the root and diverging, leveraging temporal locality in the early rounds. Collectively, these excerpts support the notion of organizing traversal work around treelets and incorporating scheduling to improve memory access patterns and performance in SIMD or GPU contexts.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.0.mechanism_summary",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  }
		],
		"reasoning": "The finegrained field value describes using a single, large grid of persistent threads that repeatedly fetch new tasks from a global work queue using atomic operations, with the goal of bypassing the hardware scheduler in workloads with divergent paths. Excerpts that discuss launching enough threads to fill the machine and then having long-running persistent threads fetch work from a global pool via an atomic counter directly illustrate this mechanism, showing how persistent workers can be used to keep the GPU busy while dynamically pulling work. Additional excerpts reinforce the concept by explicitly calling out persistent variants of traversal and the idea that memory bandwidth constraints can be mitigated or approached through such persistent designs, which aligns with the goal of maintaining throughput under irregular, data-dependent traversal paths. Further excerpts discuss the broader implications and performance benefits of persistent threading in similar traversal or packet-based contexts, supporting the overarching idea of a persistent, task-fetching worker model. Collectively, these excerpts substantiate the core mechanism of a large grid of persistent threads feeding a global work queue with atomic coordination to improve utilization in irregular, divergent traversal workloads. ",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.2.mechanism_summary",
		"citations": [
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The finegrained field value describes a mechanism where, after a traversal step, a compaction process identifies the currently active walkers across divergent warps and then regroups them into new, fully-packed warps for the next execution stage, with an implementation possibility via software or hardware like Thread Block Compaction. The most directly supporting excerpts explain dynamic warp formation and the core idea of reconciling divergent SIMD paths by regrouping threads into new warps, thereby maintaining SIMD efficiency during control-flow divergence. These excerpts also articulate how compaction within a block can share state or reconverge to a coherent warp grouping, which maps directly to the concept of collecting active walkers and repacking them for the next traversal step. Additional excerpts discuss the implementation and observed performance benefits of thread block compaction and similar active-thread strategies in SIMT/GPU contexts, including explicit references to compacting divergent threads, sharing a reconvergence mechanism, and achieving measurable speedups. The combination of these sources corroborates the notion of actively managing SIMD lane utilization by identifying active participants after each step and regrouping them for efficient subsequent computation, consistent with the described mechanism summary that mentions both software- and hardware-based TBC as viable paths.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.3.key_insight",
		"citations": [
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "**SIMD-Aware Batch Hash Tables.**",
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "Prefetching-Enabled Hash Tables.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  }
		],
		"reasoning": "The most relevant content directly describes SIMD-focused, batched hashing and vectorization, which align with applying modern hash-function work to many walkers processed in parallel. Excerpts that explicitly discuss SIMD-Aware Batch Hash Tables, which aim to parallelize hash computations across a batch, provide direct support for using a vertical/SIMD approach to hashing across multiple lanes. Descriptions of building a vector of operands to fit CPU registers, and the need for gather/scatter to access non-contiguous hash-table entries, map closely to the memory-access patterns of many walkers that diverge after a common root. References to prefetching-enabled hash tables and the benefits of batching to achieve effective prefetching further reinforce how a large batch of hash operations can be overlapped with memory latency. In addition, mentions of horizontal and vertical vectorization, and the general notion of SIMD in batch hash tables, provide context for achieving high throughput when processing many walkers in parallel. The material collectively supports the idea that a vertical/SIMD-fied hashing step for a large batch can dramatically reduce tail latency and improve throughput compared to serial processing, which is at the heart of optimizing the described 256 independent walkers traversing a binary tree with data-dependent indices.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.3.source_domain",
		"citations": [
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "**SIMD-Aware Batch Hash Tables.**",
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "Prefetching-Enabled Hash Tables.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables."
			]
		  }
		],
		"reasoning": "The core field value points to techniques for high-performance hash tables and SIMD algorithms. Excerpt 6 explicitly labels SIMD-Aware Batch Hash Tables, which directly aligns with the target domain. Excerpt 0 discusses batch hash tables in high-performance analytics, reinforcing the batch-processing aspect. Excerpt 7 describes constructing a vector of operands, a prerequisite for effectively leveraging SIMD in batch hashing. Excerpts 4 and 5 detail how to translate hashing into SIMD code and leverage gather/scatter operations to read/write in parallel, which are essential for SIMD throughput on irregular data structures. Excerpt 1, 2, and 3 further elaborate on horizontal/vertical vectorization and broader Vec-HT concepts, expanding on how vectorized hashing is implemented. Excerpt 8 introduces prefetching-enabled hash tables, addressing memory latency concerns, a key aspect of high-performance implementations. Excerpt 9 discusses batch operations and large hash tables, aligning with practical deployment concerns in SIMD hash-table contexts. Collectively, these excerpts map directly to the high-performance, SIMD-oriented hash table techniques requested and provide concrete signals ( batching, SIMD utilization, memory access patterns, prefetching, and vectorization strategies) that support the field value.",
		"confidence": "high"
	  },
	  {
		"field": "execution_and_scheduling_models.4.platform_realization",
		"citations": [
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  }
		],
		"reasoning": "The field value describes a hardware feature realization for GPUs that enables warp specialization and related hardware mechanisms to improve memory access patterns in parallel traversals. Excerpt about WASP introduces hardware and compiler support for warp specialization, explicitly aiming to overlap memory access and compute operations, which is a direct match to platform-level realization of such a feature. The follow-on excerpts further describe how warp-specialized hardware and compiler techniques yield performance gains and practical architectural details (e.g., warp-level queues, address generation, and producer-consumer synchronization) that would underpin a hardware realization of the feature. Other excerpts discuss the general benefits of warp specialization, partitioning computations, and improved throughput with warp-specific compilation strategies, reinforcing the notion that hardware support and its compiler/tooling are central to realizing the proposed feature. Taken together, these excerpts collectively support the claim that warp-specialization and its hardware/architectural support constitute a concrete realization path on GPUs, even though they do not reference Fung et al. directly. The strongest, most direct support comes from discussions of hardware-enabled warp specialization and its impact on memory access patterns, followed by empirical performance improvements and architectural notes, which together underpin the feasibility and design of such a hardware feature realization.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.4.technique",
		"citations": [
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract"
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  }
		],
		"reasoning": "The field value corresponds to a tree storage technique that groups child nodes contiguously to enhance locality and reduce memory traffic, sometimes by retaining only a minimal pointer (such as the first child) per node. Excerpts that directly state or imply storing all child nodes contiguously and reducing per-node pointers strongly support this claim. The strongest support comes from descriptions of CSB+-Trees, which explicitly describe contiguous storage of child nodes and keeping only the address of the first child, matching the notion of partial pointer elimination. Additional excerpts reiterate the cache-conscious motivation and the idea of a reduced pointer footprint (e.g., a single explicit child pointer), further aligning with the concept of pointer elimination to improve cache efficiency. Other excerpts discussing cache-conscious trees and related layouts provide contextual support but are slightly less direct about the exact contiguous storage and pointer-elimination mechanism, though still relevant to the overarching goal of reducing memory bandwidth and improving locality. Overall, the cited excerpts collectively corroborate the fine-grained field value by detailing a design where child nodes are stored contiguously and pointers are minimized to boost cache performance.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.3.technique",
		"citations": [
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "**SIMD-Aware Batch Hash Tables.**",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization."
			]
		  }
		],
		"reasoning": "The finest-grained value names a specific technique: Vectorized Hashing implemented via vertical vectorization. The most directly relevant content explicitly labels the approach as SIMD-aware and discusses how vertical vectorization translates hashing into SIMD code, including the need for gathers/scatters to access multiple hash-table entries in parallel. This aligns with the field value by identifying a concrete vectorization strategy used to accelerate batch hashing. Supporting material describes SIMD-aware batch hash tables and the use of vector operands that fit the register size, which is central to vertical vectorization. Additional excerpts describe the role of gather/scatter operations to enable in-parallel memory access across a hash table in a vectorized context, which is a natural enabler of vertical vectorization in practice. Other excerpts discuss the idea of a batch-vectorized hash table with prefetching and memory-access scheduling implications, which further grounds the technique within a practical, performance-oriented framework. Taken together, these excerpts provide a cohesive view: (a) the technique is referred to as vertical vectorization within batch hashing, (b) it relies on vector-aware data layouts and SIMD operations, and (c) it employs memory access primitives like gathers/scatters and prefetching to maintain throughput in irregular access patterns. This directly supports the field value as a named technique and situates it within the broader context of SIMD-friendly hashing and batched operations.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.0.reported_performance_gain",
		"citations": [
		  {
			"title": "Understanding the Efficiency of Ray Traversal on GPUs",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2009-08_Understanding-the-Efficiency/aila2009hpg_paper.pdf",
			"excerpts": [
			  "**3.4**\n**Persistent trace()**\nWe implemented persistent variants of the packet traversal and\nwhile-while. As shown in Table 1, the packet traversal got 1.52.2\ntimes faster, and its performance is now within 1020% of the the-\noretical upper bound for all ray types. One cannot reasonably hope\nto get much closer to the theoretical optimum because that would\nimply, among other things, optimal dual issue, complete absence of\nhardware resource conflicts, all memory access latencies to be hid-\nden, and all of our *>* 20K concurrent threads to terminate exactly at\nthe same time.",
			  "In order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty. As long as the atomic counter does not cause significant\nserialization, underutilization will not occur with this design.",
			  "These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty.",
			  "n order to study whether work distribution is a significant factor\nin efficiency of trace() kernels, we need to bypass the units. This\nis easily achieved by launching only enough threads to fill the ma-\nchine once. These long-running persistent threads can then fetch\nwork from a global pool using an atomic counter until the pool is\nempty",
			  "The persistent while-while shows remarkably similar development,\nwhich implies its performance cannot be significantly limited by\nthe memory bandwidth either. It is worth noticing that while-while\nis faster than packet traversal in all cases, and with diffuse rays the\ndifference is approximately 2X.\nThe implementation of persistent threads is given in Appendix A.",
			  "We implemented persistent variants of the packet traversal and\nwhile-while."
			]
		  }
		],
		"reasoning": "The specific field value cites a quantified performance gain: a 1.5x-2.2x speedup for packet traversal and 1.5x for per-ray traversal, plus an additional note that a 32% improvement can be gained from coordinating memory accesses via atomic coalescing in the work queue. The most directly supportive excerpt states that a persistent variant of the traversal achieves 1.52.2x speedup and is near the theoretical upper bound, which aligns with the reported range for packet traversal. This provides the clearest, explicit numeric support for at least part of the field value. Other excerpts describe the technique enabling such gains: using long-running persistent threads that fetch work from a global pool via an atomic counter, and the concept of atomic counter-based work distribution to avoid serialization and underutilization, which underpins the reported speedups. Additional mentions of persistent traversal variants and their impact on memory bandwidth further contextualize how these gains may be achieved in practice, supporting the broader claim that structuring work and memory access can yield substantial improvements. Taken together, the excerpts collectively corroborate the existence of substantial speedups through persistence and coordinated memory access, and they provide explicit numbers for similar traversal variants, with the largest emphasis on the 1.52.2x figure for packet traversal as the strongest direct match to the field value. The discussion of an atomic counter and global pool further explains the mechanism behind the reported gains, including potential 32% improvements from improved memory access coalescing in the work queue management.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.4.application_to_traversal_problem",
		"citations": [
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract"
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  }
		],
		"reasoning": "The fine-grained field value proposes a memory layout optimization for tree nodes: store all child nodes contiguously and keep only a pointer to the first child, with the second child address derived via arithmetic. This directly maps to the idea of cache-conscious or contiguous-layout trees that maximize spatial locality and reduce per-node storage overhead, enabling more nodes to reside in cache. The most supportive excerpts explicitly describe this approach: a variant of B+-trees that stores all child nodes contiguously and keeps only the address of the first child in each node. This supports the core claim that contiguity improves cache utilization and data density in memory and could be leveraged for parallel walkers sharing a root and diverging early. Additional excerpts discuss the general benefits of cache-conscious index structures and the use of compact or contiguous layouts (including variations like Cache Sensitive B+-Trees and references to Eytzinger layouts) which reinforce that such representations can yield higher cache efficiency and throughput for traversal-like workloads. Together, these excerpts corroborate the proposed data-density optimization as a viable technique to enhance traversal performance under memory bandwidth constraints.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.3.mechanism_summary",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The finegrained field value describes partitioning the tree into small subtrees called treelets sized to fit a cache level, grouping walkers into queues based on the next treelet to access, and a scheduler that loads a target treelet into cache and then processes all walkers in that queue. Directly supporting this, excerpts describe forming treelets by grouping connected BVH nodes to maximize the size of each treelet, which aligns with sizing to cache and batching walkers by the treelet they will access. They also describe batching rays visiting the same treelet to process them together to improve memory reuse, which is the same principle as batching walkers that share the same upcoming treelet. Additional excerpts discuss prefetching treelets into the GPU cache in treelet granularity, which is a concrete mechanism to ensure the treelet is in cache when the grouped walkers are processed, further matching the idea of a scheduler loading a specific treelet and then handling all relevant walkers. Some excerpts mention removing pointer-chasing by visiting the treelet first or near-simultaneous traversal perspectives, which reinforces the notion that treelet-level organization enables more coherent, cache-friendly traversal. Collectively, these excerpts substantiate the core concepts of treelet-based partitioning, queue-based grouping of walkers by upcoming treelet, and cache-driven loading and processing of those groups to minimize irregular memory access and improve throughput. The emphasis on treelets, grouping by access, and cache-aware loading directly maps to the described mechanism summary and its practical implications for traversal efficiency.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.2.primary_benefit",
		"citations": [
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages"
			]
		  }
		],
		"reasoning": "The most relevant content directly addresses strategies to counteract control-flow divergence in SIMT/SIMD execution models by regrouping or reformatting threads into coherent execution groups (warps) after divergent branches or during reconvergence. This aligns with the described technique of eliminating idle lanes caused by divergence through a periodic regrouping step, which is the core mechanism to restore near-full SIMT efficiency. Specific passages describe active thread compaction, dynamic warp formation, and maintaining a unified re-convergence point, all of which provide concrete methods and performance benefits for reducing divergence penalties. Additional related material discusses the idea of compacting divergent threads back into cohesive warps and demonstrates measurable speedups over baselines that suffer from divergence, reinforcing the connection to restoring SIMT efficiency via regrouping. Other entries discuss similar SIMT control-flow management concepts (e.g., sharing reconvergence stacks and block-wide strategies) that support the general principle of reducing divergence-induced idle lanes, though they are somewhat less direct than the core warp-compaction and Megakernel discussions. Overall, the strongest support comes from Megakernel- and warp-compaction-related literature, followed by papers that explicitly describe dynamic warp formation and thread-block compaction, which together substantiate the proposed value about near-full SIMT efficiency via periodic regrouping step.",
		"confidence": "high"
	  },
	  {
		"field": "gpu_optimization_techniques.2.reported_performance_gain",
		"citations": [
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages"
			]
		  }
		],
		"reasoning": "The target fine-grained field value claims a concrete hardware-based technique (Thread Block Compaction) yields a 22% average speedup over a baseline, with potential larger gains when combined with Treelet Scheduling. The most directly supportive excerpt explicitly reports a 22% speedup achieved by a compaction-based approach over a baseline, which aligns with the core quantitative claim of the field value. Related excerpts describe how dynamic warp formation and thread/block compaction restructure active threads to improve SIMD/SIMT efficiency and reduce divergence, which are the operational mechanisms by which TBC would realize such gains. Additional excerpts discuss coherence and batching advantages from similar compaction strategies (e.g., active thread compaction in path tracing and SIMT control-flow optimizations), which bolster the plausibility of the reported speedups in practice, even if they do not repeat the exact percentage. Collectively, these excerpts support the field value by corroborating that hardware-assisted compaction can improve utilization and throughput in divergent, irregular traversal scenarios, and that significant speedups are achievable when combining compaction with related scheduling or batching techniques. The most direct support is the explicit 22% figure; the rest provide mechanism and context that underpin why such gains are plausible and observed in related GPU traversal workloads.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.2.technique",
		"citations": [
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "found that our proposal addresses some key challenges\nof dynamic warp formation [9]. Our simulation evaluation\nquantifies that it achieves an overall 22% speedup over a\nper-warp reconvergence stack baseline for a set of divergent\napplications, while introducing no performance penalty for\na set of control-flow coherent applications",
			  "This mechanism maintains the key benefits of\nDWF of creating new warps to improve SIMD efficiency,"
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011"
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "Thread Block Com-\npaction for Efficient SIMT Control Flo",
			  " *Proc. IEEE*\n*Symp. on High-Perf. Computer Architecture (HPCA)* , pages"
			]
		  },
		  {
			"title": "[PDF] Megakernels considered harmful: wavefront path tracing on GPUs | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Megakernels-considered-harmful%3A-wavefront-path-on-Laine-Karras/90cf1ea079253c4451d4c74a37f0575e8501cdf4",
			"excerpts": [
			  "R\nThis paper explores and thoroughly analyze the concept of active thread compaction, the process of taking multiple partially-filled warps and compacting them to fewer but fully utilized warps in the context of a CUDA path tracer, and shows that this technique can indeed lead to significant improvements in SIMD utilization.",
			  "Active thread compaction for GPU path tracing]("
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf",
			"excerpts": [
			  "his reduces the control flow divergence, thereby\nimproving SIMT thread utilization, and also prevents resource us-\nage hot spots from dominating the latency-hiding capability for the\nwhole program",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherenc",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence.",
			  "The effect of improved coherence is witnessed by the speedup of\nray cast kernels by 32%, which can be attributed entirely to im-\nproved ray coherence."
			]
		  }
		],
		"reasoning": "The most pertinent excerpts describe dynamic warp formation, which explicitly regroups threads executing similar instructions into new warps to improve SIMD efficiency when control flow diverges. This directly maps to the concept of repacking work into coherent units (warps) to better utilize the SIMD/SIMT execution model in the presence of divergence. Related excerpts discuss thread block compaction, where divergent threads are compacted into new warps according to branch outcomes, thereby resynchronizing and reusing execution resources more efficiently. This evidences a concrete mechanism for warps to perform work more coherently after divergence, aligning with warp repacking principles. Additional references frame these ideas within broader SIMT control flow management and active thread compaction across GPU path-tracing and ray-tracing workloads, reinforcing that regrouping and scheduling of threads to minimize divergence is a practical, widely studied technique. Collectively, these excerpts support the idea that work compaction and warp repacking are effective strategies to maintain SIMD/SIMT efficiency when multiple parallel workers diverge on irregular traversal paths. The discussion on dynamic warp formation specifically demonstrates creating new warps from threads that share the same instruction trajectory to improve SIMD efficiency, which is a core aspect of warp repacking. The thread block compaction literature shows how compacting divergent threads into reconverged groups mitigates performance penalties from divergence, again aligning with the intended field value. The megakernel discussions extend the notion to aggressive batching and coherence improvements in GPU workloads, illustrating practical outcomes when warp and thread groupings are managed to reduce control-flow-induced inefficiencies. Overall, the excerpts collectively corroborate the field value by detailing concrete methods, benefits, and contexts in which work compaction or warp repacking improves throughput under irregular traversal patterns.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.3.application_to_traversal_problem",
		"citations": [
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "**SIMD-Aware Batch Hash Tables.**",
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel.",
			  "Prefetching-Enabled Hash Tables.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "We present Vec-HT, the first batch hash table that is fully optimized in all three dimensions;",
			  "Horizontal Vectorization and (2) Vertical Vectorization.",
			  "Vertical vectorization [ 19 ] is a more generalizable but more complex approach to benefit\nfrom SIMD in batch hash tables.",
			  "In vertical vectorization, since the hashing\nscheme must be translated into SIMD code, we need to use gathers and scatters to read/write\nfrom/to different entries of a hash table.",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  }
		],
		"reasoning": "The most relevant excerpt directly discusses SIMD-aware batch hash tables, which aligns with the idea of computing a batch of hashes for multiple walkers in parallel within SIMD lanes. It also emphasizes using the SIMD features of CPUs with a batch of operands sized to fit register constraints, which matches the concept of processing multiple walkers simultaneously. A closely related excerpt explains that vertical/horizontal vectorization in batch hash tables relies on transforming the hashing operation into SIMD code and may require gather/scatter memory accesses, thereby describing how to implement batched hashing in a vectorized fashion. Other excerpts elaborate on the need for batch operations, and the role of prefetching and batched access patterns in achieving higher throughput, which is compatible with feeding the hashed results into a vectorized probing step. Together, these excerpts provide a coherent picture of constructing a batched, SIMD-friendly hashing pipeline that could feed into a vectorized probe or traversal step, consistent with the proposed field value describing a batched, SIMD-parallel computation of hashes and a pipeline into vectorized probing.",
		"confidence": "medium"
	  },
	  {
		"field": "gpu_optimization_techniques.3.reported_performance_gain",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Figure 7 is the overall speedup and energy results of our proposed\ntreelet prefetcher using the ALWAYS heuristic, PMR scheduler, and\nwith a 512B maximum treelet size. Treelet traversal combined with\ntreelet prefetching achieves an average speedup of 32.1% over the\nbaseline RT Unit in Vulkan-Sim [ 41 ] while maintaining the same\npower consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "The treelet prefetcher\nis connected to the warp buffer so it can identify treelets that will\nbe traversed next.",
			  "We form\ntreelets by grouping connected BVH nodes to maximize the size\nof each treelet.",
			  "r simulations show treelet based traversal reduces performance\nslightly by 3.7% over a DFS baseline. However, when combined\nwith treelet prefetching, the overall speedup reaches 32.1% while\nmaintaining the same power consumption. Ma",
			  "Treelet\nprefetching also removes the need for rays to visit a node before\nfetching its child nodes, minimizing pointer-chasing dependencies."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity to the GPU's cache.",
			  "Treelet prefetching also removes the need for rays to visit a node before fetching its child nodes, minimizing pointer-chasing dependencies."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  "Aila et al. [ 1 ] improved upon this idea and\nintroduced treelets during tree traversal, batching together rays\ntraversing the same treelets and further reduced divergence. Pr"
			]
		  }
		],
		"reasoning": "The most directly supportive excerpt describes a concrete, substantial speedup (32.1%) achieved by combining treelet traversal with prefetching and a warp-aware scheduling approach, demonstrating that treelet-based techniques can yield significant performance gains in GPU ray tracing workloads. This aligns with the field values claim that combining optimizations like virtualized queues and warp repacking can boost path tracing performance, suggesting that large gains are indeed achievable under similar techniques.\n\nAdditional excerpts discuss the core mechanisms: batching rays that visit the same small subtrees (treelets) to improve memory reuse and reduce divergence, and prefetching BVH nodes at treelet granularity to the GPU cache. These pieces directly support the idea that organizing traversal around treelets and proactively bringing data into cache can yield meaningful, architecture-aware performance improvements in path tracing workloads.\n\nFurther, several excerpts note that treelet prefetching eliminates certain pointer-chasing dependencies by allowing child nodes to be fetched before their parent is visited, and that coupling treelets with warp-aware buffering can help synchronize traversal across many threads. This supports the notion that warp-level optimizations and memory-access batching (warp repacking, prefetching, and treelet batching) can be leveraged to sustain high throughput in irregular traversal patterns common to path tracing.\n\nA few excerpts reinforce that the gains are not merely theoretical but demonstrated in GPU contexts (e.g., prefetcher integration with warp buffers, and explicit performance graphs). While the exact 2.55x figure is not stated, the presence of multi-bundle speedups and architecture-aware strategies across these excerpts substantiates the central claim that combining treelet-based traversal with memory-access optimizations can yield substantial performance improvements in path tracing scenarios.\n",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.4.source_domain",
		"citations": [
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers."
			]
		  },
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  }
		],
		"reasoning": "The most directly relevant information is a direct statement naming the technique: the CSB+-Trees term is introduced as a new indexing technique. This confirms the exact field value and serves as the strongest support. Several excerpts expand on this technique by describing its core ideastoring child nodes contiguously and optimizing cache usagethereby aligning with the field value and providing concrete substance about CSB+-Trees. Others discuss closely related cache-conscious B+-tree variants and layout-conscious approaches (e.g., cache-conscious trees, layout choices) which, while not naming CSB+-Trees explicitly, support the broader concept of cache-aware B+-tree designs and their relevance to memory access patterns in database indexing. A few excerpts reiterate or paraphrase similar ideas (e.g., CSB+-Trees as part of cache-conscious indexing), which further corroborates the field value even if they do not use the exact acronym multiple times. Collectively, the strongest support comes from explicit naming of CSB+-Trees, followed by explicit descriptions of their cache-conscious properties, with additional corroborating context from related cache-conscious and layout-oriented discussions.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.4.key_insight",
		"citations": [
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract"
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  }
		],
		"reasoning": "The most directly supportive content describes a specific indexing technique where all child nodes are stored contiguously and only the first child pointer is stored, which directly aligns with eliminating redundant pointers and enforcing a contiguous layout. Such organization enables better use of cache lines because a single memory fetch can bring in multiple related nodes that are stored adjacent to each other, reducing memory traffic. Repeatedly, the excerpts emphasize that cache-conscious designs (like CSB+-Trees) improve cache behavior and line utilization by organizing children contiguously and minimizing pointer indirection. This directly corroborates the claim that a contiguous sibling layout and fewer pointers improve cache-line utilization and reduce memory traffic. Additional excerpts reinforce this by highlighting that similar contiguity and cache-conscious designs lead to faster lookups and better use of cache lines, and that variant but related cache-optimized layouts (e.g., mentioning Eytzinger layout) also aim to improve data locality and access efficiency. Taken together, these sources support the idea that removing redundant pointers and enforcing a contiguous sibling layout improves cache-line utilization and can lead to more data being brought in per fetch, which matches the described improvement in memory traffic and fetch quality. The supporting evidence is strongest where contiguity and reduced pointer usage are explicitly stated as benefits, and is supplementary where it discusses cache-consciousness in broader terms or in related layout strategies. ",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.2.source_domain",
		"citations": [
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "Use `H1` to pick a starting group.",
			  "Swiss Table has the following key characteristics:",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early."
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions."
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  }
		],
		"reasoning": "The strongest support comes from excerpts that explicitly name Swiss Table and SwissTables, describing it as a high-performance hash table design and outlining its core ideas (control bytes, group-based scanning, SIMD usage) which align with the requested high-performance hash-table family. Direct references to Swiss Table conceptssuch as control bytes, SIMD-friendly lookups, and parallel/grouped scanningtie closely to the notion of high-performance hashing implementations used by Google and related projects. Additional excerpts describe how control words and SIMD enable fast lookups, and how groups of slots are processed in parallel, which reinforces the connection to the high-performance hash-table family mentioned in the target field value. Other excerpts reference the same Swiss Table family and its characteristics (e.g., open-addressing with control bytes, 8-slot groups, and H2 fingerprint usage), further corroborating the association with high-performance hashing tech used by major players. Collectively, these excerpts establish a coherent picture of the SwissTable/SwissTables lineage as an exemplar of high-performance hash tables, consistent with the field value. ",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.1.key_insight",
		"citations": [
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  }
		],
		"reasoning": "The finegrained field value asserts that replacing explicit pointers with an implicit, arithmetic-based tree layout reduces memory traffic, improves cache utilization, and yields more predictable access patterns for hardware prefetchers. Several excerpts describe this exact idea in different flavors. Descriptions of Eytzinger-style layouts show how binary trees can be stored in array form to achieve better temporal locality and cache-friendliness, since elements nearer the root appear earlier in memory and thus are more likely to be cached on access. This supports the claim that an implicit, cache-friendly layout reduces scattered accesses by clustering traversal-related data. Additional passages discuss prefetching and explicit hints to prefetch cache lines, which aligns with making the traversal pattern more predictable for hardware prefetchers. Finally, references to cache-oblivious and recursive layouts illustrate broader strategies for organizing tree data to improve locality and bandwidth efficiency, reinforcing the stated benefits of an implicit layout. Taken together, these sources corroborate the position that moving away from pointer-based, scattered access toward an arithmetic-based, implicit layout reduces memory traffic, enhances cache utilization, and improves predictability for prefetchers during batched or parallel traversals.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.1.source_domain",
		"citations": [
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size.",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers)."
			]
		  }
		],
		"reasoning": "The most directly related material discusses a whole-family of cache-aware tree layouts and traversal optimizations that enable more predictable memory access patterns, which are foundational for vectorized or cache-conscious data structures like vectorized B-Trees. In particular, the van Emde Boas layout presents a recursive, fractal arrangement that preloads subsequent vertices to stay within a page, aligning with the goal of reducing random memory accesses in tree traversals. The discussion about static search trees and batching analyzes how to use memory layouts (such as Eytzinger) and prefetching to improve locality and enable batched processing, which is highly relevant to building vectorized or SIMD-friendly tree structures. The Eytzinger-related excerpts explain how reordering a tree into an array layout improves temporal locality and cache behavior, making it easier to feed multiple walkers in a data-parallel fashion with fewer memory stalls. Other excerpts address prefetching techniques and the idea of prefetching cache lines ahead of use, which is crucial for maintaining SIMD efficiency when traversals diverge. Collectively, these excerpts support the notion of cache-conscious data structures and vectorizable traversal strategies, even if they do not name the exact term 'Vectorized B-Trees'. The content about memory-batched/batched searches and batching in static search trees provides concrete mechanisms to exploit temporal locality and spatial locality, which are central to vectorized or SIMD-optimized tree structures.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.0.key_insight",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis"
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate)."
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions.",
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements."
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD"
			]
		  }
		],
		"reasoning": "The field value describes replacing conditional branches with branchless SIMD operations to process many walkers efficiently. Excerpts that explicitly discuss using SIMD to avoid branching by computing masks, performing comparisons in parallel, and using bitmask operations to drive control flow directly address this goal. For example, one excerpt explains loading multiple elements, performing a vectorized comparison against a key, producing a vector mask, and then extracting a scalar decision from that mask, which directly demonstrates a branchless, SIMD-driven selection mechanism. Another excerpt highlights converting and combining vector masks (movemask) to avoid branching and keep the traversal within vectorized pathways, aligning with the idea of processing 256 walkers in lockstep for decisions on left/right movement based on bitcoded results. Additional excerpts emphasize faster SIMD routines that reduce the cost of mask handling (movemask/blending) and the idea of pre-processing or organizing data to make vectorized decisions more efficient, which supports the notion of avoiding divergent branches by operating on batches of walkers. There are also excerpts describing batched or parallelized search in related structures (e.g., batched filter/search in hash-table-like data layouts) that illustrate how vectorized filtering within a chunk can substitute for per-element branching, further reinforcing the branchless, batch-processing approach for irregular access patterns. Taken together, these excerpts provide a coherent set of techniquesvectorized comparisons, masks, bitmask extraction, and data layout/batchingthat directly support the stated field value about branchless SIMD to improve throughput for many walkers with divergent paths. ",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.2.mechanism",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The most relevant excerpts directly describe a hardware or near-hardware level mechanism that prefetches an entire small subtree (treelet) when traversal enters its root, which aligns with a specialized hardware prefetcher that partitions a BVH/tree into treelets and prefetches the whole treelet. Specifically, one excerpt discusses a hardware implementation of a treelet prefetcher with a treelet popularity heuristic, indicating a concrete hardware realization of the idea. Another excerpt explicitly states that treelets are smaller subtrees used to speed up traversal and that prefetching confines accesses within a treelet, enabling prefetching to the GPU cache. Additional excerpts note that treelet prefetching reduces latency in pointer-chasing traversal by prefetching to the cache, and multiple sources discuss the general benefits of treelet-based traversal in reducing irregular memory access and improving memory reuse. Together these excerpts corroborate the concept of partitioning the BVH into treelets and leveraging a hardware/architectural mechanism to prefetch entire treelets when traversal diverges from the root, which is consistent with the target finegrained field value describing a specialized prefetcher that automatically prefetches all nodes within the treelet upon entry to the treelet root. The accompanying performance notes reinforce the practical benefits of such an approach, such as improved throughput and reduced latency.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.0.source_domain",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD"
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  }
		],
		"reasoning": "The target field value points to Vectorized B-Trees and database indexes, so excerpts that describe using SIMD to accelerate B-tree operations or to accelerate index traversal are directly relevant. The discussion of using SIMD to load and compare multiple elements in parallel, creating and manipulating vector masks, and then extracting results (such as through movemask) aligns with vectorized approaches that could optimize B-tree traversals and index searches. Descriptions that show preprocessing or reordering data to fit SIMD workflows (for example permuting results or reorganizing layouts) provide concrete techniques that could translate to faster B-tree query paths and index scans. References that discuss k-ary search on modern processors extend the same SIMD-accelerated search paradigm to multi-way search problems common in database indexes, offering complementary strategies for reducing branching and improving throughput in index traversal. Collectively, these excerpts outline practical SIMD techniques, layout optimizations, and search strategies that underpin vectorized B-tree and database index implementations. They do not present conflicting claims about the field value, and several excerpts explicitly discuss SIMD-based tree or index traversal, which supports the finegrained field value.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.0.technique",
		"citations": [
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references)."
			]
		  }
		],
		"reasoning": "The target field value is exactly about an inline software prefetching technique implemented within the main thread context to handle delinquent irregular loads, epitomized by the phrase 'Helper Without Threads'. Excerpts that explicitly describe inline software prefetching, embedded helper code in the main thread, or the concept of a customized software prefetcher tailored to the application's dataflow are the strongest matches. The following direct points connect to that field value: one excerpt explains an inline software prefetching technique that overcomes hardware constraints by inserting the helper code into the main thread itself, which aligns with the concept of performing prefetching without additional helper threads. Another excerpt discusses the idea of a helper without threads in the context of customized prefetching for delinquent irregular loads, precisely the notion of containing prefetch logic within the main execution thread. Additional excerpts further describe the notion of customized software prefetchers extracted from the applications dataflow to anticipate irregular loads, which directly supports the idea of software-driven prefetching strategies for irregular memory accesses. The remaining excerpts reinforce related themes by elaborating on how loads can be categorized (constant, striding, irregular) and how such prefetching strategies target delinquent irregular loads without relying on separate hardware or auxiliary threads, thereby underpinning the same inline, main-thread prefetching approach. Taken together, these excerpts collectively substantiate the concept of Inline Software Prefetching and the specific Helper Without Threads technique as a viable approach for mitigating memory access latency in irregular, data-dependent traversal patterns.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.2.application_to_traversal_problem",
		"citations": [
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** ."
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions."
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait."
			]
		  }
		],
		"reasoning": "The finegrained field value describes converting a per-node hash computation into a batched, SIMD-enabled lookup against a group of candidate nodes, using a fingerprint (H2) to quickly filter the matches and produce a bitmask of viable next steps. Several excerpts directly discuss this pattern: using control bytes organized to match SIMD widths to scan a group of slots at once, which enables fast identification of candidate or empty slots and early termination; representing groups of slots (for example 8 or 16) and scanning their control metadata with SIMD; and maintaining a control word that stores per-slot metadata, including an H2 fingerprint, so that a SIMD probe can compare fingerprints across the group in parallel. Additional excerpts describe the core idea of performing multiple probe steps in parallel with a control word, and that the array is partitioned into logical groups where SIMD can operate on the control data to accelerate lookups. The described mechanism of computing a hash, partitioning into a high-bits group selector and a low-bits fingerprint, and using a SIMD comparison across a set of 16 candidates to produce a match mask aligns closely with the field values proposed pipeline. Collectively, these excerpts provide the architectural knobs (group-based scanning, control bytes/fingerprint, SIMD-wide comparisons, and early-stop/grouping) that underpin the modeled hash-table-like traversal within a packed, memory-efficient layout. They also cover how such designs achieve cache-friendly, high-throughput memory access in a batched context, which reinforces the field values assertion about performance gains from SIMD-enabled, fingerprint-based group lookups in a traversal scenario.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.0.technique",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better.",
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best."
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD"
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate)."
			]
		  }
		],
		"reasoning": "The target finegrained field value is Branchless SIMD Node Search, placed in a context of parallel walkers traversing a binary tree with data-dependent memory access patterns and limited memory bandwidth. The most relevant excerpts describe SIMD-based approaches to tree-like or hierarchical search without branches, using vector comparisons to generate masks and then selecting the next step from those masks. For instance, the discussion of loading multiple elements in parallel, performing a vectorized comparison against a key, producing a vector mask, and then extracting the relevant path decision via mask handling aligns directly with a branchless, SIMD-first node-selection approach in a traversal. Further, techniques that avoid branching by computing all possible next steps in parallel and then selecting the actual next node based on a computed mask match directly map to the idea of branchless traversal of a tree under SIMD constraints. Additional excerpts discuss using movemask and vector permutation/rewriting to keep computations within SIMD units and reduce branches, which is precisely the kind of branchless decision logic that would enable 256 independent walkers to traverse a tree with data-dependent indices using limited memory bandwidth.\n\nSeveral excerpts discuss pre-processing or layout changes to improve SIMD efficiency for tree-like structures (e.g., reorganizing data layouts or using precomputed permutations) to ensure that vectorized computations can proceed with minimal branching. These align with the notion of branchless SIMD node searches where the traversal decisions are derived from parallel comparisons and masks rather than divergent branches. The inclusion of techniques for k-ary search on modern processors and the emphasis on SIMD masking to avoid branches further reinforce the applicability of a branchless, SIMD-centric node search strategy in a traversal context. The references to vectorized hashing and intra-chunk filtering indicate parallel search constructs that share the same core principle: perform work in parallel across data elements and select outcomes via masks rather than conditional branches, which is consistent with a branchless SIMD approach to node traversal. Collectively, these excerpts substantiate a design where 256 walkers can diverge along different paths while maintaining SIMD-friendly, branchless decision logic at each step, leveraging masks and bitwise_extract (movemask) operations to determine left/right children without per-walker branching.\n\nIn summary, the strongest support comes from explicit descriptions of vectorized searches that avoid branching by using masks and selective extraction, plus mentions of reorganizing data and using movemask to keep computations within SIMD lanes. Additional related material on SIMD-based k-ary searches and intra-chunk filtering provides corroborating context that parallel, branchless traversal is feasible under SIMD constraints. While some hashing-focused excerpts extend the same branchless/masked paradigm to different data structures, the core relevance remains in the branchless, SIMD-driven traversal paradigm applicable to a parallel tree-walking scenario.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.2.key_insight",
		"citations": [
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n"
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions."
			]
		  }
		],
		"reasoning": "The target field value asserts that a compact, 7-bit fingerprint (H2) is used to filter out non-matching nodes with a cheap SIMD operation, reducing the need for expensive full 64-bit key comparisons and thereby speeding up long probe chains. The most direct support comes from passages describing the use of a small H2 fingerprint within control metadata, and how SIMD-enabled scanning of control bytes or words accelerates lookups. Specifically, the material discusses: using H2 fingerprints stored alongside slots to quickly rule out non-matches, and the idea that group-based or word-level SIMD comparison of control data enables fast identification of candidate slots or empty slots, enabling early termination of probes. Additional passages describe control bytes that encode slot state and H2, and the use of SIMD to scan these control bytes or a control word to perform parallel checks. There is also explicit mention that a single, SIMD-enabled comparison against H2 can filter most non-matches, which aligns with the claim of drastically reducing full key comparisons. Furthermore, descriptions of scanning control bytes in groups, and treating the control metadata as a SIMD-friendly structure (control word/bytes) corroborate the batching and parallelism benefits described in the field value. Together, these excerpts support the idea that a compact fingerprint and SIMD/grouped control-byte strategy dramatically improves performance by avoiding full comparisons and by enabling batched, parallel memory access patterns during probe operations. The combination of H2 presence in control metadata, SIMD-enabled control-byte/group scanning, and the notion of batching probe steps into parallel work all directly map to the field values core claims.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.2.technique",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more"
			]
		  }
		],
		"reasoning": "The most directly relevant passages explicitly discuss prefetching BVH nodes in treelet granularity and batching subtree traversal to improve memory locality on GPUs. In particular, descriptions of prefetching treelets to the GPU cache, the hardware implementation of a treelet prefetcher, and the notion that memory accesses become more clustered and prefetchable within treelets strongly support the target concept of Treelet Prefetching on GPU architectures. Additional excerpts describe the benefits and rationale of treelet-based traversal for ray tracing, including reducing latency of pointer-chasing patterns and organizing accesses within subtrees, which reinforces how this technique aligns with GPU-focused optimization for irregular memory access patterns. Several excerpts also mention performance improvements and memory reuse when processing rays that visit the same treelets, which illustrates practical outcomes of applying treelet-based batching on GPUs. Collectively, these excerpts establish that Treelet Prefetching (GPU) is a technique characterized by breaking the BVH into treelets, clustering accesses to enable prefetching, and leveraging GPU caches to hide latency in divergent traversal scenarios. The content lines up with the idea of using treelet granularity to improve memory efficiency and throughput in GPU-based traversal workloads, directly supporting the stated finegrained field value.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.0.mechanism",
		"citations": [
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references)."
			]
		  }
		],
		"reasoning": "The fine-grained field value describes a software-only technique where prefetching logic is inserted directly into the application's code and runs ahead of the main computation, issuing prefetches for future data without creating separate OS threads. The most relevant excerpts state an inline software prefetching technique that inserts helper code into the main thread itself, thereby enabling prefetching without additional threads. This directly matches the described mechanism of weaving prefetching into the primary computation path. Additional highly relevant excerpts discuss customized or specialized software prefetchers that operate without access to the application source code, reinforcing the idea of software-driven, inline prefetching rather than relying on separate hardware or OS threads. Other excerpts emphasize prefetching aimed at delinquent or irregular loads, which aligns with mitigating scattered memory access patterns in irregular traversal or data-dependent indexing. References that frame the prefetching as complementary to hardware prefetchers or that categorize load types (constant, striding, irregular) provide supporting background but are one step removed from the exact inline, single-path integration described in the target mechanism. Taken together, these excerpts strongly support the concept of a software-only, inline prefetching mechanism that advances the main computation to prefetch future data without spawning new threads.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.2.technique",
		"citations": [
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "Swiss Table has the following key characteristics:",
			  "Use `H1` to pick a starting group.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed."
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "he array is broken into logical *groups* of **8 slots** each."
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait."
			]
		  }
		],
		"reasoning": "The field value refers to a SIMD-based probing approach that uses grouped control data and H1/H2 splitting to guide probes. Excerpts that discuss scanning groups of control bytes with SIMD, aligning control data to SIMD widths, and using H2 fingerprints to quickly identify candidate slots directly support the idea of SIMD group probing with hash-based splitting. Specifically, one excerpt explains that control bytes are organized to match SIMD widths and that grouping allows scanning multiple control bytes at once to identify candidates or empties, which is a core aspect of SIMD group probing. Another excerpt describes scanning a group of 8 or 16 slots efficiently using SIMD, which underpins the concept of probing within a SIMD-friendly group rather than scalar, divergent paths. Further excerpts discuss performing multiple probe steps in parallel with control word metadata and the use of a hashed fingerprint (H2) stored in control metadata to validate candidates, which directly aligns with the notion of hash-splitting (H1/H2) informing which paths to take in a SIMD-probing context. Additional excerpts illustrate how a Swiss Table-like layout relies on control bytes and explicit grouping to accelerate lookups, including the concept that full control words contain or reflect H2 data, enabling rapid exclusion of non-candidates. There is also explicit mention of SIMD-based scanning of control bytes and the role of control words to drive vectorized searches, reinforcing the relevance to the requested technique. Some excerpts provide broader context about cache-friendly and SIMD-accelerated probing in high-performance hash maps, which supports the general approach of SIMD group probing for irregular access patterns, including batched or parallel traversal scenarios where root-level convergence occurs before divergence. Collectively, these excerpts substantiate the key components of SIMD Group Probing with H1/H2 Hash Splitting: grouping of slots, control-byte metadata aligned to SIMD widths, group-based SIMD scanning, and the use of H2 fingerprints to prune probes before deeper traversal.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.4.technique",
		"citations": [
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n"
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%."
			]
		  }
		],
		"reasoning": "The fine-grained field value identifies Correlation Prefetching leveraging the Global History Buffer and DCPT. Excerpts that introduce Delta Correlating Pre-diction Tables (DCPT) as a prefetching technique, and describe its combination of Reference Prediction Tables (RPT) and PC/DC prefetching with delta correlating ideas, directly support this field value. Several excerpts explain that DCPT reduces the cost of pointer chasing in buffering structures and that it embodies a delta-correlating approach, which aligns with correlation-based prefetching strategies that can exploit shared traversal patterns across multiple walkers. Other excerpts explain the role of the Global History Buffer (GHB) in indexing and storing heuristic information related to loads, which is foundational to how correlation/prediction-based prefetchers operate and would be applicable to a batched tree traversal where multiple agents access related memory patterns. Collectively, these excerpts establish that the described technique is DCPT-based, uses a GHB, and relies on correlation to predict memory accesses, matching the requested fine-grained field value. The content also mentions performance benefits from such prefetchers, reinforcing the relevance to the research context of optimizing irregular, data-dependent memory access in a batched, SIMD-like traversal scenario.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.2.suitability_for_problem",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions.",
			  "**Figure 4: Hardware implementation of treelet prefetcher**\n**with treelet popularity threshold heuristic.**"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  }
		],
		"reasoning": "The central claim is that treelet-based or coarse-grained prefetching can mitigate pointer-chasing bottlenecks in a GPU-like, highly parallel walker traversal by clustering accesses and prefetching ahead of time. Excerpt describing prefetching BVH nodes in treelet granularity to the GPU cache explicitly states that memory accesses are now clustered within a treelet, enabling easier prefetching and reducing latency, which directly supports the idea of turning a sequence of dependent loads into a single or fewer, larger prefetch operations. Additional excerpts reinforce this by showing performance gains from treelet prefetching and by describing how ray traversal becomes more cache-friendly when memory accesses stay within treelets. Other excerpts discuss extending this idea to memory-coherence and memory reuse through treelets, framing it as a practical, hardware-friendly technique for irregular access patterns seen in ray tracing, which is analogous to the described 256-walker, data-dependent traversal. While some excerpts focus on broader performance improvements or irregular access patterns in ray tracing, their shared emphasis on treelets, clustering, and prefetching to the GPU cache provides cohesive support for the field value.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.0.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads"
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  }
		],
		"reasoning": "The target field value describes a significant performance improvement for memory-bound irregular workloads using speculative/software-directed prefetching and related approaches. Excerpts that outline inline software prefetching embedded in the main thread demonstrate how to initiate prefetches without requiring extra threads, directly supporting the core concept of improving memory access coherence and latency hiding in irregular access patterns. Excerpts that discuss helper-thread prefetching illuminate an alternative strategy to prefetch data ahead of time, which is relevant when evaluating different prefetching models for divergence-prone traversals. Excerpts that classify loads into constant, striding, and irregular categories provide essential context for why irregular accesses are problematic and how bespoke prefetching strategies can target delinquent loads. Excerpts that explicitly describe prefetching tailored to delinquent irregular loads offer a direct blueprint for applying such techniques to a batched tree-walker, where many walkers may access irregular memory as traversal decisions diverge. Excerpts noting that these approaches are complementary to hardware prefetchers further support the idea that software-directed prefetching can augment limited memory bandwidth scenarios. Additional excerpts that reiterate the concept of delinquent irregular loads and the goal of minimizing latency by proactive data fetching reinforce the applicability of the described techniques to the users scenario. While the exact numerical gains (the 2x figure and the 83% improvement) are attributed to a specific study, the collection of excerpts provides the methodological foundation (inline prefetching, delinquent-load targeting, and memory-access scheduling) that could realize such improvements in practice for a 256-walker, data-dependent traversal.\n",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.0.application_to_traversal_problem",
		"citations": [
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better.",
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best."
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate)."
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD"
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  }
		],
		"reasoning": "The most directly relevant concept is using a SIMD-generated bitmask to represent the LSBs across walkers and then using a blend operation to select the next node indices from two candidate vectors (left and right) without branching. The first excerpt explicitly discusses using SIMD to compare a set of elements, produce a bitmask from those comparisons, and then extract the first relevant index via a bit operation, which maps to forming a mask from LSBs and using it to drive selection. The second excerpt reinforces the same idea in AVX2 terms, showing how to load multiple elements, compare them in parallel, and obtain a scalar mask (movemask) to guide further action. The third excerpt expands on combining multiple vector masks into a single interpretation, illustrating how to handle multiple rounds of comparisons while maintaining a correct ordering of results, which aligns with forming a bitmask from 256 walkers and using it to pick next indices. The fifth excerpt notes a performance consideration: reducing reliance on movemask due to its cost, which informs practical constraints when implementing a branch-free blend; this supports the idea that the overall approach is viable but must be carefully optimized. The remaining excerpts discuss related SIMD-friendly traversal improvements (permuting during preprocessing to align data, faster mask handling, and higher-level batched/bulk traversal ideas) that provide context and corroborate that bitmask-based, vectorized selection is a viable strategy in complex, irregular access patterns. The additional related materials on batched hash table probing and intra-chunk vectorization further illustrate the broader family of techniques where grouping work and using masks to drive selection improves throughput under data-dependent traversal patterns. In sum, the core mechanism of forming an LSB-derived bitmask across walkers and using SIMD blend to select between candidate indices is well-supported by explicit mask-based and blend-based discussions, with practical caveats and complementary approaches documented in the surrounding excerpts.",
		"confidence": "medium"
	  },
	  {
		"field": "quantitative_performance_model.key_insight_from_model",
		"citations": [
		  {
			"title": "General Transformations for GPU Execution of Tree ...",
			"url": "https://engineering.purdue.edu/~milind/docs/sc13.pdf",
			"excerpts": [
			  "However, because the tree structures\nare irregular, and the points traversals are input-dependent, sim-\nply running multiple traversals simultaneously on the GPU can-\nnot take advantage of efficient memory accesses, seriously hinder-\ning performance (Section 2.2 discusses GPU architectures and the\nGPU performance model in more detail)."
			]
		  },
		  {
			"title": "Parallel Tree Traversal for Nearest Neighbor Query on the ...",
			"url": "http://dicl.skku.edu/publications/icpp2016.pdf",
			"excerpts": [
			  "Traversing hierarchical tree structures in an irregular manner makes it difficult to exploit parallelism since GPUs are tailored for deterministic memory ...Read more"
			]
		  },
		  {
			"title": "REGTT: Accelerating Tree Traversals on GPUs by ...",
			"url": "https://cgi.cse.unsw.edu.au/~jingling/papers/icpp16.pdf",
			"excerpts": [
			  "There are three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The thre",
			  "re three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access overhead.\n** **Load Imbalance** Different threads in the same warp may\nhave different workloads due to query-dependent",
			  "e three challenges faced in parallelizing tree traver-\nsals on GPUs due to query-dependent tree truncations:\n** **Memory-Access Irregularity** The threads that handle\ndifferent queries in the same warp may visit different\nnodes at the same time, making it hard to take advantage\nof *memory coalescing* , a well-known optimization for co-\nalescing a number of simultaneous global accesses from\nthe threads in a warp into a single *memory transaction*\nto reduce GPUs global memory access over"
			]
		  },
		  {
			"title": "SIMD Parallelization of Applications that Traverse Irregular ...",
			"url": "https://www.cs.wm.edu/~bren/files/papers/CGO13.pdf",
			"excerpts": [
			  "In order to make our approach fast, we demonstrate\nseveral optimizations including a stream compaction method\nthat aids with control flow in SIMD, a set of layouts that\nreduce memory latency, and a tiling approach that enables\nmore effective prefetching.",
			  "This paper develops support for exploiting such\ndata parallelism for a class of non-numeric, non-graphic\napplications, which perform computations while travers-\ning many independent, irregular data structure",
			  "Our work has considered specific challenges arising\nfor pointer-based traversals, which have not been considered\nin the past."
			]
		  },
		  {
			"title": "Intersection Prediction for Accelerated GPU Ray Tracing",
			"url": "https://weschang.com/publications/intersection-prediction/intersection-prediction.pdf",
			"excerpts": [
			  ". Aila et al. [ 2 ] proposed group-\ning rays into ray packets and traversing rays together, improving\nmemory coherence. "
			]
		  },
		  {
			"title": "Batching of divergent rays on GPU architectures",
			"url": "https://studenttheses.uu.nl/bitstream/handle/20.500.12932/41250/thesis_final.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "A number of advanced traversal algorithms have been devised in recent years that increase ray coherence by bundling active rays together dynamically as they ...Read more",
			  "**2.2. Ray Batching**\nPharr et al sought to address the problem as a scheduling challenge [ Pha+97 ]. They developed a system\ndesigned to handle scenes far too large for system memory, based on voxels. When tracing through a\ngiven voxel, required data would need to be explicitly fetched from disc before tracing could continue\ninto that segment. They also ran into issues with the sheer volume of rays that are spawned by the\ntree structure of by Whitted-style ray tracing [ Whi79 ], especially when they scheduled a large number\nof active concurrent traces rather than the traditional depth-first implementation. Their schedul"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Tree traversal is an intensive pointer-chasing operation, requiring traversing to a node in the tree and finding the child pointers, before being able to find the child node addresses and issue loads.",
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "The primary performance bottleneck in ray tracing is the cost of determining the closest intersection between a ray and a scene. While the scene is encoded as a tree data structure such as a Bounding Volume Hierarchy (BVH) tree to reduce the cost of finding intersections, traversing the BVH tree is still costly due to long memory latencies.",
			  "This work presents a treelet prefetching scheme to improve ray traversal performance. Conventional prefetchers like stride and stream prefetching are inadequate for ray tracing due to irregular access patterns during BVH traversal. Ray accesses exhibit little overlap and can be highly divergent, sampling independent scene areas and traversing different parts of the tree.",
			  "we propose a treelet based ray traversal algorithm with an accompanying prefetcher.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance. This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nRay traversal is typically done by traversing the BVH tree in a depth-first or breadth-first manner",
			  "We propose a treelet based traversal algorithm performed in the RT unit that transforms the sequence of memory accesses performed by each ray to be clustered within individual treelets.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  ".\nAila et al. [ 5 ] proposed to use *treelets* , which are small subtrees of\nthe overall BVH tree to speed up ray traversal. They explored using\ntreelet queues to queue up rays that visit the same treelet and pro-\ncess them together to increase memory reuse. While an interesting\nidea, their simulated architecture is different from a programmable\nGPU and they lacked an actual hardware implementation. Adopting\nthe queuing mechanism with current GPU threading models and\nmodern ray tracing APIs is non-trivial. In this work, we build off\nthe concept of treelets and propose prefetching for BVH trees at a\ntreelet granularity. Tree traversal is an intensive pointer-chasing\noperation, requiring traversing to a node in the tree and finding the\nchild pointers, before being able to find the child node addresses\nand issue loads. With treelet prefetching, as rays traverse the BVH\ntree and visit the root node of treelets, corresponding treelets can be\nprefetched to load deeper levels of the tree before they are needed.\nWe combine treelet prefetching with a treelet based traversal algo-\nrithm in the ray tracing accelerator to further reduce ray traversal\nlatency. From the limited available public information disclosed by\nGPU hardware manufacturers [ 2  4 , 9 , 11 ], it is unclear whether\nany commercial designs implement treelets and if so how.\nWe make the following contributions in this paper:\n We propose a treelet prefetching technique for ray tracing\nthat can hide the memory latency of ray traversal.\n We propose a lightweight hardware implementation of a\ntreelet based prefetcher by organizing BVH memory in a\ntreelet based layout.\n We propose a treelet based traversal algorithm that is able\nto take advantage of treelet prefetching.\n**2**",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://users.aalto.fi/~laines9/publications/ylitie2017hpg_paper.pdf",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates\non compressed wide BVHs and maintains the traversal stack in a\ncompressed format. Our method reduces the amount of memory\ntraffic significantly, which translates to 1.92.1  improvement in\nincoherent ray traversal performance compared to the current state\nof the art. Furthermore, the memory consumption of our hierarchy\nis 3560% of a typical uncompressed BVH.",
			  "They trivially allow ray-box and ray-triangle tests to be\nexecuted in parallel over multiple SIMD lanes, although this has the\ndrawback that the reserved lanes may suffer from underutilization\ndue to highly serial control code.",
			  "Managing a full traversal stack is costly in GPU ray tracers,\nbecause the caches in GPUs are too small to capture the stack traffic.\nThis tends to lead to high DRAM traffic from the traversal stacks\nonly."
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://jcgt.org/published/0011/04/01/paper.pdf",
			"excerpts": [
			  "Traversal on the GPU might be challenging due to warp divergence and incoherent\nmemory accesses.",
			  "To prevent warp divergence, the traversal\nis divided into two independent loops processing interior and leaf nodes separately\n(i.e., the *while-while* traversal)."
			]
		  },
		  {
			"title": "CPU-Style SIMD Ray Traversal on GPUs",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231583/07-1038-lier.pdf",
			"excerpts": [
			  "In this paper we describe and evaluate an implementation of CPU-\nstyle SIMD ray traversal on the GPU. We show how spreading\nmoderately wide BVHs (up to a branching factor of eight) across\nmultiple threads in a warp can improve performance while not\nrequiring expensive pre-processing. Te presented ray-traversal\nmethod exhibits improved traversal performance especially for\nincreasingly incoherent rays.",
			  "Te foundation of our approach is teaming multiple lanes of a warp\nand leting them traverse the BVH together for one single ray. Tis\nconcept mimics a regular SIMD-based BVH traversal known from\nmethods utilizing SSE and AVX extension on the CPU [ Dammertz\net al . 2008 ; Ernst and Greiner 2008 ; Wald et al . 2014 ]. But in con-\ntrast to CPUs, switching between vector instruction (e.g. parallel\nintersection tests) and scalar instruction (e.g. stack management) is\nnot easily (or even efciently) possible on GPUs. In our case, some\noperations (e.g. loading, storing, and stack management) have to\nbe handled individually and at times redundantly on each lane.\nTerefore, we supply each lane with its own copy of the ray data,\nnearest hit information, and stack pointer. However, the stack itself\nresides in shared memory and thus is not redundant.\n"
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through Compressed Wide BVHs | Research",
			"url": "https://research.nvidia.com/publication/2017-07_efficient-incoherent-ray-traversal-gpus-through-compressed-wide-bvhs",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more"
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://dl.acm.org/doi/10.1145/3105762.3105773",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format.Read more",
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH."
			]
		  },
		  {
			"title": "Efficient incoherent ray traversal on GPUs through compressed wide BVHs",
			"url": "https://www.researchgate.net/publication/318730238_Efficient_incoherent_ray_traversal_on_GPUs_through_compressed_wide_BVHs",
			"excerpts": [
			  "We present a GPU-based ray traversal algorithm that operates on compressed wide BVHs and maintains the traversal stack in a compressed format. Our method reduces the amount of memory traffic significantly, which translates to 1.9--2.1 improvement in incoherent ray traversal performance compared to the current state of the art. Furthermore, the memory consumption of our hierarchy is 35--60% of a typical uncompressed BVH."
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray Tracing of Incoherent Rays",
			"url": "https://www.researchgate.net/publication/220506625_Shallow_Bounding_Volume_Hierarchies_for_Fast_SIMD_Ray_Tracing_of_Incoherent_Rays",
			"excerpts": [
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006; Dammertz et al. 2008; Ernst and Greiner 2008]. Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ..."
			]
		  },
		  {
			"title": "Multi Bounding Volume Hierarchies",
			"url": "https://www.researchgate.net/publication/4375554_Multi_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "Wide BVHs. Ray traversal using wide BVHs is common with CPU SIMD ray traversal [Christensen et al. 2006;Dammertz et al. 2008; Ernst and Greiner 2008] . Here, multiple bounding volumes are tested simultaneously with single rays without the need for using ray packets [Benthin et al. 2007;Gunther et al. 2007]. ..."
			]
		  },
		  {
			"title": "Dynamic ray stream traversal | ACM Transactions on Graphics",
			"url": "https://dl.acm.org/doi/10.1145/2601097.2601222",
			"excerpts": [
			  "While each new generation of processors gets larger caches and more compute power, external memory bandwidth capabilities increase at a much lower pace. Additionally, processors are equipped with wide vector units that require low instruction level divergence to be efficiently utilized.",
			  "In order to exploit these trends for ray tracing, we present an alternative to traditional depth-first ray traversal that takes advantage of the available cache hierarchy, and provides high SIMD efficiency, while keeping memory bus traffic low."
			]
		  },
		  {
			"title": "Dynamic Ray Stream Traversal",
			"url": "http://cseweb.ucsd.edu/~ravir/274/15/papers/drst.pdf",
			"excerpts": [
			  "Divergence leads to underutilization since SIMD lanes will\nneed to be masked out. This is not a trait generally attributed to\ndepth-first traversal. Even when packets of rays are traced in a\nSIMD fashion, rays usually diverge quickly when incoherent, such\nas for diffuse interreflections, for example.",
			  "r algorithm is designed to\nhave a predictable memory access pattern with high data coherence,\nwhich substantially reduces the amount of memory bandwidth us-\nage in our tests.",
			  "ck-less ray traver-\nsal [Hughes and Lim 2009; Laine 2010; Hapala et al. 2011; Bar-\nringer and Akenine-Moller 2013] is a more recent endeavor that\nwas, at least initially, motivated by the high overhead of main-\ntaining a traversal stack on previous generations of G",
			  "ually, a stack is maintained that contains the next node to be pro-\ncessed during ray traversal. The technique has been combined with\nvarious forms of ray sorting and tracing of whole packets [Wald\net al. 2001] of rays to improve performance.",
			  " rays in the ray stream take the same path in the tree, which decreases\ndivergence and increases SIMD utilization.",
			  ".\nEmbree also includes a set of packet traversal kernels [Wald et al.\n2001], as well as hybrid kernels that starts with packets and\nswitches to single-ray traversal as utilization becomes low [Ben-\nthin et al. 2012]. ",
			  "s. The example renderer that supports\nthese kernels constitutes a total rewrite of the single-ray renderer us-\ning *ISPC* [Pharr and Mark 2012], which makes the entire renderer\nvectorized.",
			  "which makes the entire renderer\nvectorized. This makes it a bit difficult to compare performance di-\nrectly with our algorithm and single-ray traversal."
			]
		  },
		  {
			"title": "Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware: ACM Transactions on Architecture and Code Optimization: Vol 6, No 2",
			"url": "https://dl.acm.org/doi/10.1145/1543753.1543756",
			"excerpts": [
			  "Section Title: Dynamic warp formation: Efficient MIMD control flow on SIMD graphics hardware > Abstract\nContent:\nRecent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes.",
			  "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes."
			]
		  },
		  {
			"title": "Thread Block Compaction for Efficient SIMT Control Flow",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/wwlfung.hpca2011.pdf",
			"excerpts": [
			  "A (OpenCL) threads (work items) are issued to\nthe SIMT cores in a unit of work called a thread block\n(work group). Warps within a thread block can communi-\ncate through shared memory and quickly synchronize via\nbarriers. Thread block compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks. At a divergent branch, the\nwarps synchronize and their threads are compacted into\nnew warps according to the branch outcome of each thread.\nThe compacted warps then execute until the next branch or\nreconvergence point, where they synchronize again for fur-\nther compaction. Compaction of all the divergent threads\nafter they have reached the reconvergence point will re-\nstored their *original* warp grouping before the divergent\nbranch was e",
			  "lock compaction extends this sharing to\nexploit control flow locality among threads within a thread\nblock. Warps *within a thread block* share a block-wide re-\nconvergence stack for divergence handling instead of hav-\ning separate *per-warp* stacks.",
			  "namic warp formation (DWF) [9] regroups threads\nexecuting the same instruction into new warps to improve\nSIMD efficienc",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence.",
			  "ion results show that this compaction mechanism*\n*provides an average speedup of 22% over a baseline per-*\n*warp, stack-based reconvergence mechanism, and 17% ver-*\n*sus dynamic warp formation on a set of CUDA applications*\n*that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Thread block compaction for efficient SIMT control flow",
			"url": "https://ieeexplore.ieee.org/document/5749714/",
			"excerpts": [
			  "Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data cores to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22% over a baseline per-warp, stack-based reconvergence mechanism, and 17% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence."
			]
		  },
		  {
			"title": "Control Flow Management in Modern GPUs",
			"url": "https://arxiv.org/html/2407.02944v1",
			"excerpts": [
			  "[13] W. W. L. Fung and T. M. Aamodt, Thread block compaction for efficient simt control flow, in *International Symposium on High Performance Computer Architecture (HPCA)* , 2011",
			  " [14] W. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, Dynamic warp formation and scheduling for efficient gpu control flow, in *International Symposium on Microarchitecture (MICRO)* , 2007. "
			]
		  },
		  {
			"title": "A Scalable Multi-Path Microarchitecture for Efficient GPU ...",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/eltantawy.hpca2014.pdf",
			"excerpts": [
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "o improve SIMD units utilization for applications with un-\nstructured control flow behavior.",
			  "Evaluated on a set of\nbenchmarks with multi-path divergent control flow, our pro-\nposal achieves 32% speedup over conventional single-path",
			  "SIMT execution."
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://arxiv.org/html/2506.11273v1",
			"excerpts": [
			  "/2506.11273v1.bib12) ) used breadth-first packet traversal after a ray sorting step. They proposed the idea of sorting rays to reduce divergence in computation using a hash-based method for sorting the rays into coherent packets. T",
			  "For production rendering, not only the trace kernel but also shading might be limited by memory bandwidth. Therefore, Eisenacher et al . ( [2013](https://arxiv.org/html/2506.11273v1.bib11) ) proposed to sort termination points to improve shading performance. While this approach is designed for out-of-core path tracing, grouping shading calculations by a material also improves in-core performance for complex shaders. For highly detailed scenes, Hanika\net al . ( [2010](https://arxiv.org/html/2506.11273v1.bib15) ) proposed to use a two-level hierarchy combined with ray sorting to facilitate efficient on the fly micro-polygon tessellation. The rays are traversed through the top-level hierarchy, and they are repeatedly sorted to determine sets of rays traversing the same leaf nodes of the top-level hierarchy.",
			  "When coherence among rays exists, the packet traversal (Gunther\net al . , [2007](https://arxiv.org/html/2506.11273v1.bib14) ) exploits it by forcing a SIMD processing of a group of rays. This, on the other hand, increases inter-thread communication and synchronization. Furthermore, it assumes high ray coherence and is significantly slower than depth-first traversal for incoherent rays. Bikker ( [2012](https://arxiv.org/html/2506.11273v1.bib7) ) proposed a packet traversal algorithm that uses batching to improve data locality.",
			  "In a case when thread divergence occurs on GPU, the whole warp of threads is blocked until all its rays finish the traversal."
			]
		  },
		  {
			"title": "[PDF] Two-level ray tracing with reordering for highly complex scenes | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Two-level-ray-tracing-with-reordering-for-highly-Hanika-Keller/ee5673f3141a61924798d7642f06971dd41d871c",
			"excerpts": [
			  "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision Surfaces",
			  "Besides introducing an optimized method to determine axis aligned bounding boxes of Gregory patches restricted in the parametric domain, several techniques are introduced that accelerate the recursive subdivision process including stackless operation, efficient work distribution, and control flow optimizations.",
			  "TLDR"
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Proceedings of High Performance Graphics",
			"url": "https://dl.acm.org/doi/10.5555/2977336.2977343",
			"excerpts": [
			  "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time ... Stackless multi-BVH traversal for CPU, MIC and GPU ray tracing."
			]
		  },
		  {
			"title": "Extending GPU Ray-Tracing Units for Hierarchical Search ...",
			"url": "https://engineering.purdue.edu/tgrogers/publication/barnes-micro-2024/barnes-micro-2024.pdf",
			"excerpts": [
			  "Binder and A. Keller, Efficient Stackless Hierarchy Traversal on. GPUs with Backtracking in Constant Time, in Proceedings of High. Performance Graphics (HPG).Read more"
			]
		  },
		  {
			"title": "A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees",
			"url": "https://jcgt.org/published/0014/01/03/paper.pdf",
			"excerpts": [
			  "BINDER, N. AND KELLER, A. Efficient stackless hierarchy traversal on GPUs with back- tracking in constant time. In Proceedings of High ...Read more"
			]
		  },
		  {
			"title": "Efficient stackless hierarchy traversal on GPUs with backtracking in constant time | Research",
			"url": "https://research.nvidia.com/publication/2016-06_efficient-stackless-hierarchy-traversal-gpus-backtracking-constant-time",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling and use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or state memory. We show that the next node in such a traversal actually can be determined in constant time and state memory. In fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and outperforms the fastest stack-based algorithms on GPUs. In addition, it reduces memory access during traversal, making it a very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal on GPUs with ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/1c026ceb-0c54-4e20-9fd2-2ff77222894d/content",
			"excerpts": [
			  "The fastest acceleration schemes for ray tracing rely on traversing a bounding volume hierarchy (BVH) for efficient culling",
			  "nd use backtracking, which in the worst case may expose cost proportional to the depth of the hierarchy in either time or*\n*state memory",
			  "We show that the next node in such a traversal actually can be determined in constant time and state memory.",
			  "*fact, our newly proposed parallel software implementation requires only a few modifications of existing traversal methods and*\n*outperforms the fastest stack-based algorithms on GPUs.",
			  " addition, it reduces memory access during traversal, making it a*\n*very attractive building block for ray tracing hardware."
			]
		  },
		  {
			"title": "Efficient Stackless Hierarchy Traversal with Backtracking in ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2016/2016-HPG-Binder-StacklessTraversalPerfectHash.pdf",
			"excerpts": [
			  "Efficient Stackless Hierarchy Traversal with Backtracking in Constant Time",
			  "Results: Performance in M rays/s, NVIDIA Titan X, for Primary/Shadow/Diffuse Rays"
			]
		  },
		  {
			"title": "Intel 64 and IA-32 Architectures Software Developer's Manual ...",
			"url": "https://kib.kiev.ua/x86docs/Intel/SDMs/326018-062.pdf",
			"excerpts": [
			  "Page 1. Intel 64 and IA-32 Architectures. Software Developer's Manual ... _mm512_i64gather_epi64( __m512i vdx, void * base ... Intel C/C++ Compiler Intrinsic ..."
			]
		  },
		  {
			"title": "\n\tGather of byte/word with avx2 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Gather-of-byte-word-with-avx2/td-p/921687",
			"excerpts": [
			  "1) Use _mm256_i32gather_epi32. We would fetch an extra 16-bits that we do not want for each voxel, and then either mask off the extra bits or ...Read more"
			]
		  },
		  {
			"title": "_mm_i32gather_epi32, _mm256_i32gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-1F275401-A760-49B1-944A-B02C075514D8.htm",
			"excerpts": [
			  "Gathers 2/4 doubleword values from memory referenced by the given base address, dword indices, and scale. The corresponding Intel AVX2 instruction is"
			]
		  },
		  {
			"title": "_mm_i64gather_epi32, _mm256_i64gather_epi32",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/compiler_c/main_cls/intref_cls/common/intref_avx2_mm256_i64gather_epi32.htm",
			"excerpts": [
			  "Gather 2/4 doubleword values from memory referenced by the given base address, qword indices and scale. The corresponding Intel AVX2 instruction is VPGATHERQD."
			]
		  },
		  {
			"title": "An introduction to Arm Scalable Vector Extensions",
			"url": "https://epicure-hpc.eu/wp-content/uploads/2025/03/SVE_Vectorization_Ricardo_Fonseca.pdf",
			"excerpts": [
			  "SVE gather / scatter operations.  Gather operations.  Gather scalar values ...  https://developer.arm.com/documentation/101458/2404/Optimize.  Arm ...Read more"
			]
		  },
		  {
			"title": "Mirror of Intel Intrinsics Guide",
			"url": "https://www.laruence.com/sse/",
			"excerpts": [
			  "Intel. Intrinsics Guide. Technologies. MMX. SSE. SSE2. SSE3. SSSE3. SSE4.1. SSE4.2 ... This intrinsic is provided for conversion between little and big endian ...Read more"
			]
		  },
		  {
			"title": "Intrinsics for Integer Gather and Scatter Operations",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-35D298CC-B89B-4B38-856B-FCD0EBB3AA23.htm",
			"excerpts": [
			  "**_mm512_i32gather_epi32**\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i32gather_epi32**\nSection Title: Intrinsics for Integer Gather and Scatter Operations\nContent:\nGather int32 from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination.",
			  "**_mm512_mask_i64gather_epi64**\nGathers int64 from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale ). Gathered elements are merged into destination using writemask k (elements are copied from src when the corresponding mask bit is not set).",
			  "**_mm512_i32scatter_epi32**\nScatters int32 from a into memory using 32-bit indices. 32-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ).",
			  "**_mm512_mask_i32scatter_epi64**\nScatters int64 from a into memory using 32-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 32-bit element in vindex (each index is scaled by the factor in scale ) subject to mask k (elements are not stored when the corresponding mask bit is not set).",
			  "**_mm512_i64scatter_epi64**\nScatters int64 from a into memory using 64-bit indices. 64-bit elements are stored at addresses starting at base_addr and offset by each 64-bit element in vindex (each index is scaled by the factor in scale )."
			]
		  },
		  {
			"title": "Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions",
			"url": "http://www.physics.ntua.gr/~konstant/HetCluster/intel2021.7/HPC/cpp_compiler/cpp_compiler_classic_dev_guide/GUID-D77C7B04-9104-4AFE-A29B-005683AC9F78.html",
			"excerpts": [
			  "\nThe prototypes for Intel Advanced Vector Extensions 512 (Intel AVX-512) intrinsics are located in the zmmintrin.h header file.\nTo u",
			  "To use these intrinsics, include the immintrin.h file as follows:",
			  "ement.\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Data Types for Intel AVX-512 Intrinsics\nC",
			  "s:\nIntel AVX-512 intrinsics have vector variants that use __m128 , __m128i , __m128d , __m256 , __m256i , __m256d , __m512 , __m512i , and __m512d data types.",
			  ".\nSection Title: Intrinsics for Intel Advanced Vector Extensions 512 (Intel AVX-512) Instructions > Naming and Usage Syntax\nCon"
			]
		  },
		  {
			"title": "algorithm - What do you do without fast gather and scatter in AVX2 instructions? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/51128005/what-do-you-do-without-fast-gather-and-scatter-in-avx2-instructions",
			"excerpts": [
			  "**AVX2 has gathers (not scatters), but they're only fast on Skylake and newer** . They're ok on Broadwell, slowish on Haswell, and slow on AMD. (Like one per 12 clocks for Ryzen's `vpgatherqq` ). See http://agner.org/optimize/ and other performance links in [the x86 tag wiki](https://stackoverflow.com/tags/x86/info) .\nIntel's optimization manual has a small section on manual gather / scatter (using insert/extract or `movhps` ) vs. hardware instructions, possibly worth reading. In this case where the indices are runtime variables (not a constant stride or something), I think Skylake can benefit from AVX2 gather instructions here",
			  "**extract indices, manually gather into a vector with `vmovq` / `vmovhps` for a SIMD `vpor` , then scatter back with `vmovq` / `vmovhps`** .Just like a HW gather/scatter, **correctness requires that all indices are unique** , so you'll want to use one of the above options until you get to that point in your algo. (vector conflict detection + fallback would not be worth the cost vs. just always extracting to scalar: [Fallback implementation for conflict detection in AVX2](https://stackoverflow.com/questions/44843518/fallback-implementation-for-conflict-detection-in-avx2) ).See [selectively xor-ing elements of a list with AVX2 instructions](https://stackoverflow.com/questions/50583718/selectively-xor-ing-elements-of-a-list-with-avx2-instructions) for an intrinsics version.",
			  "**AVX2 `vpgatherqq` for the gather ( `_mm256_i64gather_epi64(sieveX, srli_result, 8)` ), then extract indices and manually scatter.** So it's exactly like the manual gather / manual scatter, except you replace the manual gather with an AVX2 hardware gather. (Two 128-bit gathers cost more than one 256-bit gather, so you would want to take the instruction-level parallelism hit and gather into a single 256-bit register).",
			  "Possibly a win on Skylake (where `vpgatherqq ymm` is 4 uops / 4c throughput, plus 1 uop of setup), but not even Broadwell (9 uops, one per 6c throughput) and definitely not Haswell (22 uops / 9c throughput). You do need the indices in scalar registers anyway, so you're *only* saving the manual-gather part of the work. That's pretty cheap.",
			  "**manual gather/scatter: 20 uops, 5 cycles of front-end throughput** (Haswell / BDW / Skylake). Also good on Ryzen.",
			  "**Skylake AVX2 gather / manual scatter: Total = 18 uops, 4.5 cycles of front-end throughput.** (Worse on any earlier uarch or on AMD).",
			  "\nvextracti128 indices (1 uop for port 5)",
			  "2x vmovq extract (2p0)",
			  "2x vpextrq (4 = 2p0 2p5)",
			  "`vpcmpeqd ymm0,ymm0,ymm0` create an all-ones mask for `vpgatherqq` (p015)",
			  "`vpgatherqq ymm1, [rdi + ymm2*8], ymm0` 4 uops for some ports.",
			  "`vpor ymm` (p015)",
			  "vextracti128 on the OR result (p5)\n",
			  "2x vmovq store (2x 1 micro-fused uop, 2p23 + 2p4). Note no port7, we're using indexed stores.",
			  "2x vmovhps store (2x 1 micro-fused uop, 2p23 + 2p4)."
			]
		  },
		  {
			"title": "c++ - AVX2 Gather Instruction Usage Details - Stack Overflow",
			"url": "https://stackoverflow.com/questions/58832024/avx2-gather-instruction-usage-details",
			"excerpts": [
			  "The offsets in `vindex` are in bytes. Therefore, you gather 32-bit integer values from addresses `{arr, arr+2, arr+4, ...}` .",
			  "Either change these indexes from `{0,2,4...}` to `{0,8,16,...}` , or update the scale factor as:\nThis prints out the expected values."
			]
		  },
		  {
			"title": "Arm C Language Extensions",
			"url": "https://arm-software.github.io/acle/main/acle.html",
			"excerpts": [
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B)",
			  "\nThe SVE ACLE intrinsics have the form:\nwhere the individual parts are as follows:\n**base**\nFor most intrinsics this is the lower-case name of an SVE\ninstruction, but with some adjustments:The most common change is to drop `F` , `S` and `U` if they\nstand for floating-point, signed and unsigned respectively,\nin cases where this would duplicate information in the type\nsuffixes below.Simple non-extending loads and non-truncating stores drop the\nsize suffix ( `B` , `H` , `W` or `D` ), which would always duplicate\ninformation in the suffixes.Conversely, extending loads always specify an explicit extension\ntype, since this information is not available in the suffixes.\nA sign-extending load has the same base as the architectural\ninstruction (for instance, `ld1sb` ) while a zero-extending load replaces\nthe `s` with a `u` (for instance, `ld1ub` for a zero-extending `LD1B` ).\nThus [`svld1ub_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1ub_u32) zero-extends 8-bit data to a vector of `uint32_t` s while [`svld1sb_u32`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svld1sb_u32) sign-extends 8-bit data to a vector of `uint32_t` s.",
			  "| LD1D (vector plus immediate) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather%5B) |",
			  "| LD1D (scalar plus vector) | [`svld1_gather`](https://developer.arm.com/architectures/instruction-sets/intrinsics/:@navigationhierarchieselementbitsize=%5B64%5D&q=svld1_gather_%5B) ",
			  "| ST1D (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) |",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| ST1W (vector plus immediate) | [`svst1_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1_scatter%5B) , [`svst1w_scatter`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svst1w_scatter%5B) |\n",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |",
			  "| COMPACT | [`svcompact`](https://developer.arm.com/architectures/instruction-sets/intrinsics/=svcompact) |"
			]
		  },
		  {
			"title": "Arm Scalable Vector Extension and application to Machine ...",
			"url": "https://developer.arm.com/-/media/Arm%20Developer%20Community/PDF/Arm-scalable-vector-extensions-and-application-to-machine-learning.pdf",
			"excerpts": [
			  "the function svld1_gather_u32base_offset_s32 , with signature\nsvint32_t svld1_gather_u32base_offset_s32(svbool_t pg, svuint32_t bases, int64_t\noffset)\nis a *gather load* ( ld1_gather ) of *signed 32-bit integer* ( _s32 ) from a vector of *unsigned 32-bit integer* base\naddresses ( _u32base ) plus an *offset in bytes* ( _offset ).",
			  "The SVE ACLE are compatible with C++ overloading and C _Generic association, so that the names\ncan be contracted removing those parts that can be derived from the arguments types.",
			  "The SVE ACLE (or ACLE hereafter) is a set of functions and types that exposes the vectorization capabilities of\nSVE at C/C++ level.",
			  "They introduce a set of *size-less* types and *intrinsic functions* that a C/C++ compiler can directly convert into\nSVE assembly.",
			  "An additional svbool_t type is defined to represent predicates for masking operations.",
			  "SVE intrinsic functions",
			  "The naming convention of the intrinsic functions in the SVE ACLE is described in detail in section 4 of the SVE\nACLE document (ARM limited 2017b).",
			  "Most of them are in the form: svbase[_disambiguator][_type0][_type1]...[_predication] .",
			  "For example, the name of the intrinsic svadd_n_u16_m , with signature svuint16_t svadd_n_u16_m(svbool_t\npg, svuint16_t op1, uint16_t op1) , describes a vector *addition* ( add ) of *unsigned 16-bit integer* ( u16 ),\nwhere one of the arguments is a scalar ( _n ) and the predication mode is *merging* ( _m ).",
			  "Some of the functions, like loads and stores, have a different form for the names, with additional tokens that\nspecify the addressing mode.",
			  "All the examples of this document use the short form. For simplicity, we also assume no aliasing, meaning that"
			]
		  },
		  {
			"title": "Arm C Language Extensions for SVE - Version 00bet6",
			"url": "https://rci.stonybrook.edu/sites/default/files/documents/acle_sve_100987_0000_06_en.pdf",
			"excerpts": [
			  "COMPACT: Compact vector and fill with zero",
			  "These functions concatenate the active elements of the input vector, filling any remaining elements with\nzero.",
			  "6.23. Predicate creation",
			  "6.23.1. PTRUE: Return an all-true predicate for a given pattern",
			  "These functions return an all-true predicate for a particular vector pattern and element size. When an\nelement has more than one predicate bit associated with it, only the lowest of those bits is ever true.",
			  "There are two forms: one with a _pat suffix that takes an explicit vector pattern and one without a _pat\nsuffix in which the pattern is implicitly SV_ALL .",
			  "svbool_t **svptrue_b8** ()",
			  "svbool_t **svptrue_b16** ()",
			  "svbool_t **svptrue_b32** ()",
			  "svbool_t **svptrue_b64** ()"
			]
		  },
		  {
			"title": "ARM's Scalable Vector Extensions: A Critical Look at SVE2 ...",
			"url": "https://gist.github.com/zingaburga/805669eb891c820bd220418ee3f0d6bd",
			"excerpts": [
			  "Under ACLE, NEON  SVE value transfer must go through memory. Interestingly ... SVE adds support for gather/scatter operations, which helps vectorize ..."
			]
		  },
		  {
			"title": "_mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32",
			"url": "http://portal.nacad.ufrj.br/online/intel/compiler_c/common/core/GUID-8B147603-6A6A-4F23-8529-1609A13AB784.htm",
			"excerpts": [
			  "extern __m512i __cdecl _mm512_i32gather_epi32(_m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "Section Title: _mm512_i32[ext]gather_epi32/ _mm512_mask_i32[ext]gather_epi32 > Syntax",
			  "tent:\n| extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "extern __m512i __cdecl _mm512_mask_i32gather_epi32(_m512 v1_old, __mmask16 k1, __m512i index, void const* mv, _MM_UPCONV_EPI32_NONE, int scale, _MM_HINT_NONE);",
			  "The non-masked variant of the intrinsic is equivalent to the masked variant with full mask ( k1 =0xffff).",
			  ":\nGather int32 vector with int32 indices. Corresponding instruction is VPGATHERDD . This intrinsic only applies to Intel Many\nIntegrated Core Architecture (Intel MIC Architecture).\n",
			  "Up-converts a set of 16 memory locations pointed by base address mv and int32 index vector index with scale scale , and gathers them into a int32 vector.",
			  "The resulting vector for the masked variant is populated by elements for which the corresponding bit in the writemask vector k1 is set. The remaining elements of the resulting vector for the masked variant is populated by corresponding elements from v1_old .",
			  "Returns the result of the up-convert load operation."
			]
		  },
		  {
			"title": "x86 Intrinsics Cheat Sheet",
			"url": "https://db.in.tum.de/~finis/x86%20intrinsics%20cheat%20sheet%20v1.0.pdf",
			"excerpts": [
			  "AVX2\n**mi** **i64gather_epi32**\n( **i*** ptr, **mi** a, **i** s)\n**FOR** j := 0 to 1;\ni := j*32\nm := j*64\n**dst** [i+31:i] :=\n*(ptr + a[m+63:m]*s])\n**dst** [MAX:64] := 0\nMask Gather\nmask_ *i32* / *i64* gather\nepi32-64,ps/d\n**NOTE:** Same as gather but takes\nan additional mask and src\nregister. Each element is only\ngathered if the highest\ncorresponding bit in the mask is\nset. Otherwise it is copied from\nsrc. Memory does not need to\nbe aligned.",
			  "AVX2\n**mi mask_i64gather_epi32** ( **mi** src,\n**i*** ptr, **mi** a, **mi** mask, **i32** s)\n**FOR** j := 0 to 1; i:=j*32;m:=j*64\n**IF** mask[i+31]\n**dst** [i+31:i]:=*(ptr+a[i+63:i]*s)\nmask[i+31] := 0\n**ELSE**\n**dst** [i+31:i] := src[i+31:i]\nmask[MAX:64] := 0\n**dst** [MAX:64] := 0\n256bit Insert\ninsertf128\nsi256,ps/d\n**m** **insertf128_ps** ( **m** a, **m** b, **ii** i)\n**dst** [255:0] := a[255:0]\nsel := i*128\n**dst** [sel+15:sel]:=b[127:0]"
			]
		  },
		  {
			"title": "Filtering a Vector with SIMD Instructions (AVX-2 and AVX-512) | Quickwit",
			"url": "https://quickwit.io/blog/simd-range",
			"excerpts": [
			  "Let's start with `compact` .\nAVX2 does not exactly have an instruction for this. In the 128-bits world, [`PSHUFB`](https://www.felixcloutier.com/x86/pshufb.html#:~:text=PSHUFB%20performs%20in%2Dplace%20shuffles,leaving%20the%20shuffle%20mask%20unaffected.) is a powerful instruction that\nlets you apply a permutation over the bytes of your register.",
			  "The equivalent instruction exists and is called `vpshufd` , but there is a catch: it only\napplies two disjoint permutations within the two 128-bits lanes, which is perfectly useless to us.",
			  "This is a very common pattern in AVX2 instructions. Instructions crossing that dreaded 128bit-lane are seldom.",
			  "\nFortunately, applying a permutation over `u32s` (which is what we need) is actually possible,\nvia the `VPERMPS` instruction [__mm256_permutevar8x32_epi32](https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-vector-extensions-2/intrinsics-for-permute-operations/mm256-permutevar8x32-epi32.html) .",
			  "Then I did what every sane engineer should do. I asked [Twitter](https://twitter.com/fulmicoton/status/1539534316405161984) (Ok I am lying a bit, at the time of the tweet I was playing with SSE2).",
			  "a single byte. The instruction is called `VMOVMSKPS` . I could not find it because it is presented as a floating point instruction to extract the sign of a bunch of 32-bits floats.\n"
			]
		  },
		  {
			"title": "On the usage of the Arm C Language Extensions for a High ...",
			"url": "https://hal.science/hal-03029933v1/document",
			"excerpts": [
			  "In our case, we\nuse the following code:\nauto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "The indices in this resulting vector will serve as the base for\naccessing the following 124 points of these elements.",
			  "the gath-\nering step of our kernel, we first need to gather indices of\nthe first point of each element considered.",
			  "At order 4, each\nelement is composed of 125 points.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "SVE provides the svindex_u32() intrinsics to\nfill a vector with multiples of a given value.",
			  "In our case, we\nuse the following code:",
			  "auto\nv s t r i d e s\n= svindex u32 (0 u ,\n125u ) ;",
			  "At iteration i , the first element of the vector is at position\ni*svcntw()*125 and we need to duplicate it and add the\nvstrides values to obtain the indices of the first point of\neach element in the vector:"
			]
		  },
		  {
			"title": "Introduction to SVE",
			"url": "https://documentation-service.arm.com/static/67ab35a4091bfc3e0a9478b5?token=",
			"excerpts": [
			  "The ACLE (Arm C Language Extension) for SVE defines which SVE instruction functions\nare available, their parameters and what they do.",
			  " To use the ACLE intrinsics,\nyou must include the header file arm_sve.h, which contains a list of vector types and instruction\nfunctions (for SVE) that can be used in C/C++.",
			  "The following example C code has been manually optimized with SVE intrinsics:\n//intrinsic_example.c\n\\ <arm_sve.h>\nsvuint64_t uaddlb_array(svuint32_t Zs1, svuint32_t Zs2)\n{\n// widening add of even elements\nsvuint64_t result = svaddlb(Zs1, Zs2);\nreturn result;\n}"
			]
		  },
		  {
			"title": "\n\tAgner's tables show the - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-ISA-Extensions/Throughput-MUL-FMA-Broadwell/m-p/1151730/highlight/true",
			"excerpts": [
			  "Agner's tables show the throughput for sequences of independent instructions, and the latency for sequences of dependent instructions."
			]
		  },
		  {
			"title": "Release Notes for Intel Intrinsics Guide",
			"url": "https://www.intel.com/content/www/us/en/developer/articles/release-notes/intrinsics-guide-release-notes.html",
			"excerpts": [
			  "Removed extended gather/scatter intrinsics."
			]
		  },
		  {
			"title": "Surprising new feature in AMD Ryzen 3000 | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24302057",
			"excerpts": [
			  " But the scatter/gather instructions do random access memory operations. You have one SIMD register with a 8 (or whatever the width is) indexes to be applied to a base address in a scalar register, and the hardware then goes and does 8 separate memory operations on your behalf, packing the results into a SIMD register at the end.",
			  "That has to hit the cache 8 times in the general case. It's extremely expensive as a single instruction, though faster than running scalar code to do the same thing.",
			  "I looked Agner's tables, and was curious how Intel fared with it. All numbers are reciprocal throughput. So how many cycles per instruction in throughput. zen 2 has it's gather variants mostly 9 and 6 cycles and one variant with 16. Broadwell has only 6,7 and 5 cycles.",
			  "Skylake has mostly 4 and 2 and one variant with 5.",
			  "Now I was surprised by Agners figures for zen2 LOOP and CALL which both have reciprocal throughput of 2. Being equal to doing with just normal jump instructions."
			]
		  },
		  {
			"title": "4. Instruction tables",
			"url": "https://www.agner.org/optimize/instruction_tables.pdf",
			"excerpts": [
			  "VPGATHERDD\nx,[r+s*x],x\n24\n5\nP0123\nAVX2",
			  "VPGATHERDD\ny,[r+s*y],y\n42\n8",
			  "VPGATHERQQ\nx,[r+s*x],x\n18\n4",
			  "VPGATHERQQ\ny,[r+s*y],y\n24\n5",
			  "VPSCATTERDD\n[r+s*x]{k},x\n27\n6",
			  "VPSCATTERQQ\n[r+s*z]{k},z\n48\n12",
			  "VPCOMPRESSD/Q\nv{k},v\n2\n2",
			  "VPEXPANDB/W/D/Q\nx{k},x\n2\n2"
			]
		  },
		  {
			"title": "(PDF) Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://www.academia.edu/145129609/Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Section Title: Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads\nContent:\n[Karthik Sankaranarayanan](https://independent.academia.edu/KarthikSankaranarayanan)\n2020, ArXiv\nvisibility\n\ndescription See full PDF download Download PDF\nbookmark Save to Library share Share\nclose"
			]
		  },
		  {
			"title": "Prefetching for complex memory access patterns",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-923.pdf",
			"excerpts": [
			  "This thesis makes three contributions. I first contribute an automated software prefetch- ing compiler technique to insert high-performance prefetches into ...Read more"
			]
		  },
		  {
			"title": "On Reusing the Results of Pre-Executed Instructions in a ...",
			"url": "https://dl.acm.org/doi/abs/10.1109/L-CA.2005.1",
			"excerpts": [
			  "Previous research on runahead execution took it for granted as a prefetch-only technique. Even though the results of instructions independent of an L2 miss ..."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/pdf/2505.21669",
			"excerpts": [
			  "Hardware prefetcher designs have been previously created with pointer-chasing access patterns in mind. Some techniques compute and store jump ...",
			  "oth et al. describe a prefetching technique in [ **50** ] that utilizes dependency chains to\ndetermine the shape of a linked data structure node in hardware, and issue requests ac-\ncordingly. While this approach requires no modification of the executable, it cannot learn\nthe full structure of an object without observing accesses to all children. Additionally, the\nauthors limit their prefetcher to only a single node ahead, meaning prefetches are only\ntriggered when the core issues new loads, *and* blocks must be returned from the memory\nsystem before new prefetches can be issuedleading to significantly worse performance\nas effective memory access latencies incr",
			  " [ **37** ] build on dependence based prefetching (DBP) as described in [ **50** ] to en-\nable *timely* prefetches of children; this is needed as increased memory access times require\nprefetches to be issued earlier to avoid stalls."
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references).",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "Observing the backward slice shown in Figure 3a, we see that\nthe one cycle in the graph is comprised of a single instruction\n0x6cf , *i.e.* , the stride address increment, and that it is the only\nloop-carried dependence in this backward slice.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "Helper Without Threads: Customized\nPrefetching for Delinquent Irregular Loads",
			  "We duplicate the backward slice code\nand assign new registers to it. By analogy, this code is the carrot\nand the main computation is the horse.",
			  "Prior to the entry into\nthe loop, the carrot is first extended *k* iterations ahead of the horse.\nWe call this phase in the dynamic execution the *head start* phase.",
			  "After the entry into the loop, the carrot locks steps with the horse\nand stays a constant *k* iterations ahead. We call this phase in the\ndynamic execution the *stay ahead* phase.",
			  "During the last *k* iterations\nof the loop, the carrot ceases to stay ahead and merges with the\nhorse. We call this phase of dynamic execution the *join* phase.",
			  "**4**",
			  "**M** **ETHOD**",
			  "In the previous section, we explained the problem of memory-\nbound DILs through a hash table example and outlined the\nchallenges in implementing a prefetcher with helper threads",
			  "Here,\nwe will outline our approach to a solution, with a reminder that we\nwant to create a prefetcher implementation without threads.",
			  "We exclude\nsuch scenarios by design for two reasons: first, such situations\nare rare and second, prefetcher complexity increases tremendously\nin such cases.",
			  "To see why, let us consider the example of the\nbinary tree where both the paths are equally likely. If we want to\nprefetch *k* iterations ahead, then there are 2 *k* possible addresses\nto prefetch.",
			  "We have the option of either prefetching all of those\naddresses or implementing a software-based branch predictor to\nselect one of the addresses to prefetch."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "Content:\nAugust 2020\nDOI: [10.48550/arXiv.2009.00202](https://doi.org/10.48550/arXiv.2009.00202)",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Improved Prefetching Techniques for Linked Data Structures",
			"url": "https://arxiv.org/html/2505.21669v1",
			"excerpts": [
			  "[52] Sankaranarayanan, K., Lin, C.-K., and Chinya, G. N. Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads. ArXiv (Sept. 2020).",
			  "These two benchmarks ( bintree_dfs and bintree_bfs ) perform common traversals of binary tree data structures to compute a sum.",
			  "The DFS benchmark is implemented recursively, while the BFS benchmark uses a std::queue from the C++ STL.",
			  "This tree is staticonly lookups are performed.",
			  "In [ [61](https://arxiv.org/html/2505.21669v1.bib61) ] , Wang et al. describe a prefetch engine that utilizes hints from a compiler analysis to inform issued requests.",
			  "Notably, the analysis detects common cases of recursive pointers as used in linked-list traversals, which hardware then exploits to issue prefetches of the LDS up to six levels deep.",
			  "However, layout information is not provided to the hardware prefetch engineit instead speculates on values in a cache block being pointers, falling victim to the same cache pollution flaws as other CDPs.",
			  "Nodes are also assumed to be smaller than two cache blocks, which may not hold across real-world applications.",
			  "The researchers also state the technique does not perform well on trees.",
			  "Most importantly, prefetch requests for an LDS are issued sequentially, and thus performance degrades with increased effective memory access times."
			]
		  },
		  {
			"title": "(PDF) On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor",
			"url": "https://www.academia.edu/126501330/On_Reusing_the_Results_of_Pre_Executed_Instructions_in_a_Runahead_Execution_Processor",
			"excerpts": [
			  "Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs.",
			  "runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient.",
			  "In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance."
			]
		  },
		  {
			"title": "Pointer Cache Assisted Prefetching - Computer Science",
			"url": "https://cseweb.ucsd.edu/~calder/papers/MICRO-02-PCache.pdf",
			"excerpts": [
			  ".\nSpeculative precomputation [6] works by identifying the\nsmall number of static loads, known as delinquent loads, that\nare responsible for the vast majority of memory stall cycles.",
			  "\nPrecomputation slices (p-slices), sequences of dependent in-\nstructions which, when executed, produce the address of a\nfuture delinquent load, are extracted from the program be-\ning accelerated. ",
			  " When an instruction in the non-speculative\nthread that has been identified as a trigger instruction reaches\nsome point in the pipeline (typically commit or rename),\nthe corresponding p-slice is spawned into an available SMT\nthread context.\n",
			  "Speculative slices [28] focus largely on the use of precom-\nputation to predict future branch outcomes and to correlate\npredictions to future branch instances in the non-speculative\nthread, but they also support load prefetching.",
			  "\nSoftware controlled pre-execution [13] focuses on the use\nof specialized, compiler inserted code that is executed in\n",
			  "e trace to extract data reference sequences that fre-\nquently repeat in the same order. At this point, the system\ninserts prefetch instructions to detect and prefetch these fre-\nquent data references.",
			  "The sampling and optimization are\ndone dynamically at runtime with very low overhead.\n*",
			  "The Pointer Cache holds mappings between heap point-\ners and the address of the heap object they point to.",
			  "\nThe primary function of the pointer cache is to break the\nserial dependence chains in pointer chasing code. ",
			  ". When one\nload depends on the data loaded by another, a cache miss by\nthe first load forces the second load to stall until the first load\ncompletes.",
			  "When executing a long sequence of such depen-\ndent pointer-chasing loads, instructions can only be executed\nat the speed of the serial accesses to memory.",
			  "*\nOnly pointer loads are candidates to be inserted into the\npointer cache.",
			  "The\nprograms static instructions are analyzed in the reverse order\nof execution from a delinquent load, building up a slice of in-\nstructions the load is directly and indirectly dependent upon.",
			  "Slice construction terminates when an-\nalyzing an instruction far enough from the delinquent load\nthat a spawned thread can provide a timely prefetch, or when\nfurther analysis will add additional instructions to the slice\nwithout providing further performance benefits.",
			  ". In this form,\na slice consists of a sequence of instructions in the order they\nwere analyzed.",
			  "The single path slices constructed in this work are trig-",
			  "Jump pointers are a software technique for prefetching\nlinked data structures.",
			  "Artificial jump pointers are extra\npointers stored into an object that point to other objects some\ndistance ahead in the traversal order.",
			  "Natural jump pointers are existing pointers in the\ndata structure used for prefetching.",
			  "These techniques were introduced by\nLuk and Mowry [12] and refined in [11] and [17].",
			  "Chilimbi and Hirzel [4] proposed an automated\nsoftware approach based on correlation. Their scheme first\ngathers a data reference profile via sampling. Next, they pro-",
			  "Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching. This paper proposes the use of a pointer ...Read more",
			  "However, traditional prefetching techniques have diffi- culty with sequences of irregular accesses. A common ex- ample of this type of access is pointer chains, ..."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Speculative precomputation: long-range prefetching of delinquentloads",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Deep-Learning-Driven Prefetching for Far Memory",
			"url": "https://arxiv.org/html/2506.00384v1",
			"excerpts": [
			  "Section Title: Deep-Learning-Driven Prefetching for Far Memory > 1. Introduction",
			  "Many data-center workloads including graph processing ( [PageRank ,](https://arxiv.org/html/2506.00384v1.bib25) ; [han2024graph ,](https://arxiv.org/html/2506.00384v1.bib20) ) , tree and index structures ( [guttman1984r ,](https://arxiv.org/html/2506.00384v1.bib19) ; [gusfield1997algorithms ,](https://arxiv.org/html/2506.00384v1.bib18) ) , pointer chasing ( [hsieh2017implementing ,](https://arxiv.org/html/2506.00384v1.bib24) ) , and recursive data structures ( [harold2004xml ,](https://arxiv.org/html/2506.00384v1.bib21) ) exhibit memory access patterns that defy rule-based prefetching.",
			  "If these access patterns could be learned and predicted accurately, far-memory systems could proactively fetch data and mitigate the performance penalties associated with remote access, even in the absence of new hardware.",
			  "Figure 1. FarSight Achieving Three Key Goals Together. FarSight, FastSwap ( [fastswap ,](https://arxiv.org/html/2506.00384v1.bib2) ) , and Hermit ( [hermit ,](https://arxiv.org/html/2506.00384v1.bib35) ) are far-memory systems that run in the Linux kernel. Voyager ( [voyager ,](https://arxiv.org/html/2506.00384v1.bib39) ) , Hashemi etal. ( [pmlr-v80-hashemi18a ,](https://arxiv.org/html/2506.00384v1.bib22) ) , and Twilight ( [duong2024twilight ,](https://arxiv.org/html/2506.00384v1.bib15) ) are micro-architecture CPU cache prefetchers implemented in simulation or with offline traces.",
			  "Content:"
			]
		  },
		  {
			"title": "An Event-Triggered Programmable Prefetcher for Irregular ...",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/programmableprefetcher.pdf",
			"excerpts": [
			  "Ainsworth and T. M. Jones. Graph prefetching using data structure knowledge. In ICS, 2016. [2] S. Ainsworth and T. M. Jones. Software prefetching for indirect ...Read more"
			]
		  },
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads ...",
			"url": "https://www.bohrium.com/paper-details/helper-without-threads-customized-prefetching-for-delinquent-irregular-loads/867745821129441724-108609",
			"excerpts": [
			  "Download the full PDF of Helper Without Threads: Customized Prefetching for Delinquent. Includes comprehensive summary, implementation ..."
			]
		  },
		  {
			"title": "[PDF] Speculative precomputation: long-range prefetching ...",
			"url": "https://www.semanticscholar.org/paper/Speculative-precomputation%3A-long-range-prefetching-Collins-Wang/cd42d31aa8f4d07a41556ee4640cb47d3401b9ef",
			"excerpts": [
			  "This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of ..."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~tmj32/papers/docs/ainsworth19-tocs.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automatically generate software prefetches for indirect\nmemory accesses, a special class of irregular memory accesses often seen in high-performance workloads.",
			  "Across a set of memory-bound benchmarks, our automated pass achieves average\nspeedups of 1.3  for an Intel Haswell processor, 1.1  for both an ARM Cortex-A57 and Qualcomm Kryo,\n1.2  for a Cortex-72 and an Intel Kaby Lake, and 1.35  for an Intel Xeon Phi Knights Landing, each of which\nis an out-of-order core, and performance improvements of 2.1  and 2.7  for the in-order ARM Cortex-A53\nand first generation Intel Xeon Phi.",
			  "Hardware prefetchers in real systems focus on stride patterns [ 10 , 12 , 18 ,\n37 , 40 ]. These pick up and predict regular access patterns, such as those in dense-matrix and array\niteration, based on observation of previous addresses being accessed."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses | Department of Computer Science and Technology",
			"url": "https://www.cst.cam.ac.uk/blog/tmj32/software-prefetching-indirect-memory-accesses",
			"excerpts": [
			  "In the paper we create a compiler pass that will automatically identify opportunities to insert prefetches where we find these memory-indirect accesses.",
			  "One of these is that there must be an induction variable within the transitive closure of the source operand (which, for a load, is the operand that calculates the address to load from).",
			  "This means we search backwards through the data dependence graph, starting at the load, until we find this induction variable.",
			  "When we have identified loads that need to be prefetched then we duplicate all of the necessary computation to calculate the address and insert the software prefetch instructions.",
			  "We also have to add code around any loads that are part of this address computation to prevent them from causing errors at runtime if they calculate an invalid address, for example running beyond the end of an array.",
			  "There are more details in the paper including information about how we schedule the prefetches so that data is available immediately before being used."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf",
			"excerpts": [
			  "This paper develops a novel compiler pass to automat-\nically generate software prefetches for indirect memory\naccesses, a special class of irregular memory accesses of-\nten seen in high-performance workloads. We evaluate this\nacross a wide set of systems, all of which gain benefit from\nthe technique. We then evaluate the extent to which good\nprefetch instructions are architecture dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Categ",
			  " dependent. Across a set\nof memory-bound benchmarks, our automated pass achieves\naverage speedups of 1.3 ** and 1.1 ** for an Intel Haswell pro-\ncessor and an ARM Cortex-A57, both out-of-order cores,\nand performance improvements of 2.1 ** and 2.7 ** for the\nin-order ARM Cortex-A53 and Intel Xeon Phi.\n***Cat"
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective: ACM Transactions on Computer Systems: Vol 36, No 3",
			"url": "https://dl.acm.org/doi/10.1145/3319393",
			"excerpts": [
			  "CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being required.",
			  ")Indirect memory accesses have irregular access patterns that limit the performance\nof conventional software and hardware-based prefetchers. To address this problem,\nwe propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect\nmemory ...",
			  "Software prefetching for indirect memory accesses\")CGO '17: Proceedings of the 2017 International Symposium on Code Generation and OptimizationMany modern data processing and HPC workloads are heavily memory-latency bound. A\ntempting proposition to solve this is software prefetching, where special non-blocking\nloads are used to bring data into the cache hierarchy just before being require"
			]
		  },
		  {
			"title": "Filtered runahead execution with a runahead buffer | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/2830772.2830812",
			"excerpts": [
			  "Section Title: Filtered runahead execution with a runahead buffer",
			  "Section Title: Filtered runahead execution with a runahead buffer > References",
			  "Authors : [Milad Hashemi](# \"Milad Hashemi\") Milad Hashemi",
			  "Milad Hashemi",
			  "The University of Texas at Austin",
			  "Pages 358 - 369",
			  "https://doi.org/10.1145/2830772.2830812"
			]
		  },
		  {
			"title": "Copyright by Milad Olia Hashemi 2016",
			"url": "https://repositories.lib.utexas.edu/bitstreams/4d988cbc-f809-418f-972d-8202b4c72bf4/download",
			"excerpts": [
			  "Jeffery A. Brown, Hong Wang, George Chrysos, Perry H. Wang, and John P.\nShen.\nSpeculative precomputation on chip multiprocessors.\nIn *Workshop on*\n*Multithreaded Execution, Architecture, and Compilation* , 2001.",
			  "Luis Ceze, James Tuck, Josep Torrellas, and Calin Cascaval. Bulk disambiguation\nof speculative threads in multiprocessors. In *ISCA* , 2006.",
			  "Murali Annavaram, Jignesh M. Patel, and Edward S. Davidson. Data prefetching\nby dependence graph precomputation. In *ISCA* , 2001."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "putation*\n*enables*\n*effective*\n*cache*\n*prefetching for even irregular memory access behavior, by*\n*using an alternate thread on a multithreaded or multi-core*\n*architecture. This paper describes a system that constructs*\n*and runs precomputation based prefetching threads via*\n*event-driven dynamic optimization. Precomputation threads*\n*are dynamically constructed by a runtime compiler from the*\n*programs frequently executed hot traces, and are adapted*\n*to the memory behavior automatically. Both construction*\n*and execution of the prefetching threads happen in another*\n*thread, imposing little overhead on the main thread. This*\n*paper also presents several techniques to accelerate the pre-*\n*computation threads, including colocation of p-threads with*\n*hot traces, dynamic stride prediction, and automatic adap-*\n*tation of runahead and jumpstart distan",
			  "While in-\nlined prefetches are typically effective for simple addressing\npatterns (e.g., strided addresses), p-thread based prefetching\nhas the potential to handle more complex address patterns\n(e.g. pointer chasing), or accesses embedded in more com-\nplex control flow. This is because the prefetching address is\ncomputed via actual code extracted from the main thread.",
			  "ead.\nA successful precomputation-based prefetcher must ad-\ndress several challenges. It must be able to determine the\nproper distance by which the prefetching thread should lead\nthe main thread, and it should have the ability to control that\ndistance. It must create lightweight threads that can actually\nproceed faster than the main thread, so that they stay out in\nfront. It must prevent p-threads from diverging from the ad-\ndress stream of the main thread, or at least detect when it\nhas happened. This divergence may be the result of control\nflow or address value speculation in the p-thread. Runaway\nprefetching may unnecessarily displace useful data, resulting\nin more data cache m",
			  " sophistication of slice creation.\nMore recent work by Lu, et al. [12] dynamically con-\nstructs p-slices via a runtime optimizer running on an idle\ncore. A single user-level thread is multiplexed to detect the\nprograms phases, construct the p-thread code, and perform\nprecomputation prefetching.",
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load.",
			  "r work enables new levels of adaptability by generat-\ning and improving p-threads within a dynamic optimization\nframework. In addition, it also introduces new techniques\nto push the p-thread in front of the main thread, to further\nstreamline the p-threads, and to detect and recover p-threads\nthat get off track.",
			  "**Loop Re-rolling**  A hot trace may contain multiple\ncopies of the same code due to loop unrolling done during\nstatic compilation. We perform loop *re-rolling* for the p-\nslice (i.e. removing the redundant loop copies) to reduce\nduplicated computation inside a p-slice. This optimization\nincreases the granularity at which we can set the prefetch\nrun-ahead distance, since the prefetch distance is always an\nintegral number of iterations.",
			  "**Object-Based Prefetching**  We perform same-object\nbased prefetching, as in our prior work on inline prefetch-\ning [26]. Same-object prefetching clusters prefetches falling",
			  "**P-Thread Jump Starting**\nSometimes, the only way to get the prefetch thread ahead\nof the main thread is to give it a head start. Existing dy-\nnamic precomputation schemes (e.g.\n[12]) typically start\np-threads from the same starting point (same iteration) as the\nmain thread.",
			  "**6**\n**Results**\nThis section evaluates the cost and performance of our dy-\nnamically generated precomputation based prefetching tech-\nnique.",
			  "The jump start allows the p-thread\nto get out in front more quickly. Jump start distances are\nrepaired when p-threads are frequently blocked (i.e., when\ntheir potential is not fully released). We observe as much as\na 25% performance improvement from *applu* , 40% from gal-\ngel, 14% from *mcf* , and 11% from *gap* . The average speedup\nis 39%, which is 17% better than previous techniques (in-\ncluding store prefetches).",
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "Our approach is complementary to and does not interfere with existing hardware prefetchers since we target only delinquent irregular load instructions (those with no constant or striding address patterns).",
			  "For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support.",
			  "Delinquent Irregular Loads",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://hlitz.github.io/papers/crisp.pdf",
			"excerpts": [
			  "We also compare CRISP to a hardware-only design referred to as IBDA which performs load slice extraction via iterative backwards dependency analysis. IBDA ...Read more"
			]
		  },
		  {
			"title": "Caching and Performance of CPUs - Jyotiprakash's Blog",
			"url": "https://blog.jyotiprakash.org/caching-and-performance-of-cpus",
			"excerpts": [
			  "Nonblocking Caches to Handle Multiple Misses:** Nonblocking or \"lockup-free\" caches take cache optimization a step further by allowing out-of-order execution. With a nonblocking cache, the CPU doesn't need to stall on a cache miss; instead, it can continue to fetch other instructions while waiting for the missing data. This technique, referred to as \"hit under miss\" or \"miss under miss,\" reduces effective miss penalties by overlapping misses and allowing more efficient cache utilizatio",
			  "ltibanked Caches to Increase Cache Bandwidth:** Instead of treating the cache as a single large block, **multibanked caches** split it into independent banks that can support simultaneous accesses. This approach effectively increases cache bandwidth by allowing multiple memory accesses at the same time.",
			  "For example, modern processors like the Intel Core i7 use multiple banks in their L1 cache, enabling up to two memory accesses per clock cycle. By using **sequential interleaving** , addresses are spread evenly across different banks, ensuring that memory accesses are well distributed, reducing contention, and improving cache performance."
			]
		  },
		  {
			"title": "Understanding the Backward Slices of Performance ...",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/2000/slice.isca.pdf",
			"excerpts": [
			  "backward slice (the subset of the program that relates to*\n*the instruction) of these performance degrading instructions, if*\n*small compared to the whole dynamic instruction stream, can be*\n*pre-executed to hide the instructions latenc",
			  " be effective with respect to a given instruction, a pre-execu-\ntion technique needs three things.",
			  "First, at an *initiation point* ahead\nof the instructions execution, the pre-execution technique needs to\nknow that the performance degrading instruction *will* be executed.",
			  "Second, it has to know which other instructions contribute to the\nperformance degrading instruction.",
			  "-fetches.\nPre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. U",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "e backward slice\ncomprises all of the instructions in the program that contribute,\neither directly or indirectly, to its computation, either through val-\nues or control decisions.",
			  "The key to answering all of these questions lies in the backward\nslice of the performance degrading instruction.",
			  "Pre-execution of branches is like data memory pre-fetching in\nthat the slice needs to compute the input operands of the branch in\norder to evaluate the branch. Unlike the previous two cases, this\npre-executed branch outcome (and perhaps target) needs to be\nbound to a particular dynamic branch instance to fully benefit from\nthe pre-execution.",
			  "n general, pre-execution amounts\nto guessing the existence of a future performance degrading\ninstruction and executing it (or what we think it will be) some time\nprior to its actual encounter in the machine, thereby at least par-\ntially hiding its latency"
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "In this section requirements for\neffective direct pre-execution are examined and hardware\nmechanisms supporting these requirements are described.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses",
			"url": "https://llvm.org/devmtg/2017-03/assets/slides/software_prefetching_for_indirect_memory_accesses.pdf",
			"excerpts": [
			  "Software Prefetching for Indirect. Memory Accesses. Sam Ainsworth and Timothy M. Jones. Computer Laboratory. Page 2. What should we software prefetch? Stride ...Read more"
			]
		  },
		  {
			"title": "Long-range Prefetching of Delinquent Loads",
			"url": "http://cseweb.ucsd.edu/~tullsen/isca2001.pdf",
			"excerpts": [
			  "Speculative Precomputation, a tech-*\n*nique that uses idle thread contexts in a multithreaded ar-*\n*chitecture to improve performance of single-threaded appli",
			  "ecula-\ntive threads are spawned under one of two conditions: when\nencounteringa basic trigger, which occurs when a designated\ninstruction in the main thread reaches a particular pipeline\nstage (such as the commit stage), or a chaining trigger, when\none speculative thread explicitly spawns another.",
			  "A speculative thread is spawned by allocating a hardware\nthread context, copying necessary live-in values into its reg-\nister file, and providing the thread context with the address of\nthe first instruction of the threa",
			  "If a free hardware context\nis not available the spawn request is ignored.",
			  "Necessary live-in values are always copied into the thread\ncontext when a speculative thread is spawned.",
			  "peculative threads execute precomputation slices (p-\nslices), which are sequences of dependent instructions which\nhave been extracted from the non-speculative thread and\ncompute the address accessed by delinquent loads.",
			  "When\na speculative thread is spawned, it precomputes the address\nexpected to be accessed by a future delinquent load, and\nprefetches the data.",
			  "wo primary forms of Speculative Precom-*\n*putation are evaluat",
			  "Delinquent Loads",
			  "We find that in most programs the set of delinquent\nloads is quite small; commonly 10 or fewer static loads cause\nmore than 80% of L1 data cache misses.",
			  " precomputa-\ntion slices used by our work are constructed within an in-\n ...",
			  "ulative precom-\nputation could be thought of as a special prefetch mech-\nanism that effectively targets load instructions that tradi-\ntionally have been difficult to handle via prefetching, such\nas loads that do not exhibit predictable access patterns and\nchains of dependent load",
			  "Speculative threads can be spawned",
			  "hardware structure is called the Outstanding Slice\nCounter (OSC). This structure tracks, for a subset of delin-\nquent loads, the number of instances of delinquent loads\nfor which a speculative thread has been spawned but for\nwhich the main thread has not yet committed the corre-\nsponding load."
			]
		  },
		  {
			"title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3319393",
			"excerpts": [
			  "S. Ainsworth and Timothy M. Jones. 2017. Software prefetching for indirect memory accesses. In *Proceedings of the International Symposium on Code Generation and Optimization (CGO17)* . Navigate to citation 1 citation 2",
			  "Sam Ainsworth and Timothy M. Jones. 2018. An event-triggered programmable prefetcher for irregular workloads. In *Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)* . Navigate to citation 1"
			]
		  },
		  {
			"title": "Orchestrated Scheduling and Prefetching for GPGPUs",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/orchestrated-gpgpu-scheduling-prefetching_isca13.pdf",
			"excerpts": [
			  "In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General. Purpose Graphics Processing Unit (GPGPU) ...Read more"
			]
		  },
		  {
			"title": "Evaluating and Mitigating Bandwidth Bottlenecks Across ...",
			"url": "https://users.cs.utah.edu/~vijay/papers/ispass17.pdf",
			"excerpts": [
			  "For instance, we observe that to prevent throttling of L1 cache, increasing the L1 bandwidth by increasing the MSHRs to handle more outstanding misses can lead ..."
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://hparch.gatech.edu/papers/lee_micro10.pdf",
			"excerpts": [
			  " **In some cases, blindly applying prefetching degrades perfor-**\n**mance. To reduce such negative effects, we propose an** ***adaptive***\n***prefetch throttling*** **scheme, which permits automatic GPGPU**\n**application- and hardware-specific adjustment. We show that**\n**adaptation reduces the negative effects of prefetching and can**\n**even improve performance. Overall, compared to the state-of-**\n**the-art software and hardware prefetching, our MT-prefetching**\n**improves performance on average by 16% (software pref.) / 15%**\n**(hardware pref.) on our benchmarks.**",
			  "IP may not be useful in two cases. The first case is when\ndemand requests corresponding to prefetch requests have al-\nready been generated. This can happen because warps are not\nexecuted in strict sequential order.",
			  ".\nThe second case is when the warp that is prefetching is\nthe last warp of a thread block and the target warp (i.e.\nthread block to which the target warp belongs) has been\nassigned to a different core.",
			  " Inter-thread Prefetching (IP):* One of the main differ-\nences between GPGPU applications and traditional applica-\ntions is that GPGPU applications have a significantly higher\nnumber of thread",
			  "er warp training:* Stream and stride detectors must be\ntrained on a per-warp basis, similar to those in simulta-\nneous multithreading architectures. This aspect is criti-\ncal since many requests from different warps can easily\nconfuse pattern detector",
			  "*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "The key idea behind hardware IP is that when an application\nexhibits a strided memory access pattern across threads at the\nsame PC, one thread generates prefetch requests for another\nthread.",
			  "able*\n*Hardware*\n*Prefetcher*\n*Training:*\nCurrent\nGPGPU applications exhibit largely regular memory access\npatterns, so one might expect traditional stream or stride\nperfetchers to work well. However, because the number of\nthreads is often in the hundreds, traditional training mecha-\nnisms do not scale.",
			  "Here, we describe extensions to the traditional training\npolicies, for program counter (PC) based stride prefetch-\ners [4, 11], that can overcome this limitation. This basic idea\ncan be extended to other types of prefetchers as well.",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  "\n*2) Inter-thread Prefetching in Hardware:* We propose a\nhardware-based inter-thread prefetching (IP) mechanism, in\naddition to our software-based IP scheme (Section III-A).\n",
			  " *Stride promotion:* Since memory access patterns are\nfairly regular in GPGPU applications, we observe that\nwhen a few warps have the same access stride for a given\nPC, all warps will often have the same stride for the\nPC.",
			  "Based on this observation, when at least three PWS\nentries for the same PC have the same stride, we promote\nthe PC stride combination to the *global stride (GS)* table.",
			  "IV. U NDERSTANDING U SEFUL VS . H ARMFUL\nP REFETCHING IN GPGPU",
			  "o as** ***many-thread aware prefetching*** **(MT-prefetching) mecha-*",
			  "nisms.",
			  "access behavior among fine-grained threads.",
			  "For hardware MT-**",
			  "The key ideas behind our MT-prefetching schemes are\n(a) per-warp-training and stride promotion, (b) inter-thread\nprefetching, and (c) adaptive throttling.",
			  "mechanism.",
			  "In some cases, blindly applying prefetching degrades perfor-**",
			  "mance. To reduce such negative effects, we propose an** ***adaptive***",
			  "prefetch throttling*** **scheme, which permits automatic GPGPU**",
			  "application- and hardware-specific adjustment.",
			  "We show that**",
			  "adaptation reduces the negative effects of prefetching and can**",
			  "even improve performance.",
			  "III. P REFETCHING M ECHANISMS FOR GPGPU",
			  "This section describes our *many-thread aware* prefetching",
			  "(MT-prefetching) schemes, which includes both hardware",
			  "and software mechanisms.",
			  "To support these schemes, we\naugment each core of the GPGPUs with a prefetch cache and\na prefetch engine.",
			  "The prefetch cache holds the prefetched\nblocks from memory and the prefetch engine is responsible\nfor throttling prefetch requests (see Section V).",
			  "*A. Software Prefetching*",
			  "We refer to our software prefetching mechanism as *many-*",
			  " refer to our software prefetching mechanism as *many-*\n*thread aware software prefetching* (MT-SWP). MT-SWP con-\nsists of two components: conventional *stride prefetching* and\na newly proposed *inter-thread prefetching* (IP).",
			  "a newly proposed *inter-thread prefetching* (IP).",
			  "*1) Stride Prefetching:* This mechanism is the same as the",
			  "1) Stride Prefetching:* This mechanism is the same as the\ntraditional stride prefetching mechanism. The prefetch cache\nstores any prefetched blocks.",
			  "tions is that GPGPU applications have a significantly higher\nnumber of threads.",
			  "As a result, the execution length of each\nthread is often very short.",
			  "Figure 3 shows a snippet of sequen-\ntial code with prefetch instructions and the equivalent CUDA\ncode without prefetch instructions.",
			  "In the CUDA code, since\nthe loop iterations are parallelized and each thread executes\nonly one (or very few) iteration(s) of the sequential loop, there\nare no (or very few) opportunities to insert prefetch requests",
			  "This can happen because warps are not\nexecuted in strict sequential order.",
			  "For example, when T32\ngenerates a prefetch request for T64, T64 might have already\nissued the demand request corresponding to the prefetch\nrequest generated by T32.",
			  "hese prefetch requests are usu-\nally merged in the memory system since the corresponding\ndemand requests are likely to still be in the memory system",
			  "Unless inter-core merging occurs\nin the DRAM controller, these prefetch requests are useless.",
			  "This problem is similar to the out-of-array-bounds problem\nencountered when prefetching in CPU systems.",
			  "Nevertheless,\nwe find that the benefits of IP far outweigh its negative effects.",
			  "*B. Hardware Prefetching*",
			  "We refer to our hardware prefetching mechanism as\nthe *many-thread aware hardware prefetcher* (MT-HWP).",
			  "T-HWP has (1) enhanced prefetcher training algorithms\nthat provide improved scalability over previously proposed\nstream/stride prefetchers and (2) a hardware-based inter-\nthread prefetching (IP) mechanism.",
			  "This information is stored in a separate table called an\nIP table.",
			  "*3) Implementation:* Figure 6 shows the overall design of\nthe MT-HWP, which consists of the three tables discussed\nearlier: PWS, GS, and IP tables.",
			  " practice, overly aggressive prefetching can have a nega-\ntive effect on performance.",
			  " principal memory latency tolerance mechanism in a\nGPGPU is multithreading. Thus, if a sufficient number of\nwarps and/or enough computation exist, memory latency can"
			]
		  },
		  {
			"title": "CTA-aware Prefetching for GPGPU - Computer Engineering",
			"url": "https://ceng.usc.edu/techreports/2014/Annavaram%20CENG-2014-08.pdf",
			"excerpts": [
			  "Lee et al. [21] proposed a software and hardware\nbased many-thread aware prefetching which basically commands\nthreads to prefetch data for the other threads.",
			  "They exploit the\nfact that the memory addresses are referenced using thread id in\nmany GPU applications."
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/",
			"excerpts": [
			  "To optimize memory access on NVIDIA GPUs, prefetching can be employed in software when excess warps are insufficient to hide memory latency.",
			  "A synchronization within the loopfor example, `syncthreads` constitutes a memory fence and forces the loading of `arr` to complete at that point within the same iteration, not PDIST iterations later. The fix is to use asynchronous loads into shared memory, the simplest version of which is explained in the [Pipeline interface](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) section of the CUDA programmer guide.",
			  "Prefetching can be implemented by unrolling loops, storing prefetched values in registers or shared memory, and using techniques like batched or rolling prefetching, with the latter being more effective when combined with asynchronous memory copies.",
			  "Confirm that not all memory bandwidth is being used.",
			  "Confirm the main reason warps are blocked is **Stall Long Scoreboard** , which means that the SMs are waiting for data from DRAM.",
			  "This leads to the improved shared memory results shown in Figure 2. A prefetch distance of just 6, combined with asynchronous memory copies in a rolling fashion, is sufficient to obtain optimal performance at almost 60% speedup over the original version of the code."
			]
		  },
		  {
			"title": "Near-Side Prefetch Throttling - of Wim Heirman",
			"url": "https://heirman.net/papers/pact2018.pdf",
			"excerpts": [
			  " near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine. The MSHR tracks outstand-\ning cache misses triggered by both demand requests (application\nloads and stores), and prefetches; the extra state allows for detec-\ntion of late prefetches. The state machine periodically computes\nthe fraction of late prefetches, and updates the optimal prefetch\ndistance.\n",
			  "Maintaining a small fraction\n(e.g., 10%) of late prefetches does not harm performance as long as\nthe late prefetches are only late by a small amount, i.e., the demand\naccess is made only just before the prefetch request completes.",
			  "Near-Side Prefetch Throttling",
			  " We show that near-side throttling can be extended to mul-\ntiple prefetchers per core, where it will naturally throttle\nthose prefetchers that yield no useful requests, allowing for\na diverse set of prefetch algorithms to co-exis",
			  "e throttling detects bandwidth\nsaturation locally (through memory latency), so no global coordi-\nnation is needed between the per-core prefetchers in a many-core\narchitecture, or even between multiple prefetch algorithms on a\nsingle core.\n",
			  "ng detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost. This makes near-side throttling superior over traditional\nfar-side throttling as it is able to provide even slightly better per-\nformance (9.6% vs. 9.4%), at a far cheaper implementation cost,\nand is more widely applicable to other use cases such as software\nprefetching and control of multiple hardware prefetchers.",
			  "The basic concept of near-side prefetch throttling (NST) is to\ndetect late prefetches, and tune the prefetcher aggressiveness such\nthat the amount of late prefetches is balanced around a small but\nnon-zero fraction of all prefetches.",
			  "NEAR-SIDE PREFETCH THROTTLING**",
			  "ur\nsolution is cheap to implement in hardware, includes throttling on\noff-chip bandwidth saturation, applies to both hardware and soft-\nware prefetching, and can control multiple concurrent prefetchers\nwhere it will naturally allow the most useful prefetch algorithm\nto generate most of the requests",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "he aim of prefetch throttling is to allow the prefetcher to run\nahead far enough such that prefetches are timely, but prevent it\nfrom running too far ahead into a region where the prefetch al-\ngorithm can no longer accurately predict the applications access\npattern which leads to useless prefetches",
			  "In a many-core processor, the prefetchers in each core can be con-\ntrolled independently based on their own specific late prefetch\nfraction. This allows for heterogenous applications or multi-\nprogramming workloads, and will tune each prefetchers distance\nto the specific access pattern it is experienci",
			  "Our proposed implementation of near-side prefetch throttling\nconsists of some extra state in the processors miss status holding\nregister (MSHR) and a state machine.",
			  "sing detailed simulations we measure application performance\nover a range of workloads, and show that our method can quickly\nadapt to application behavior, to match, or in some cases exceed,\nthe best static optimal prefetch distance with only minimal hard-\nware cost"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power ...Read more",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Hardware Design of DRAM Memory Prefetching Engine for ...",
			"url": "https://www.mdpi.com/2227-7080/13/10/455",
			"excerpts": [
			  "Inter-warp prefetching mechanisms are based on the detection of stride patterns and base addresses in different warps. A many-thread-aware prefetching mechanism ...Read more"
			]
		  },
		  {
			"title": "APAC: An Accurate and Adaptive Prefetch Framework with ...",
			"url": "https://par.nsf.gov/servlets/purl/10251073",
			"excerpts": [
			  "Near-side prefetch throttling (NST) [9] only adjusts the aggressiveness of prefetching based on the fraction of late prefetchers, which has a relatively small ...Read more"
			]
		  },
		  {
			"title": "PPT - Many-Thread Aware Prefetching Mechanisms for GPGPU Application PowerPoint Presentation - ID:5741796",
			"url": "https://www.slideserve.com/phil/many-thread-aware-prefetching-mechanisms-for-gpgpu-application",
			"excerpts": [
			  "akash\n**[Motivation](https://image3.slideserve.com/5741796/motivation-l.jpg \"motivation\")**  Memory latency hiding through multithread prefetching schemes  Per-warp training and Stride promotion  Inter-thread Prefetching  Adaptive Throttling  Propose software and hardware prefetching mechanisms for a GPGPU architecture  Scalable to large number of threads  Robustness through feedback and throttling mechanisms to avoid degraded performance\n",
			  "rmance\n**[Memory Latency Hiding techniques](https://image3.slideserve.com/5741796/memory-latency-hiding-techniques-l.jpg \"memory latency hiding techniques\")**  Multithreading  Thread level and Warp level context switching  Utilization of complex cache memory hierarchies  Using L1, L2, DRAMs than accessing Global Memory each time  Prefetching  Insufficient thread-level parallelism  Memory request merging Thread1 Thread2 Thread1 Thread3",
			  "MT-HWP  Stride Promotion  Considering the stride pattern is the same across all warps for a given PC, PWS is monitored for three accesses  If found same stride, promote the PWS to Global Stride(GS) table, if not, retain in PWS  Inter-thread Prefetching  Monitor stride pattern across threads at the same PC, for 3 memory accesses  If found same, stride information is stored in the IP table",
			  "MT-HWP  Implementation  When there are hits in both GS and IP, GS is given preference because  Strides"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications | Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2010.44",
			"excerpts": [
			  "Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract\nContent:\nWe consider the problem of how to improve memory latency tolerance in massively multithreaded GPGPUs when the thread-level parallelism of an application is not sufficient to hide memory latency. One solution used in conventional CPU systems is prefetching, both in hardware and software. However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously. This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms. Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads. For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, blindly applying prefetching degrades performance. To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment. We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Overall, compared to the state-of-the-art software and hardware prefetching, our MT-prefetching improves performance on average by 16%(software pref.) / 15% (hardware pref.) on our benchmarks.",
			  "Section Title: Many-Thread Aware Prefetching Mechanisms for GPGPU Applications > Abstract",
			  "However, we show that straightforwardly applying such mechanisms to GPGPU systems does not deliver the expected performance benefits and can in fact hurt performance when not used judiciously.",
			  "This paper proposes new hardware and software prefetching mechanisms tailored to GPGPU systems, which we refer to as many-thread aware prefetching (MT-prefetching) mechanisms.",
			  "Our software MT-prefetching mechanism, called inter-thread prefetching, exploits the existence of common memory access behavior among fine-grained threads.",
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism.",
			  "In some cases, blindly applying prefetching degrades performance.",
			  "To reduce such negative effects, we propose an adaptive prefetch throttling scheme, which permits automatic GPGPU application- and hardware-specific adjustment.",
			  "We show that adaptation reduces the negative effects of prefetching and can even improve performance.",
			  "Content:"
			]
		  },
		  {
			"title": "[PDF] Near-side prefetch throttling: adaptive prefetching for high-performance many-core processors | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Near-side-prefetch-throttling%3A-adaptive-prefetching-Heirman-Bois/39e6013cbe45a288431ddb4269611a483c90bbb9",
			"excerpts": [
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases. Expand",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "Expand",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "The near-side throttling (NST) proposal performs similar to the state-of-the-art feedback-directed prefetching (FDP), even though it has a significantly lower implementation cost, can react more quickly to changes in application behavior and is applicable to a more varied set of use cases.",
			  "TLDR"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Treelet Prefetching For Ray Tracing",
			  "To address this, we propose treelet prefetching to reduce the latency of ray traversal.",
			  "When a ray visits a treelet root node, we prefetch the corresponding treelet, enabling deeper levels of the tree to be fetched in advance.",
			  "Our approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching.",
			  "Abstract",
			  "Abstract",
			  "Ray tracing is traditionally only used in offline rendering to produce images of high fidelity because it is computationally expensive.",
			  "Recent Graphics Processing Units (GPUs) have included dedicated accelerators to bring ray tracing to real-time rendering for video games and other graphics applications.",
			  "These accelerators focus on finding the closest intersection between a ray and a scene using a hierarchical tree data structure called a Bounding Volume Hierarchy (BVH) tree.",
			  "However, BVH tree traversal is still very costly due to divergent rays accessing different parts of the tree, with each ray following a unique pointer-chasing sequence that is difficult to optimize with traditional methods.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal."
			]
		  },
		  {
			"title": "Fast Ray Sorting and Breadth-First Packet Traversal for ...",
			"url": "https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2010/GPU-RayTracing.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "ur approach uses a hardware prefetcher with a two-stack treelet based traversal algorithm, maximizing the benefits of treelet prefetching. O",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "Treelets are smaller subtrees created by splitting the BVH tree.",
			  "This reduces the latency associated with pointer-chasing during tree traversal.",
			  "\nDaniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, and Jir Bittner. 2021. A Survey on Bounding Volume Hierarchies for Ray Tracing. Computer Graphics Forum (2021).\n[Go",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Meister%2C+Shinji+Ogaki%2C+Carsten+Benthin%2C+Michael%C2%A0J.+Doyle%2C+Michael+Guthe%2C+and+Jir%C3%AD+Bittner.+2021.+A+Survey+on+Bounding+Volume+Hierarchies+for+Ray+Tracing.+Computer+Graphics+Forum+%282021%29.)",
			  "[32]",
			  "Bochang Moon, Yongyoung Byun, Tae-Joon Kim, Pio Claudio, Hye-Sun Kim, Yun-Ji Ban, Seung Woo Nam, and Sung-Eui Yoon. 2010. Cache-Oblivious Ray Reordering. ACM Transactions on Graphics (TOG) (2010).",
			  "[Google Scholar](https://scholar.google.com/scholar?q=Bochang+Moon%2C+Yongyoung+Byun%2C+Tae-Joon+Kim%2C+Pio+Claudio%2C+Hye-Sun+Kim%2C+Yun-Ji+Ban%2C+Seung%C2%A0Woo+Nam%2C+and+Sung-Eui+Yoon.+2010.+Cache-Oblivious+Ray+Reordering.+ACM+Transactions+on+Graphics+%28TOG%29+%282010%29.)",
			  "[33]",
			  "Paul Arthur Navratil, Donald S. Fussell, Calvin Lin, and William R. Mark. 2007. Dynamic Ray Scheduling to Improve Ray Coherence and Bandwidth Utilization. In IEEE Symposium on Interactive Ray Tracing. 95104.\n[Dig",
			  "[Digital Library](/doi/10.1109/RT.2007.4342596)",
			  "[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FRT.2007.4342596)",
			  "[34]"
			]
		  },
		  {
			"title": "Many-Thread Aware Prefetching Mechanisms for GPGPU ...",
			"url": "https://info.computer.org/csdl/proceedings-article/micro/2010/05695538/12OmNzcPApv",
			"excerpts": [
			  "For hardware MT-prefetching, we describe a scalable prefetcher training algorithm along with a hardware-based inter-thread prefetching mechanism. In some cases, ...Read more"
			]
		  },
		  {
			"title": "(PDF) Adaptive Prefetching for Fine-grain Communication in PGAS Programs",
			"url": "https://www.researchgate.net/publication/381186663_Adaptive_Prefetching_for_Fine-grain_Communication_in_PGAS_Programs",
			"excerpts": [
			  "In this work, we present an adaptive prefetching optimization that can be applied to PGAS programs with irregular memory access patterns. We ...Read more"
			]
		  },
		  {
			"title": "System level cache prefetching algorithms for complex ...",
			"url": "https://lup.lub.lu.se/student-papers/record/9159122/file/9159147.pdf",
			"excerpts": [
			  "These merges occur when a prefetch request is late and a demand request is coming at the same time. The throttling behaviour can be observed in table 1. Early ...Read more"
			]
		  },
		  {
			"title": "Boosting Application Performance with GPU Memory Prefetching - Technical Blog - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/boosting-application-performance-with-gpu-memory-prefetching/209210",
			"excerpts": [
			  "This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.Read more"
			]
		  },
		  {
			"title": "Many-thread aware hardware prefetcher (MT-HWP). | Download Scientific Diagram",
			"url": "https://www.researchgate.net/figure/Many-thread-aware-hardware-prefetcher-MT-HWP_fig7_221005092",
			"excerpts": [
			  "h consists of the three tables discussed earlier: PWS, GS, and IP tables. The ",
			  "The IP and GS tables are indexed in parallel with a PC address.",
			  "adaptive MT-HWP pro- vides a 29% performance improvement over the baseline."
			]
		  },
		  {
			"title": "Some issues regarding the use of prefetch in the cuda kernel - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/some-issues-regarding-the-use-of-prefetch-in-the-cuda-kernel/334568",
			"excerpts": [
			  "e difference between cp.async and normal loads is the means of re-synchronization of the asynchronous operations.",
			  "For normal loads from global memory, the re-synchronization works with the help of the long scoreboard, as soon as the result registers are used in instructions dependent on the result.",
			  "For cp.async, the results are transferred into shared memory. You have to use cp.async.wait_group or cp.async.wait_all for re-synchronization."
			]
		  },
		  {
			"title": "Combining Local and Global History for High Performance ...",
			"url": "https://jilp.org/dpc/online/papers/00dimitrov.pdf",
			"excerpts": [
			  "To capture delta correlation, a delta buffer is included in the prefetch function, which keeps the delta information when a linked list is traversed in the GHB.Read more"
			]
		  },
		  {
			"title": "L5 - Advanced Memory Prefetching Techniques",
			"url": "https://coconote.app/notes/5551d401-c94c-4d86-bb70-c67c9e8bb912",
			"excerpts": [
			  "Delta correlation prefetchers track repeating delta patterns between accessed addresses to predict future accesses. Correlation-based ...Read more"
			]
		  },
		  {
			"title": "Pointer jumping - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Pointer_jumping",
			"excerpts": [
			  "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs.Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov Predictors",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=joseph_isca97.pdf",
			"excerpts": [
			  "The Markov prefetcher provides the best performance across nll\nthe applications, particularly when applied to both the instruction\nand data caches."
			]
		  },
		  {
			"title": "A Survey on Recent Hardware Data Prefetching ...",
			"url": "https://arxiv.org/pdf/2009.00715",
			"excerpts": [
			  "99] D. Joseph and D. Grunwald, Prefetching Using Markov Predictors, in *Proceedings of the International Symposium on*\n*Computer Architecture (ISCA)* , pp. 252263, 1997"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1999/jmp-ptr.isca.pdf",
			"excerpts": [
			  "bstract**\n*Current techniques for prefetching linked data structures*\n*(LDS) exploit the work available in one loop iteration or*\n*recursive call to overlap pointer chasing latency. Jump-*\n*pointers, which provide direct access to non-adjacent*\n*nodes, can be used for prefetching when loop and recur-*\n*sive procedure bodies are small and do not have sufficient*\n*work to overlap a long latency. This paper describes a*\n*framework for jump-pointer prefetching (JPP) that sup-*\n*ports four prefetching idioms: queue, full, chain, and root*\n*jumping and three implementations: software-only, hard-*\n*ware-only, and a cooperative software/hardware tech-*\n*nique.",
			  "n a suite of pointer intensive programs, jump-*\n*pointer prefetching reduces memory stall time by 72% for*\n*software, 83% for cooperative and 55% for hardware, pro-*\n*ducing speedups of 15%, 20% and 22% respecti",
			  "general purpose technique for tol-\nerating serialized latencies that result from LDS tra-\nversal.\nBy storing explicit jump-pointers to nodes\nseveral hops away, JPP overcomes the pointer-chasing\nproblem.\nIt is able to generate prefetch addresses\ndirectly, rather than in a serial fashion, and is effective\neven in situations where not enough work is available\nto hide latencies by scheduling.\n**",
			  "P implementations: software-only,\nhardware-only, and cooperative.\nFor those programs\nwith appreciable memory latency components, these\nimplementations reduce overall observed memory\nlatency by 72%, 55%, and 83%, respectively and\nachieve speedups of 15%, 22%, and 20%.\n",
			  "**Abstract**",
			  "*Current techniques for prefetching linked data structures*",
			  "\n*(LDS) exploit the work available in one loop iteration or*",
			  "*recursive call to overlap pointer chasing latency. Jump-*",
			  "*pointers, which provide direct access to non-adjacent*",
			  "*nodes, can be used for prefetching when loop and recur-*",
			  "*sive procedure bodies are small and do not have sufficient*",
			  "*work to overlap a long latency. This paper describes a*",
			  "*framework for jump-pointer prefetching (JPP) that sup-*",
			  "*ports four prefetching idioms: queue, full, chain, and root*",
			  "*jumping and three implementations: software-only, hard-*",
			  "*ware-only, and a cooperative software/hardware tech-*",
			  "*nique.*",
			  "*On a suite of pointer intensive programs, jump-*",
			  "*pointer prefetching reduces memory stall time by 72% for*",
			  "*software, 83% for cooperative and 55% for hardware, pro-*",
			  "*ducing speedups of 15%, 20% and 22% respectively.*",
			  "*\n**1 Introduction**",
			  "Linked data structures (LDS) are common in many appli-",
			  "cations, and their importance is growing with the spread of",
			  "object-oriented programming.",
			  "The popularity of LDS",
			  "stems from their flexibility, not their performance. LDS",
			  "access, often referred to as *pointer-chasing* , entails chains",
			  "of data dependent loads that serialize address generation",
			  "and memory access.",
			  "In traversing an LDS, these loads",
			  "often form the programs critical path.\nC",
			  "Consequently,",
			  "when they miss in the cache, they can severely limit paral-",
			  "lelism and degrade performance.",
			  "Prefetching is one way to hide LDS load latency and",
			  " ... \nefficiently [6] and to parallelize searches and reductions on",
			  "lists [9].",
			  "Discussions of maintaining recursion-avoiding",
			  "traversal *threads* in non-linear data structures can be found",
			  "in data structures literature [18]. As noted earlier, Luk and",
			  "Mowry [11] suggested the use of programmer controlled",
			  "jump-pointers for prefetching. We are not aware of any",
			  "implementations, actual or proposed,\nof hardware or",
			  "cooperative jump-pointer prefetching.",
			  "\n**6 Summary and Future Directions**\n",
			  "In this paper, we describe the general technique of jump-",
			  "pointer prefetching (JPP) for tolerating linked structure",
			  "(LDS) access latency. JPP is effective when limited work",
			  "is available between successive dependent accesses (e.g., a",
			  "***Figure***",
			  "***7.***",
			  "***Tolerating***",
			  "***longer***",
			  "***memory***",
			  "***latencies.***",
			  "*Execution times for health: the first group of bars uses*",
			  "*the base configuration (70 cycle memory latency), the*",
			  "*second and third simulate long memory latency (280*",
			  "cycles).*",
			  "*In terms of prefetching, the first two*",
			  "*configurations use a jump interval (the distance*",
			  "*\n*between a jump-pointers home and target nodes) of 8,*",
			  "the third uses an interval of 16.*",
			  "0.0",
			  "0.5",
			  "1.0",
			  "1.5",
			  "2.0",
			  "2.5",
			  "3.0",
			  "MemLat=70",
			  "Interval=8",
			  "Interval=8",
			  "BDSCH",
			  "BDSCH",
			  "BDSCH",
			  "MemLat=280",
			  "MemLat=280",
			  "Interval=16",
			  "Normalized Execution Time",
			  "memory latency",
			  "Compute time",
			  "**Legend: B:** Base",
			  "**D:** DBP",
			  "**S:** Software JPP",
			  "**C:** Cooperative JPP",
			  "**H:** Hardware JPP",
			  "tight pointer chasing loop) to enable aggressive scheduling"
			]
		  },
		  {
			"title": "The Efficacy of Software Prefetching and Locality ...",
			"url": "https://jilp.org/vol6/v6paper7.pdf",
			"excerpts": [
			  "In jump pointer prefetching, additional pointers are inserted into a dynamic data structure\nto connect non-consecutive link elements.",
			  "These jump pointers allow prefetch instructions to\nname link elements further down the pointer chain ( i.e. a prefetch distance, PD , away which\nis computed as in Sections 3.1 and 3.2) without sequentially traversing the intermediate links.",
			  "Consequently, prefetch instructions can overlap the fetch of multiple link elements simultaneously\nby issuing prefetches through the memory addresses stored in the jump pointers.",
			  "Jump pointer prefetching, however, cannot prefetch the first PD link nodes in a linked list\nbecause there are no jump pointers that point to these early nodes.",
			  "To enable prefetching of early\nnodes, jump pointer prefetching can be extended with prefetch arrays [7]. In this technique, an\narray of prefetch pointers is added to every linked list to point to the first PD link nodes.",
			  "Hence,\nprefetches can be issued through the memory addresses in the prefetch arrays before traversing\neach linked list to cover the early nodes, much like the prologue loops in affine array and indexed\narray prefetching prefetch the first PD array elements.",
			  "Figure 5 Part(A) illustrates the addition\nof a prologue loop that performs prefetching through a prefetch array.",
			  "6.3 ccmalloc and Prefetching",
			  "Software prefetching for pointer-chasing codes suffers high overhead to create and manage jump\npointers, as described in Section 5.2.",
			  "owever, jump pointers may not be necessary when prefetch-\ning is combined with ccmalloc memory allocation",
			  "Since intelligent allocation places link nodes\ncontiguously in memory, prefetch instructions can access future link nodes by simple indexing, just\nas for affine array accesses.",
			  "Figure 9 shows the effect of ccmalloc on nodes linked together by\npointers.",
			  "From the right-hand part of the figure, it is intuitive that a compiler can insert prefetches\nfor list nodes further down the list using the size of a node and the location of the first node.",
			  "This approach, which we call index prefetching [1, 37], was originally proposed in [8].",
			  "With in-\ndex prefetching, the jump pointers can be removed, thus eliminating all the overhead associated\nwith jump pointer prefetchin",
			  "To quantify this benefit, we created index prefetching versions for\nHealth and MST , and show the results for these benchmarks in Figure 18.",
			  "(We did not create an\nindex prefetching version for EM3D , our third pointer-chasing benchmark, since it already achieves\nhigh performance with normal software prefetching as shown in Figures 10 and 13)."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta- ...",
			"url": "https://jilp.org/vol13/v13paper2.pdf",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " Correlating Prediction\nTables (DCPT). DCPT builds upon two previously proposed prefetcher techniques, com-\nbining them and refining their ideas to achieve better perf"
			]
		  },
		  {
			"title": "Prefetching",
			"url": "https://ece752.ece.wisc.edu/lect15-prefetching.pdf",
			"excerpts": [
			  "Markov prefetching forms address correlations",
			  "Joseph and Grunwald (ISCA 97)",
			  "Uses global memory addresses as states in the Markov graph",
			  "Correlation Table *approximates* Markov graph",
			  "obal History Buffer (GHB)\n Holds miss address\nhistory in FIFO order",
			  "Global History Buffer",
			  "Delta-based prefetching leads to much smaller table than\nclassical Markov Prefetching",
			  "Delta-based prefetching can remove compulsory misses"
			]
		  },
		  {
			"title": "Data prefetching on in-order processors",
			"url": "https://upcommons.upc.edu/server/api/core/bitstreams/2268c547-2c42-49d5-9e73-7578ebe3758e/content",
			"excerpts": [
			  "[14] K. J. Nesbit and J. E. Smith, Data cache prefetching using a global history buffer, Software, IEE Proceedings-, 2004. [15] S. Srinath, O. Mutlu, H ...Read more"
			]
		  },
		  {
			"title": "Feedback Mechanisms for Improving Probabilistic Memory ...",
			"url": "https://www.cs.utexas.edu/~lin/papers/hpca09.pdf",
			"excerpts": [
			  "The efficiency of stream prefetching has been improved by Nesbit and. Smith [18], who introduce the Global History Buffer to im- prove prefetch effectiveness ...Read more"
			]
		  },
		  {
			"title": "Data Access History Cache and Associated Data Prefetching ...",
			"url": "http://www.cs.iit.edu/~scs/assets/files/SC07_DAHC.pdf",
			"excerpts": [
			  "Nesbit and Smith proposed a global history buffer for data prefetching in [14] and. [15]. The similarity between their work and our work is that both attempt ..."
			]
		  },
		  {
			"title": "TDT4260 Computer Architecture Mini-Project",
			"url": "https://www.nichele.eu/files/nichele_tdt4260.pdf",
			"excerpts": [
			  "[14] M. Grannaes, M. Jahre and L. Natvig. Multi-level Hardware Prefetching. Using Low Complexity Delta Correlating Prediction Tables with Partial. Matching.Read more"
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer",
			"url": "https://www.researchgate.net/publication/3215463_Data_Cache_Prefetching_Using_a_Global_History_Buffer",
			"excerpts": [
			  "This research is to design a history table-based linear analysis ... This paper studies hardware prefetching for second-level (L2) caches."
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | Proceedings of the 10th International Symposium on High Performance Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1109/HPCA.2004.10030",
			"excerpts": [
			  "A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction.",
			  "The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.",
			  "Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods.",
			  "Global History Buffer prefetching can increase correlation prefetching performance by 20% and cut its memory traffic by 90%. Furthermore, the Global History Buffer can make correlations within a loads address stream, which can increase stride prefetching performance by 6%. "
			]
		  },
		  {
			"title": "Data Cache Prefetching Using a Global History Buffer | IEEE Micro",
			"url": "https://dl.acm.org/doi/abs/10.1109/MM.2005.6",
			"excerpts": [
			  "By organizing data cache prefetch information in a new way, a GHB supports existing prefetch algorithms more effectively than conventional prefetch tables. It reduces stale table data, improving accuracy and reducing memory traffic. It contains a more complete picture of cache miss history and is smaller than conventional tables",
			  "The structure is based on a Global History Buffer that holds the most\nrecent miss addresses in FIFO order.",
			  "Linked lists within this global history buffer",
			  "HPCA '04: Proceedings of the 10th International Symposium on High Performance Computer ArchitectureA new structure for implementing data cache prefetching is proposed and analyzed via",
			  "A new structure for implementing data cache prefetching is proposed and analyzed via"
			]
		  },
		  {
			"title": "DATA CACHE PREFETCHING USING A GLOBAL ...",
			"url": "https://minds.wisconsin.edu/bitstream/1793/11158/1/file_1.pdf",
			"excerpts": [
			  "As a circular buffer, the GHB prefetching\nstructure eliminates many problems associat-\ned with conventional tables. First, the GHB\nFIFO naturally gives table space priority to\nthe most recent history, thus eliminating the\nstale-data problem.",
			  "dex table entries contain point-\ners into the GHB.",
			  "The GHB is larger, with a size chosen to hold\na representative portion of the miss address\nstream. Last, and perhaps most important, a\ndesigner can use the ordered global history to\ncreate more-sophisticated prefetching meth-\nods than conventional stride and correlation\nprefetchin"
			]
		  },
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement"
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%.",
			  "In\nfigure 7 we show the average portion of deltas that can be\nrepresented with a given amount of bits across all SPEC2006\nbenchmarks.",
			  "Although the coverage steadily increases with the amount\nof bits used, speedup has a distinct knee at around 7 bits.",
			  "In figure 8 we show the geometric mean of speedups as\na function of the number of deltas per table entry.",
			  "One of the main differences between DCPT and PC/DC is\nthat DCPT stores deltas, while PC/DC stores entire addresses\nin its GHB.",
			  "the deltas are usually quite small, fewer\nbits are needed to represent a delta than a full address."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Prefetching using Markov predictors | Proceedings of the 24th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/264107.264207",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the *Markov prefetcher.* This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetching *multiple reference predictions* from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "Prefetching using Markov predictors for ISCA 1997 - IBM Research",
			"url": "https://research.ibm.com/publications/prefetching-using-markov-predictors--1",
			"excerpts": [
			  "Prefetching is one approach to reducing the latency of memory operations in modern computer systems.",
			  "In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs.",
			  "The Markov prefetcher is distinguished by prefetching multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor.",
			  "This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor.",
			  "In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization."
			]
		  },
		  {
			"title": "CRISP: Critical Slice Prefetching",
			"url": "https://people.ucsc.edu/~hlitz/papers/crisp.pdf",
			"excerpts": [
			  " Prefetching using Markov predictors.\n",
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using Markov predictors.",
			  "Tempo-\nral prefetchers [ 8 , 49 , 54 , 119 , 122 ] track the temporal order of\ncache line accesses based on Markov prefetching [ 55 ] introducing\nsignificant storage overheads in the order of megabytes in con-\ntrast to CRISP",
			  " Runahead prefetchers [ 6 , 33 , 48 , 82  84 , 89 , 95 ] and\nhelper threads [ 21 , 23 , 24 , 70 , 73 , 74 , 110 , 117 , 123 , 124 ] prefetch\nirregular memory accesses as in linked-list traversals, however,\nthey introduce significant hardware complexity or consume sepa-\nrate SMT-threads [ 34 ] whereas CRISP requires only minimal hard-\nware modifications. Bra",
			  "CRISP can be\ncombined with these prior approaches to increase coverage by\nreducing the miss penalty of irregular memory accesses. T"
			]
		  },
		  {
			"title": "Perceptron-Based Prefetch Filtering - Engineering People Site",
			"url": "https://people.engr.tamu.edu/djimenez/pdfs/ppf_isca2019.pdf",
			"excerpts": [
			  "7.2\nLookahead Prefetchers\nUnlike spatial prefetchers, lookahead prefetchers take program order\ninto account when they make predictions. Shevgoor et al. propose\nthe Variable Length Delta Prefetcher (VLDP) [ 35 ], which correlates\nhistories of deltas between cache line accesses within memory pages\nwith the next delta within that page. SPP [ 2 ] and KPCs prefetching\ncomponent [ 36 ] are more recent examples of lookahead prefetchers.\n",
			  "Ishii et al. propose the Access Map Pattern\nMatching prefetcher (AMPM) [ 11 ], which creates a map of all ac-\ncessed lines within a region of memory, and then analyzes this map\non every access to see if any fixed-stride pattern can be identified\nand prefetched that is centered on the current access.",
			  "In a single core configuration, PPF increases performance by\n3.78% compared to the underlying prefetcher, SPP.",
			  "In a multi-core\nsystem running a mixes of memory intensive SPEC CPU 2017 traces,\nPPF saw an improvement of 11.4% over SPP for a 4-core system,\nand 9.65% for an 8-core system.",
			  "Michaud proposes the\nBest-Offset Prefetcher [ 34 ], which determines the optimal offset to\nprefetch while considering memory latency and prefetch timeliness.",
			  "DRAM-Aware\nAMPM (DA-AMPM) [ 32 ] is a variant of AMPM that delays some\nprefetches so they can be issued together with others in the same\nDRAM row, increasing bandwidth utilization.",
			  "Pugsley et al. pro-\npose the Sandbox Prefetcher [ 33 ], which analyzes candidate fixed-\noffset prefetchers in a sandboxed environment to determine which is\nmost suitable for the current program phase."
			]
		  },
		  {
			"title": "Building Efficient Neural Prefetcher",
			"url": "https://www.memsys.io/wp-content/uploads/2023/09/3.pdf",
			"excerpts": [
			  "] Doug Joseph and Dirk Grunwald. 1997. Prefetching using markov predictors. In\n*Proceedings of the 24th annual international symposium on Computer architecture* .\n252",
			  "chun Kim, Seth H Pugsley, Paul V Gratz, AL Narasimha Reddy, Chris Wilker-\nson, and Zeshan Chishti. 2016. Path confidence based lookahead prefetching.\nIn *2016 49th Annual IEEE/ACM International Symposium on Microarchitecture*\n*(MICRO)* . IEEE, 112."
			]
		  },
		  {
			"title": "Arsenal of Hardware Prefetchers",
			"url": "https://www.arxiv.org/pdf/1911.10349v1",
			"excerpts": [
			  "ature Path Prefetcher** (SPP) [ 5 ] stores the stride pat-\nterns in a compressed form in the signature table (ST). Each\nentry in the ST is used to index into the pattern table (PT),\nwhich is used to predict the next stride and also contains the\nconfidence for the current prefetch. The signature is then up-\ndated with the latest stride and is used to recursively lookup\nthe PT to predict more strides. This goes on until the confi-\ndence, which is multiplied with the last prefetch confidenc"
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching For Linked Data ...",
			"url": "https://www.scribd.com/document/861770035/9",
			"excerpts": [
			  "This paper presents a framework for jump-pointer prefetching (JPP) aimed at improving the performance of linked data structures (LDS) by ...Read more"
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "Jump pointers, which provide direct access to non-adjacent nodes, can be used for prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures",
			  "New technique: Jump Pointer Prefetching",
			  "Creates parallelism",
			  "Hides arbitrary latency",
			  "Choice of implementation: software, hardware, cooperative",
			  "Problem: Pointer chasing latency",
			  "pointer loads",
			  "Jump pointer prefetching:\nOverlap pointer loads with each other anyway!"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  " uses the underlying data of the application, and provides an 11.3% speedup using no additional processor state. By adding less than 1/2% space 2 overhead to the second level cache, performance can be further increased to 12.6% across a range of \"real world\" applications.\n*",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems.",
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "There are a number of ways to\nidentify \"likely\" addresses.",
			  "Roth\net al.\nintroduced dependence-based techniques for capturing\nproducer-consumer load pairs [ 12].",
			  "his paper investigates a technique that predicts addresses in\npointer-intensive applications using a hardware only technique with\nno built-in biases toward the layout of the recursive data struc-\nSection Title: A state",
			  "ability to \"run ahead\" of an application has been shown to be a re-\nquirement for pointer-intensive applications [12], which tradition-\nally do not provide sufficient computational work for masking the\nprefetch latency.",
			  "me hybrid prefetch engines [13] do have the\nability to run several instances ahead of the processor, but require\napriori\nknowledge of the layout of the data structure, and in some\ncases, the traversal order of the structu",
			  "es. Content-based prefetching works by examining the content of data as it is moved from memory to the caches. Data values that are likely to be addresses are then translated and pushed to a prefetch buffer. Con",
			  "N\nIn early processor designs, the performance of the processor and\nmemory were comparable, but in the last 20 years their relative\nperformances have steadily diverged [4], with the performance im-\nprovemen"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  "Content-Directed Data Prefetching, a data*\nprefetching architecture that exploits the memory allocation used\nby operating systems and runtime systems to improve the perfor-\nmance of pointer-intensive applications constructed using modem\nlanguage systems. This technique is modeled after conservative\ngarbage collection, and prefetches \"likely\" virtual addresses ob-\nserved in memory references",
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "Prefetching:- - CSE IITM",
			"url": "https://www.cse.iitm.ac.in/~pritam/prefetching.pdf",
			"excerpts": [
			  "**PRITAM MAJUMDER, PACE LAB, CSE DEPT., IIT MADRAS** **9**\nCollins Jamison\nCalder Brad\nTullsen Dean M\nPointer Cache Assisted Prefetching\n2002\nMICRO"
			]
		  },
		  {
			"title": "CS 473 Homework 3 (due 12/20/03) Fall 2004",
			"url": "https://jeffe.cs.illinois.edu/teaching/473/hw3final.pdf",
			"excerpts": [
			  "Storing a complete binary search tree in the van Emde Boas layout allows us to perform any search in O(logB N) memory operations in the cache-oblivious model.Read more"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://diglib.eg.org/bitstreams/c41b2a27-076a-4252-a346-584b2deac5fd/download",
			"excerpts": [
			  "Yoon et al. [YM06] proposed a cache-oblivious. BVH layout for collision detection which applied to a k- d tree for ray tracing, resulted ...Read more",
			  "The common DFS layout performed worst for all node layouts in both memory areas. Excluding layouts that use statistics the equally simple to construct BFS ...Read more",
			  "BVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and p",
			  "reelet based DFS/BFS (TDFS/TBFS):** A treelet is a\nconnected sub-tree of a BVH. For this layout treelets of\nnodes that were accessed above a certain threshold are\nbuilt",
			  "The internal memory layout of a treelet can be cho-\nsen freely. By always adding nodes just to the front or the\nback of the merge queue we automatically obtain a treelet\nin DFS or BFS order. Finally the node order of the whole\ntree is obtained by lining up the nodes of all treelets.",
			  "A32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines)",
			  "A16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache line",
			  "**Table 1:** *Scenes used for benchmarking",
			  "ivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and par",
			  " cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.",
			  "**5.1. Node Layouts**",
			  "e classic BVH node data structure stores a bounding vol-\nume along with pointers to its children. We follow Aila et\nal. [ AL09 ], i.e., a node does not store its bounding box, but\nthe bounding boxes of its children. Both children are fetched\nand tested together, which is more efficient for GPUs due\nto increased instruction level parallelism and allows rough\nfront to back traversal. Depending on the data layout, the\nsize of such a node is at least 56 bytes (2 float values for\nminimum/maximum per dimension and child plus pointers).",
			  "*AoS:** 64 bytes, including 8 bytes padding (fitting 2 nodes\nin one 128B cache line",
			  "*SoA32_24:** 32 + 24 bytes, min/max x/y both children,\nmin/max z both children and pointers, plus 8 bytes\npadding (fitting 4 nodes across 2 128B cache lines",
			  "*SoA16_8:** 3 ** 16 + 8 bytes, min/max x/y child1, min/max\nx/y child2, min/max z both children, pointers (fitting 8\nnodes across 4 128B cache lin",
			  "We also analyzed an SoA8 layout which fitted 16 nodes in\n7 cache lines. As it consistently performed much worse than\nthe other layouts, we excluded it from our experiments.",
			  "**5.2. Tree Layouts**",
			  "A tree layout describes how nodes are grouped in memory.",
			  "We analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:",
			  "DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf",
			  "BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches",
			  "vEB):** A cache-oblivious tree layout\n[ vEB75 ]",
			  "ext we describe our two proposed layouts depending on\nnode access statistics which use a threshold *p* ",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "ped subtrees (SWST):** Swap the sub-trees of a\nnode in a depth-first layout if the fraction of left child ac-\ncesses compared to all child accesses is below *p* ** [ 0 *,* 0 *.* 5 ] .\nLeft children of the nodes form a path whose nodes are\naccessed the most and are spread over fewer cac",
			  "We analyzed six different tree layouts. The first four layouts are two common layouts and two cache-efficient layouts. We further propose two more layouts.Read more"
			]
		  },
		  {
			"title": "[2209.09166] Cache-Oblivious Representation of B-Tree Structures",
			"url": "https://arxiv.org/abs/2209.09166",
			"excerpts": [
			  "CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout with packed memory array.",
			  "In the use of the vEB layout mostly search complexity was considered, so far.",
			  "We describe how to build an arbitrary tree in vEB layout if we can simulate its depth-first search.",
			  " The data structure allows searching with an optimal I/O complexity \\mathcal{O}(\\log_B{N}) and is stored in linear space. ",
			  "ave an amortized I/O complexity \\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)",
			  " amortized time complexity \\mathcal{O}(S\\cdot\\log^2 N) , where S is the size of the subtree and N the size of the whole stored tree. R",
			  "Rebuilding an existing subtree saves the multiplicative \\mathcal{O}(\\log^2 N) in both complexities if the number of vertices on individual tree levels is not changed; it is paid only for the inserted/removed vertices otherwise.",
			  "We propose a general data structure CORoBTS for storing B-tree-like search trees dynamically in a cache-oblivious way combining the van Emde Boas memory layout ...Read more"
			]
		  },
		  {
			"title": "Cache-oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/paper.pdf",
			"excerpts": [
			  "The van Emde Boas layout proceeds recursively. Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree. Suppose first that *h* is\na power of 2. Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + 1. This breaks the tree into the *top recursive subtree* *A* of\nheight *h/* 2, and several *bottom recursive subtrees* *B* 1 , *B* 2 , . . . , *B* ** , each of height *h/* 2",
			  "eaf nodes have the same number of children, then the recursive subtrees all\nhave size roughly\n**\n*N* , and ** is roughly\n**\n*N* . The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2",
			  " memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail* . Each level of detail is a partition of the tree into disjoint recursive\nsubtrees. In the finest level of detail, 0, each node forms its own recursive subtree. In\nthe coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subt",
			  "ur layout is a modified version of Prokops layout for a complete binary tree\nwhose height is a power of 2 [40, pp. 6162]. We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48]. ",
			  "One useful consequence of our rounding scheme is the following.",
			  "Lemma 2.1. *At level of detail* *k* *all recursive subtrees except the one containing*\n ... \ntree, even though the relative order of elements changes very little. Instead we use a\npacked-memory array from Section 2.3 to store the van Emde Boas layout, allowing\nus to make room for changes in the tree.",
			  "n additional technical complication arises in maintaining pointers to nodes that\nmove during an update. We search in the top tree by following pointers from nodes\nto their children, represented by indices into the packed-memory array. When we\ninsert or delete an element in the packed-memory array, an amortized *O* (log 2 *N* ) ele-\nments move. Any element that is moved must let its parent know where it has go",
			  "Thus, each node must have a pointer to its parent, and so each node must also let\nits children know where it has moved.",
			  " nodes that move can have\n*O* (log 2 *N* ) children scattered throughout the packed-memory array, each in separate\nmemory blocks. Thus an insertion or deletion can potentially induce *O* (log 2 *N* ) mem-\nory transfers to update these disparately located pointers. We can afford this large\nCACHE-OBLIVIOUS B-TREES\n11\ncost in an amortized sense by adding another level of (log *N* ",
			  "verall structure of our cache-oblivious B-tree therefore has three levels. The\ntop level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array. The middle level is a collection o",
			  "The van Emde Boas layout proceeds recursively.",
			  "Let *h* be the height of the tree,\nor more precisely, the number of levels of nodes in the tree.",
			  "Conceptually split the tree at the middle level of edges, between nodes\nof height *h/* 2 and *h/* 2 + ",
			  "The memory-transfer analysis views the van Emde Boas layout at a particular\n*level of detail",
			  "Each level of detail is a partition of the tree into disjoint recursive\nsubtrees.",
			  "In the finest level of detail, 0, each node forms its own recursive subtree.",
			  "the coarsest level of detail, ** log 2 *h* ** , the entire tree forms the unique recursive subtree.",
			  "The key property of the van Emde Boas layout is that, at any\nlevel of detail, each recursive subtree is stored in a contiguous block of memory.",
			  "cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.",
			  "op level is a weight-balanced B-tree on ( *N/* log 2 *N* ) elements stored according to a\nvan Emde Boas layout in a packed-memory array",
			  "The middle level is a collection of",
			  "We call the layout the *van Emde Boas*\n*layout* because it resembles the van Emde Boas data structure [47, 48",
			  "The layout of the tree is obtained by\nrecursively laying out each subtree and combining these layouts in the order *A* , *B* 1 ,\n*B* 2 , . . . , *B* ** ; see Figure 2.1.\nI",
			  " The cost of any search\nin this layout is (1 + log *B* +1 *N* ) memory transfers, which is optimal up to constant\nfactors."
			]
		  },
		  {
			"title": "Cache Oblivious Search Trees via Binary ...",
			"url": "https://www.cs.au.dk/~gerth/papers/soda02.pdf",
			"excerpts": [
			  "The basic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22]. The static structures are maintained\nby global rebuilding, i.e. they are rebuilt each time the\ndynamic tree has doubled or halved in size.",
			  "ion.\nFor storing n elements, our proposal uses (1 +  ) n\ntimes the element size of memory, and performs searches\nin worst case O (log B n ) memory transfers, updates\nin amortized O ((log 2 n ) / ( B )) memory transfers, and\nrange queries in worst case O (log B n + k/B ) memory\ntransfers, where k is the size of the output.",
			  " For random searches, we\ncan expect the top levels of the trees to reside in cache.\nFor the remaining levels, a cache fault should happen at\nevery level for the BFS layout, approximately at every\nsecond level for the DFS layout (most nodes reside in the\nsame cache line as their left child), and every (log B n )\nlevels for the van Emde Boas layout.",
			  "In the last part of this paper, we try to assess\nmore systematically the impact of the memory layout\nof search trees by comparing experimentally the effi-\nciency of the cache-oblivious van Emde Boas layout with\na cache-aware layout based on multiway trees, and with\nclassical layouts such as Breath First Search (BFS),\nDepth First Search (DFS), and inorde",
			  " trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "Inside main memory, the BFS is best, but looses by\na factor of five outside. The tree optimized for page size\nis the best outside main memory, but looses by a factor\nof two inside. Remarkably, the van Emde Boas layout\nis on par with the best throughout the range.",
			  "4.3\nConclusion.\nFrom the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "It also appears that in the area of search trees, the\nnice theoretical properties of cache obliviousness seems\nto carry over into practice: in our experiments, the van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  ".\nFigure 4 compares the time for random searches in\nimplicit layouts. For sizes up to cache size ( n = 2 16 ), it\nappears that the higher instruction count for navigating\nin an implicit layout dominates the running times: most\ngraphs are slightly higher than corresponding graphs\nin Figure 3, and the van Emde Boas layout (most\ncomplicated address arithmetic) is the slowest while the\nBFS layout (simplest address arithmetic) is fastest. ",
			  "In particular, our data structure avoids the\nuse of weight balanced B -trees, and can be implemented\nas just a single array of data elements, without the use of\npointers.",
			  "sic idea of our data structure is to maintain\na dynamic binary tree of height log n + O (1) using\nexisting methods [2, 14], embed this tree in a static\nbinary tree, which in turn is embedded in an array\nin a cache oblivious fashion, using the van Emde Boas\nlayout [5, 19, 22",
			  " For storing n elements, our data structure uses\n(1 +  ) n times the element size of memory. ",
			  "Searches are\nperformed in worst case O (log B n ) memory transfers,\nupdates in amortized O ((log 2 n ) / ( B )) memory trans-\nfers, and range queries in worst case O (log B n + k/B )\nmemory transfers, where k is the size of the output.",
			  "From the experiments reported in\nthis paper, it is apparent that the effects of the memory\nhierarchy in todays computers play a dominant role for\nthe running time of tree search algorithms, already for\nsizes of trees well within main memory.",
			  "he van\nEmde Boas layout was competitive with cache aware\nstructures, was better than structures not optimized for\nmemory access for all but the smallest n , and behaved\nrobustly over several levels of the memory hierarchy.",
			  "The basic idea of our data structure is to maintain a\ndynamic binary tree of height log n + O (1) using existing\nmethods, embed this tree in a static binary tree, which\nin turn is embedded in an array in a cache oblivious\nfashion, using the van Emde Boas layout of Prokop.",
			  "he\nvan Emde Boas layout , was proposed by Prokop [19,\nSection 10.2], and is related to a data structure of\nvan Emde Boas [21, 22]."
			]
		  },
		  {
			"title": "Cache-Oblivious Dynamic Search Trees Zardosht Kasheff",
			"url": "https://people.csail.mit.edu/bradley/papers/Kasheff04.pdf",
			"excerpts": [
			  "The tree is laid out in memory in a recursive\nfashion. Let *h* be the height of the binary tree. For simplicity, assume *h* is a power of 2. Let\n*N* be the number of nodes in the tree. We divide the tree into two sections. The first section\nis the top half containing a subtree, sharing the same root as the tree, of height *h/* 2 with\n**\n*N* nodes. The second section is the bottom half containing 2 *h/* 2 subtrees of height *h/* 2,\neach containing about\n**\n*N* nodes. This represents subtree *A* in Figure 2-3. The idea is to\n19\nfirst layout the top half recursively. Then layout the remaining 2 *h/* 2 subtrees recursively in\norder. This represents subtrees *B* 1 *, B* 2 *, . . . , B* *l* in Figure 2-3. In memory, the entire subtree\n*A* would be laid out first, followed by *B* 1 *, . . . , B* *l* . We assume the binary tree is full and\nbalanced. If *h* is not a power of 2, the bottom half of the tree is chosen such that its height\nis a power of 2. Figure 2-3 shows the layout of a binary tree with height 5.\n",
			  "The locations of the children of node *i* are 2 *i* and 2 *i* + 1. Thus, the\nlocation of children may be implicitly calculated. Implicit calculations of children makes the\n35",
			  "The tree is represented in memory as an array. The value at location *i* of the array\ncorresponds to some node of the tree. We need a way of computing the location of the left\nand right children of node *i* . One solution is to have the array store pointers, but pointers\ncost space. Instead, we wish to have an array such that the root of the tree is the first\nelement of the array, and for a given node located at array location *i* , the locations of the\nnodes two children are easily found. This chapter provides details.",
			  "ch that the tree\nmay be traversed with *O* (1+log *B* ( *N* )) memory transfers, which is asymptotically optimal [7].\n**"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/CELBVH.pdf",
			"excerpts": [
			  "he-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (CALB",
			  "he COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH. We use *OpenCCL",
			  "The Eurographics Association and Blackwell Publishing 2006.",
			  "e compare our cache-efficient layouts with other layouts in the*\n*context of collision detection and ray tracing.",
			  "r benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applicatio",
			  " VEB lay-\nout is computed recursively. The tree is partitioned with a hor-\nizontal line so that the maximum height of the tree is divided\ninto half. The resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmos",
			  "he resulting sub-trees are linearly stored by first\nplacing the root sub-tree followed by other sub-trees from left-\nmost to rightmost.",
			  " the performance of our cache-oblivious\nlayout of BVHs (COLBVH) with different layouts includ-\ning depth-first layout(DFL) of the BVH, breadth-first lay-\nout(BFL), van Emde Boas layout (VEB) [vEB77], cache-\noblivious mesh layout (COML) [YLPM05], and a cache-\naware layout obtained by explicitly setting cache size into our\ncache-oblivious layout algorithm (C",
			  "Our layout*\n*computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the",
			  "The COML, as explained in Sec. 4.3, is computed by\nconstructing an undirected graph. This is accomplished by\ngenerating edges between parent and child nodes and between\nnearby nodes on the same level of the BVH.",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime appli",
			  "n our benchmarks, our layouts consistently show better performance*\n*over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the*\n*underlying algorithms or runtime applic",
			  ".\nHavran analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.2.\n**Layouts of geometric meshes:** Many algor",
			  "8.2. Comparison with Cache-Oblivious Mesh Layouts",
			  "We have tested the performance of the OBB-tree collision\n ... \ngraph. The edge creation methods for BVHs described in\nYoon et al. [YLPM05] do not adequately represent access\npatterns of the travers",
			  "Our greedy algorithm is\nbased on greedy heuristics to compute cache-coherent layouts\nbased on parent-child locality. T",
			  "There are several areas for future work. We would like to\nextend our probability formulation that predicts runtime data\naccess patterns of collision queries to consider other proximity\nqueries such as minimum separation distance. W",
			  " The Eurographics Association and Blackwell Publishing 2006.\n",
			  "an analyzes various layouts of BVHs in the context of ray\ntracing and improves the performance by using a compact lay-\nout representation of BVHs [Hav97]. Yoon et al. [YLPM05]\npropose a cache-oblivious mesh layout algorithm to compute\nlayouts of geometric meshes and bounding volume hierar-\nchies. We compare our approach with this algorithm in Sec-\ntion 8.",
			  "outs of geometric meshes:** Many algorithms and repre-\nsentations have been proposed to compute coherent layouts for\nspecialized applications. Rendering sequences (e.g., triangle\nstrips) [Dee95,Hop99] are used to improve rendering through-\nput by optimizing the vertex cache hits in the GPU. Isenburg\nand Gumhold [IG03] propose processing sequences, includ-\ning streaming meshes [IL04], as an extension of rendering se-\nquences for large-data processing. In these cases, global mesh\naccess is restricted to a fixed traversal order. Many algorithms\nuse space filling curves [Sag94] to compute cache-friendly\nlayouts of volumetric grids or height fields. These layouts\nare widely used to improve performance of image process-\ning [VG91] and terrain or volume visualization [PF01,L",
			  "We introduce a new probabilistic\nmodel to predict the runtime access patterns of BVHs based on\nlocalities.",
			  "ecifically, we utilize two types of localities during\ntraversal of a BVH: parent-child and spatial localities between\nthe accessed node",
			  "In this section, we define two localities that are used to com-\npute a cache-efficient layout of a BV",
			  "to achieve this\ngoal, we recursively compute the clusters. We first decompose\nthe BVH into a set of clusters and recursively decompose each\ncluster. In this case, the cache block boundaries can lie any-\nwhere within a layout that corresponds to the nodes of these\nclusters. Therefore, we need to compute a cache-efficient or-\ndering of the clusters computed at each level of recursion.\nOur algorithm has two different components that handle\nparent-child and spatial localities. In particular, the first part\nof our algorithm decomposes a BVH into a set of clusters that\nminimize the cache misses for parent-child locality. The clus-\nters are classified as a root cluster and child clusters. The root\ncluster contains the root node of the BVH and child clusters\nare created for each node outside the root cluster whose par-\nent node is in the root cluster (see the middle image in Fig.\n4). The second part of the algorithm computes an ordering of\nthe clusters and stores the root cluster at the beginning of the\nordering. The ordering of child clusters is computed by con-\nsidering their spatial locality. Then, we can merge two child\nclusters if it can further decrease the size of the working set.\nWe recursively apply this two-fold procedure to compute an\nordering of all the BVs in the BVH"
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://meistdan.github.io/publications/bvh_star/paper.pdf",
			"excerpts": [
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged."
			]
		  },
		  {
			"title": "A Survey on Bounding Volume Hierarchies for Ray Tracing",
			"url": "https://diglib.eg.org/bitstream/handle/10.1111/cgf142662/v40i2pp683-712.pdf",
			"excerpts": [
			  "SWST aims to achieve better cache\nlocality by swapping subtrees of a node in a depth-first layout. If\nthe right child is more accessed than the left, the nodes subtrees\nare exchanged.",
			  "he latter, treelet-based layouts, divide a BVH into\ntreelets by merging the most frequently accessed nodes. The differ-\nence between TDFS and TBFS is whether the treelets are created\nin depth-first or breadth-first order",
			  " authors compared the pro-\nposed layouts against DFS, BFS, van Emde Boas layout, and COL-\nBVH, showing that TDFS achieves the highest speedup on average.",
			  "Liktor and Vaidyanathan [ LV16 ] proposed a two-level clustering\nscheme, which decomposes a given BVH into clusters similar to\nCOLBVH. The key difference is the use of two different types of\nclusters to further reduce bandwidth and cache misses.",
			  "s first recursively decomposed into a specified number of *address*\n*clusters* (ACs), in which child pointers can be represented with re-\nduced precision (i.e., child pointers are compressed). Next, *cache*\n*clusters* (CCs) are recursively generated within each AC. CCs are\ncache-aware, meaning that their size is determined to fit withi",
			  "In this report, we review the basic principles of bounding volume hierarchies as well as advanced state of the art methods with a focus on the construction and ...Read more"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies | Request PDF",
			"url": "https://www.researchgate.net/publication/220507680_Cache-Efficient_Layouts_of_Bounding_Volume_Hierarchies",
			"excerpts": [
			  "Yoon and Manocha [YM06] proposed a node layout algorithm known as cache-oblivious BVH (COLBVH) that recursively decomposes clusters of nodes and works without prior knowledge of the cache, such as the block size.",
			  "In initialization, each node is assigned the probability that the node is accessed, given that the cluster's root is already accessed."
			]
		  },
		  {
			"title": "CacheEfficient Layouts of Bounding Volume Hierarchies - Yoon - 2006 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2006.00970.x",
			"excerpts": [
			  "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			  "Section Title: Cache-Efficient Layouts of Bounding Volume Hierarchies > Abstract",
			  "*We present a novel algorithm to compute cache-efficient layouts of bounding volume hierarchies (BVHs) of polygonal models. Our approach does not make any assumptions about the cache parameters or block sizes of the memory hierarchy. We introduce a new probabilistic model to predict the runtime access patterns of a BVH. Our layout computation algorithm utilizes parent-child and spatial localities between the accessed nodes to reduce both the number of cache misses and the size of the working set. Our algorithm also works well for spatial partitioning hierarchies including kd-trees. We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of millions of triangles. We compare our cache-efficient layouts with other layouts in the context of collision detection and ray tracing. In our benchmarks, our layouts consistently show better performance over other layouts and improve the performance of these applications by* 26% ** 300% *without any modification of the underlying algorithms or runtime applications.*",
			  "Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Hierarchy and Geometric Transformations"
			]
		  },
		  {
			"title": "Memory Hierarchy Sensitive Graph Layout",
			"url": "https://arxiv.org/pdf/1203.5675",
			"excerpts": [
			  "The VEB\nlayout is a layout of a tree that is done by repeatedly splitting it at\nthe middle and *recursively* laying out all the component subtrees\nin contiguous units of memory.",
			  "In the figure, the tree of depth *D* is\nsplit into a subtree (rooted at the original tree) of depth *D*\n2 and this\nis recursively laid out first.",
			  " Next, the remaining subtrees, *O* ( 2\n*D*\n2 )\nin number, are laid out recursively.",
			  "The VEB layout is complex\nto setup and maintain for trees and difficult to apply to graphs in\ngeneral.",
			  "The first step in applying it to a graph is to traverse the\ngraph and prepare a sub-graph in the form of a tree that covers it.",
			  "Figure 2 shows graphically how this might be done. Assume\nan algorithm *P* *i* that aims to copy a tree while traversing it, into\nblocks that fit into the cache at level *i* . Using breadth first search,\nit can discover the entire subtree that fits into a block at level *i*",
			  "This is shown in the Figure and corresponds (roughly)\nto the recursive layout achieved by VEB. The key difference is\nthat we *know* where to cut the spanning tree based on runtime\ninformation about the memory hierarchy rather than simply using\nhalf the diameter of the graph.",
			  "he van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7]",
			  "A third class of techniques (including the one in this paper) are\nused at runtime. One approach is to control memory allocation.",
			  "The van Emde Boas layout [20]\nforms the basis for many cache oblivious designs including those\nfor cache oblivious B-trees [7].",
			  "he knee around the tree depth of 18. This is because be-\nyond that depth the tree no longer fits in the 6MB last level cache\nleading to a sudden increase in query time",
			  "ble 1.** Cachegrind miss rates",
			  "Figure 4.** Binary Search Tree Performan",
			  "Not every level of cache has an equal impact on performance."
			]
		  },
		  {
			"title": "Depth first or breadth first ordering in binary search trees? - Computer Science Stack Exchange",
			"url": "https://cs.stackexchange.com/questions/51443/depth-first-or-breadth-first-ordering-in-binary-search-trees",
			"excerpts": [
			  "There's a paper on this: [Khuong and Morin. Array Layouts For Comparison-Based Searching](http://arxiv.org/pdf/1509.05053.pdf)\nThey compare the Eytzinger, B-Tree, Van Emde Boas, and sorted array layouts and conclude that Eytzinger works best. The reasons are fairly complex, since things like simple address arithmetic and branch predictability combine with memory prefetch and processor features like speculative execution. They also rely on doing a fair amount of extra work by prefetching blocks which have only a small chance of matching the search argument.\nHowever they do give a clear exposition of each mechanism.",
			  "The ordering I've actually heard most for making cache-friendly BSTs is the [van Emde Boas layout](http://www.cs.au.dk/~gerth/papers/soda02.pdf) , which is formed as follows:\nIf the tree has height two or less, lay it out in DFS or BFS order (they're the same here).\nOtherwise, split the tree at the middle level into a \"top tree\" of the first half of the nodes and up to $\\sqrt{n}$ \"bottom trees\" formed from the lower levels. Recursively compute the van Emde Boas layouts of each of these trees, then concatenate them together in order.\n ..."
			]
		  },
		  {
			"title": "\n\t\tCache Oblivious Search Trees via Binary Trees of Small Height\n\t\t\t\t\t\t\t| BRICS Report Series\n\t\t\t",
			"url": "https://tidsskrift.dk/brics/article/view/21696",
			"excerpts": [
			  "We propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.",
			  "For storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "The source code of the programs, our scripts and tools, and the data we present here are available online under ftp.brics.dk/RS/01/36/Experiments/.",
			  "Section Title: Cache Oblivious Search Trees via Binary Trees of Small Height > Abstract",
			  "Content:\nWe propose a version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds. In particular, our data structure avoids the use of weight balanced B-trees, and can be implemented as just a single array of data elements, without the use of pointers. The structure also improves space utilization.\nFor storing n elements, our proposal uses (1+epsilon)n times the element size of memory, and performs searches in worst case O(log_B n) memory transfers, updates in amortized O((log^2 n)/(epsilon B)) memory transfers, and range queries in worst case O(log_B n + k/B) memory transfers, where k is the size of the output.\nThe basic idea of our data structure is to maintain a dynamic binary tree of height log n + O(1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.\nWe also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "Van Emde Boas tree",
			"url": "https://en.wikipedia.org/wiki/Van_Emde_Boas_tree",
			"excerpts": [
			  "A van Emde Boas tree also known as a vEB tree or van Emde Boas priority queue, is a tree data structure which implements an associative array with m-bit ...Read more"
			]
		  },
		  {
			"title": "The Cost of Cache-Oblivious Searching",
			"url": "https://www3.cs.stonybrook.edu/~bender/newpub/2011-algorithmica-BenderBrFa-co-searching.pdf",
			"excerpts": [
			  "atic cache-oblivious search tree is built as follows: Embed a complete binary tree with\nN nodes in memory, conceptually splitting the tree at half its height, thus obtaining (\n\nN ) subtrees each\nwith (\n\nN ) nodes. Lay out each of these trees contiguously, storing each recursively in memory. This type\nof recursive layout is commonly referred to in the literature as a van Emde Boas layout because it is remi-\nniscent of the recursive structure of the van Emde Boas tree [37,38]. The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26",
			  " The static cache-oblivious search tree\nis a basic building block of most cache-oblivious search structures, including the (dynamic) cache-oblivious\nB-tree [14,15,15,22,32] and other cache-oblivious search structures [1,6,11,12,1621,25,26]. ",
			  "We present the following results:",
			  "We present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.",
			  " Intuitively, the improvement of uneven splitting, as compared to the even splitting in the standard van\nEmde Boas layout, is likely to be due to the generation of a variety of subtree sizes at each recursive\nlevel of the layout. Such a variety will on any search path reduce the number of subtrees that can have\nparticularly bad sizes compared to the block size B",
			  "Finally, we demonstrate that it is harder to search in the cache-oblivious model than in the DAM model.\nPreviously the only lower bound for searching in the cache oblivious model was the log B N lower bound\nfrom the DAM model. We prove a lower bound of lg e log B N memory transfers for searching in the\naverage case in the cache-oblivious model.",
			  "\n We then present a class of generalized van Emde Boas layouts that optimizes performance through\nthe use of uneven splits on the height of the tree. For any constant  > 0, we optimize the layout\nachieving a performance of [lg e +  + O (lg lg B/ lg B )] log B N + O (1) expected memory transfers. ",
			  "In this section we give a tight analysis of the cost of searching in a binary tree stored using the van Emde. Boas layout [31]. As mentioned earlier, in the vEB ...Read more",
			  "s.\nWe present the following results:\n We give an analysis of Prokops static cache-oblivious search tree [31], proving that searches perform\nat most 2\n\n1 +\n3\n\nB\n\nlog B N + O (1) expected memory transfers; the expectation is taken only over the\nrandom placement of the data structure in memory. This analysis is tight to within a 1 + o (1) factor.\n ",
			  "in the vEB layout, the tree is split evenly by height, except for\nroundoff. Thus, a tree of height h is split into a top tree of height  h/ 2  and bottom tree of height  h/ 2  . It\nis known [15,22] that the number of memory transfers for a search is 4 log B N in the worst case ; we give a\nmatching configuration showing that this analysis is tight. We then consider the average-case performance\nover all starting positions of the tree in memory, and we show that the expected search cost is 2(1 +\n3 /\n\nB ) log B N + O (1) memory transfers, which is tight within a 1 + o (1) factor. We assume that the data\nstructure begins at a random position in memory; if there is not enough space, then the data structure\nwraps around to the first location in memory.\nA",
			  "trees.\nThe generalized vEB layout is as follows: Suppose the complete binary tree contains N  1 = 2 h  1\nnodes and has height h = lg N . Let a and b be constants such that 0 < a < 1 and b = 1  a . Conceptually\nwe split the tree at the edges below the nodes of depth  ah  . This splits the tree into a top recursive subtree\nof height  ah  , and k = 2  ah  bottom recursive subtrees of height  bh  . Thus, there are roughly N a bottom\nrecursive subtrees and each bottom recursive subtree contains roughly N b nodes. We map the nodes of the\ntree into positions in the array by recursively laying out the subtrees contiguously in memory. The base case\nis reached when the trees have one node, as in the standard vEB layout.\nWe",
			  "We find the values of a and b that yield a layout whose memory-transfer cost is arbitrarily close to\n[lg e + O (lg lg B/ lg B )] log B N + O (1) for a = 1 / 2   and large enough N . We focus our analysis on the first\nlevel of detail where recursive subtrees have size at most the block size B . In our analysis memory transfers\ncan be classified in two types. There are V path-length memory transfers , which are caused by accessing\ndifferent recursive subtrees in the level of detail of the analysis, and there are C page-boundary memory"
			]
		  },
		  {
			"title": "Memory Layouts for Binary Search",
			"url": "https://cglab.ca/~morin/misc/arraylayout/",
			"excerpts": [
			  "`veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature.",
			  "In many settings B-trees (with a properly\nchosen value of B) are best. In others, the Eytzinger layout\nwins. In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "In many settings B-trees (with a properly\nchosen value of B) are best.",
			  "In others, the Eytzinger layout\nwins.",
			  "In others, still, the van Emde Boas layout is the winner\n(at least for large enough array sizes).",
			  "Which of these array memory layouts is fastest?",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "The answer is complicated, and it seems to depend on the data\nsize, the cache size, the cache line width, and the relative\ncache speed.",
			  "For an example, consider the following two graphs, generated\nby running the same code on two different Intel machines.",
			  "In\nthe left graph, the Eytzinger layout is almost as slow as a\nplain sorted array while the van Emde Boas and B-tree layouts\nare more than twice as fast.",
			  "In the right graph, the Eytzinger layout and b-tree are the\nfastest, the sorted array is still the slowest, and the vEB layout\nis somewhere in betweeen (for array sizes).",
			  "veb` : An implicit binary search tree packed\ninto an array using the van Emde Boas layout seen in\nthe cache-oblivious literature"
			]
		  },
		  {
			"title": "Cache oblivious search trees via binary trees of small height | Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms",
			"url": "https://dl.acm.org/doi/10.5555/545381.545386",
			"excerpts": [
			  "Section Title: Cache oblivious search trees via binary trees of small height",
			  "ng Brodal](# \"Gerth Stlting Brodal\") Gerth Stlting Brodal\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81409594931) , [Rolf Fagerberg](# \"Rolf Fagerberg\") Rolf Fagerberg\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100166398) , [Riko Jacob](# \"Riko Jacob\") Riko Jacob\nUniversity of Aarhus, Ny Munkegade, DK-8000 rhus C, Denmark\n[View Profile](/profile/81100438116) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386)\n[SODA '02: Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms](/doi/proceedings/10.5555/545381)\nPages 39 - 48\nPublished : 06 January 2002 [Publication History](#) To view Publication History, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F545381.545386) [](# \"Check for updates on crossmark\")\n... citation ... Downloads\n[](#) To get citation alerts, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.5555%2F54",
			  "For storing *n* elements, our proposal uses (1 + ) *n* times the element size of memory, and performs searches in worst case *O* (log *B* *n* ) memory transfers, updates in amortized *O* ((log 2 *n* )/( *B* )) memory transfers, and range queries in worst case *O* (log *B* *n + k/B* ) memory transfers, where *k* is the size of the output.The ",
			  "The basic idea of our data structure is to maintain a dynamic binary tree of height log *n+O* (1) using existing methods, embed this tree in a static binary tree, which in turn is embedded in an array in a cache oblivious fashion, using the van Emde Boas layout of Prokop.",
			  "We also investigate the practicality of cache obliviousness in the area of search trees, by providing an empirical comparison of different methods for laying out a search tree in memory."
			]
		  },
		  {
			"title": "[PDF] Cache Oblivious Search Trees via Binary Trees of Small Height | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache-Oblivious-Search-Trees-via-Binary-Trees-of-Brodal-Fagerberg/da35c4651414b03abe079b5c8454948c59f372c0",
			"excerpts": [
			  "Cache Oblivious Search Trees via Binary Trees of Small Height",
			  "A version of cache oblivious search trees which is simpler than the previous proposal of Bender, Demaine and Farach-Colton and has the same complexity bounds is proposed, and can be implemented as just a single array of data elements, without the use of pointers.",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Efficient Layouts of Bounding Volume Hierarchies",
			"url": "http://gamma.cs.unc.edu/COLBVH/",
			"excerpts": [
			  "We present a novel algorithm to compute cache-efficient layouts of\nbounding volume hierarchies (BVHs) of polygonal\nmodels.",
			  "We does not make any\nassumptions about the cache parameters or block sizes of the memory hierarchy.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH.",
			  "Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%\nwithout any modification of the underlying algorithms or runtime\napplications.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing.",
			  "Paper: [Cache-Efficient Layouts of Bounding Volume Hierarchies](CELBVH.pdf) , Computer graphics forum (Eurographics), volume 25, issue 3, 2006, pp. 507-516",
			  "Section Title: by [Sung-Eui Yoon](http://jupiter.kaist.ac.kr/~sungeui/) and [Dinesh Manocha](http://www.cs.unc.edu/~dm/) .",
			  "llision Detection between Hugo and 1M Power Plant Models:** The hugo robot model is placed inside the power plant model, whose\noverall shape is shown on the right. We are able to achieve 35%--2600%\nperformance improvement in collision detection by using our cache-efficient\nlayouts of the OBBTree over other tested layouts.",
			  "We introduce a new probabilistic model to predict the runtime access patterns\nof a BVH. Our layout computation algorithm utilizes parent-child and spatial\nlocalities between the accessed nodes to reduce both the number of cache\nmisses and the size of working set.",
			  "We compare our cache-efficient layouts with other\nlayouts in the context of collision detection and ray tracing. In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%",
			  "without any modification of the underlying algorithms or runtime\napplications.",
			  "We use our algorithm to compute layouts of BVHs and spatial partitioning hierarchies of large models composed of\nmillions of triangles.",
			  " In our\nbenchmarks, our layouts consistently show better performance over other layouts\nand improve the performance of these applications by 26%--300%"
			]
		  },
		  {
			"title": "[PDF] CacheEfficient Layouts of Bounding Volume Hierarchies | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Cache%E2%80%90Efficient-Layouts-of-Bounding-Volume-Yoon-Manocha/77fdc59d6f28a3eb5da7c9fc134205a8f498f306",
			"excerpts": [
			  "A novel algorithm to compute cacheefficient layouts of bounding volume hierarchies (BVHs) of polygonal models and a new probabilistic model to predict the runtime access patterns of a BVH is introduced",
			  "Published in Computer graphics forum 1 September 2006\n",
			  "TLDR"
			]
		  },
		  {
			"title": "Cache-Oblivious B-Trees",
			"url": "https://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/",
			"excerpts": [
			  "Demaine, and Martin Farach-Colton, Cache-Oblivious B-Trees, SIAM Journal on Computing, volume 35, number 2, 2005, pages 341358. Abstract: This paper presents ...Read more"
			]
		  },
		  {
			"title": "Concurrent Cache-Oblivious B-Trees - People",
			"url": "https://people.csail.mit.edu/bradley/papers/BenderFiGi05.pdf",
			"excerpts": [
			  "e first review the performance models used to analyze cache-\nefficient data structures, and then review three variations on serial\ncache-oblivious B-trees.",
			  "ernal-memory data structures, such as B-trees, are tradition-\nally analyzed in the *disk-access model (DAM)* [1], in which internal\nmemory has size *M* and is divided into blocks of size *B* , and exter-\nnal memory (disk) is arbitrarily larg",
			  "Performance in the DAM\nmodel is measured in terms of the number of block transfers.",
			  "Thus\nB-trees implement searches asymptotically optimally in the DAM\nmodel.",
			  "The *cache-oblivious model* [15,26] is like the DAM model in that\nthe objective is to minimize the number of data transfers between\ntwo levels.",
			  "A CO B-tree [8] achieves nearly optimal locality of reference at\nevery level of the memory hierarchy. It optimizes simultaneously\nfor first- and second-level cache misses, page faults, TLB misses,\ndata prefetching, and locality in the disk subsystem."
			]
		  },
		  {
			"title": "Cost of Divergence in Ray Tracing: Performance ...",
			"url": "http://prism.sejong.ac.kr/dossa-5/dossa_paper/paper3_hansung-kim-cost-of-divergence-for-ray-tracing-camera-ready-2nd.pdf",
			"excerpts": [
			  "\nFigure 5(a) shows a top-down breakdown of the total CPU\nexecution cycles for BVH traversal operations, measured with\nVTune. We observe that *Back-end stall* , where the pipeline\nis stalled by memory operations, is the dominant bottleneck,\nfollowed by *Bad speculation*",
			  "To mitigate\nthis, we modified the memory allocator implementation in\nEmbree [8] to allocate BVH in a single 1GB hugepage, which\nled to an end-to-end rendering speedup of 6.4%.",
			  "Divergent memory access is a major source of latency.",
			  "d stall* cycles reveals that most of the memory stalls are\ncaused by TLB misses. This is possible due to the virtually-\nindexed physically-tagged nature of L1 data cache, where TLB\nmisses can delay tag access latency and thereby the hit latency\nof the cache.",
			  "Ray sorting improves memory\ndivergence by mapping coherent rays to adjacent threads, and\nthis is evidenced by the fewer number of L1 sectors per\nrequest.",
			  "IMT execution divergence** severely limits per-\nformance of software-based GPU ray tracing. For our CUDA\nworkload, 44% of the threads in a single SIMD unit is inactive\nduring execution. This is because threads mapped to divergent\nrays unavoidably execute divergent branches or terminate early\nduring traversal, causing them to become inactive due to the\nlockstep execution of SI"
			]
		  },
		  {
			"title": "OPTIMIZING QUERY TIME IN A BOUNDING VOLUME ...",
			"url": "https://benedikt-bitterli.me/bvh-report.pdf",
			"excerpts": [
			  "To reduce the likelihood of this occurring, we implemented\na special tree layout from literature, the van Emde Boas\nordering[ **?** ], which is a cache-oblivious layout designed to\nkeep certain subtrees close together in memory.",
			  "The van Emde Boas ordering of a single node is the node\nitself.",
			  "The van Emde Boas ordering of a tree *T* of depth *d* *T*\nis the van Emde Boas ordering of the top subtree ending at\ndepth ** *d* *T*\n2 ** followed by the van Emde Boas ordering of all\nchild subtrees rooted at depth ** *d* *T*\n2 ** + 1 .\nIn",
			  ".\nInformally, this guarantees that any subtree is likely to\nbe stored in a contiguous memory segment; in other words,\nthe traversal algorithm is likely to work in a locally contigu-\nous memory segment for many traversal steps before mak-\ning a large jump through memory.",
			  "Although this should increase performance in theory, in\npractice, no performance improvement can be observed. It\nappears that TLB misses do not play a significant role in the\ntraversal performance."
			]
		  },
		  {
			"title": "Cache-Oblivious Algorithms and Data Structures",
			"url": "https://erikdemaine.org/papers/BRICS2002/paper.pdf",
			"excerpts": [
			  "A comparison of cache aware and cache oblivious static search trees using program in- strumentation. In Experimental Algorithmics: From Algorithm Design to.Read more"
			]
		  },
		  {
			"title": "Cache-oblivious algorithms and data structures",
			"url": "https://scispace.com/pdf/cache-oblivious-algorithms-and-data-structures-jjcrutokhi.pdf",
			"excerpts": [
			  "Prokop in [60] proposed static cache-oblivious search trees with search cost\nO (log B N ) I/Os, matching the search cost of standard (cache-aware) B-trees [17].",
			  "\nThe search trees of Prokop are related to a data structure of van Emde Boas [67,\n68], since the recursive layout of a search tree generated by Prokops scheme re-\nsembles the layout of the search trees of van Emde Boas.",
			  "The constant in the\nO (log B N ) search cost was studied in [21], where it is proved that no cache-\noblivious algorithm can achieve a performance better than log 2 e  log B N I/Os,\ni.e. a factor  1 . 44 slower than a cache-aware algorithm.",
			  "Dynamic B-trees were first presented by Bender et al. [22] achieving searches\nin O (log B N ) I/Os and updates requiring amortized O (log B N ) I/Os.",
			  "A cache-oblivious dictionary based on exponential search trees was presented\nin [19]."
			]
		  },
		  {
			"title": "Interactive Visualization and Collision Detection using ...",
			"url": "https://sgvr.kaist.ac.kr/~sungeui/thesis/phd_thesis_yoon_2005.pdf",
			"excerpts": [
			  "We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layou",
			  "erall, our approach offers the following\nbenefits:\n1. **Generality:** Our algorithm is general and applicable to all kind of BVHs. It\ndoes not require any knowledge of cache parameters or block sizes of a memory\nhierarchy.\n159\n2. **Applicability:** Our algorithm does not require any modification of BVH-based\nalgorithms or the runtime application. We simply compute cache-oblivious lay-\nouts of BVHs without making any assumptions about the applications.\n3. **Improved performance:** Our layouts reduce the number of cache misses during\ntraversals of BVHs. We are able to improve the performance of collision queries\nduring dynamic simulation by 2 ** 5 times by using our layouts. Main improve"
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different ...",
			"url": "https://download.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Wodniok_2013_GCB.pdf",
			"excerpts": [
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the\nsurface areas of its grand-parent and parent.\nNext we describe our ",
			  " Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**5.2. Tree Layouts**\nA tree layout describes how nodes are grouped in memory.\nWe analyzed six different tree layouts. The first four layouts\nare two common layouts and two cache-efficient layouts. We\nfurther propose two more layouts. The idea behind them is\nto compute a path traced image at a relatively low sample\nrate as a pre-process, recording the number of accesses for\neach BVH node. We then use the access statistics to guide\nthe two layouting methods. Layouts not using statistics are:\n** **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.\n** **Breadth-first-search (BFS):** Nodes are ordered as visited\nby a breadth-first traversal visiting the left child node first.\nThis fits best for rays traversing neighboring branches.\n** **van Emde Boas (vEB):** A cache-oblivious tree layout\n[ vEB75 ] described in Section 2 .\n** **COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of the",
			  "In global memory we have\nachieved a runtime reduction by 1%6%. We gained a 30%\n40% runtime reduction compared to the baseline in global\nmemory when the BVH is stored in texture memory simi-\nlar to Aila et al. [ ALK12",
			  " **Depth-first-search (DFS):** Nodes are ordered as visited\nby a pre-order traversal. This layout performs best with\ncoherent rays since a cache line is potentially filled with\nnodes on the path to the leaf.",
			  ":** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing. Beginning with all *n* nodes in a root cluster, the\ntree is recursively decomposed into clusters of ** ** *n* + 1 **\n1 ** nodes. Nodes are merged into root clusters depending\non their access probability computed from the ratio of th",
			  "**COLBVH (COL):** A cache-oblivious tree layout mainly\nused for collision detection [ YM06 ] but also applicable to\nraytracing."
			]
		  },
		  {
			"title": "Bandwidth-Efficient BVH Layout for Incremental Hardware ...",
			"url": "https://diglib.eg.org/server/api/core/bitstreams/e9925803-977c-47e1-b1fd-a11633cdd229/content",
			"excerpts": [
			  "*Address Cluster (AC):** A continuous address space that can be\nreferenced by a small pointer. If the original BVH can address\n2 *n* nodes, the maximum size of an AC is 2 *n* */* 2 .",
			  "We achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address spac",
			  " present a novel BVH memory layout for incremental traver-\nsal that improves cache locality and compresses the child pointers,\nin order to effectively reduce the bandwidth.",
			  "therefore propose a two-level clustering scheme that al-**\n**lows node reordering while storing** ***two small*** **child pointers on**\n**the footprint of a regular pointer** :",
			  "n describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footpr",
			  "OLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next.",
			  "L1 bandwidth** In Fig. 8 A we show the overall bandwidth require-\nments of traversing the same set of rays using different BVH node\nlayo",
			  "4. Two-Level Clustering",
			  "In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft",
			  " consider that storing *P* *Le ft* allows less nodes per\ncache line. The size *|* *P* *|* can be up to 4 bytes, which is small\ncompared to conventional BVH encodings used by previous wo",
			  "However, a node pair can be quantized to as small as 8 bytes using\nDFL (Sec. 6.1 ), whereas clustering would require up to 12 bytes.",
			  "C* can maintain the node size of the depth-first layout (or\neven reduce it), while the *CC* reorders nodes within the same *AC* for\nthe best cache utilization.",
			  "5. Algorithm",
			  "In order to effectively reduce the working set, we must carefully\nselect the nodes for each cluster. We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ].",
			  " We adopt the probabilistic model\nproposed by Yoon and Manocha [ YM06 ]. They attempt to order the\nnodes according to the most likely traversal path based on *parent-*\n*child* and *spatial locality* .",
			  "ssuming that all traversal paths start at the root of the tree, their\n*COLBVH (Cache-Oblivious Layout of BVHs)* algorithm iteratively\nmerges the child nodes that are the most likely to be traversed next",
			  "4.1. Glue Nodes",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  " use of small pointers limits the number of nodes within an *AC* .\nIn order to support larger BVHs, we need a new node type that\npoints outside this limited range. We call these glue nodes re-\nferring to their connecting role: they store a single full-precision\npointer to the root of a new address cluste",
			  "lue nodes* only generate bandwidth when the\nchild node is traversed, not when accessing the parent node.",
			  "Glue Nodes",
			  "7.2. Bandwidth Analysis",
			  "*A novel node layout and addressing scheme*",
			  " achieve memory bandwidth reduction at two levels: we com-\npress the child pointers by forming clusters within the BVH,\neach within an arbitrarily reduced-precision address space. We\nthen choose the order of nodes inside these clusters to maxi-\nmize the cache line locality. We introduce a new node type to\nreference address-space changes during traversal. This keeps the\nnode sizes uniform, which is more suited for a fixed function\nhardware.",
			  "ct node ordering** schemes can eliminate a few child point-\ners from the BVH. Depth-first layout (DFL) places the left child\ndirectly after the parent node, therefore only the the right pointer\nis required. Alternatively, two sibling nodes can be stored sequen-\ntially [ AL09 ]. Besides compression, these layouts can also improve\ncache locality, since child nodes are often tested together following\nthe parent during traversal. Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent",
			  "ubtree partitioning** methods first decompose the BVH into clus-\nters of nodes, each containing one or more subtrees. By optimizing\nthe node order for multiple traversal paths it can further improve\ncache locality. Moreover, the size of the child pointers within clus-\nters may be reduced. This optimization was presented for BSP trees\nby Havran [ Hav97 ]. Gil and Itai [ GI99 ] showed that cache local-\nity for tree traversal can be significantly improved if the clusters\nof nodes are generated top-down, by greedily merging the children\nwith the highest probability. Yoon et al. [ YM06 ] applied this theory\nto kd-tree based ray travers",
			  "**8-byte Internal Node:**",
			  "6 x 6 bits per plane",
			  "6 bits",
			  "2 bits",
			  "2 x 10 bits",
			  "Figure 3: A 2D illustration of our quantized storage of sibling nodes\nwith parent-plane sharing (top). The layout of our internal nodes\n(bottom). We store 2 bits to indicate leaves, one low-precision\npointer per child, and 6 plane offsets (z-axis not shown). Finally,\nour reuse mask is set to 1 if the corresponding plane belongs to the\nleft child.",
			  "Nah et al. [ NPK ** 10 ] improved cache lo-\ncality using an *ordered depth-first layout* (ODFL), storing the child\nnode with the largest surface area, next to the parent.\n**",
			  " compare our method (green) with the standard ODFL (gray).\nWhen scaling the L2 cache with a fixed L1, we see a different\ntrend: as the capacity of L2 increases, the reduction achieved by our\nmethod slowly diminishes. Our explanation is that the outstanding\nmisses from L2 become less and less coherent and since more of the\nfrequently traversed nodes reside inside L2, the clustering heuris-\ntic cannot predict the outgoing access pattern anymore. There is\nanother interesting trend regarding the utilization of the traversal\nunit, which increases with the L2 capacity.",
			  "hen describing how changing the order of nodes can affect band-\nwidth, we *assumed that any*  *layout fits into the same memory*\n*footprint* . In practice this may not hold. When using depth-first lay-\nout (DFL), an internal node can be represented as { *P* *Right* *|* *BV* }: the\npointer to the right child node, and the node bounds ( *BV* ). Since the\nleft child directly follows, *P* *Le ft* can be omit",
			  "ache Cluster (CC):** A small set of nodes that fits within a\ncache line, created within an *AC* ",
			  " Two-Level Clustering*",
			  " Two-Level Clustering*"
			]
		  },
		  {
			"title": "The Ultimate Guide to Bounding Volume Hierarchies",
			"url": "http://www.lufei.ca/posts/BVH.html",
			"excerpts": [
			  "Alternatively, proposals including Cache-Oblivious BVH (COLBVH) 19 and Swapped Subtrees (SWST) 20 organize the tree into subtree clusters in memory.Read more"
			]
		  },
		  {
			"title": "Further Reading",
			"url": "https://pbr-book.org/4ed/Primitives_and_Intersection_Acceleration/Further_Reading",
			"excerpts": [
			  "Vaidyanathan et al. ( [2016](:Vaidyanathan2016) ), who introduced a\nreduced-precision representation of the BVH that still guarantees\nconservative intersection tests with respect to the original BVH.",
			  "Liktor\nand Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node\nrepresentation based on clustering nodes that improves cache performance\nand reduces storage requirements for child node pointers.",
			  "Ylitie et\nal. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs\ninto wider BVHs with more children at each node, from which they derived a\ncompressed BVH representation that shows a substantial bandwidth reduction\nwith incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )\ndeveloped an algorithm for efficiently traversing such wide BVHs using a\nsmall stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing\nsets of adjacent leaf nodes of BVHs under the principle that most of the\nmemory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described\nan approach that saves both computation and storage by taking advantage of\nshared planes among the bounds of the children of a BVH node.",
			  "Other work in the area of space-efficient BVHs includes that of",
			  "reduced-precision representation of the BVH that still guarantees",
			  "conservative intersection tests with respect to the original BVH.",
			  "and reduces storage requirements for child node pointers. Ylitie et",
			  "with incoherent rays. Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "small stack. Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "Section Title: Further Reading > Grids > Bounding Volume Hierarchies",
			  "Liktor",
			  "Liktor",
			  "and Vaidyanathan ( [2016](:Liktor2016) ) introduced a BVH node",
			  "representation based on clustering nodes that improves cache performance",
			  "representation based on clustering nodes that improves cache performance",
			  "and reduces storage requirements for child node pointers.",
			  "Ylitie et",
			  "al. ( [2017](:Ylitie2017) ) showed how to optimally convert binary BVHs",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "into wider BVHs with more children at each node, from which they derived a",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "compressed BVH representation that shows a substantial bandwidth reduction",
			  "with incoherent rays.",
			  "Vaidyanathan et al. ( [2019](:Vaidyanathan2019) )",
			  "developed an algorithm for efficiently traversing such wide BVHs using a",
			  "small stack.",
			  "Benthin et al. ( [2018](:Benthin2018) ) focused on compressing",
			  "sets of adjacent leaf nodes of BVHs under the principle that most of the",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "memory is used at the leaves, and Lin et al. ( [2019](:Lin2019) ) described",
			  "an approach that saves both computation and storage by taking advantage of",
			  "an approach that saves both computation and storage by taking advantage of",
			  "shared planes among the bounds of the children of a BVH node.",
			  "shared planes among the bounds of the children of a BVH node."
			]
		  },
		  {
			"title": "High-Performance Graphics 2016",
			"url": "https://diglib.eg.org/collections/c76a314b-e5ee-4fc4-8162-19c780bda7db",
			"excerpts": [
			  "Reduced precision bounding volume ... Moreover, as BVH nodes become comparably small to practical cache line sizes, the BVH is cached less efficiently.Read more"
			]
		  },
		  {
			"title": "Performance Comparison of Bounding Volume Hierarchies ...",
			"url": "https://www.researchgate.net/publication/284233414_Performance_Comparison_of_Bounding_Volume_Hierarchies_and_Kd-Trees_for_GPU_Ray_Tracing",
			"excerpts": [
			  "The BVH node takes 64 bytes: 24 bytes for each of the children axis-aligned bounding boxes (AABB), two 4 byte offsets to the left and right ...Read more"
			]
		  },
		  {
			"title": "[PDF] Bandwidth-efficient BVH layout for incremental hardware traversal | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Bandwidth-efficient-BVH-layout-for-incremental-Liktor-Vaidyanathan/23e7f72b23dbbc376dd46dd681e0edcde34a6037",
			"excerpts": [
			  "This paper introduces a novel memory layout and node addressing scheme and map it to a system architecture for fixed-function ray traversal and demonstrates a significant reduction in memory bandwidth, compared to previous approaches."
			]
		  },
		  {
			"title": "What is a GPU warp? | Modular",
			"url": "https://docs.modular.com/glossary/gpu/warp",
			"excerpts": [
			  "Threads in a warp can access contiguous memory locations efficiently through memory coalescing. The hardware automatically synchronizes threads within a warp ..."
			]
		  },
		  {
			"title": "Introduction to the HIP programming model  HIP 7.1.52802 Documentation",
			"url": "https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html",
			"excerpts": [
			  "Coalescing memory accesses means aligning and organizing these accesses so that multiple threads in a warp can combine their memory requests into the fewest ...Read more"
			]
		  },
		  {
			"title": "definition - In CUDA, what is memory coalescing, and how is it achieved? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/5041328/in-cuda-what-is-memory-coalescing-and-how-is-it-achieved",
			"excerpts": [
			  "A coalesced memory transaction is one in which all of the threads in a half-warp access global memory at the same time.Read more"
			]
		  },
		  {
			"title": "Accessing same global memory address within warps - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/accessing-same-global-memory-address-within-warps/66574",
			"excerpts": [
			  "If a warp accesses the same addresses several times, then the memory instruction is coalesced. Internally, NVIDIA GPUs only support gather instructions.Read more"
			]
		  },
		  {
			"title": "On Ray Reordering Techniques for Faster GPU Ray Tracing",
			"url": "https://meistdan.github.io/publications/raysorting/paper.pdf",
			"excerpts": [
			  " employed techniques such as path tracing produce increasingly\nincoherent ray sets due to scattering on diffuse and glossy surfaces.",
			  "\nTracing incoherent rays is much more costly than tracing coherent\nones due to higher memory bandwidth, higher cache miss rate,\nand computational divergence. ",
			  ". A number of techniques were pro-\nposed to mitigate this issue that usually use the wavefront path\ntracing combined with ray reordering, packet tracing, or ray space\nhierarchies.",
			  "The core of the ray tracing based algorithms is evaluating ray\nscene intersections, which is often referred to as the trace kernel. In\nour paper, we revisit the basic problem of sorting rays to produce\ncoherent subsets of rays in order to accelerate the trace kernel. We",
			  "We\nfocus on methods that are fully agnostic to the particular trace ker-\nnel and the employed acceleration data structure.",
			  "Such techniques\nalready appeared in the literature [Aila and Karras 2010; Costa et al .\nI3D 20, May 57, 2020, San Francisco, CA, USA\n",
			  "USA\nMeister, Bokansk, Guthe, and Bittner\n2015; Moon et al . 2010; Reis et al . 2017], but we feel there is a need\nfor their thorough comparison and deeper analysis.",
			  "\nWe aim at the following contributions:\n We summarize previously published methods for ray reorder-\ning suitable for GPU ray tracing.",
			  "We propose a method for sorting key computation that aims\nto maximize ray coherence by using a novel termination\npoint estimation technique.\n",
			  "We show the current limits of the trace acceleration using an\n ...",
			  "\nRays in three-dimensional space can be represented as points in\na five-dimensional space (ray space), where three dimensions rep-\nresent ray origins, and two dimensions represent ray directions.",
			  "cing on GPUs, Aila and\nLaine [2009] also evaluated a hash-based sorting criterion based on\ninterleaving ray origin and normalized ray direction. At that time,\nthe sorting overhead was too large to improve the overall rendering\ntime. Ano",
			  ".\nIn a case when thread divergence occurs on GPU, the whole warp\nof threads is blocked until all its rays finish the traversal. Aila and\nLaine [2009] proposed to increase SIMD efficiency by replacing al-\nready finished rays with new ones from a global queue.",
			  " Techniques\nsuch as speculative traversal slightly increase the redundancy of ray\nintersection tests because they work on possibly terminated rays.\n",
			  "Moon et al .\n[2010]. They propose to sort rays using an estimated termination\npoint that is calculated by ray tracing a simplified scene that fits\ninto the main memory. The approach is, however, only suitable for\nout-of-core ray tracing due to the expensive hit point estimation.",
			  "the sorting overhead was too large to improve the overall rendering\ntime."
			]
		  },
		  {
			"title": "Designing Fast Architecture-Sensitive Tree Search on",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2043652.2043655",
			"excerpts": [
			  "We explore latency hiding techniques for CPUs and GPUs to improve instruction throughput, resulting in better SIMD utilization. This article is an extended ...Read more"
			]
		  },
		  {
			"title": "Cpu Cache",
			"url": "https://paul.bone.id.au/blog/2019/05/01/cpu-cache/",
			"excerpts": [
			  "In other words, the cache manages 64-byte long (and aligned) blocks, or lines of memory. Managing cache in lines improves its use, since if you ...Read more"
			]
		  },
		  {
			"title": "Global memory access - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/global-memory-access/44288",
			"excerpts": [
			  "In general if you can have each thread load a 128 bit segment(16 bytes) then this will usually be faster than a 32 bit(4 bytes) or 64(8 bytes) bit word per ...Read more"
			]
		  },
		  {
			"title": "Memory transaction size - CUDA",
			"url": "https://forums.developer.nvidia.com/t/memory-transaction-size/8856",
			"excerpts": [
			  "In device 1.2+ (G200), you can use a transaction size as small as 32 bytes as long as each thread accesses memory by only 8-bit words.Read more"
			]
		  },
		  {
			"title": "Adapting Tree Structures for Processing with SIMD ...",
			"url": "https://openproceedings.org/2014/conf/edbt/ZeuchFH14.pdf",
			"excerpts": [
			  "We adapt the. B+-Tree and prefix B-Tree (trie) by changing the search al- gorithm on inner nodes from binary search to k-ary search. The k-ary search enables ...Read more",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "We adapt the *B* + -Tree and the prefix B-Tree for SIMD\nusage by incorporating k-ary search.",
			  "Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search",
			  "The remainder of this paper is structured as follows. Sec-\ntion 2 covers preliminaries of our work. First, we discuss the\nSIMD chipset extension of modern processors and their op-\nportunities. Furthermore, we outline the *k-ary search* idea\nas the foundation for our work. Sections 3 and 4 cover our\nadaption of a *B* + -Tree (called *Segment-Tree* ) and prefix B-\nTree (called *Segment-Trie* ) using k-ary search.",
			  "The optimized Seg-\nTrie provides a constant 14 fold speedup independently of\ntree depth and an eight fold reduced memory consumption\ncompared to the original *B* + -Tree.",
			  "Like the *B* + -Tree using binary search, the Seg-Tree adds one\nnode to the traversal path for each increase in tree depth.",
			  "The smallest data type that can currently be processed by\nthe *SIMD Extensions* is 8-bit [2]. This restriction limits a\nfurther increase in tree depth."
			]
		  },
		  {
			"title": "A High Throughput B+tree for SIMD Architectures",
			"url": "https://www.ece.lsu.edu/lpeng/papers/tpds-20-1.pdf",
			"excerpts": [
			  "AbstractB+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent.Read more",
			  "armonia, a\nnovel B+tree structure, to bridge the gaps between B+tree\nand SIMD architectures.",
			  "The key region stores the nodes with its keys in a breadth-\nfirst order",
			  "The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region.",
			  "Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality.",
			  "Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "aluations on a 28-core INTELCPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than\nthat of CPU-based HB+Tree [1], a recent state-of-the-art solution. And on a Volta TITAN VGPU, it can achieve up to 3.6 billion queries per\nsecond, which is about 3.4X faster than that of GPU-based HB+Tree.",
			  "The key region stores the nodes with its keys in a\nbreadth-first order.",
			  "To make it more efficient, Harmonia also includes two optimizations: partially-sorted\naggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization.",
			  "ap in Memory Access Requirement. Each B+tree query\nneeds to traverse the tree from root to leaf. This traversal\nbrings lots of indirect memory accesses, which is propor-\ntional to tree height",
			  "Gap in Memory Divergence. Since the target leaf node of a\nquery is generally random, multiple queries may traverse the",
			  "e\ngaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodes with its keys in a\nbreadth-first order. The child region is organized as a prefix-sum array, which only stores each nodes first child index in the key region. Since\nthe prefix-sum child region is small and the childrens index can be retrieved through index computations, most of it can be stored in on-chip\ncaches, which can achieve good cache locality. To",
			  "Partially-Sorted Aggregation (PSA)\n",
			  "narrowed thread-group traversal (NTG)."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decompose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frustums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized ray-primitive intersections.",
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level.",
			  "We demonstrate our algorithm with area light sources to get a soft shadow effect and show that our concept is reasonable for GPU implementation. For the same data sets and ray-primitive intersection routines our pipeline is 3x faster than an optimized standard depth first ray tracing implemented in one kernel."
			]
		  },
		  {
			"title": "Fast Ray Sorting and BreadthFirst Packet Traversal for GPU Ray Tracing - Garanzha - 2010 - Computer Graphics Forum - Wiley Online Library",
			"url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01598.x",
			"excerpts": [
			  "We utilize the well known parallel primitives scan and segmented scan in order to process irregular data structures, to remove the need for a stack, and to minimize branch divergence in all stages.",
			  "Our ray sorting stage is based on applying hash values to individual rays, ray stream compression, sorting and decompression.",
			  "Our breadth-first BVH traversal is based on parallel frustum-bounding box intersection tests and parallel scan per each BVH level."
			]
		  },
		  {
			"title": "fast architecture sensitive tree search on modern CPUs ...",
			"url": "http://kaldewey.com/pubs/FAST__SIGMOD10.pdf",
			"excerpts": [
			  "t **FAST** (Fast Architecture Sensitive Tree)\nsearch algorithm that exploits high compute in modern processors\nfor index tree traversal. FAST is a binary tree, managed as a hier-\narchical tree whose elements are rearranged based on architecture\nfeatures like page size, cache line size, and SIMD width of underly-\ning hardware. We",
			  "eliminate the impact of latency with\nhierarchically blocked tree, software pipelining, and prefetches.",
			  "Having eliminated memory latency impact, we show how to ex-",
			  "cache line have the minimum number of cache misses, they found",
			  "that TLB misses are much higher than on trees with large node\nsizes, thus favoring large node sizes. Chen at al. [9] also concluded\nthat having a B+-tree node size larger than a cache line performs\nbetter and proposed pB+-trees, which tries to minimize the increase\nof cache misses of larger nodes by inserting software prefetches.",
			  "In order to efficiently use the compute performance of processors,\nit is imperative to eliminate the latency stalls, and store/access trees\nin a SIMD friendly fashion to further speedup the run-time.",
			  "*Hierarchical Blocking**\nWe advocate building binary trees (using the keys of the tuple)\nas the index structure, with a layout optimized for the specific ar-\nchitectural features.",
			  "For tree sizes larger than the LLC, the per-\nformance is dictated by the number of cache lines loaded from the\nmemory, and the hardware features available to hide the latenc",
			  "architecture features like page size, cache\nline size, and SIMD width of the underlying hardware.",
			  "this paper, we present FAST, an extremely fast architecture\nsensitive layout of the index tree. FAST is a binary tree logically\norganized to optimize for architecture features like page size, cache\nline size, and SIMD width of the underlying hardware. FAST elimi-\nnates impact of memory latency, and exploits thread-level and data-\nlevel parallelism on both CPUs and GPUs to achieve 50 million\n(CPU) and 85 million (GPU) queries per second, 5X (CPU) and\n1.7X (GPU) faster than the best previously reported performance\non the same architectures.",
			  "ompression techniques have been\nused to overcome disk I/O bottleneck by increasing the effective\nmemory capacity [15, 17, 20]. The transfer unit between mem-\nory and processor cores is a cache line. Compression allows each\ncache line to pack more data and increases the effective memory\nbandwidth. This increased memory bandwidth can improve query\nprocessing speed as long as decompression overhead is kept mini-\nmal [19, 3"
			]
		  },
		  {
			"title": "(PDF) FAST: fast architecture sensitive tree search on modern CPUs and GPUs",
			"url": "https://www.researchgate.net/publication/221213860_FAST_fast_architecture_sensitive_tree_search_on_modern_CPUs_and_GPUs",
			"excerpts": [
			  "FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware.",
			  " FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures.",
			  "FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.",
			  "In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree."
			]
		  },
		  {
			"title": "CUDA - Coalescing memory accesses and bus width - Stack Overflow",
			"url": "https://stackoverflow.com/questions/12589416/cuda-coalescing-memory-accesses-and-bus-width",
			"excerpts": [
			  "The memory bus of your GPU isn't simply 48 bytes wide (which would be quite cumbersome as it is not a power of 2). Instead, it is composed of 6 memory channels of 8 bytes (64 bits) each. Memory transactions are usually much wider than the channel width, in order to take advantage of the memory's burst mode. Good transaction sizes start from 64 bytes to produce a size-8 burst, which matches nicely with 16 32-bit words of a half-warp on compute capability 1.x devices.",
			  "128 byte wide transactions are still a bit faster, and match the warp-wide 32-bit word accesses of compute capability 2.0 (and higher) devices. Cache lines are also 128 bytes wide to match. Note that all of these accesses must be aligned on a multiple of the transaction width in order to map to a single memory transaction.",
			  "Now regarding your actual problem, **the best thing probably is to do nothing and to let the cache sort it out** . This works the same way as you would explicitly do in shared memory, just that it is done for you by the cache hardware and no code is needed for it, which should make it slightly faster. The only thing to worry about is to have enough cache available so that each warp can have the necessary 32324 bytes = 4kbytes of cache for word wide (e.g. float) or 8kbytes for double accesses.",
			  "For purposes of coalescing, as you stated, you should focus on making the 32 threads in a warp access contiguous locations, preferably 32-byte or 128-byte aligned as well. Beyond that, don't worry about the physical address bus to the DRAM memory. The memory controller is composed of mostly independent partitions that are each 64bits wide. Your coalesced access coming out of the warp will be satisfied as quickly as possible by the memory controller. A single coalesced access for a full warp (32 threads) accessing an int or float will require 128 bytes to be retrieved anyway, i.e. multiple transactions on the physical bus to DRAM.",
			  "When you are operating in caching mode, you can't really control the granularity of requests to global memory below 128 bytes at a time, anyway."
			]
		  },
		  {
			"title": "The granularity of L1 and L2 caches - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/the-granularity-of-l1-and-l2-caches/290065",
			"excerpts": [
			  ")\nI am currently studying CUDA.\nAcorrding to the 2022 CUDA C Programming Guide, A cache line is 128 bytes and maps to a 128 byte aligned segment in device memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte memory transactions, whereas memory accesses that are cached in L2 only are serviced with 32-byte memory transactions. Caching in L2 only can therefore reduce over-fetch, for example, in the case of scattered memory accesses.",
			  "In modern GPUs (say, Pascal and newer) both the L1 and L2 cache can be populated sector-by-sector. The minimum granularity is 1 sector or 32 bytes. The cache line tag, however, applies to 4 sectors (in each case) that comprise the 128-byte cache line. You can adjust L2 cache granularity."
			]
		  },
		  {
			"title": "Coalesced Memory Read Question - CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/coalesced-memory-read-question/41565",
			"excerpts": [
			  "From CUDA Best Programming Guide I know that GPU reading 128 byte word for one warp transaction. Thats mean, each warp(32 threads) can easy read any 4 byte data type in coalesced way during one cycle.",
			  "Im no expert but I think youre right. If you had 32 float3s, it would take 3 warp cycles, no matter how you arrange the data.",
			  "SoA approach would still take 3 warp cycles as well. No matter how you slice it, youre getting a full read.",
			  "Yes, multiple transactions will be issued.",
			  "SoA as a general recommendation is a good idea, but there shouldnt be any problem (no difference in efficiency) with loading a float4 per thread."
			]
		  },
		  {
			"title": "CUDA Performance Optimization",
			"url": "https://juser.fz-juelich.de/record/915940/files/04-mhrywniak-perf_opt.pdf",
			"excerpts": [
			  "To achieve coalesced global memory access: Usually: Fix your access pattern. Try to use shared memory (but first, check cache behavior). Look for different way ..."
			]
		  },
		  {
			"title": "An Evaluation of B-tree Compression Techniques | The VLDB Journal | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1007/s00778-025-00950-8",
			"excerpts": [
			  "Among them, B-tree compression is an important technique introduced as early as the 1970s to improve both space efficiency and query performance ...Read more"
			]
		  },
		  {
			"title": "fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu- ...",
			"url": "https://scispace.com/pdf/fast-ray-sorting-and-breadth-first-packet-traversal-for-gpu-42t1vrahpt.pdf",
			"excerpts": [
			  "e present a novel approach to ray tracing execution on commodity graphics hardware using CUDA. We decom-*\n*pose a standard ray tracing algorithm into several data-parallel stages that are mapped efficiently to the massively*\n*parallel architecture of modern GPUs. These stages include: ray sorting into coherent packets, creation of frus-*\n*tums for packets, breadth-first frustum traversal through a bounding volume hierarchy for the scene, and localized*\n*ray-primitive int",
			  "r breadth-\nfirst BVH frustum traversal is based on the full parallel\nscan for all frustums (and rays) per each BVH level and\ndoes not use a stack. ",
			  "This algorithm amortizes the cost of node access\npattern among the rays.",
			  "Ray sorting timings are not taken\ninto account when we evaluate [ AL09 ] performance."
			]
		  },
		  {
			"title": "Megakernels Considered Harmful: Wavefront Path Tracing ...",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2013/Laine-MegakernelsConsideredHarmful.pdf",
			"excerpts": [
			  "llows further optimizations**\n**Collecting requests in operation-specific queues and scheduling**\n**them individually**\n**This is the big one!** **Really hard to do in the megakernel approach**\n****\n**Path state must reside in memory**\n**A simple loop-based method can keep it in registers**\n**Not as bad as it sounds if we use a good memory layout (SOA",
			  "**Step 2: Per-Operation Queues**\n**Allocate a queue for each primitive operation request**\n**Extension ray casts**\n**Shadow ray casts**\n**New path generation**\n**Material evaluations**\n***With separate queues for individual materials***\n**Place requests compactly (i.e., no gaps) into queues**\n**When executing, use one thread per request**\n**Every thread will have an item to work on**\n**Every thread will be doing the same thing, so theres very**\n**little execution divergence!**"
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol263-ecoop2023/LIPIcs.ECOOP.2023.27/LIPIcs.ECOOP.2023.27.pdf",
			"excerpts": [
			  "In recent years, the increasing demand for high-performance analytics on big data has led the\nresearch on batch hash tables.",
			  "It is shown that this type of hash table can benefit from the\ncache locality and multi-threading more than ordinary hash tables.",
			  "Moreover, the batch design\nfor hash tables is amenable to using advanced features of modern processors such as prefetching\nand SIMD vectorization.",
			  "While state-of-the-art research and open-source projects on batch hash\ntables made efforts to propose improved designs by better usage of mentioned hardware features,\ntheir approaches still do not fully exploit the existing opportunities for performance improvements.",
			  "Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing.",
			  "To allow developers to fully take\nadvantage of its performance, we recommend a high-level batch API design.",
			  "Our experimental\nresults show the superiority and competitiveness of this approach in comparison with the alternative\nimplementations and state-of-the-art for the data-intensive workloads of relational join processing,\nset operations, and sparse vector processing.",
			  "The SIMD is a hardware\nfeature that allows the simultaneous execution of an operation on a vector of values.",
			  "On\nthe other hand, prefetching is a hardware feature that allows the program to request future\nmemory accesses in advance and asynchronous to the other computations.",
			  "We will cover the\nmore-detailed definitions of these two concepts later in this section.",
			  " 1 ], Horton [ 9 ] and Cuckoo++[ 23 ] have focused on improving the\nperformance of batch hash tables by applying SIMD and prefetching techniques to a specific\ntype of SIMD-aware batch hash table designs called Bucketized Cuckoo Hash Tables (BCHTs)",
			  " Vec-HT, a parallel, SIMD-vectorized,\nand prefetching-enabled hash table for fast batch processing. ",
			  "**SIMD-Aware Batch Hash Tables.**",
			  "To use SIMD features of a CPU in an operation (logical,\narithmetic, memory, etc.), we first need to construct a vector of operands that fit the CPU\nregister size. T",
			  "Prefetching-Enabled Hash Tables.",
			  "Modern CPUs support hardware and software prefetch-\ning. Prefetching improves the performance of a program by amortizing the costs of memory\naccess over tim",
			  "In hash tables, regardless of the hashing scheme, accessing entries is based on the value\nof the computed hash for each provided key.",
			  "To have an\neffective prefetching in hash tables we need (1) a batch of operations and (2) a large hash\ntable.",
			  " \nFigure 6 depicts a generic and high-level algorithm for combining prefetching with vertical\nvectorization (based on the assumption that we take the group-prefetching approach instead\nof standard prefetching",
			  "By having a group of keys as input, before starting the vertical\nvectorization, we define a loop over the group keys (prefetching loop).",
			  "foreach\ngroup in array by GROUP_SIZE {",
			  "// prefetching\nstage",
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "An Efficient Vectorized Hash Table for Batch Computations",
			"url": "https://www.pure.ed.ac.uk/ws/files/459637586/ShaikrokhiShaikhhaECOOP2023AnEfficientVectorizedHashTable.pdf",
			"excerpts": [
			  "The gather/scatter operations provide the ability to load/write from/into different parts of\nthe memory in parallel."
			]
		  },
		  {
			"title": "Conditions of coalescing global memory into few transactions - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums",
			"url": "https://forums.developer.nvidia.com/t/conditions-of-coalescing-global-memory-into-few-transactions/109481",
			"excerpts": [
			  "The (L2) cache can act as a coalescing buffer by collecting write activity from multiple instructions, before it is written out to DRAM, in presumably a minimized set of transactions. This is possible in part because L2 has write-back, not write-through, behavior.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "the maximum transaction size per thread is 16 bytes (per instruction). This is covered in the programming guide.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp."
			]
		  },
		  {
			"title": "CUDA on WSL User Guide  CUDA C++ Best Practices Guide 13.1 documentation",
			"url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/",
			"excerpts": [
			  "A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.",
			  "For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.",
			  "On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.",
			  "On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.",
			  "The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the *k* -th thread accesses the *k* -th word in a 32-byte aligned array. Not all threads need to participate."
			]
		  },
		  {
			"title": "Memory Latency - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/memory-latency",
			"excerpts": [
			  "eturning to Little's Law, we notice that it assumes that the full bandwidth be utilized, meaning, that all 64 bytes transferred with each memory block are useful bytes actually requested by an application, and not bytes that are transferred just because they belong to the same memory block. When any amount of data is accessed, with a minimum of one single byte, the entire 64-byte block that the data belongs to is actually transferred. To make sure that all bytes transferred are useful, it is necessary that accesses are *coalesced* , i.e. requests from different threads are presented to the [memory management unit](/topics/computer-science/memory-management-unit \"Learn more about memory management unit from ScienceDirect's AI-generated Topic Pages\") (MMU) in such a way that they can be packed into accesses that will use an entire 64-byte block",
			  " Typical latencies are 4 cycles for the L1 cache, 12 cycles for the [L2 cache](/topics/computer-science/l2-cache \"Learn more about L2 cache from ScienceDirect's AI-generated Topic Pages\") , and roughly 150-200 cycles for main memory. This memory hierarchy enhances memory performance in two ways. On one hand, it reduces the memory latency for recently used data. On the other hand, it reduces the number of accesses to the main memory, thereby limiting the usage of the network interconnect and the bandwidth demand. Indeed, accesses to L1 and L2 caches do not require network activation because they are part of the [processor socket](/topics/computer-science/processor-socket \"Learn more about processor socket from ScienceDirect's AI-generated Topic Pages\")",
			  "The size of memory transactions varies significantly between Fermi and the older versions. In compute 1.x devices (G80, GT200), the coalesced memory transaction size would start off at 128 bytes per memory access. This would then be reduced to 64 or 32 bytes if the total region being accessed by the coalesced threads was small enough and within the same 32-byte aligned block. This memory was not cached, so if threads did not access consecutive memory addresses, it led to a rapid drop off in memory bandwidth. Thus, if thread 0 reads addresses 0, 1, 2, 3, 4, , 31 and thread 1 reads addresses32, 32, 34, , 63, they will not be coalesced. In fact, the hardware will issue one read request of at least 32 bytes for each thread. The bytes not used will be fetched from memory and simply be discarded. Thus, without careful consideration of how memory is used, you can easily receive a tiny fraction of the actual bandwidth available on the device.",
			  "The situation in Fermi and Kepler is much improved from this perspective. Fermi, unlike compute 1.x devices, fetches memory in transactions of either 32 or 128 bytes. A 64-byte fetch is not supported. By default every memory transaction is a 128-byte cache line fetch. Thus, one crucial difference is that access by a stride other than one, but within 128 bytes, now results in cached access instead of another memory fetch. This makes the GPU model from Fermi onwards considerably easier to program than previous generations.",
			  "The 16 LSUs distributes 64 of the 128 bytes to the registers used by the first half-warp of warp 0. In the next cycle, the 16 LSUs distribute the remaining 64 bytes to the register used by the other half-warp. However, warp 0 still can not progress as it has only one of the two operands it needs for the multiply. It thus does not execute and the subsequent bytes arriving from the coalesced read of a for the other warps are distributed to the relevant registers for those warps.",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data. For most programs, the data brought into the cache will then allow a cache hit on the next [loop iteration](/topics/computer-science/loop-iteration \"Learn more about loop iteration from ScienceDirect's AI-generated Topic Pages\") .",
			  "There are, however, some areas where the cache causes Fermi and Kepler to operate slower than previous generation GPUs. On compute 1.x devices, memory transactions would be progressively reduced in size to as little as 32 bytes per access if the [data item](/topics/engineering/data-item \"Learn more about data item from ScienceDirect's AI-generated Topic Pages\") was small. Thus, a kernel that accesses one data element from a widely dispersed area in memory will perform poorly on any cache-based architecture, CPU, or GPU. The reason for this is that a single-element read will drag in 128 bytes of data.",
			  "ple, the MMU can only find 10 threads that read 10 4-byte words from the same block, 40 bytes will actually be used and 24 will be discarded. It is clear that coalescing is extremely important to achieve high memory utilization, and that it is much easier when the access pattern is regular and contiguous. Th"
			]
		  },
		  {
			"title": "CUDA C++ Programming Guide (Legacy)  CUDA C++ Programming Guide",
			"url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/",
			"excerpts": [
			  "By default page-locked host memory is allocated as cacheable. It can optionally be allocated as *write-combining* instead by passing flag `cudaHostAllocWriteCombined` to `cudaHostAlloc()` . Write-combining memory frees up the hosts L1 and L2 cache resources, making more cache available to the rest of the application. In addition, write-combining memory is not snooped during transfers across the PCI Express bus, which can improve transfer performance by up to 40%.",
			  "ng from write-combining memory from the host is prohibitively slow, so write-combining memory should in general be used for memory that the host only writ",
			  "Using CPU atomic instructions on WC memory should be avoided because not all CPU implementations guarantee that functionality.",
			  "An access policy window specifies a contiguous region of global memory and a persistence property in the L2 cache for accesses within that region.",
			  "The code example below shows how to set an L2 persisting access window using a CUDA Stream.",
			  "When a kernel subsequently executes in CUDA `stream` , memory accesses within the global memory extent `[ptr..ptr+num_bytes)` are more likely to persist in the L2 cache than accesses to other global memory locations.",
			  "The `hitRatio` parameter can be used to specify the fraction of accesses that receive the `hitProp` property.",
			  "For example, if the L2 set-aside cache size is 16KB and the `num_bytes` in the `accessPolicyWindow` is 32KB:",
			  "With a `hitRatio` of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.",
			  "Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://www.researchgate.net/publication/2623917_Making_B-Trees_Cache_Conscious_in_Main_Memory",
			"excerpts": [
			  "CSB+Tree (Rao and Ross 2000) improves key density and reduces cache accesses and misses by storing only the address of the first child and ...Read more"
			]
		  },
		  {
			"title": "TRAVERSING A BVH CUT TO EXPLOIT RAY COHERENCE",
			"url": "https://www.scitepress.org/Papers/2011/33634/33634.pdf",
			"excerpts": [
			  "algorithms used for traversing a subtree are due to. (Aila and Laine, 2009). They are the persistent packet and the persistent while-while and will be ..."
			]
		  },
		  {
			"title": "How do cache lines work?",
			"url": "https://stackoverflow.com/questions/3928995/how-do-cache-lines-work",
			"excerpts": [
			  "Modern PC memory modules transfer 64 bits (8 bytes) at a time, in a burst of eight transfers, so one command triggers a read or write of a full cache line from ...Read more"
			]
		  },
		  {
			"title": "Search Lookaside Buffer: Efficient Caching for Index Data ...",
			"url": "https://wuxb45.github.io/papers/slb.pdf",
			"excerpts": [
			  "The CPU cache can leverage access locality to keep the most frequently used part of an index in it for fast access. However, the traversal on the index to a ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Proceedings of the 2001 ACM symposium on Applied computing",
			"url": "https://dl.acm.org/doi/10.1145/372202.372329",
			"excerpts": [
			  "The B+-Tree is the most popular index structure in database systems. In this paper, we present a fast B+-Tree ... Read More  Storage systems for movies-on ...Read more"
			]
		  },
		  {
			"title": "Batch-construction of B+-trees | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Batch-construction-of-B%2B-trees-Kim-Won/a4f0cb5e06927a905cccb25056730b2a95e48d06",
			"excerpts": [
			  "An algorithm for batchconstructing the B+-tree, the most widely-used index structure in database systems, which achieves up to 28 times performance gain ..."
			]
		  },
		  {
			"title": "Fast Divergent Ray Traversal by Batching Rays in a BVH",
			"url": "https://jbikker.github.io/literature/Fast%20Divergent%20Ray%20Traversal%20by%20Batching%20Rays%20in%20a%20BVH%20-%202016.pdf",
			"excerpts": [
			  "In this work we propose a batching traversal scheme\ncalled RayCrawler.",
			  "Our scheme operates on a hierarchy of\nBVHs by splitting an existing BVH into two separate layers,\ncreating a top-level tree and multiple small trees that fit in the\nL2 cache of modern CPUs.",
			  "The Top-BVH traversal stack of the ray is\nstored to resume traversal later on.",
			  "he traversal algorithm starts by first traversing each\nray depth-first through the Top-BVH; once the ray reaches\na leaf node of the Top-BVH, the ray is batched at the Leaf-\nBVH that the leaf node is pointing to and the traversal of the\nray is suspended.",
			  "This system tries to amortize the cost of retrieving a Leaf-\nBVH from memory by traversing many batched rays through\nthe Leaf-BVH once it has been loaded into cache",
			  "hed rays. The scheme\nachieves modest speedups compared to a single-ray traver-\nsal algorithm for secondary rays and proves that a batching\nscheme can outperform a naive single-ray traversal approach\nfor highly divergent rays",
			  "he comparisons in this work are made with the algo-\nrithms implemented in the Embree framework version 2.7.1",
			  "s section gives a short overview of the data structure used\nthroughout the paper and an overview of the traversal al-\ngorithm. The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched",
			  "We achieve this by splitting a regular 4-wide BVH in two\nlayers. The rays are batched in the leaf nodes of the top\nlayer before traversing the bottom layer of the data struc-\ntur",
			  " The goal of our scheme is to improve cache effi-\nciency by batching rays together before traversing parts of\nthe scene, amortizing memory reads over the batched rays."
			]
		  },
		  {
			"title": "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/3170492.3136044",
			"excerpts": [
			  "RaTrace: simple and efficient abstractions for BVH ray traversal algorithms",
			  "GPCE 2017: Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and ExperiencesIn order to achieve the highest possible performance, the ray traversal and intersection\nroutines at the core of every high-performance ray tracer are usually hand-coded,\nheavily optimized, and implemented separately for each hardware platformeven ...[Read More](# \"",
			  "Stackless traversal algorithms for ray tracing acceleration structures require significantly\nless storage per ray than ordinary stack-based ones. This advantage is important for\nmassively parallel rendering methods, where there are many rays in flight. ...[Read More]",
			  "Efficient stack-less BVH traversal for ray tracing",
			  "SCCG '11: Proceedings of the 27th Spring Conference on Computer GraphicsWe propose a new, completely iterative traversal algorithm for ray tracing bounding\nvolume hierarchies that is based on storing a parent pointer with each node, and on\nusing simple state logic to infer which node to traverse next."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "https://minkhollow.ca/Courses/461/Notes/Trees/Resources/rao.pdf",
			"excerpts": [
			  "we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "CSS-Trees augment binary search\nby storing a directory structure on top of the\nsorted list of elements.",
			  "We proposed a new index\nstructure called Cache-Sensitive Search Trees\n(CSS-Tree) that has even better cache behavior\nthan a B + -Tree. ",
			  "With a large amount of RAM, most of the\nindexes can be memory resident.",
			  "Our experiments are performed for 4-byte keys\nand 4-byte child pointers. Theoretically, B + -Trees\nwill have 30% more cache misses than CSB + -Trees.",
			  "In this paper, we proposed a new index structure\ncalled a CSB + -Tree. CSB + -Trees are obtained by\napplying partial pointer elimination to B + -Trees.",
			  "As the gap between CPU and memory speed is\nwidening, CSB + -Trees should be considered as a re-\nplacement for B + -Trees in main memory database",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees."
			]
		  },
		  {
			"title": "Cache Craftiness for Fast Multicore Key-Value Storage",
			"url": "https://pdos.csail.mit.edu/papers/masstree:eurosys12.pdf",
			"excerpts": [
			  "Masstree uses a combination of old and new techniques\nto achieve high performance [8, 11, 13, 20, 2729]. It\nachieves fast concurrent operation using a scheme inspired\nby OLFIT [11], Bronson *et al.* [9], and read-copy up-\ndate [28]. Lookups use no locks or interlocked instructions,\nand thus operate without invalidating shared cache lines and\nin parallel with most inserts and update",
			  "Masstree shares a single tree among all cores to avoid load\nimbalances that can occur in partitioned designs.",
			  "The tree\nis a trie-like concatenation of B + -trees, and provides high\nperformance even for long common key prefixes, an area in\nwhich other tree designs have trouble.",
			  "Masstree uses a\nwide-fanout tree to reduce the tree depth, prefetches nodes\nfrom DRAM to overlap fetch latencies, and carefully lays\nout data in cache lines to reduce the amount of data needed\nper node.",
			  "Masstree achieves six to ten million operations per\nsecond on parts AC of the benchmark, more than 30 ** as\nfast as VoltDB [5] or MongoDB [2",
			  "e contributions of this paper are as follows. First, an\nin-memory concurrent tree that supports keys with shared\nprefixes efficiently. Second, a set of techniques for laying\nout the data of each tree node, and accessing it, that reduces\nthe time spent waiting for DRAM while descending the tree.\nThird, a demonstration that a single tree shared among mul-\ntiple cores can provide higher performance than a partitioned\ndesign for some workloads. Fourth, a complete design that\naddresses all bottlenecks in the way of million-query-per-\nsecond performance",
			  "Masstree provides high\nconcurrency from the start."
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory",
			"url": "https://dl.acm.org/doi/pdf/10.1145/335191.335449",
			"excerpts": [
			  "ache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a\nvariant of B + -Trees that stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node.",
			  "We propose a new indexing technique called\nCache Sensitive B + -Trees (CSB + -Trees).",
			  "t stores all the child nodes of any\ngiven node contiguously, and keeps only the address of\nthe first child in each node. The rest of the children can\nbe found by adding an offset to that address. ",
			  "Since only\none child pointer is stored explicitly, the utilization of\na cache line is high.",
			  "In Section 2 we survey related work on cache\noptimization.\nIn Section 3 we introduce our\nnew CSB + -Tree and its variants.",
			  "Full CSB + -Trees are better than B + -Tree in all\naspects except for space.\nWhen space overhead\nis not a big concern,\nFull CSB + -Tree is the\nbes"
			]
		  },
		  {
			"title": "Cache craftiness for fast multicore key-value storage | Proceedings of the 7th ACM european conference on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/2168836.2168855",
			"excerpts": [
			  "J. Rao and K. A. Ross. Making B+-trees cache conscious in main memory. SIGMOD Record, 29:475--486, May 2000.",
			  ". Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: A cache-sensitive parallel external sort. The VLDB Journal, 4(4):603--627, 1995.\n"
			]
		  },
		  {
			"title": "What is Memory Coalescing? | GPU Glossary",
			"url": "https://modal.com/gpu-glossary/perf/memory-coalescing",
			"excerpts": [
			  "Memory coalescing takes advantage of the internals of DRAM technology to enable\nfull bandwidth utilization for certain access patterns. Each time a DRAM address\nis accessed, multiple consecutive addresses are fetched together in parallel in\na single clock. For a bit more detail, see Section 6.1 of [the 4th edition of Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311) ;\nfor comprehensive detail, see Ulrich Drepper's excellent article [*What Every Programmer Should Know About Memory*](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf) .\nThe access and transfer of these consecutive memory locations is referred to as\na *DRAM burst* . If multiple concurrent logical accesses are serviced by a single\nphysical burst, the access is said to be *coalesced* . Note that a physical\naccess is part of a memory transaction, terminology you may see elsewhere in\ndescriptions of memory coalescing.\nOn CPUs, a similar mapping of bursts onto cache lines improves access\nefficiency. As is common in GPU programming, what is automatic cache behavior in\nCPUs is here programmer-managed."
			]
		  },
		  {
			"title": "Request Hedging vs Request Coalescing: A Software Engineers Guide to Optimizing Distributed Systems | by Sourav Chaurasia | Medium",
			"url": "https://medium.com/@mr.sourav.raj/request-hedging-vs-request-coalescing-a-software-engineers-guide-to-optimizing-distributed-fdcc6590ba9d",
			"excerpts": [
			  "**Request Coalescing** , also known as request deduplication or the singleflight pattern, is a resource optimization technique that merges multiple identical concurrent requests into a single execution. When multiple clients request the same resource simultaneously, only one actual request is executed, and all requesters share the result.",
			  "**Benefits:**",
			  "**Significant resource savings** : Can reduce duplicate executions by 7090% in high-concurrency scenarios.",
			  "**Improved cache efficiency** : Better hit rates when multiple requests need the same data.",
			  "**Prevents thundering herd** : Avoids overwhelming backend services during spikes.",
			  "**Lower infrastructure costs** : Reduced CPU, memory, and network usage.",
			  "**Drawbacks:**",
			  "**No individual latency improvement** : Doesnt help single request performance.",
			  "**Implementation complexity** : Requires careful key generation and cleanup.",
			  "**Memory overhead** : Must track pending requests and their futures.",
			  "**Potential bottlenecks** : Shared operations can become single points of failure.",
			  "**Discords Message Storage:** Discord uses request coalescing to manage its trillion-message database efficiently, preventing duplicate queries for the same message data.",
			  "**CDN Cache Filling:** Content delivery networks coalesce multiple requests for the same uncached resource, fetching it once and serving all waiting clients.",
			  "**Authentication Token Refresh:** When multiple requests need to refresh an expired token simultaneously, coalescing ensures only one refresh operation occurs.",
			  "**Database Query Optimization:** High-traffic applications coalesce identical database queries to reduce load and improve response times.",
			  "**Memory vs CPU Balance** Coalescing trades memory (for tracking pending requests) against CPU and network resources (avoiding duplicate work).",
			  "**Key Strategy Impact:** The effectiveness depends heavily on your key generation strategy:\nToo specific: Minimal coalescing benefit.\nToo general: Risk of sharing inappropriate results.\nOptimal: Balance between specificity and reuse.",
			  "\n**Cleanup Strategies:** Implement proper cleanup to prevent memory leaks",
			  "**Resource efficiency matters** : Infrastructure costs or capacity are constraints.",
			  "**High concurrency** : Many clients are requesting identical data simultaneously.",
			  "**Expensive operations** : Computationally intensive or slow database queries.",
			  "**Cache scenarios** : Filling caches or warming up cold data.",
			  "Request hedging and request coalescing represent two fundamental approaches to optimizing distributed systems. Hedging prioritizes user experience through latency reduction, while coalescing prioritizes system efficiency through resource optimization."
			]
		  },
		  {
			"title": "Two-Minute Tech Tuesdays - Request Coalescing - Resources",
			"url": "https://info.varnish-software.com/blog/two-minutes-tech-tuesdays-request-coalescing",
			"excerpts": [
			  "This episode of Two Minute Tech Tuesdays is about request coalescing, a core feature in Varnish that is used to reduce the stress on origin servers when multiple requests are trying to fetch the same uncached content.",
			  "What happens when multiple clients request the same uncached content from Varnish? Does Varnish open up the same amount of requests to the origin, and potentially destabilize the entire platform under heavy load (the Thundering Herd effect)? Luckily Varnish is not sensitive to the Thundering Herd. It identifies requests to the same uncached resource, queues them on a waiting list, and only sends a single request to the origin. As the origin responds, Varnish will satisfy the entire waiting list in parallel, so there's no [head-of-line blocking](https://en.wikipedia.org/wiki/Head-of-line_blocking) : everyone gets the content at exactly the same time. So request coalescing will effectively merge multiple potential requests to the origin into a single request.",
			  "The advantages are pretty straightforward: * Less pressure on the origin server",
			  "Less latency for queued clients",
			  "In terms of domains of application, request coalescing is useful for: * Long tail content that doesn't always end up in the cache",
			  "This only applies to cacheable content. Uncacheable content cannot take advantage of request coalescing. With uncacheable content we mean content that uses set cookie headers, or that has cache control response headers that deliberately bypass the cache.",
			  "With serialization we mean items on the waiting list being processed serially, rather than in parallel.",
			  "doing so we avoid potential serialization. With serialization we mean items on the waiting list being processed serially, rather than in parallel. This has a very detrimental effect on the performance and the quality of experience for the user, because in this case there actually is head-of-line blocking. Luc",
			  "Luckily Varnish Configuration Language (VCL) has provisions for that:",
			  "set beresp.ttl = 120s;",
			  "set beresp.uncacheable = true;"
			]
		  },
		  {
			"title": "Lecture 19c: Decoupled Access-Execute",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=onur-digitaldesign_comparch-2022-lecture19c-dae-beforelecture.pdf",
			"excerpts": [
			  "+ Execute stream can run ahead of the access stream and vice versa. + If A is waiting for memory, E can perform useful work.Read more",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)\nD",
			  "Decoupled Access/Execute (DAE)",
			  "Decoupled Access/Execute (DAE)",
			  "Idea: Decouple operand\naccess and execution via\ntwo separate instruction\nstreams that communicate\nvia ISA-visible queues .",
			  "Smith,  Decoupled Access/Execute\nComputer Architectures ,  ISCA 1982,\nACM TOCS 1984.",
			  "Synchronizes the two upon control flow instructions (using branch queues)",
			  "Advantages:\n+ Execute stream can run ahead of the access stream and vice\nversa\n+ If A is waiting for memory, E can perform useful work\n+ If A hits in cache, it supplies data to lagging E\n+ Queues reduce the number of required registers\n+ Limited out-of-order execution without wakeup/select complexi",
			  "Disadvantages:\n-- Compiler support to partition the program and manage queues\n-- Determines the amount of decoupling\n-- Branch instructions require synchronization between A and E"
			]
		  },
		  {
			"title": "Caches and Memory Systems Part 3: Miss penalty reduction",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch04-part3-MoreOnCaches-MissPenaltyReduction.pdf",
			"excerpts": [
			  "MSHR = Miss Status/Handler Registers (Kroft*). Each entry in this queue keeps track of status of outstanding memory requests to one complete memory line.  ...Read more"
			]
		  },
		  {
			"title": "Unweaving Warp Specialization",
			"url": "https://rohany.github.io/blog/warp-specialization/",
			"excerpts": [
			  "CUDA-DMA separated the warps into memory loading warps and compute warps; the loader warps issue loads and signal the compute warps when the ...Read more",
			  "Warp specialization uses this property of warp divergence to restructure GPU programs. A standard GPU program executes the same logic on each ...Read more"
			]
		  },
		  {
			"title": "18.5 Decoupled Access-Execute - CS Notes",
			"url": "https://cs.shivi.io/01-Semesters-(BSc)/Semester-2/Digital-Design-and-Computer-Architecture/Lecture-Notes-2023/18.5-Decoupled-Access-Execute",
			"excerpts": [
			  "The core idea of DAE is to **decouple the instruction stream into two separate streams**: an **Access stream** and an **Execute stream**.",
			  "The Access stream (executed by an Access Processor) primarily handles memory operations (loads and stores), including address calculations and data fetching.",
			  "The Execute stream (executed by an Execute Processor) handles computational operations (arithmetic, logic, etc.) and potentially control flow.",
			  "\nThese two streams communicate and synchronize through **ISA-visible queues**",
			  "Load instructions in the Access stream, upon fetching data from memory, deposit it into a data queue visible to the Execute stream.",
			  "Conversely, Execute stream instructions that produce values needed for address calculations in the Access stream deposit those values into a queue visible to the Access stream.",
			  "Control flow synchronization (like branches) is handled via a separate branch queue.",
			  "The compiler analyzes the program, identifies operations belonging to each stream, and generates two distinct instruction sequences that explicitly communicate via queue operations inserted by the compiler.",
			  "The Decoupled Access-Execute paradigm offers several advantages:\n**Latency Tolerance:** The asynchronous execution of the Access and Execute streams allows for tolerance of memory latency (if the Access stream can run ahead) and computation latency (if the Execute stream can run ahead). This is a key benefit over traditional in-order pipelines.",
			  "**Queue-based Communication:** Communication via ISA-visible queues is simpler to implement and potentially more scalable than the complex tag-matching and broadcast mechanisms in OoO processors.",
			  "**Potential for Specialization:** The Access and Execute processors can be specialized and optimized for their respective tasks (e.g., the Access processor could have specialized address calculation units, while the Execute processor could focus on arithmetic pipelines).",
			  "**Astronautics ZS-1:** The Astronautics ZS-1 processor, designed by James E. Smith, is an example of a DAE machine that dynamically steers instructions from a single stream into separate Access (A) and Execute (X) pipelines. These pipelines operate in order internally but are decoupled from each other and communicate via queues."
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-F00/handouts/papers/p231-smith.pdf",
			"excerpts": [
			  "Oata\nfetched\nfrom\nmemory\nis\neither\nused\ninternally\nin\nthe\nA-\nprocessor,\nor\nis\nplaced\nin\na FIFO queue\nand is\nsent\nto\nthe\nE-processor.\nThis\nis\nthe\nAccess\nto\nExecute\nQueue,\nor\nAEQ.The\nE-processor\nremoves\noperands\nfrom the\nAEQ as it\nneeds them and places\nany results\ninto\na second\nFIFO queue,\nthe\nExecute\nto Access Queue or EAQ.",
			  "simplest\nacces$exe,t:',\n(DAE)\nform,\na\ndecoupled\narchitecture\nis\nseparated\ninto\ntwo major\nfunctional\nunits,\neach with\nits\nown\ninstruction\nstream\n(Fig.\n1).\nThese are the Access\nProcessor\nor A-processor\nand the\nExecute\nProcessor\nor\nE-processor.\nEach unit\nhas\nits\nown distinct\nset\nof\nregisters,\nin\nthe\nA-processor\nthese\nare\ndenoted\nas\nregisters\nAO,\nAl,\n. . . .\nin\nthe\nE-\nprocessor\nthey\nare X0, Xl,\n. . . .",
			  "The two processors\nexecute\nseparate\nprograms\nwith\nsimilar\nstructure,\nbut\nwhich\nperform\ntwo\ndifferent\nfunctions.\nThe A-processor\nperforms\nall\noperations\nnecessary\nfor\ntransferring\ndata\nto and\nfrom\nmain\nmemory.\nThat\nis,\nit\ndoes all\naddress\ncomputation\nand performs\nall\nmemory read and write\nrequests.\nIt\nwould\nalso\ncontain\nthe\noperand\ncache,\nif\nthe\nsystem\nhas one.",
			  "The A-processor\nissues\nmemory stores\nas soon\nas it\ncomputes\nthe\nstore\naddress;\nit\ndoes not wait\nuntil\nthe\nstore\ndata\nis\nreceived\nvia\nthe\nEAQ.\nStore\naddresses\nawaiting\ndata\nare\nheld\ninternally\nin\nthe\nWrite"
			]
		  },
		  {
			"title": "Decoupled Access/Execute Computer Architectures",
			"url": "https://cseweb.ucsd.edu/classes/wi09/cse240c/Slides/26_decoupled.pdf",
			"excerpts": [
			  "**DEA**\n Two cooperative, co-dependent processors\n Access processor\n address generation\n memory requests\n Integer ops (sometimes)\n Execute processor\n Floating point\n Complex integer ops (sometimes)",
			  "**DEA**",
			  " Complex integer ops (sometimes)\n*",
			  "**DEA vs. CRAY-1**",
			  "**Instantiations of DEA**",
			  " Astronautics ZS-1 (James Smith)\n",
			  "Access processor",
			  "address generation",
			  "memory requests",
			  "Integer ops (sometimes)",
			  "Execute processor",
			  "Floating point",
			  "Architecture",
			  "queues",
			  "MAP-200"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1067649.801719",
			"excerpts": [
			  "An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues.",
			  "A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams.",
			  "Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.",
			  "Finally, the problem of deadlock in such a system is discussed, and one possible solution is given."
			]
		  },
		  {
			"title": "Stop Crying Over Your Cache Miss Rate: Handling ...",
			"url": "https://www.epfl.ch/labs/lap/wp-content/uploads/2019/06/AsiaticiFeb19_StopCryingOverYourCacheMissRateHandlingEfficientlyThousandsOfOutstandingMissesInFpgas_FPGA19.pdf",
			"excerpts": [
			  "could generate even more\nrequests per cycle with no fundamental limitations on the total num-\nber of in-flight operations",
			  "a shared MHA to maximize the merging opportunities.",
			  "In general, the cache requires 15 block RAMs per 32 kB per cache way, the MSHR buffer requires 1 block RAM per 512 MSHRs per cuckoo hash table for storage plus ...Read more"
			]
		  },
		  {
			"title": "Addressing Isolation Challenges of Non-blocking Caches ...",
			"url": "https://www.ittc.ku.edu/~heechul/papers/taming-rtsj17.pdf",
			"excerpts": [
			  "Miss Status Holding\nRegisters (MSHRs), which track the status of outstanding cache-misses, can be a sig-\nnificant source of contention that is not addressed by conventional cache partitioning.",
			  "e propose to dynamically control the num-\nber of usable MSHRs in the private L1 caches",
			  "We add two\nhardware counters *TargetCount* and *V alidCount* for each L1 cache controller.",
			  "The *V alidCount* tracks the number of total valid MSHR entries (i.e., entries with\noutstanding memory requests) of the cache and is updated by the hardware.",
			  "The\n*TargetCount* defines the maximum number of MSHRs that can be used by the core\nand is set by the system software (OS).",
			  " By control-\nling the value of *TargetCount* , the OS can effectively control the cores local MLP.",
			  "The added area and logic complexity is minimal as we only need two additional\ncounter registers and one comparator logic.",
			  "The degree of parallelism supported by\na memory subsystem is called *Memory-Level Parallelism (MLP)* [13].",
			  "Non-blocking\ncaches are essential to provide high MLP in multicore processors.",
			  "When a cache-miss occurs on a non-blocking cache, the cache controller records\nthe miss on a special register, called Miss Status Holding Register (MSHR) [27],",
			  "The request is managed at a cache-line\ngranularity. Multiple misses to the same cache-line are merged and notified together\nby a single MSHR entry.",
			  "The MSHR entry is cleared when the corresponding mem-\nory request is serviced from the lower-level memory hierarchy. ",
			  " other words, cache hit re-\n ... \ndynamic CPU and memory frequency scaling. Table 1 shows the basic characteristics\nof the five CPU architectures we used in our experime",
			  "3.2 Memory-Level Parallelism\nWe first identify memory-level parallelism (MLP) of the multicore architectures using\nan experimental method described in [11].",
			  "The method uses a pointer-chasing micro-\nbenchmark shown in Figure 3 to identify memory-level parallelism.",
			  "Latency\nis a pointer chasing synthetic benchmark, which accesses a randomly shuffled single\nlinked list. Due to data dependencies, Latency can only generate one outstanding re-\nquest at a time."
			]
		  },
		  {
			"title": "Scalable Cache Miss Handling for High Memory-Level ...",
			"url": "https://iacoma.cs.uiuc.edu/iacoma-papers/micro06_mshr.pdf",
			"excerpts": [
			  " a line is *primary* if there is currently no outstand-\ning miss on the line and, therefore, a new MSHR needs to be allo-\ncated. ",
			  "A miss is *secondary* if there is already a pending miss on the\nline. In this case, the existing MSHR for the line can be augmented to\nrecord the new miss, and no request is issued to memory. In this case,\nthe MSHR for a line keeps information for all outstanding misses on\nthe line.",
			  " each miss, it contains a *subentry* (in contrast to an *en-*\n*try* , which is the MSHR itself",
			  "Once an MHA exhausts its MSHRs or subentries, it\n*locks-up* the cache (or the corresponding cache bank).",
			  "From then on,\nthe cache or cache bank rejects further requests from the processor.",
			  "This may eventually lead to a processor stall.",
			  "They used a design where each cache bank has its own\nMSHR fi le (Figure 1(b)), but did not discuss the MSHR itself"
			]
		  },
		  {
			"title": "CudaDMA | Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis",
			"url": "https://dl.acm.org/doi/10.1145/2063384.2063400",
			"excerpts": [
			  "Section Title: CudaDMA: optimizing GPU memory bandwidth via warp specialization > Abstract\nContent:\nAs the computational power of GPUs continues to scale with Moore's Law, an increasing number of applications are becoming limited by memory bandwidth. We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories. Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms. DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout. To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns. Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Research",
			"url": "https://research.nvidia.com/publication/2011-11_cudadma-optimizing-gpu-memory-bandwidth-warp-specialization",
			"excerpts": [
			  "We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories.",
			  "Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms.",
			  "To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns.",
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic micro-benchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.",
			  "DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout."
			]
		  },
		  {
			"title": "Leveraging Warp Specialization for High Performance on GPUs",
			"url": "https://cs.stanford.edu/~sjt/pubs/ppopp14.pdf",
			"excerpts": [
			  "e crucial insight for warp specialization is that while con-\ntrol divergence within a warp results in performance degradation,\ndivergence between warps does not.",
			  "med barriers pro-\nvide two operations: *arrive* and *sync* . Arrive is a non-blocking op-\neration which registers that a warp has arrived at a barrier and then\ncontinues execution. Sync is a blocking operation that waits until\nall the necessary warps have arrived or synced on the barrie",
			  "Warp-specialized partitioning provides a useful mechanism for\nDSL compilers when grappling with computations that exhibit\nboth irregularity and large working sets.",
			  "The mapping compilation stage is responsible for taking in an\narbitrary dataflow graph of operations and mapping it onto the\nspecified number of warps and available GPU memories.",
			  "It is important to\nnote that named barriers support synchronization between arbitrary\nsubsets of warps within a CTA, including allowing synchronization\nbetween a single pair of warps as in this example.",
			  "rtitioning computations us-\ning warp specialization allows Singe to deal efficiently with the\nirregularity in both data access patterns and computation.",
			  "Consumer Warp",
			  "Producer Warp",
			  "(signal begin)",
			  "(wait until ready)",
			  "bar.sync",
			  "bar.sync",
			  "(wait until begin)",
			  "bar.arrive",
			  "bar.arrive",
			  "(signal ready)",
			  "producer-consumer named barrier is used\nto indicate to the QSSA warps when the needed values from the\nnon-QSSA warps have been written into shared memory.",
			  "arp-specialized programs also require more expressive syn-\nchronization mechanisms",
			  "Warp specialization exploits the division\nof a thread block into warps to partition computations into sub-\ncomputations such that each sub-computation is executed by a\ndifferent warp within a thread block.",
			  "However, by using inline PTX statements, a CUDA program\nhas access to a more expressive set of intra-CTA synchronization\nprimitives referred to as *named barriers*",
			  "Using arrive and sync operations, programmers can encode\nproducer-consumer relationships in warp-specialized programs.",
			  "gure 2 illustrates using two named barriers to coordinate move-\nment of data from a producer warp (red) to a consumer warp (blue)\nthrough a buffer in shared memory.",
			  "he\nconsumer warp signals the buffer is ready by performing a non-\nblocking arrive operation.",
			  "Since the arrive is non-blocking, the\nconsumer warp is free to perform additional work while waiting\nfor the buffer to be filled.",
			  "At some point the consumer warp blocks\non the second named barrier waiting for the buffer to be full.",
			  "The\nproducer warp signals when the buffer is full using a non-blocking\narrive operation on the second named barrier.",
			  "Named Barrier 0",
			  "Named Barrier 1"
			]
		  },
		  {
			"title": "Optimizing GPU Memory Bandwidth via Warp Specialization",
			"url": "https://ppl.stanford.edu/papers/sc11-bauer.pdf",
			"excerpts": [
			  " root cause of this entanglement is the require-\nment encouraged by the CUDA programming model that\nthreads of a CTA perform both memory accesses and com-\nputation. By creating specialized warps that perform inde-\npendent compute and memory operations we can tease apart\nthe issues that affect memory performance from those that\naffect compute performan",
			  "The simplest approach to writing code using CudaDMA is\nto allocate a separate buffer for each transfer to be performed\nand to associate a cudaDMA object with each buffer. We refer\nto this approach as single buffering since there is a single\nbuffer for each transfer being performed by a set of DMA\nwarps.",
			  "arp specialization allows subsets of threads within\na CTA to have their behavior tuned for a particular purpose\nwhich enables more efficient consumption of constrained re-\nsources",
			  "If single buffering is not exploiting enough MLP to keep\nthe memory system busy, an alternative is to create two\nbuffers with two sets of DMA warps for transferring data.\nWe call this two-buffer technique double buffering ",
			  "DMA warps also improve programmer productivity\nby decoupling the need for thread array shapes to match\ndata layout.",
			  "Named Barrier 0",
			  "Named Barrier 1",
			  "compute warps",
			  "DMA warps",
			  "start_async_dma()",
			  "wait_for_dma_fnish()",
			  "wait_for_dma_start()",
			  "(bar.sync)",
			  "(bar.sync)",
			  "fnish_async_dma()",
			  "(bar.arrive)",
			  "(bar.arrive)",
			  "The\nproducer/consumer nature of our synchronization mecha-\nnisms allow the programmer to employ a variety of tech-\nniques to overlap communication and computation",
			  "e accomplish fine-grained synchronization by using in-\nlined PTX assembly to express named barriers .",
			  "med\nbarriers are hardware resources that support a barrier oper-\nation for a subset of warps in a CTA and can be identified\nby a unique name (e.g. immediate value in PTX).",
			  "There are\ntwo named barriers associated with every cudaDMA object.\nTwo barriers are required to track whether the data buffer\nin shared memory is full or empty.",
			  "e use the PTX in-\nstruction bar.arrive , which allows a thread to signal its ar-\nrival at a named barrier without blocking the threads execu-\ntion",
			  "This functionality is useful for producer-consumer\nsynchronization by allowing a producer to indicate that a\ndata transfer has finished filling a buffer while permitting\nthe producer thread to continue to perform work.",
			  "Similarly,\na consuming thread can use the same instruction to indicate\nthat a buffer has been read and is now empty.",
			  "For blocking\noperations we use the PTX instruction bar.sync to block\non a named barrier.",
			  "CudaDMA has shown the benefits of emulating an asyn-\nchronous DMA engine in software on a GP",
			  "The CudaDMA approach to GPU programming is therefore\ngeneral enough to be applied to any CUDA program.",
			  "CudaDMA encapsulates this technique in order to make it\nmore generally available to a range of application workloads.",
			  "By decoupling the compute and\nDMA warps, CudaDMA enables this approach without\nsacrificing memory system performance.",
			  "CudaDMA enables the\nprogrammer to decouple the shape of data from how the\ndata is transferred by creating specialized DMA warps for"
			]
		  },
		  {
			"title": "[PDF] Decoupled access/execute computer architectures",
			"url": "https://www.semanticscholar.org/paper/a9212cdd44437a13fe507b3794a3ba52d424961a",
			"excerpts": [
			  "Decoupled access/execute computer architectures  James E. Smith  Published in ACM Transactions on Computer 1 November 1984  Computer Science.Read more",
			  "An architecture for high-performance scalar computation with a high degree of decoupling between operand access and execution is proposed and discussed, ..."
			]
		  },
		  {
			"title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
			"url": "https://arxiv.org/html/2510.14719v2",
			"excerpts": [
			  "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT ...Read more"
			]
		  },
		  {
			"title": "Using Littles Law to Measure System Performance | Kevin Sookocheff",
			"url": "https://sookocheff.com/post/modeling/littles-law/",
			"excerpts": [
			  "Little's Law is a useful tool for software architecture because it provides a simple way to measure the effect of changes to a system.Read more"
			]
		  },
		  {
			"title": "decoupled access/execute computer architectures",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2022/lib/exe/fetch.php?media=smith-1982-decoupled-access-execute-computer-architectures.pdf",
			"excerpts": [
			  "In\nits\nsimplest\nform,\na\ndecoupled\naccess/execute\n(DAE) architecture\nis\nseparated\ninto two major functional units, each with its own\ninstruction stream (Fig. 1).\nThese are the Access\nProcessor or A-processor and the Execute Processor\nor E-processor.",
			  "Data fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ",
			  "The A-processor performs all\noperations necessary for transferring data to and\nfrom main memory.\nThat is, it does all address\ncomputation and performs all memory read and write\nrequests.\nIt would also contain the operand\ncache, if the system has one.\nData fetched from\nmemory\nis\neither\nused internally\nin\nthe\nA-\nprocessor,\nor is placed in a FIFO queue and is\nsent to the E-processor.\nThis is the Access to\nExecute Queue, or\nAEQ.The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "The A-processor issues memory stores as soon\nas it computes the store address; it does not wait\nuntil\nthe store data is\nreceived via the EAQ.",
			  "The E-processor\nremoves\noperands from the AEQ as it needs them and places\nany results into a second FIFO queue, the Execute\nto Access Queue or EAQ.",
			  "t.\nThere is also a \"Branch From\nQueue\" (BFQ) instruction that is conditional on\nthe branch outcome at the head of the branch queue\ncoming from the\nopposite processor.\n",
			  ".\nThus conditional branches\nappear in the two processors as complementary\npairs.",
			  " deadlock can occur if\nboth the AEQ and EAQ are full and both processors\nare blocked by the full queues, or if both queues\nare empty and both processors are blocked by the\nempty queues.\n"
			]
		  },
		  {
			"title": "Singe | Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming",
			"url": "https://dl.acm.org/doi/10.1145/2555243.2555258",
			"excerpts": [
			  "Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block.",
			  "Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "WASP: Exploiting GPU Pipeline Parallelism with Hardware ...",
			"url": "https://www.nealcrago.com/wp-content/uploads/WASP_HPCA2024_preprint.pdf",
			"excerpts": [
			  "enables fine-grained streaming and gather memory access**\n**patterns through the use of warp-level register file queues**\n**and hardware-accelerated address generati",
			  "e architect hardware\nqueues in WASP by mapping them as circular buffers in\nthe existing register file space. ",
			  "In this work, we present WASP, hardware and compiler\nsupport for warp specialization, a powerful technique for\noverlapping memory access and compute operations to accom-\nplish better GPU performance.",
			  "WASP compiler improves runtime\nperformance over state-of-the-art GPUs by 23%, and by 47%\nwhen combined with the new WASP hardware."
			]
		  },
		  {
			"title": "Little's Law as Viewed on Its 50th Anniversary",
			"url": "https://people.cs.umass.edu/~emery/classes/cmpsci691st/readings/OS/Littles-Law-50-Years-Later.pdf",
			"excerpts": [
			  ".\nLittles Law says that the average number of items in a\nqueuing system, denoted *L* , equals the average arrival rate\nof items to the system, ** , multiplied by the average waiting\ntime of an item in the system, *W* . Thus,\n*L* = ** *W* *0*\nF",
			  "\nThe two biggest fields in which Littles Law regularly\ncomes into play are operations management (OM) and\ncomputer architecture (computers).",
			  "Latency is a key performance measure for computers\n(low is good) and has an unspoken average implicitly\nattached, but *response time* seems more meaningful for a\nlayperson.",
			  "e\ncomputer architect wants to understand what is going on so\nas best to design the system.",
			  "Computer memory comes in various *tiers* with different\ninherent response times. The tiers range from CPU cache\n(fast), to DRAM (dynamic random access memory) (not\nquite as fast), to RAM (random access memory) (a lit-\ntle slower), all the way to solid-state drives and finally to\nnetwork attached storage (banks of hard disk drives).",
			  "The server CPU contains microprocessors, each of which\ncontains cores, perhaps 8 or 16 of them. Each is a von\nNeumann computer. Now we are coming to the queues. The\npiece of code the computer is executing is aptly called a\nthread (of instructions). The author visualizes it as a list of\ninstructions running down his penciled code sheet. Oh, oh,\nthe instruction calls for a piece of data that is somewhere\nelse in memory, electronically far away from the thread.\nThe thread STOPS and sends out a request for that data."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://dl.acm.org/doi/abs/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982. ... 1984. ISSN ...Read more",
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings ... James E Smith profile image James E. Smith. Department of Electrical and ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Overview and Code Examples",
			"url": "https://www.nvidia.com/content/PDF/GDC2011/Brucek_KhailanySC11.pdf",
			"excerpts": [
			  "Warp Specialization. CudaDMA enables warp specialization: DMA warps. Maximize MLP. Compute warps. No stalls due to memory. CudaDMA objects manage warp ...Read more"
			]
		  },
		  {
			"title": "CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization | Request PDF",
			"url": "https://www.researchgate.net/publication/220782141_CudaDMA_Optimizing_GPU_Memory_Bandwidth_via_Warp_Specialization",
			"excerpts": [
			  "Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications ...Read more"
			]
		  },
		  {
			"title": "Verification of Producer-Consumer Synchronization in GPU ...",
			"url": "https://legion.stanford.edu/pdfs/weft.pdf",
			"excerpts": [
			  "To perform synchronization be- tween different warps, warp-specialized kernels use the producer- consumer named barriers available in PTX[1] on NVIDIA GPUs.Read more"
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures | ACM Transactions on Computer Systems",
			"url": "https://dl.acm.org/doi/10.1145/357401.357403",
			"excerpts": [
			  "SMITH, J.E. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture (May), 1982."
			]
		  },
		  {
			"title": "Singe: Leveraging Warp Specialization for High Performance on GPUs | Request PDF",
			"url": "https://www.researchgate.net/publication/262368469_Singe_Leveraging_Warp_Specialization_for_High_Performance_on_GPUs",
			"excerpts": [
			  "We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs.",
			  "Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers.",
			  "Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation.",
			  "Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories.",
			  "We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels."
			]
		  },
		  {
			"title": "Decoupled access/execute computer architectures",
			"url": "https://alastairreid.github.io/RelatedWork/papers/smith:tocs:1984/",
			"excerpts": [
			  "Decoupled access/execute computer architectures. James E. Smith [doi] [Google Scholar] [DBLP] [Citeseer]. ACM Transactions on Computer Systems 2(4)Read more"
			]
		  },
		  {
			"title": "(PDF) A decoupled access-execute architecture for ...",
			"url": "https://www.researchgate.net/publication/326637256_A_decoupled_access-execute_architecture_for_reconfigurable_accelerators",
			"excerpts": [
			  "PDF | Mapping computational intensive applications on reconfigurable technology for acceleration requires two main implementation parts: (a) ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114% ( ...Read more"
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "This paper demonstrates significant performance gains on a commodity 8-core multicore machine running a variety of codes transformed with DSWP+. Formats ...Read more"
			]
		  },
		  {
			"title": "Scalable-Grain Pipeline Parallelization Method for Multi- ...",
			"url": "https://inria.hal.science/hal-01513778/file/978-3-642-40820-5_23_Chapter.pdf",
			"excerpts": [
			  "Some approaches partition individual instructions across pro- cessors, such as the decoupled software pipeline (DSWP) method [10], while.Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "1.1 Problem. Decoupled software pipelining [5] presents an easy way to automatically ex- tract thread-level parallelism for general loops in any program.Read more"
			]
		  },
		  {
			"title": "decoupled-software-pipelining-creates-parallelization- ...",
			"url": "https://scispace.com/pdf/decoupled-software-pipelining-creates-parallelization-tyrvhzup00.pdf",
			"excerpts": [
			  "An automatic parallelization technique, DSWP splits the loop body into several stages distributed across multiple threads, and executes them in a pipeline."
			]
		  },
		  {
			"title": "\nParallel Techniques of the Sequential Codes Based on Multi-core",
			"url": "https://scialert.net/fulltext/?doi=itj.2013.1673.1684",
			"excerpts": [
			  "DSWP parallelizes a loop by partitioning the body of the loop into a sequence of pipeline stages. Each stage is then executed by a separate thread. The threads ...Read more"
			]
		  },
		  {
			"title": "PIPELINED MULTITHREADING TRANSFORMATIONS AND ...",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_ram.pdf",
			"excerpts": [
			  "tolerate variable latency and to overcome scope restrictions imposed by single PC ar-\nchitectures, a program transformation called *decoupled software pipelining* (DSWP) is pre-\nsented in this chapter",
			  "SWP avoids heavy hardware usage by attacking the fundamental\nproblem of working within a single-threaded execution model, and moving to a multi-\nthreaded execution model.",
			  "Only useful, about-to-execute instructions are even brought into\nthe core. The rest are conveniently left in the instruction cache or their results committed\nand retired from the processor core.",
			  "Unlike the single-threaded multi-core techniques pre-\nsented in Table 2.1, DSWP is an entirely *non-speculative* techniqu",
			  "Each DSWP thread\nperforms useful work towards program completion.",
			  "The concurrent multithreading model of DSWP means that each participating thread\ncommits its register and memory state concurrently and independently of other threads.",
			  "The current thread model for DSWP is as follows. Execution begins as a single thread,\ncalled *primary thread* . It spawns all necessary *auxiliary threads* at the beginning of a\nprogram",
			  "When the primary thread reaches a DSWPed loop, auxiliary threads are set up\nwith necessary loop live-in values.",
			  "Similarly, upon loop termination, loop live-outs from\nauxiliary threads have to be communicated back to the primary thread."
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization. They also promise much wider flexi-\nbility than data-parallelism-centric methods for processors, aiming\nfor the effective parallelization of general-purpose application",
			  "In this paper, we provide another method to decouple control-\nflow regions of serial programs into concurrent tasks, exposing\npipeline and data parallelism",
			  "The power and simplicity of the\nmethod rely on the restriction that all streams should retain a\nsynchronous semantics [8].",
			  "ics [8]. It amounts to checking the sufficient\ncondition that the source and target of any decoupled dependence\nare control-dependent on the same node in the control dependence\ntree (this assumes structured control flow).",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "For example, when\nthere are no dependences between loop iterations of a DSWP stage,\nthe incoming data can be distributed over multiple data-parallel\nworker threads dedicated to this stage, while the outgoing data can\nbe merged to proceed with downstream pipeline stages."
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformati",
			  "WP operates by partitioning the instructions of a\nloop among a sequence of loops. The new loops are concurrently\nexecuted on different threads, with dependences among them flow-\ning in a single direction, thus forming a pipeline of threads.",
			  "The performance results indicate\nthe potential of this technique to exploit iteration-level parallelism\nin loops that cannot be parallelized as DOALL.",
			  "e evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining with the Synchronization Array",
			"url": "https://liberty.princeton.edu/Publications/pact04_dswp.pdf",
			"excerpts": [
			  " software pipelining (DSWP), a technique that stati-*\n*cally splits a single-threaded sequential loop into multi-*\n*ple non-speculative threads, each of which performs use-*\n*ful computation essential for overall program correctne",
			  "g threads execute on thread-parallel architec-*\n*tures such as simultaneous multithreaded (SMT) cores or*\n*chip multiprocessors (CMP), expose additional instruction*\n*level parallelism, and tolerate latency better than the orig-*\n*inal single-threaded RDS lo",
			  "accomplish this,\nthe paper presents the *synchronization array* which ap-\npears to the ISA as a set of queues capable of supporting\nboth out-of-order execution and speculative issue",
			  "e traversal and computation threads behave as a tra-\nditional decoupled producer-consumer pair.",
			  "DSWP threads these loops\nfor parallel execution on SMT or CMP processors.",
			  "ever, for this threading technique to be effective, a very\nlow overhead communication and synchronization mecha-\nnism between the threads is required. "
			]
		  },
		  {
			"title": "CoroBase: Coroutine-Oriented Main-Memory Database ...",
			"url": "http://vldb.org/pvldb/vol14/p431-he.pdf",
			"excerpts": [
			  "Data stalls are a major overhead in main-memory database engines\ndue to the use of pointer-rich data structures. ",
			  ". Lightweight corou-\ntines ease the implementation of software prefetching to hide data\nstalls by overlapping computation and asynchronous data prefetch-\ning. ",
			  "Coroutine-to-transaction models transactions as coroutines\nand thus enables inter-transaction batching, avoiding application\nchanges but retaining the benefits of prefetching. W",
			  ". We show that\non a 48-core server, CoroBase can perform close to 2  better for\nread-intensive workloads and remain competitive for workloads\nthat inherently do not benefit from software prefetching.\n*",
			  "CoroBase is open-source at https://github.com/sfu-dis/corobase ."
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://medium.com/nlnetlabs/adapting-radix-trees-15fe7d27c894",
			"excerpts": [
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the ...Read more"
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/pf_final.pdf",
			"excerpts": [
			  "For index searches, pB+-Trees reduce this problem by having wider nodes than the natural data transfer size,. e.g., eight vs. one cache lines (or disk pages).",
			  "To accelerate searches, pB+-Trees use prefetching to e ectively create wider nodes than the natural data trans- fer size: e.g., eight vs. one cache lines or ...Read more",
			  "prefetching pointer-linked data structures (i.e. linked-lists, trees, etc.) in general-purpose applications. Assuming that three nodes worth of computation ..."
			]
		  },
		  {
			"title": "The Taming of the B-Trees - ScyllaDB",
			"url": "https://www.scylladb.com/2021/11/23/the-taming-of-the-b-trees/",
			"excerpts": [
			  "There is a good and pretty cheap optimization to mitigate this spike that we've called linear root. The leaf root node grows on demand, ...Read more"
			]
		  },
		  {
			"title": "Static B-Trees - Algorithmica",
			"url": "https://en.algorithmica.org/hpc/data-structures/s-tree/",
			"excerpts": [
			  "The [first]() is based on the memory layout of a B-tree, and, depending on the array size, it is up to 8x faster than `std::lower_bound` while using the same space as the array and only requiring a permutation of its eleme",
			  "[second]() is based on the memory layout of a B+ tree, and it is up to 15x faster than `std::lower_bound` while using just 6-7% more memory  or 6-7% **of** the memory if we can keep the original sorted array.",
			  "o distinguish them from B-trees  the structures with pointers, hundreds to thousands of keys per node, and empty spaces in them  we will use the names *S-tree* and *S+ tree* respectively to refer to these particular memory layouts [1](:1) .",
			  ":\nTo find the lower bound, we need to fetch the $B$ keys in a node, find the first key $a_i$ not less than $x$, descend to the $i$-th child  and continue until we reach a leaf node. There is some variability in how to find that first key. For example, we could do a tiny internal binary search that makes $O(\\log B)$ iterations, or maybe just compare each key sequentially in $O(B)$ time until we find the local lower bound, hopefully exiting from the loop a bit early.",
			  "ly.\nBut we are not going to do that  because we can use [SIMD](/hpc/simd) . It doesnt work well with branching, so essentially what we want to do is to compare against all $B$ elements regardless, compute a bitmask out of these comparisons, and then use the `ffs` instruction to find the bit corresponding to the first non-lesser element",
			  "In AVX2, we can load 8 elements, compare them against the search key, producing a [vector mask](/hpc/simd/masking/) , and then extract the scalar mask from it with `movemask` . Here is a minimized illustrated example of what we want to do:\n",
			  "nt:\nThis instruction converts 32-bit integers stored in two registers to 16-bit integers stored in one register  in our case, effectively joining the vector masks into one. Note that weve swapped the order of comparison  this lets us not invert the mask in the end, but we have to subtract [2](:2) one from the search key once in the beginning to make it correct (otherwise, it works as `upper_bound` ).\nT",
			  "\nThe problem is, it does this weird interleaving where the result is written in the `a1 b1 a2 b2` order instead of `a1 a2 b1 b2` that we want  many AVX2 instructions tend to do that. To correct this, we need to [permute](/hpc/simd/shuffling) the resulting vector, but instead of doing it during the query time, we can just permute every node during preprocessing:\nN",
			  "This new SIMD routine is significantly faster because the extra `movemask` is slow, and also blending the two masks takes quite a few instructions. Unfortunately, we now cant just do the `res = btree[k][i]` update anymore because the elements are permuted. We can solve this problem with some bit-level trickery in terms of `i` , but indexing a small lookup table turns out to be faster and also doesnt require a new branch:\nThis",
			  ":\nAll this work saved us 15-20% or so:\nIt doesnt feel very satisfying so far, but we will reuse these optimization ideas later.\nThere are two main problems with the current implementation:\nThe `update` procedure is quite costly, especially considering that it is very likely going to be useless: 16 out of 17 times, we can just fetch the result from the last block.\nWe do a non-constant number of iterations, causing branch prediction problems similar to how it did for the [Eytzinger binary search](../binary-search/) ; you can also see it on the graph this time, but the latency bumps have a period of $2^4$.\nTo address these problems, we need to change the layout a little bit.\n"
			]
		  },
		  {
			"title": "Adapting Radix Trees",
			"url": "https://blog.nlnetlabs.nl/adapting-radix-trees/",
			"excerpts": [
			  "The Adaptive Radix Tree (ART)",
			  "Section Title: The Adaptive Radix Tree (ART) > *Adaptively sized nodes*",
			  "Horizontal compression is applied by adapting the size of inner nodes. Nodes can have sizes of 4, 16, 48 and 256, each capable of holding the respective number of references to child nodes, and are grown/shrunk as needed.",
			  "Fixed sizes are used to minimize the number of memory (de)allocations.",
			  "Nodes of sizes up to 48, map keys onto edges using two separate vectors.",
			  "Nodes of size 16 make use of 128-bit SIMD instructions ( [SSE2](https://en.wikipedia.org/wiki/SSE2) and [NEON](https://en.wikipedia.org/wiki/ARM_architecture(Neon)) ) to map a key onto an edge to improve performance.\n",
			  "Section Title: The Adaptive Radix Tree (ART) > Path compression",
			  "Inner nodes that have only one child are merged with their parent and each node reserves a fixed number of bytes to store the prefix.",
			  "If more space is required, lookups simply skip the remaining number of bytes and compare the search key to the key of the leaf once it arrives there.",
			  "Section Title: The Adaptive Radix Tree (ART) > Lazy expansion",
			  "Inner nodes are created only to distinguish at least two leaf nodes. Paths are truncated. Pointer tagging is used to tell inner nodes apart from leaf nodes."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://db.in.tum.de/~leis/papers/ART.pdf",
			"excerpts": [
			  "**ode16:** This node type is used for storing between 5 and\n16 child pointers. Like the Node4 , the keys and pointers\nare stored in separate arrays at corresponding positions, but\nboth arrays have space for 16 entries.",
			  "ART adapts the\nrepresentation of every individual node, as exemplified in\nFigure 1. By adapting each inner node *locally* , it optimizes\n*global* space utilization and access efficiency at the same ",
			  "Radix trees consist of two types of nodes: Inner nodes,\nwhich map partial keys to other nodes, and leaf nodes, which\nstore the values corresponding to the keys. The most efficient\nrepresentation of an inner node is as an array of 2 *s* pointer",
			  "A useful property of radix trees is that the order of the keys\nis not random as in hash tables; rather, the keys are ordered\nbitwise lexicographically.",
			  "Two additional techniques, path\ncompression and lazy expansion, allow ART to efficiently\nindex long keys by collapsing nodes and thereby decreasing\nthe tree height.",
			  "e use a small number of node types, each with a different\nfanout. Depending on the number of non-null children, the\nappropriate node type is used. ",
			  "the space consumption per key is bounded\nto 52 bytes, even for arbitrarily long keys. We show\nexperimentally, that the space consumption is much lower\nin practice, often as low as 8.1 bytes per key.",
			  " **ode4:** The smallest node type can store up to 4 child\npointers and uses an array of length 4 for keys and another\narray of the same length for pointers. The keys and pointers\nare stored at corresponding positions and the keys are sorted.",
			  "The height (and complexity) of radix trees depends on\nthe length of the keys but in general not on the number\nof elements in the tree.",
			  "Instead of using\na list of key/value pairs, we split the list into one key part\nand one pointer part. This allows to keep the representation\ncompact while permitting efficient search:",
			  "**ode48:** As the number of entries in a node increases,\nsearching the key array becomes expensive. Therefore, nodes\nwith more than 16 pointers do not store the keys explicitly.\n",
			  " lookup**\n**performance surpasses highly tuned, read-only search trees, while**\n**supporting very efficient insertions and deletions as we",
			  "en though ARTs performance**\n**is comparable to hash tables, it maintains the data in sorted**\n**order, which enables additional operations like range scan and**\n**pref"
			]
		  },
		  {
			"title": "Adaptive Radix Tree",
			"url": "https://pages.cs.wisc.edu/~yxy/cs764-f22/slides/L16.pdf",
			"excerpts": [
			  "**Key idea** : Use a small node type\nwhen only a small number of\nchildren pointers exist",
			  "t\nKey Idea: Adaptive Radix Tree",
			  "**Node4** and **Node16**\n**Node48**\n**Node256**\n 256 child pointers indexed with\npartial key byte directly\n",
			  "Inner Node Structure",
			  "\n**Node4** and **Node16**\n Store up to 4 (16) partial keys\nand the corresponding pointers\n Each partial key is one byte\n Use SIMD instructions to\naccelerate key search\n**No",
			  "**Node48**",
			  "**Node256**",
			  "256 entries indexed with partial\nkey byte directly",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "ode4** and **Node16**\n**Node48**\n 256 entries indexed with partial\nkey byte directly\n Each entry stores a one-byte index\nto a child pointer array\n Child pointer array contains 48\npointers to children nodes\n**Node256",
			  "**Lazy expansion** : remove path to\nsingle leaf",
			  "**Path compression** : merge one-way\nnode into child node",
			  "ART requires at most **52 bytes** of memory to index a key"
			]
		  },
		  {
			"title": " Beating hash tables with trees? The ART-ful radix trie | Paper Trail ",
			"url": "https://www.the-paper-trail.org/post/art-paper-notes/",
			"excerpts": [
			  "The most significant change that ART makes to the standard trie structure is that it introduces the\nability to change the datastructure used for each internal node depending on how many children the\nnode actually has, rather than how many it might have.",
			  ".\nPointers are assumed to be 8 bytes, so a single `Node4` is 36 bytes, so sits in a single cache\nline. The search loop can also be unrolled. Finally, by not early-exiting from the loop, we can hint\nto the compiler that it need not use a full branch, but can just use a conditional `cmov` [predicated instruction]",
			  "Nodes with from 5 to 16 children have an identical layout to `Node4` , just with 16 children per node:\nKeys in a `Node16` are stored sorted, so binary search could be used to find a particular key. Since\nthere are only 16 of them, its also possible to search all the keys in parallel using SIMD. What\nfollows is an annotated version of the algorithm presented in the papers Fig 8.\nThis is superior to binary-search: no branches (except for the test when bitfield is 0), and all the\ncomparisons are done in parallel.",
			  "The next node can hold up to three times as many keys as a `Node16` . As the paper says, when there\nare more than 16 children, searching for the key can become expensive, so instead the keys are\nstored implicitly in an array of 256 indexes. The entries in that array index a separate array of up\nto 48 pointers.\nThe idea here is that this is superior to just storing an array of 256 `Node` pointers because you\ncan store 48 children in 640 bytes (where 256 pointers would take 2k). Looking up the pointer does\ntake an extra indirection:\nThe paper notes that in fact only 6 bytes (i.e. \\(log_2(48)\\)) are needed for each index; both in\nthe paper and here its simpler to use a byte per index to avoid any shifting and masking.",
			  " final node type is the traditional trie node, used when a node has between 49 and 256 children.\nLooking up child pointers is obviously very efficient - the most efficient of all the node types -\nand when occupancy is at least 49 children the wasted space is less significant (although not 0 by\nany stretch of the imagination)"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://www.eecs.umich.edu/techreports/cse/02/CSE-TR-468-02.pdf",
			"excerpts": [
			  "A design decision that is consistently used in cache-conscious tree-based indices is defining the node size\nto be equal to the size of the L2 data cache line.",
			  "nalogous\nto the traditional B+-tree where a node size is equal to a disk page to minimize the number of page ac-\ncesses during a search, the node size for the CSB\n -tree is set equal to a processor cache line to minimize\nthe number of cache miss",
			  "The work by Rao and Ross has been extended in recent years in a number\nof different ways, including handling variable key length attributes efficiently [4] and for architectures that\nsupport prefetching [9].",
			  "In a recent paper, Chen, Gibbons and Mowry [9] examined the cache behavior of B+trees and CSB\n -\ntrees. They conclude that the CSB\n -trees produce very deep trees which cause many cache misses as a\nsearch traverses down the tree. They propose a prefetching-based solution, in which the node size of a\nB+tree is larger than the cache line size, and special prefetching instructions are manually inserted into th",
			  ". The CSB\n -tree eliminates child node pointers in the non-leaf\nnodes, allowing additional keys to be stored in a node which improves cache line utilization. An"
			]
		  },
		  {
			"title": "Effect of Node Size on the Performance of Cache- ...",
			"url": "https://pages.cs.wisc.edu/~jignesh/publ/cci.pdf",
			"excerpts": [
			  "uthors also investi-\ngated dynamic indexing techniques in the main-memory environ-\nment in [27], proposing a cache-conscious variation of the tradi-\ntional B+-tree, called the CSB + -tree. The CSB + -tree eliminates\nchild node pointers in the non-leaf nodes, allowing additional keys\nto be stored in a node which improves cache line utilizatio",
			  "the\nnode size for the CSB + -tree is set equal to a processor cache line to\nminimize the number of cache misses.",
			  "In a recent paper, Chen, Gibbons and Mowry [10] examined\nthe cache behavior of B+trees and CSB + -trees. They conclude\nthat the CSB + -trees produce very deep trees which cause many\ncache misses as a search traverses down the tree. They propose a\nprefetching-based solution, in which the node size of a B+tree is\nlarger than the cache line size, and special prefetching instructions\nare manually inserted into the B+tree code to prefetch cache lines\nand avoid stalling the processor.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  " this work, the authors show how the pB+-\ntree index can be efficiently constructed onto disk pages, which\nare generally much larger in size than the index node. This paper\nnicely demonstrates the practical implications of utilizing a cache-\nsensitive main-memory index in a disk-based environment.",
			  "We also recommend larger node\nsizes for the CSB + -tree, but our recommendation is not based on\nusing special hardware prefetch instructions. Rather, we recognize\nthat node size influences a number of different factors besides cache\nmisses, and that overall performance is improved by carefully con-\nsidering the effect of node size on all these factor",
			  " [27] is an adaptation of the ubiquitous B + -tree for\nmain memory databases [27]. The CSB + -tree is an important data\nstructure for memory-resident databases as it has been shown to\noutperform other cache-conscious, tree-based indices as well as tra-\nditional, tree-based indices in memory-resident databases",
			  "sing a first-order analytical model of the search performance,\nwe show that the conventional choice of setting the node size\nequal to the cache line size is often suboptimal. This design\nchoice focuses on reducing the number of cache misses, but\nignores the effect on the number of instructions that are exe-\ncuted, the number of conditional branches mispredicted, and\nthe number of TLB misses",
			  "The work by Rao and Ross\nhas been extended in recent years in a number of different ways,\nincluding handling variable key length attributes efficiently [5] and\nfor architectures that support prefetching [10].",
			  "Chen, Gibbons, Mowry, and Valentin also propose a version of\ntheir prefetching B+-tree optimized for disk pages, called Fractal\npB+-trees\n[11]."
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/238799599_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "As the speed gap between main memory and modern processors continues to widen, the cache behavior becomes more important for main memory database systems (MMDBs).",
			  "Indexing technique is a key component of MMDBs. Unfortunately, the predominant indexes  B+-trees and T-trees  have been shown to utilize cache poorly, which triggers the development of many cache-conscious indexes, such as CSB+-trees and pB+-trees.",
			  "The J+-tree stores all the keys in its leaf nodes and keeps the reference values of leaf nodes in a Judy structure, which makes J+-tree not only hold the advantages of Judy (such as fast single value search) but also outperform it in other aspects.",
			  "For example, J+-trees can achieve better performance on range queries than Judy. The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans.",
			  "asstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary l",
			  "Recent research addressed the importance of optimizing L2 cache utilization in the design of main memory indexes and proposed the so-called cache-conscious indexes such as the CSB+-tree. However, none of these indexes took account of concurrency control, which is crucial for running the real-world main memory database applications involving index updates and taking advantage of the off-the-shelf multiprocessor systems for scaling up the performance of such applications. O"
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.semanticscholar.org/paper/The-adaptive-radix-tree%3A-ARTful-indexing-for-Leis-Kemper/6abf5107efc723c655956f027b4a67565b048799",
			"excerpts": [
			  "\nMain memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree Expand",
			  "Section Title: The adaptive radix tree: ARTful indexing for main-memory databases",
			  "[Viktor Leis](/author/Viktor-Leis/1787012) , [A. Kemper](/author/A.-Kemper/144122431) , [Thomas Neumann](/author/Thomas-Neumann/143993045)",
			  "Computer Science",
			  "[View on IEEE](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812 \"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6544812\")",
			  "[www-db.in.tum.de](http://www-db.in.tum.de/%7Eleis/papers/ART.pdf \"http://www-db.in.tum.de/%7Eleis/papers/ART.pdf\")",
			  "Save to Library Save",
			  "Create Alert Alert",
			  "Cite",
			  "Share",
			  "442 Citations",
			  "Content:"
			]
		  },
		  {
			"title": "Beautiful branchless binary search | Hacker News",
			"url": "https://news.ycombinator.com/item?id=35737862",
			"excerpts": [
			  "A significant speedup of a classic BS could be achieved by switching to vectorized search when the remaining range has a width of 3-4 SIMD lines."
			]
		  },
		  {
			"title": "An Eight-Dimensional Systematic Evaluation of Optimized ...",
			"url": "http://www.vldb.org/pvldb/vol11/p1550-schulz.pdf",
			"excerpts": [
			  "e implemented the algorithms described above and opti-\nmized the resulting code by eliminating branches, unrolling\nloops and adding software prefetching",
			  " vec-\ntorized the sequential, binary and uniform k-ary search us-\ning AVX2 instruction",
			  "One way to eliminate branches is by branch predication,\ni.e., both sides of a branch are executed and only the effects\nof the side that was actually needed are kept.",
			  "he x86 ar-\nchitecture supports branch predication via conditional move\ninstructions ( cmov )",
			  "el et al. [12] propose to use SIMD to parallelize in-\ndex computations and key comparisons of a k-ary search by\nfitting the *k* ** 1 separator elements and their indices in a\nsingle SIMD regist",
			  "Assuming AVX2 with 32-bit indices and keys, *k* = 9.",
			  "e vectorized implementa-\ntions were again tuned like the scalar variants.",
			  "The sequential search is vectorized by simply loading and\ncomparing *m* consecutive keys in parallel each iteration,\nwhere *m* is the number of keys per SIMD w",
			  "The binary search can be vectorized by viewing the array\nas a sequence of SIMD word sized blocks."
			]
		  },
		  {
			"title": "Avx2/branch-optimized binary search in .NET  GitHub",
			"url": "https://gist.github.com/buybackoff/f403be01486220baba8a9d4fe22c3cf6",
			"excerpts": [
			  "Binary search is theoretically optimal, but it's possible to speed it up substantially using AVX2 and branchless code even in .NET Core.",
			  "Memory access is the limiting factor for binary search. When we access each element for comparison a cache line is loaded, so we could load a 32-byte vector almost free, check if it contains the target value, and if not - reduce the search space by `32/sizeof(T)` elements instead of 1 element. This gives quite good performance improvement (code in `BinarySearch1.cs` and results in the table 1 below).",
			  "The linear search was not using AVX2, and for linear AVX2 should definitely work, shouldn't it!? With vectorized linear search and some additional branching optimization the performance is improved by *additional* 30-50% for the most relevant N (code in `BinarySearch2.cs` and results in the table 2 below)."
			]
		  },
		  {
			"title": "(PDF) k-ary search on modern processors",
			"url": "https://www.researchgate.net/publication/220706895_k-ary_search_on_modern_processors",
			"excerpts": [
			  "Section Title: k-ary search on modern processors > Abstract and Figures",
			  "2.3Binary Search With SIMD Instructions",
			  "Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements.",
			  "The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions."
			]
		  },
		  {
			"title": "The adaptive radix tree | Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013)",
			"url": "https://dl.acm.org/doi/10.1109/ICDE.2013.6544812",
			"excerpts": [
			  "Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck.",
			  "To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory.",
			  "Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well.",
			  "ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes.",
			  "Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.",
			  "Authors : [Viktor Leis]() , [Alfons Kemper]() , [Thomas Neumann]() [Authors Info & Claims]()",
			  "Pages 38 - 49",
			  "Contents",
			  "Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches."
			]
		  },
		  {
			"title": "How I implemented an ART (Adaptive Radix Trie) data structure in Go to increase the performance of my database 2x | by Farhan Ali Khan | techlog | Medium",
			"url": "https://medium.com/techlog/how-i-implemented-an-art-adaptive-radix-trie-data-structure-in-go-to-increase-the-performance-of-a8a2300b246a",
			"excerpts": [
			  "The paper introduces [ART](https://db.in.tum.de/~leis/papers/ART.pdf) , an adaptive radix tree (trie) for efficient indexing in main memory. It quotes the lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. It maintains the data in sorted order, which enables additional operations like range scan and prefix lookup."
			]
		  },
		  {
			"title": "Making B+-Trees Cache Conscious in Main Memory",
			"url": "http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf",
			"excerpts": [
			  "Tree puts all the child\nnodes for a given node contiguously in an array and\nstores only the pointer to the first child node.",
			  "a CSB + -Tree has fewer\npointers per node than a B + -Tree.",
			  "By having fewer\npointers per node, we have more room for keys and\nhence better cache performance.",
			  "We achieve this\ngoal by balancing the best features of the two index\nstructures.",
			  "CSB + -Trees utilize more keys per cache line, and\nare thus more cache conscious than B + -Trees.",
			  "like a CSS-Tree, which requires batch updates, a\nCSB + -Tree is a general index structure that sup-\nports efficient incremental updates.",
			  "SB + -Trees need to maintain the property that\nsibling nodes are contiguous, even in the face of\n ... \nsystem). As observed in [RR99, CLH98], B + -Trees\nwith node size of a cache line have close to optimal\nperf",
			  "S-Trees were proposed in [RR99].\nThey improve on B + -Trees in terms of search per-\nformance because each node contains only keys,\nand no pointers.",
			  "Child nodes are identified by\nperforming arithmetical operations on array offsets.",
			  "Compared with B + -Trees, CSS-Trees utilize more\nkeys per cache line, and thus need fewer cache\naccesses and fewer cache misses.",
			  "The use of arithmetic to identify children requires\na rigid storage allocation policy.",
			  "This approach allows good\nutilization of a cache line.",
			  "We get away with fewer pointers by using a\nlimited amount of arithmetic on array offsets,\ntogether with the pointers, to identify child nodes.",
			  " describe\nvariants with more pointers per node. T",
			  "Cache Sensitive B + -Trees (CSB + -Trees)",
			  "Cache Sensitive B + -Trees (CSB + -Trees)"
			]
		  },
		  {
			"title": "Effect of node size on the performance of cache-conscious B + -trees | Request PDF",
			"url": "https://www.researchgate.net/publication/314796211_Effect_of_node_size_on_the_performance_of_cache-conscious_B_-trees",
			"excerpts": [
			  "Graefe and Larson [2001] summarized techniques of improving cache performance on B-tree indexes. Hankins and Patel [2003] explored the effect of node size on the performance of CSB+-trees and found that using node sizes larger than a cache line size (i.e., larger than 512 bytes) produces better search performance. While trees with nodes that are of the same size as a cache line have the minimum number of cache misses, they found 2 1B refers to 1 billion. ...",
			  "Both methods show that the best search performance is achieved by using a node size larger than 160 bytes. The best results are received when the node size is up to 3072 bytes [Han03] . For an insert operation the effect of node size is the opposite. ...",
			  " -tree has been further optimized using a variety of techniques, such as prefetching [4], storing only partial keys in nodes [5], and choosing the node size more carefully",
			  "Hankins and Patel [HP03] report that by tuning the node size of a CSB + -tree to be significantly bigger than cache line size they are able to reduce the number of instructions executed, TLB misses and branch mispredictions. ...",
			  "Hankins and Patel [37] studied the node size of CSB + -Trees on a Pentium III machine and concluded that it is desirable to use larger node size to reduce the number of tree levels because every level of the tree experiences a TLB miss and a fixed instruction overhead.",
			  "Chen et al. proposed pB+-tree [7] to use larger index nodes and rely on prefetching instructions to bring index nodes into cache before nodes are accessed.",
			  "Masstree [29] is a trie of B + -trees to efficiently handle keys of arbitrary length.",
			  "In this paper, we present NitroGen, a framework for utilizing code generation for speeding up index traversal in main memory database systems."
			]
		  },
		  {
			"title": "A case study for Adaptive Radix Tree index",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S0306437921001228",
			"excerpts": [
			  "In the previous sections, we proposed a solution for cracking the Adaptive Radix Tree (ART), a popular IMDB index. ... Viktor Leis, Alfons Kemper, Thomas Neumann, ..."
			]
		  },
		  {
			"title": "A Case Study for Adaptive Radix Tree Index",
			"url": "https://arxiv.org/pdf/1911.11387",
			"excerpts": [
			  "Leis, A. Kemper, and T. Neumann. 2013. The adaptive radix tree: ARTful indexing for main-memory databases. In In Proceedings of the 29th ..."
			]
		  },
		  {
			"title": "FB -tree: A Memory-Optimized B -tree with Latch-Free Update",
			"url": "https://www.vldb.org/pvldb/vol18/p1579-li.pdf",
			"excerpts": [
			  "The Adaptive Radix Tree (ART) adaptively uses four different node layouts ... [43] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive ..."
			]
		  },
		  {
			"title": "Adaptive Hybrid Indexes",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/146253/3514221.3526121.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "[31] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix. Tree: ARTful Indexing for Main-Memory Databases. In ICDE, Vol. 13. 3849. [32] ..."
			]
		  },
		  {
			"title": "The adaptive radix tree: ARTful indexing for main-memory ...",
			"url": "https://www.researchgate.net/publication/261087784_The_adaptive_radix_tree_ARTful_indexing_for_main-memory_databases",
			"excerpts": [
			  "Leis et al. [42] proposed a fast and space-efficient inmemory trie called ... Alfons Kemper  Thomas Neumann. The two areas of online transaction ..."
			]
		  },
		  {
			"title": "Fractal Prefetching B -Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/fpbtree.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effec- tively optimizes both CPU cache and disk performance on modern processors, ...Read more"
			]
		  },
		  {
			"title": "Amanieu/brie-tree: SIMD-optimized B+ ...",
			"url": "https://github.com/Amanieu/brie-tree",
			"excerpts": [
			  "A fast B+ Tree implementation that uses integer keys. The API is similar to the standard library's BTreeMap with some significant differences:.Read more"
			]
		  },
		  {
			"title": "ARTful indexing",
			"url": "http://daslab.seas.harvard.edu/classes/cs265/files/presentations/CS265_presentation_Sinyagin.pdf",
			"excerpts": [
			  "(P) The Adaptive Radix Tree: ARTful indexing for main-memory databases. Viktor Leis, Alfons Kemper, Thomas Neumann. International Conference on Data ..."
			]
		  },
		  {
			"title": "Implicit Static B-trees - Algorithmica",
			"url": "https://algorithmica.org/en/b-tree",
			"excerpts": [
			  "This is a follow up on a [previous article](https://algorithmica.org/en/eytzinger) about using Eytzinger memory layout to speed up binary search.",
			  "B-trees generalize the concept of binary search trees by allowing nodes to have more than two children.",
			  "Instead of single key, a B-tree node contains up to \\(B\\) sorted keys may have up to \\((B+1)\\) children, thus reducing the tree height in \\(\\frac{\\log_2 n}{\\log_B n} = \\frac{\\log B}{\\log 2} = \\log_2 B\\) times.",
			  "They were primarily developed for the purpose of managing on-disk databases, as their random access times are almost the same as reading 1MB of data sequentially, which makes the trade-off between number of comparisons and tree height beneficial.",
			  "In our implementation, we will make each the size of each block equal to the cache line size, which in case of `int` is 16 elements.",
			  "s.\nNormally, a B-tree node also stores \\((B+1)\\) pointers to its children, but we will only store keys and rely on pointer arithmetic, similar to the one used in Eytzinger array:\nThe root node is numbered \\(0\\) .\nNode \\(k\\) has \\((B+1)\\) child nodes numbered \\(\\{k \\cdot (B+1) + i\\}\\) for \\(i \\in [1, B]\\) .\nKeys are stored in a 2d array in non-decreasing order. If the length of the initial array is not a multiple of \\(B\\) , the last block is padded with the largest value if its data type.",
			  "Back in the 90s, computer engineers discovered that you can get more bang for a buck by adding circuits that do more useful work per cycle than just trying to increase CPU clock rate which [cant continue forever](https://en.wikipedia.org/wiki/Speed_of_light) .",
			  "This worked [particularly well](https://finance.yahoo.com/quote/NVDA/) for parallelizable workloads like video game graphics where just you need to perform the same operation over some array of data. This this is how the concept of *SIMD* became a thing, which stands for *single instruction, multiple data* .",
			  "ta* .\nModern hardware can do [lots of stuff](https://software.intel.com/sites/landingpage/IntrinsicsGuide) under this paradigm, leveraging *data-level parallelism* . For example, the simplest thing you can do on modern Intel CPUs is to:\nload 256-bit block of ints (which is \\(\\frac{256}{32} = 8\\) ints),\nload another 256-bit block of ints,\nadd them together,\nwrite the result somewhere else\nand this whole transaction costs the same as loading and adding just two intswhich means we can do 8 times more work. Magic!\n",
			  "c!\nSo, as we promised before, we will perform all \\(16\\) comparisons to compute the index of the right child node, but we leverage SIMD instructions to do it efficiently. Just to clarifywe want to do something like this:\nbut ~8 times faster.\n"
			]
		  },
		  {
			"title": "k-Ary Search on Modern Processors",
			"url": "https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_People/Profs/rgemulla/publications/schlegel09search.pdf",
			"excerpts": [
			  "In this paper, we take a closer look at *k* -ary search on\nSIMD architectures. Our goal is to determine which SIMD",
			  "data-intensive applications like sorting [2, 4],\nhash-based search [11], and relational query processing [5,",
			  "12] can also benefit from SIMD instructions.",
			  "This paper\ncomplements these techniques by providing efficient meth-\nods for sort-based search.\n",
			  "We classify the instructions into instructions\nfor data loading, element-wise instructions, and horizontal",
			  "e assume throughout\nthat the registers of the processor of interest are vectors of\n*k* ** 1 scalars",
			  "Table 1 lists the worst-case number of iterations performed\nby binary search (Bin), SIMDized binary search (Bin[ *k* ** 1])\nand *k* -ary search for various dataset sizes and values of *k",
			  "learly, *k* -ary search is the more attractive the larger the\ndataset and the larger the value of *k* .",
			  "uture gen-\nerations of processors will support much larger values of *k*\nand thus provide further efficiency gains",
			  "r example, Intel\nrecently announced that its upcoming processors will sup-\nport the AVX instruction set [6] with 256-bit vector registers\n( *k* = 9 for 32-bit keys) for the 2010 processor generation and\nup to 512-bit vector registers ( *k* = 17) for later generations.",
			  "Similarly, the upcoming Larrabee GPGPU [9]a hybrid be-\ntween a GPU and a multi-core CPUprovides 16-way vec-\ntor registers ( *k* = 17) for integer, single-precision float, and\ndouble-precision float instructi",
			  "**2.**",
			  "**PREREQUISITES**",
			  "**2.1**",
			  "**Binary Search**",
			  "Binary search is a dichotomic divide-and-conquer search",
			  "Binary search is a dichotomic divide-and-conquer search",
			  " restrict our attention to the case where the keys are of a\ntype natively supported by the underlying processor archi-\ntectures, i.e., integer and floating point types.",
			  "**3.2**",
			  "**On a Sorted Array**",
			  "p S1 consists of two parts: (a) calculate the indexes\nof the separators and (b) load them into *R* . Substep (a) is\nrequired because the *k* ** 1 separators are stored in non-\ncontiguous memory locations, see Figure 4",
			  "In this case, balanced search trees appear to be the method\nof choice. We conjecture that SIMD instructions will also\nbe valuable to speed up search on those trees, but this is\nbeyond the scope of our current work.",
			  "**4.2**",
			  "**IBM Cell BE: PPE**",
			  "The first experiment was conducted on a PPE core of the\nCell Broadband Engine in Sonys Playstation3.",
			  "gure 6a shows the results of our PPE experiments. For\nsmall datasetsup to 2 16 keysBin performs best. The rea-\nson is that the 128-bit AltiVec unit with its vector registers\noperates concurrently with the scalar integer/floating-point\nregisters and there is no way to directly move data between\nboth types of registers.",
			  "rs.\nTherefore, scalar replication and\nthe horizontal-sum instructions are expensive on the Pow-\nerPC platform. As a consequence, the benefit of fewer iter-\n",
			  "Performance Improvements and Energy Efficiency"
			]
		  },
		  {
			"title": "k-ary search on modern processors | Proceedings of the Fifth International Workshop on Data Management on New Hardware",
			"url": "https://dl.acm.org/doi/10.1145/1565694.1565705",
			"excerpts": [
			  "This paper presents novel tree-based search algorithms that exploit the SIMD instructions found in virtually all modern processors. The algorithms are a natural extension of binary search: While binary search performs one comparison at each iteration, thereby cutting the search space in two halves, our algorithms perform *k* comparisons at a time and thus cut the search space into *k* pieces. On traditional processors, this so-called *k* -ary search procedure is not beneficial because the cost increase per iteration offsets the cost reduction due to the reduced number of iterations. On modern processors, however, multiple scalar operations can be executed simultaneously, which makes *k* -ary search attractive. In this paper, we provide two different search algorithms that differ in terms of efficiency and memory access patterns. Both algorithms are first described in a platform independent way and then evaluated on various state-of-the-art processors. Our experiments suggest that *k* -ary search provides significant performance improvements (factor two and more) on most platforms."
			]
		  },
		  {
			"title": "DEX: Scalable Range Indexing on Disaggregated Memory",
			"url": "https://www.vldb.org/pvldb/vol17/p2603-lu.pdf%3C/ee%3E",
			"excerpts": [
			  "The adaptive radix tree:\nARTful indexing for main-memory databases."
			]
		  },
		  {
			"title": "[PDF] Improving index performance through prefetching | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/99d4b6749d93726e007da067c7b36cb06321d1a3",
			"excerpts": [
			  "### Prefetching J+-Tree: A Cache-Optimized Main Memory Database Index Structure",
			  "The pJ+-tree index exploits prefetching techniques to further improve the cache behavior of J+-trees and yields a speedup of 2.0 on range scans and can provide better performance on both time (search, scan, update) and space aspects.",
			  "### Fractal prefetching B+-Trees: optimizing both cache and disk performance",
			  "Fractal prefetching B+-Trees are proposed, which embed \"cache-optimized\" trees within \"disk-optimization\" trees, in order to optimize both cache and I/O performance.",
			  "### Redesigning database systems in light of cpu cache prefetching",
			  "This thesis investigates a different approach: reducing the impact of cache misses through a technique called cache prefetching, and presents a novel algorithm, Inspector Joins, that exploits the free information obtained from one pass of the hash join algorithm to improve the performance of a later pass.",
			  "### BS-tree: A gapped data-parallel B-tree",
			  "BS-tree is proposed, an in-memory implementation of the B+-tree that adopts the structure of the disk-based index, setting the node size to a memory block that can be processed fast and in parallel using SIMD instructions.",
			  "### Making B+- trees cache conscious in main memory",
			  "CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node, and introduces two variants of CSB+, which can reduce the copying cost when there is a split and preallocate space for the full node group to reduce the split cost."
			]
		  },
		  {
			"title": "M.Sc. Thesis",
			"url": "https://skemman.is/bitstream/1946/7489/1/MSc_Arni-Mar-Jonsson.pdf",
			"excerpts": [
			  "Prefetching B + -tree (pB + -tree) [9] is another main-memory indexing method, which\nis almost identical to the B + -tree. It uses prefetching (see Section 2.3.3), which allows it\nto have nodes wider than a cache-line, without having to wait an entire cache-miss latency\nfor each cache-line accessed. Wider nodes result in shallower trees, and the benefits of\nhaving a shallow tree usually outweigh the prefetching overhead.",
			  "any modern CPUs have parallel memory systems. They can fetch multiple cache-lines\nfrom main-memory at the same time. Programs can instruct the CPU to fetch a given\ncache-line by issuing so-called prefetch instructions. Multiple prefetch instructions can\nbe executed at the same time. For example, the Alpha 21264 CPU has a 150 cycle cache-\nmiss latency. It can, however, fetch 15 cache-lines at the same time, meaning that a\ncache-line can be delivered to cache every 10 cycles. If a program knows what cache-line\nit will need 150 cycles later, it can prefetch it and 150 cycles later it is available in c",
			  "This way the perceived cache-miss latency is 0 cycles, even though the actual cache-miss\nlatency is unchanged",
			  "**Node Layout and Prefetching**",
			  "Pointer elimination and prefetching as described in this chapter are complementary tech-\nniques [9]. It is possible to increase the node width of the CSB + -tree and prefetching\nnodes like the pB + -tree does, resulting in the Prefetching CSB + -Tree (pCSB + -tree).\nThat way, you get better performance than the pB + -tree, since the branch factor is slightly\nhigher.",
			  "The cache-performance of the pCSB + -tree can be found using the same method as pB + -\ntree and doubling the branch factor.",
			  "Indicespredominantly B + -treesare a key performance component of DBMSs. Un-\nfortunately, however, B + -trees have been shown to utilize cache memory poorly [9], trig-\ngering the development of many cache-conscious indices. The CSS-tree [26] and CSB + -\ntree [25] improve cache performance by not storing pointers to all the children of a node,\neffectively compacting the index structure and improving locality.",
			  "In their seminal paper, Ailamaki et al. [2] showed that less than half of the CPU\ntime for commercial DBMSs is spent on computations.",
			  "he Prefetching CSB + -Tree",
			  "he Prefetching CSB + -Tree",
			  "**General Description**",
			  "**General Description**",
			  "Chapter 7",
			  "**Conclusions**",
			  "In this thesis we have studied the performance of the pB + -tree on the Itanium 2 processor."
			]
		  },
		  {
			"title": "(PDF) Improving Index Performance through Prefetching",
			"url": "https://www.researchgate.net/publication/2528098_Improving_Index_Performance_through_Prefetching",
			"excerpts": [
			  "This paper proposes and evaluates Prefetching B -Trees#, which use prefetching to accelerate two important operations on B -Tree indices: searches and range scans.",
			  "To accelerate searches, pB -Trees use prefetching to e create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B -Tree, thereby decreasing the number of expensive misses when going from parenttochild without signi increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.2.5 for main-memory B -Trees.",
			  "In addition, it outperforms and is complementary to #Cache-SensitiveB -Trees.",
			  "To accelerate range scans, pB -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys.",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "The Adaptive Radix Tree: ARTful Indexing for Main- ...",
			"url": "https://www.yumpu.com/en/document/view/21981799/the-adaptive-radix-tree-artful-indexing-for-main-memory-",
			"excerpts": [
			  "The Adaptive Radix Tree: ARTful Indexing for Main-Memory ..."
			]
		  },
		  {
			"title": "Adaptive Radix Tree (ART) Index - All things DataOS",
			"url": "https://dataos.info/resources/stacks/flash/art/",
			"excerpts": [
			  "Node Types in ART: Node4: Supports up to 4 child pointers. Node16: Supports up to 16 child pointers. Node48: Supports up to 48 child pointers. Node256 ...Read more"
			]
		  },
		  {
			"title": "Adaptive Radix Tree for Databases | PDF | Cpu Cache",
			"url": "https://www.scribd.com/document/189896702/Art",
			"excerpts": [
			  "In this work, we present the adaptive radix tree (ART) which is a fast and space-efcient in-memory indexing structure specically tuned for modern hardware.Read more"
			]
		  },
		  {
			"title": "[PDF] Making B+- trees cache conscious in main memory | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Making-B%2B-trees-cache-conscious-in-main-memory-Rao-Ross/e542bb567661be100fe983209f223146ab437520",
			"excerpts": [
			  "A new indexing technique called CSB+-Trees is proposed that stores all the child nodes of any given node contiguously, and keeps only the address of the ..."
			]
		  },
		  {
			"title": "Array Layouts for Comparison-Based Searching",
			"url": "https://arxiv.org/abs/1509.05053",
			"excerpts": [
			  "For larger values of n, we arrive at the surprising conclusion that the Eytzinger layout is usually the fastest. The latter conclusion is ...Read more"
			]
		  },
		  {
			"title": "Making B+- trees cache conscious in main memory | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/10.1145/335191.335449",
			"excerpts": [
			  "Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees.",
			  "However, CSS-Trees are designed for decision support workloads with relatively static data.",
			  "Although B + -Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers.",
			  "Nevertheless, for applications that require incremental updates, traditional B + -Trees perform well.",
			  "Our goal is to make B + -Trees as cache conscious as CSS-Trees without increasing their update cost too much.",
			  "We propose a new indexing technique called Cache Sensitive B + -Trees (CSB + -Trees).",
			  "It is a variant of B + -Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node.",
			  "The rest of the children can be found by adding an offset to that address.",
			  "Since only one child pointer is stored explicitly, the utilization of a cache line is high.",
			  "CSB + -Trees support incremental updates in a way similar to B + -Trees.",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Section Title: Making B+- trees cache conscious in main memory > Abstract",
			  "Content:",
			  "Content:",
			  "We also introduce two variants of CSB + -Trees. Segmented CSB + -Trees divide the child nodes into segments.",
			  "Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node.",
			  "Segmented CSB + -Trees can reduce the copying cost when there is a split since only one segment needs to be moved.",
			  "Full CSB + -Trees preallocate space for the full node group and thus reduce the split cost.",
			  "Our performance studies show that CSB + -Trees are useful for a wide range of applications."
			]
		  },
		  {
			"title": "Understanding the efficiency of ray traversal on GPUs | Proceedings of the Conference on High Performance Graphics 2009",
			"url": "https://dl.acm.org/doi/10.1145/1572769.1572792",
			"excerpts": [
			  "Content:"
			]
		  },
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This works well for simple access patterns, like iterating over array in increasing or decreasing order, but for something complex like what we have here its not going to perform well.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree .",
			  "This blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.",
			  "The goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!",
			  "The source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Speeding up independent binary searches by interleaving them  Daniel Lemire's blog",
			"url": "https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/",
			"excerpts": [
			  "Each data access is done using fewer than 10 instructions in my implementation, which is far below the number of cycles and small compared to the size of the instruction buffers, so finding ways to reduce the instruction count should not help.",
			  "redit: This work is the result of a collaboration with Travis Downs and Nathan Kurz, though all of the mistakes are mine.\nD",
			  "Daniel Lemire, \"Speeding up independent binary searches by interleaving them,\" in *Daniel Lemire's blog* , September 14, 2019, https://lemire.me/blog/2019/09/14/speeding-up-independent-binary-searches-by-interleaving-them/ .",
			  "The condition move instructions are pretty much standard and old at this point.",
			  "On Cannon Lake, however, you should be able to do better.",
			  "9 on Cannon Lake, 7 on Skylake, 5 on Skylark"
			]
		  },
		  {
			"title": "cwisstable/DESIGN.md at main",
			"url": "https://github.com/google/cwisstable/blob/main/DESIGN.md",
			"excerpts": [
			  "The Abseil implementation implements SwissTable via the raw_hash_set type, which provides sufficient extension points that all of the various flavors of hash ...Read more"
			]
		  },
		  {
			"title": "Facebook open-sources F14 algorithm for faster and memory-efficient hash tables",
			"url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/facebook-open-sources-f14-algorithm-for-faster-and-memory-efficient-hash-tables?srsltid=AfmBOorcYqrbSM-yIqjNiyPD-2c3or2pVOwzIgzCg7M0yVcU3kq050zF",
			"excerpts": [
			  "F14 helps the hash tables provide a faster way for maintaining a set of keys or map keys to values, even if the keys are objects, like strings.Read more"
			]
		  },
		  {
			"title": "A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort | Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/abs/10.1145/2588555.2610522",
			"excerpts": [
			  "This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy.Read more"
			]
		  },
		  {
			"title": "abseil / Swiss Tables Design Notes",
			"url": "https://abseil.io/about/design/swisstables",
			"excerpts": [
			  "Within Swiss tables, the result of the hash function produces a 64-bit hash\nvalue. We split this value up into two parts:\nH1, a 57 bit hash value, used to identify the element index within the table\nitself, which is truncated and modulated as any normal hash value would be for\nlookup and insertion purposes.\nH2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "Swiss tables hold a densely packed array of metadata, containing presence\ninformation for entries in the table. This presence information allows us to\noptimize both lookup and insertion operations. This metadata adds one byte of\noverhead for every entry in the table.",
			  "The metadata of a Swiss table stores presence information (whether the element\nis empty, deleted, or full). Each metadata entry consists of one byte, which\nconsists of a single control bit and the 7 bit H2 hash. The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "When searching for items in the table we use [SSE instructions](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) to scan for\ncandidate matches. The process of finding an element can be roughly summarized\nas follows:\nUse the H1 hash to find the start of the bucket chain for that hash.\nUse the H2 hash to construct a mask.\nUse SSE instructions and the mask to produce a set of candidate matches.\nPerform an equality check on each candidate.\nIf no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates. Note that a deleted element does not cease\nprobing, though an empty element would.\nSteps 2+3 can be summarized visually as:\nEquivalent code for this lookup appears below:\nThis process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "Use the H2 hash to construct a mask.",
			  "Perform an equality check on each candidate.",
			  "For performance reasons, it is important that you use a hash function that\ndistributes entropy across the entire bit space well (producing an [avalanche effect]",
			  "This process is an important performance trick. Because we can winnow 16\ncandidates down to only those with a matching H2 hash in only a few\ninstructions, we are functionally searching very deep probe chains\ninexpensively.",
			  "This metadata adds one byte of\noverhead for every entry in the table.",
			  "H2, the remaining 7 bits of the hash value, used to store metadata for this\nelement. The H2 hash bits are stored separately within the metadata section of\nthe table.",
			  "The control bit, in\ncombination with the value in the H2 section of the metadata, indicates whether\nthe associated hash element is empty, present, or has been deleted.",
			  "Use the H1 hash to find the start of the bucket chain for that hash.",
			  "Use SSE instructions and the mask to produce a set of candidate matches.",
			  "If no element is found amongst the current candidates, perform probing to\ngenerate a new set of candidates.",
			  "The H2 hash bits are stored separately within the metadata section of\nthe table."
			]
		  },
		  {
			"title": "Inside Googles Swiss Table: A High-Performance Hash Table Explained | by Donghyung Ko | Medium",
			"url": "https://koko8624.medium.com/open-addressing-hash-table-df7c1ef4f420",
			"excerpts": [
			  "For a keys hash, split into:\n`H1` : upper 57 bits to compute the group index\n`H2` : lower 7 bits as the fingerprint stored in the control byte",
			  "SIMD-compare that groups 16 control bytes against `H2` (and against `EMPTY` / `DELETED` ).",
			  "\nControl bytes are organized in groups that match common SIMD widths (e.g., 16 bytes = 128 bits). By scanning 16 control bytes(can be various) at once (via SIMD), the implementation can quickly find candidate or empty slots and stop early",
			  "It maximizes cache efficiency and lookup speed through the use of **control bytes** and **group-based scanning** with **SIMD** instructions.",
			  "**Swiss Table** is a high-performance hash table design for C++, introduced in 2017 by Google engineers **Sam Benzaquen, Alkis Evlogimenos, Matt Kulukundis, and Roman Perepelitsa** .",
			  "It uses an **open addressing** hash table structure.",
			  "In an open addressing hash table, when a hash collision occurs, the algorithm probes other empty slots within the table to find a location where the key can be placed.",
			  "Typical control byte states:\n`Empty` : `0x80`  slot unused\n`Deleted` : `0xFE`  slot deleted\n`Full` : `0x00`  `0x7F`  slot occupied (stores a 7-bit H2 fingerprint)\n",
			  "Linear Probing ( `i + j` ): Move one slot ( `j` ) at a time from the original hash position ( `i` )",
			  "Quadratic Probing( `i + j` ): Move `j` slots from the original hash position ( `i` )",
			  "Swiss Table has the following key characteristics:",
			  "It combines **linear probing** with the **Robin Hood hashing algorithm** to resolve collisions.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split)",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > SIMD & Groups (H1/H2 Split) > Lookup flow (high level):",
			  "Use `H1` to pick a starting group.",
			  "Only for matching positions, check the actual keys; if `EMPTY` is observed, stop early.",
			  "Section Title: **Inside Googles Swiss Table: A High-Performance Hash Table Explained** > Open Addressing Hash Table"
			]
		  },
		  {
			"title": "APT-GET | Proceedings of the Seventeenth European Conference on Computer Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3492321.3519583",
			"excerpts": [
			  "imin Chen, Anastassia Ailamaki, Phillip B Gibbons, and Todd C Mowry. 2004. Improving Hash Join Performance through Prefetching. In *Proceedings of the 20th International Conference on Data Engineering.* 116.",
			  "mir Roth, Andreas Moshovos, and Gurindar S Sohi. 1998. Dependence based prefetching for linked data structures. *ACM SIGOPS Operating Systems Review* 32, 5 (1998), 115--126",
			  "mison D Collins, Hong Wang, Dean M Tullsen, Christopher Hughes, Yong-Fong Lee, Dan Lavery, and John P Shen. 2001. Speculative pre-computation: Long-range prefetching of delinquent loads. In *Proceedings 28th Annual International Symposium on Computer Architecture.* IEEE, 14--25."
			]
		  },
		  {
			"title": "MetaSys-open-source-cross-layer-metadata- ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/MetaSys-open-source-cross-layer-metadata-management_taco22-arxiv22.pdf",
			"excerpts": [
			  "[30] Chi-Keung Luk and T. C. Mowry. Cooperative Prefetching: Compiler and Hardware Support for Effective Instruction Prefetching in Modern\nProcessors. In *MICRO* , 1998.",
			  "[31] Trishul M Chilimbi and Martin Hirzel. Dynamic Hot Data Stream Prefetching for General-Purpose Programs. In *PLDI* , 2002."
			]
		  },
		  {
			"title": "References - Engineering Information Technology",
			"url": "https://user.eng.umd.edu/~blj/memory/Book-References.pdf",
			"excerpts": [
			  "A. Roth, A. Moshovos, and G. Sohi. 1998. Dependence\nbased prefetching for linked data structures. In\nThe 8th Int. Conf. on Architectural Support for\nProgramming Languages and Operating Systems\n(ASPLOS), pp. 115126, October 1998.",
			  "A. Roth and G. S. Sohi. 1999. Effective jump-pointer\nprefetching for linked data structures. In Proc. 26th Int.\nSymp. on Computer Architecture (ISCA), Atlanta, GA,\nMay 1999.",
			  "E. Rotenberg, S. Bennett, and J. E. Smith. 1996.\nTrace cache: A low latency approach to high\nbandwidth instruction fetching. In Proc. 29th Ann.\nACM/IEEE Int. Symp. on Microarchitecture (MICRO-\n29), pp. 2435, Paris, France, December 1996"
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/248209.237190",
			"excerpts": [
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the ...Read more"
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program.",
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses.",
			  "By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal.",
			  "To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/384265.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "Dependence based prefetching for linked data structures > Abstract",
			  "Content:\nWe introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Analysis of Cache Behavior and Performance of Different BVH ...",
			"url": "https://www.dominikwodniok.de/publications/Wodniok_EGPGV2013.pdf",
			"excerpts": [
			  "The best performing combination is the TDFS BVH layout with a threshold of 0.6 using the. AoS node layout. It is only marginally faster than the base- line ...Read more"
			]
		  },
		  {
			"title": "Why is SOA (Structures of Arrays) faster than AOS?",
			"url": "https://www.reddit.com/r/C_Programming/comments/9jg7hy/why_is_soa_structures_of_arrays_faster_than_aos/",
			"excerpts": [
			  "I have heard that SOA (Structures of Arrays) is faster due to CPU caching then AOS (Array of Structures). But that doesn't really make sense to me.Read more"
			]
		  },
		  {
			"title": "c - cache locality for a binary tree - Stack Overflow",
			"url": "https://stackoverflow.com/questions/31281905/cache-locality-for-a-binary-tree",
			"excerpts": [
			  "If you want data to be collocated to take advantage of cache locality a simple alternative is to allocate an array of the struct and then allocate from that ...Read more"
			]
		  },
		  {
			"title": "Help with BVH memory access optimizations",
			"url": "https://www.reddit.com/r/GraphicsProgramming/comments/1f90xbi/help_with_bvh_memory_access_optimizations/",
			"excerpts": [
			  "pointer chasing and frequent cache misses.",
			  "flatten the tree.",
			  "Note that `sizeof(BVHNode)` is exactly 32, so a node and one of its children should be pulled together to a cache line as far as I understand. This has made the program twice as much slower!",
			  "Surprisingly this did not improve performance. It is actually comparable to the pointer-based implementation.",
			  "My understanding is that this makes traversal slow due to pointer chasing and frequent cache misses.",
			  "So I quickly moved to flatten the tree."
			]
		  },
		  {
			"title": "Binary search is a pathological case for caches - Paul Khuong: some Lisp",
			"url": "https://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/",
			"excerpts": [
			  "Binary search is a pathological case for caches",
			  "\nBinary search suffers from a related ailment when executed on medium\nor large vectors of almost power-of-two size (in bytes)",
			  "Two vectors of 64KB each, allocated at 0x10000 and 0x20000, have the unfortunate property that, for each index, the data at that index in both vectors will map to the same cache lines in a direct-mapped cache with 1024 lines of 64 bytes each.",
			  "the L1D cache has 512 lines of 64 bytes each, L2 4096\nlines, and L3 196608 lines (12 MB). Crucially, the L1 and L2 caches are\n8-way set-associative, and the L3 16-way.",
			  "There are workarounds, on both the hardware and software sides.",
			  "cache lines are power-of-two sizes, as are L1 and L2 set (buckets) counts."
			]
		  },
		  {
			"title": "Locality, B-trees, and splay trees",
			"url": "https://www.cs.cornell.edu/courses/cs312/2004sp/lectures/lec24.html",
			"excerpts": [
			  "Having caches only helps if when the processor needs to get some data, it is\nalready in the cache. Thus, the first time the processor access the memory, it\nmust wait for the data to arrive. On subsequent reads from the same location,\nthere is a good chance that the cache will be able to serve the memory request\nwithout involving main memory. Of course, since the cache is much smaller than\nthe main memory, it can't store all of main memory. The cache is constantly\nthrowing out information about memory locations in order to make space for new\ndata. The processor only gets speedup from the cache if the data fetched from\nmemory is still in the cache when it is needed again. When the cache has the\ndata that is needed by the processor,  it is called a **cache hit** . If\nnot, it is a **cache miss** . The ratio of the number of hits to misses is\ncalled the **cache hit ratio** .",
			  "Caches improve performance when memory accesses exhibit: reads from memory\ntends to request the same locations repeatedly, or at least memory locations\nnear previous requests. A tendency to revisit the same or nearby locations is\nknown as **locality** . Computations that exhibit locality will have a relatively high cache hit\nratio.",
			  "at caches actually store chunks of memory rather than individual words of\nmemory. So a series of memory reads to nearby memory locations are likely to\nmostly hit in the cache. When there is a cache miss, a whole sequence of memory\nwords is requested from main memory at once, because it is cheaper to read\nmemory that way. The cache records cached memory locations in units of **cache\nlines** whose size depends on the size of the cache (typically 4 - 32 words).",
			  "The same idea can be applied to trees. Binary trees are not good for locality\nbecause a given node of the binary tree probably occupies only a fraction of a\ncache line. **B-trees** are a way to get better locality. As in the hash\ntable trick above, we store several elements in a single node -- as many as will\nfit in a cache line."
			]
		  },
		  {
			"title": "SoA vs AoS: Data Layout Optimization | ML & CV Consultant - Abhik Sarkar",
			"url": "https://www.abhik.ai/concepts/performance/soa-vs-aos",
			"excerpts": [
			  "is seemingly simple decision can result in **10-100x performance differences** in modern computing system",
			  "The choice between AoS and SoA affects everything from CPU cache efficiency to SIMD vectorization capabilities to GPU memory coalescing patterns.",
			  "Memory Access Pattern Animation"
			]
		  },
		  {
			"title": "Array of Structs and Struct of Arrays | *^^*  HWisnu's blog   ^^",
			"url": "https://hwisnu.bearblog.dev/array-of-structs-and-struct-of-arrays/",
			"excerpts": [
			  "s (AoS) can be less cache-friendly because accessing multiple fields of a single element requires loading multiple cache lines. ",
			  "oA is almost 30% faster based on this simple example, the difference will get significantly magnified with more complex scenario, sometimes SoA (optimized) is 10x faster compared to AoS as shown in this [great article"
			]
		  },
		  {
			"title": "AoS and SoA - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/AoS_and_SoA",
			"excerpts": [
			  "**Structure of arrays** ( **SoA** ) is a layout separating elements of a [record](/wiki/Record_(computer_science) \"Record (computer science)\") (or 'struct' in the [C programming language](/wiki/C_(programming_language) \"C (programming language)\") ) into one parallel array per [field](/wiki/Field_(computer_science) \"Field (computer science)\") . [[ 1 ]]() The motivation is easier manipulation with packed [SIMD instructions](/wiki/SIMD_instruction \"SIMD instruction\") in most [instruction set architectures](/wiki/Instruction_set_architecture \"Instruction set architecture\") , since a single [SIMD register](/wiki/SIMD_register \"SIMD register\") can load [homogeneous data](/w/index.php?title=Homogeneous_data&action=edit&redlink=1 \"Homogeneous data (page does not exist)\") , possibly transferred by a wide [internal datapath](/wiki/Internal_datapath \"Internal datapath\") (e.g. [128-bit](/wiki/128-bit \"128-bit\") ). If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line. The downside is requiring more [cache ways](/wiki/Cache_way \"Cache way\") when traversing data, and inefficient [indexed addressing](/wiki/Indexed_addressing \"Indexed addressing\") .",
			  "For example, to store *N* points in 3D space using a structure of arrays:",
			  "array of structures",
			  " ( **AoS** ) is the opposite (and more conventional) layout, in which data for different fields is interleaved. This is often more intuitive, and supported directly by most [programming languages](/wiki/Programming_languages \"Programming languages\") .",
			  "AoS vs. SoA presents a choice when considering 3D or [4D vector](/wiki/4D_vector \"4D vector\") data on machines with four-lane SIMD hardware. SIMD ISAs are usually designed for homogeneous data, however some provide a [dot product](/wiki/Dot_product \"Dot product\") instruction [[ 5 ]]() and additional permutes, making the AoS case easier to handle.",
			  "Although most [GPU](/wiki/Graphics_processing_unit \"Graphics processing unit\") hardware has moved away from 4D instructions to scalar [SIMT](/wiki/Single_instruction,_multiple_threads \"Single instruction, multiple threads\") pipelines, [[ 6 ]]() modern [compute kernels](/wiki/Compute_kernel \"Compute kernel\") using SoA instead of AoS can still give better performance due to memory coalescing. [[ 7 ]]()",
			  "SoA is mostly found in languages, libraries, or [metaprogramming](/wiki/Metaprogramming \"Metaprogramming\") tools used to support a [data-oriented design](/wiki/Data-oriented_design \"Data-oriented design\") . Examples include:\n\"Data frames\", as implemented in [R](/wiki/R_(programming_language) \"R (programming language)\") , [Python](/wiki/Python_(programming_language) \"Python (programming language)\") 's Pandas package, and [Julia](/wiki/Julia_(programming_language) \"Julia (programming language)\") 's DataFrames.jl package, are interfaces to access SoA like AoS.\nThe Julia package StructArrays.jl allows for accessing SoA as AoS to combine the performance of SoA with the intuitiveness of AoS."
			]
		  },
		  {
			"title": "\n\tBuilding BVH4 or BVH8 - Intel Community\n",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/td-p/1132356",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure ..."
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies(originally ...",
			"url": "https://www.embree.org/papers/2018-HPG-compressedleafbvh.pdf",
			"excerpts": [
			  "For each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "brees fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is\nstored as 2 ** 3 bytes, for the boxs *lower* and *upper* bounds,\nrequiring 48 bytes for all 8 children. Including the 8 child\npointers, this sums to a total of 136 bytes, slightly more than\nhalf an uncompressed BVH8 multi-node",
			  "sting 1: Illustration of the three BVH node types.**\n**Top: Embrees regular BVH8 nodes contain 8 point-**\n**ers and float boxes (256 bytes). Middle: Embrees**\n**quantized QBVH8 nodes contain 8 pointers, 8 quan-**\n**tized bounding boxes, and 6 floats to specify the**\n**dequantization domain (136 bytes). Bottom: At the**\n**leaf level, our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the no",
			  " resulting BVHwhich we call a Compressed-\nLeaf BVH ( CLBVH )has two multi-node types: regular BVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original BVH8 multi-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1).\nSince",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).",
			  "Exactly\nwhat primitive data is stored in a leaf depends on the BVH\ntype: for triangles, it is either a list of triangle4 structures,\nfully pre-gathered vertices of four triangles in SoA layout;\nor a list of triangle4i with four triangles worth of vertex\nindices.",
			  "our method introduces an even smaller**\n**compressed BVH node type (72 bytes)knowing it**\n**will only contain leaf nodesand omits the pointers**\n**by storing the primitive data right after the nod",
			  " eliminating the primitive pointers for the leaf nodes our\ncompressed multi-leaf node requires only 72 bytes. Compared\nto an uncompressed BVH8 multi-node (256 bytes) this yields\na compression factor of over 3 ** .",
			  "We implement and evaluate the previously discussed strategy\nwithin a modified version of Embree 3.0.",
			  "mpressed-Leaf*\n*Bounding Volume Hierarchies* (CLBVH), which strike a bal-\nance between compressed and non-compressed BVH layouts",
			  "\nOur CLBVH layout introduces dedicated compressed multi-\nleaf nodes where most effective at reducing memory use, and\nuses regular BVH nodes for inner nodes and small, isolated\nleaves. We show that when implemented within the Embree\nray tracing framework, this approach achieves roughly the\nsame memory savings as Embrees current compressed BVH\nlayout, while maintaining almost the full performance of its\nfastest non-compressed BVH.",
			  "In Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf.",
			  "s fully compressed QBVH8 structure employs a sim-\nilar approach to Ylitie et al. [ 11 ]. In the QBVH8 layout, the 8\nchild bounding boxes are expressed relative to the parents\nbounding box, and quantized to 8-bit fixed point values. Each\nQBVH8 multi-node stores the parent bounding box in the form\nof its *start* and *extent* , stored as two 3-dimensional single\nprecision vectors (2 ** 12 bytes). Each childs bounding box is",
			  "cus on compressing just the leaf nodes, by introduc-\ning dedicated *compressed multi-leaf nodes* . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\ntraverse interior nodes.\n**",
			  "**2**\n**RELATED WORK**\nAcceleration structures for ray tracing have a long history;\na survey on the general concepts can be found in Havrans\nthesis [ 4 ]. Today, most ray tracers use some sort of BVH,\ntypically with a branching factor of 4 or 8, and in some\ncases 16 [ 2 , 3 , 7 , 9 , 10 ]. While in the past each ray tracer\nimplemented its own acceleration structures and traversal\nmethods, the last few years have seen the emergence of\ncommonly accepted ray tracing libraries such as Embree\nfor CPUs [ 10 ], and OptiX for GPUs [ 7 ], both of which use\nwide BVHs.",
			  "n**\nIn Embrees BVH8 data layout, each multi-node contains 8\nbounding boxes and 8 (64-bit) child pointers (see Listing 1).\nFor each individual node in such a multi-node, the childRef\nvalue encodes whether the node is an inner or leaf node; as\nwell as the pointer. For inner nodes the pointer refers to\nanother BVH8 multi-node; for leaves it points to the leafs *leaf*\n*data* , the list of primitive data belonging to the leaf."
			]
		  },
		  {
			"title": "Hardware-Accelerated Dual-Split Trees",
			"url": "https://hwrt.cs.utah.edu/papers/hardware_dual-split_trees.pdf",
			"excerpts": [
			  "A wider node (4 or 8 children) is attractive to current CPU\nor GPU architectures as computations on these wider nodes can benefit from SIMD computation.",
			  " BVH8 node has 200 bytes (192 bytes\nfor child bounding boxes, 4 bytes for offset, and 4 bytes for node types).",
			  "BVH4 and BVH8 nodes\nstore degenerate bounding boxes for empty children."
			]
		  },
		  {
			"title": "Building BVH4 or BVH8",
			"url": "https://community.intel.com/t5/Intel-Embree-Ray-Tracing-Kernels/Building-BVH4-or-BVH8/m-p/1132356/highlight/true",
			"excerpts": [
			  "Hello, I am trying to create a High Quality BVH4 structure using the BVH builder tutorial. I am aware that it is creating a BVH2 structure and"
			]
		  },
		  {
			"title": "Shallow Bounding Volume Hierarchies for Fast SIMD Ray ...",
			"url": "https://jo.dreggn.org/home/2008_qbvh.pdf",
			"excerpts": [
			  "The result is a large BVH node with a size of 128 bytes which\nperfectly matches the caches of modern computer hardware.",
			  "The 4 bounding boxes are stored in structure-of-arrays (SoA)\nlayout for direct processing in SIMD registers.",
			  "The leaf bounding box is already\nstored in the parent and the leaf data can be encoded directly\ninto the corresponding child integer.",
			  "We use the sign of the child index to encode whether a node\nis a leaf or an inner node.",
			  "The tree data is kept in a linear array of memory\nlocations, so the child pointer can be stored as integer indices\ninstead of using platform-dependent pointers.",
			  "Four\nchildren are created for each node.",
			  "Given a ray, the stack-based traversal algorithm starts by si-\nmultaneously intersecting the ray with the four bounding boxes\ncontained in the root SIMD BVH Node using SIMD instructions",
			  "The pointers of the children with a non-empty bounding box\nintersection are sorted and then pushed on the stack.",
			  "The ray is prepared prior to traversal by replicating the values\nfor the maximum ray distance ( tfar ), the origin, and reciprocal\nof the direction across a SIMD register."
			]
		  },
		  {
			"title": "Exploiting Local Orientation Similarity for Efficient Ray ...",
			"url": "https://www.embree.org/papers/2014-HPG-hair.pdf",
			"excerpts": [
			  "a BVH with a branching factor of four that will allow for al-\nways intersecting four child-bounds in parallel (Section 3.4 ).",
			  "This data-parallel intersection in particular requires that ev-\nery group of four sibling nodes have to be of the same type:\nIf only one prefers OBBs, all four nodes have to be OBB\nnodes.",
			  "**Node References.** A node stores 64-bit *node references* to\npoint to its children. These node references are decorated\npointers, where we use the lower 4 bits to encode the type of\ninner node we reference (AABB node or OBB node) or the\nnumber of hair segments pointed to by a leaf node. During\ntraversal we can use simple bit operations to separate the\nnode type information from the aligned pointer.",
			  "**AABB nodes.** For nodes with axis aligned bounds, we store\nfour bounding boxes in a SIMD friendly structure-of-array\nlayout (SOA). In addition to the single-precision floating\npoint coordinates for the four AABBs this node also stores\nthe four 64-bit node references, making a total of 128 bytes\n(exactly two 64-byte cache lines)."
			]
		  },
		  {
			"title": "Faster Incoherent Ray Traversal Using 8-Wide AVX ...",
			"url": "https://www.cs.ubbcluj.ro/~afra/publications/afra2013tr_mbvh8.pdf",
			"excerpts": [
			  ";\nThe node size for MBVH4 is 128 bytes, and for MBVH8\nit is 256 bytes.\nT",
			  "\nThe multi-triangles, similar to the bounding boxes in the\nnodes, have a fixed-size SoA layout to facilitate SIMD pro-\ncessing. The triangle data is pregathered to maximize inter-\nsection performance at the cost of higher memory usage.\n**"
			]
		  },
		  {
			"title": "Compressed-Leaf Bounding Volume Hierarchies",
			"url": "https://diglib.eg.org/bitstream/handle/10.1145/3231578-3231581/06-1025-benthin.pdf",
			"excerpts": [
			  ".\n**4.1**\n**Node Compression and Decompression**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf.",
			  "**struct** BVH8MultiNode {\nbox3f childBounds [8]; //one float box per child\nuint64 childRef [8]; }; // child pointers",
			  "**struct** QBVH8MultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nuint64 childRef [8]; }; // child pointers",
			  "**struct** CLBVHMultiNode {\nvec3f start , extent; // shared full -prec. start/extent\nbox3ui8 childBounds [8]; //8-bit fixed -point child boxes\nLeafPrimData childPrims [0]; // implicit pointer\n}; // leaf data stored right behind this node",
			  "ion**\nIn Embrees BVH8 data layout, each multi-node contains 8 bounding\nboxes and 8 (64-bit) child pointers (see Listing 1). For each individual\nnode in such a multi-node, the childRef value encodes whether the\nnode is an inner or leaf node; as well as the pointer. For inner nodes\nthe pointer refers to another BVH8 multi-node; for leaves it points\nto the leafs *leaf data* , the list of primitive data belonging to the\nleaf. E",
			  "**4**\n**IMPLEMENTATION**\nWe implement and evaluate the previously discussed strategy within\na modified version of Embree 3.0.",
			  "The key to improving fast ray tracing is the use of acceleration data\nstructures. Though indispensable for performance, such structures\nrequire both time and memory to be built and stored. In particu-\nlar, the memory overhead of the acceleration structure can be a",
			  "roduces dedicated compressed multi-leaf nodes where most effec-\ntive at reducing memory use, and uses regular BVH nodes for inner\nnodes and small, isolated leaves. We show that when implemented\nwithin the Embree ray tracing framework, this approach achieves\nroughly the same memory savings as Embrees compressed BVH\nlayout, while maintaining almost the full performance of its fastest\nnon-compressed BVH.\n**CC"
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH ...Read more"
			]
		  },
		  {
			"title": "[PDF] Efficient incoherent ray traversal on GPUs through ...",
			"url": "https://www.semanticscholar.org/paper/Efficient-incoherent-ray-traversal-on-GPUs-through-Ylitie-Karras/7d4816055b1a9aecb75f36e5e6ab5948b1354ed1",
			"excerpts": [
			  "Efficient incoherent ray traversal on GPUs through compressed wide BVHs  H. Ylitie, Tero Karras, S. Laine  Published in High Performance Graphics 28 July 2017 ..."
			]
		  },
		  {
			"title": "(PDF) Compressed-leaf bounding volume hierarchies",
			"url": "https://www.researchgate.net/publication/326762122_Compressed-leaf_bounding_volume_hierarchies",
			"excerpts": [
			  "Compressed-Leaf Bounding Volume Hierarchies (CLBVH), which strike a balance between compressed and non-compressed BVH layouts. Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "Typically, wide BVHs use a data layout where all of an\nindividual nodes N children are stored together in a consec-\nutive block, typically in a SoA data layout. This allows for\naddressing all of a parents N children with a single pointer\nand aids vectorization, but slightly confuses the terminology\nof what a node in a wide BVH actually i",
			  "Throughout the\npaper we will refer to each group of N siblings as a N-wide\nmulti- node, with each sibling consisting of N individual nodes",
			  "s . Our approach\nachieves similar or better compression to fully-compressed\nBVHs, while having nearly the same traversal performance\nas uncompressed BVHs, as no decompression is required to\n ... ",
			  " summary, our resulting BVHwhich we call a Compressed-\nLeaf BVH (\nCLBVH\n)has two multi-node types: regular\nBVH8\nmulti-nodes containing 8 individual nodes, each of which\ncould be a regular inner node, or a regular individual leaf\nnode, just as in Embrees original\nBVH8\nmulti-nodes; and\nour new compressed multi-leaf node which stores (up to) 8\nindividual leaves, in compressed form (also see Listing 1)",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "ompared\nto an uncompressed\nBVH8\nmulti-node (256 bytes) this yields\na compression factor of over 3  .",
			  "CLBVH-fast\nuses our\nCLBVH\nnodes with regular,\nuncompressed leaf data, while\nCLBVH-compact\nperforms the\nleaf data compression described in Section 4.5.\nWhen looking at only the\nQBVH8\nand\nCLBVH-fast\nvariants,\nthe relative memory savings of both are higher, since they\nare no longer as dominated by the leaf data cost.",
			  "For an additional roughly 5% of performance,\nCLBVH-compact\ncan save even more memory, reaching, on average, a nearly\n3  reduction in total memory "
			]
		  },
		  {
			"title": "Compressed-leaf bounding volume hierarchies | Proceedings of the Conference on High-Performance Graphics",
			"url": "https://dl.acm.org/doi/abs/10.1145/3231578.3231581",
			"excerpts": [
			  "We propose and evaluate what we call *Compressed-Leaf Bounding Volume Hierarchies* (CLBVH), which strike a balance between compressed and non-compressed BVH layouts.",
			  "Our CLBVH layout introduces dedicated compressed multi-leaf nodes where most effective at reducing memory use, and uses regular BVH nodes for inner nodes and small, isolated leaves.",
			  "We show that when implemented within the Embree ray tracing framework, this approach achieves roughly the same memory savings as Embree's compressed BVH layout, while maintaining almost the full performance of its fastest non-compressed BVH."
			]
		  },
		  {
			"title": "Carsten Benthin, Ingo Wald, Sven Woop, Attila fra",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2018/Short-Papers-Session2/HPG2018_CompressedLeafBoundingVolumeHierarchies.pdf",
			"excerpts": [
			  "Tweak top-down BVH builder to primarily generate all-leaf multi-nodes",
			  "Store referenced primitive data directly behind all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "CLBVH = regular BVH with compressed all-leaf multi-nodes",
			  "Leaf-Data 0",
			  "Leaf-Data 1",
			  "Leaf-Data 2",
			  "Leaf-Data 3",
			  "Leaf-Data 4",
			  "Leaf-Data 5",
			  "Leaf-Data 6",
			  "Leaf-Data 7",
			  "EVEN FURTHER COMPRESSION CLBVH\n(compact)",
			  "Extract shared features in geometry data in all-leaf multi-node",
			  "Object IDs, vertices, vertex indices, shader IDs, etc."
			]
		  },
		  {
			"title": "Efficient Incoherent Ray Traversal on GPUs Through ...",
			"url": "https://pdfs.semanticscholar.org/f65c/b99458aaf6c9f609e7950711188ea97a025e.pdf",
			"excerpts": [
			  "8-wide BVH constructed with SAH-optimal widening",
			  "Compressed node storage format",
			  "Quantization grid position and scale\nstored in parent node",
			  "Quantize child node AABBs to a local grid",
			  "Quantize child node AABBs to a local grid"
			]
		  },
		  {
			"title": "[PDF] Accelerating Graph Analytics on CPU-FPGA Heterogeneous Platform | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Accelerating-Graph-Analytics-on-CPU-FPGA-Platform-Zhou-Prasanna/5cf2f8dd9491b2cf0c42b0d2a6ccdae5c764f906",
			"excerpts": [
			  "Graphicionado: A high-performance and energy-efficient accelerator for graph analytics  Computer Science, Engineering. 2016 49th Annual IEEE/ACM International ..."
			]
		  },
		  {
			"title": "Two-way skewed-associative caches | by Arpit Gupta | Medium",
			"url": "https://medium.com/@arpitguptarag/two-way-skewed-associative-caches-9bab2c36dcee",
			"excerpts": [
			  "Dr Andre Seznec describes a new design skewed-associative caches which helps in improving miss rate while not compromising with the frequency at the same time.Read more"
			]
		  },
		  {
			"title": "US20160188476A1 - Hardware prefetcher for indirect access patterns \n      - Google Patents",
			"url": "https://patents.google.com/patent/US20160188476A1/en",
			"excerpts": [
			  "Two techniques address bottlenecking in processors. The first is indirect prefetching. The technique can be especially useful for graph analytics and sparse ..."
			]
		  },
		  {
			"title": "ShadowLoad: Injecting State into Hardware Prefetchers",
			"url": "https://misc0110.net/web/files/shadowload_asplos25.pdf",
			"excerpts": [
			  "ShadowLoad exploits the missing isolation of hardware prefetcher state between applications or privilege levels on. Intel and AMD CPUs up to ( ...Read more"
			]
		  },
		  {
			"title": "Looking Under the Hood of CPU Cache Prefetching",
			"url": "https://dbis.cs.tu-dortmund.de/storages/dbis-cs/r/papers/2024/sw-prefetching-survey/sw-prefetching.pdf",
			"excerpts": [
			  "Both Intel and AMD note that these non-temporally prefetched cache lines are prioritized for quicker eviction [4, 13]. 2.3 Limitations. Modern ...Read more",
			  "Interplay with Hardware Prefetchers",
			  " saw that software prefetching has its limits when it is nec-\nessary to prefetch large data blocks at once. To circumvent this\nlimitation and associated costs, a collaboration between software\nand hardware prefetching emerges as a promising solution, form-\ning a symbiotic relationship: Software prefetching takes the initial\nstep by early hinting about unpredictable data accesses; hardware\nprefetchers can take over, incrementally fetching the remaining\ndata line by line. In this section, we delve into their cooperative\ndynamics, emphasizing how strategic software prefetching *stimu-*\n*lates* hardware prefetchers beyond the typical dependence on load\nmiss",
			  "ardware Prefetcher Patterns.* In our in-depth analysis, we con-\ncentrate on Intel processors, which offer valuable insights into\nprefetching dynamics by implementing sampling of data addresses\nand access latency through the Linux *perf* interface [ 1 ]. Intel uti-\nlizes multiple core-local prefetchers that serve the L 1 and L 2 caches\nacross both modern and legacy processor generations [ 13 , 32 ]. The\nL1d prefetchers monitor load streams within a single cache line,\nsuccessively requesting the next line. Meanwhile, L 2 prefetchers\nstudy the patterns of cache line requests traversing through the\nL 2 cache. Here, the *stream prefetcher* initiates prefetches based on\na sequence of consecutive cache line requests, while the *adjacent*\n*prefetcher* consistently loads a pair of neighboring cache lines at\nonce. This architectural design prompts two critical lines of inquiry:\nDo software and hardware prefetchers have any interaction? If yes,\nwhat are the impacts of different prefetch instructions?",
			  "Discussion.* The measured results allow us to conclude that hardware\nand software prefetchers interact with each other, even on different\nhardware platforms. Leveraging this interaction becomes essential\nwhen large amounts of data should be transferred from memory to\nthe CPU cachegiven the limitations of LFB s highlighted in Sec-\ntion 3.2. Consider the *range scan* of tree structures as an example:\nAlthough accessed in a logical order, leaf nodes are scattered in\nmemory and accessed in a seemingly random fashion from the\nmemory subsystems point of view. While software prefetching can\nsignal to the hardware what leaf nodes will be accessed shortly, thus\ninitiating the transfer into the cache, the hardware prefetcher can\neffectively take over once it recognizes the sequential access pat-\ntern. This principle also extends to morsel-driven database engines,\nsuch as HyPer [ 17 ] and Umbra [ 29 , 36 ], which process fine-grained\npackages of tuples"
			]
		  },
		  {
			"title": "Graphicionado: A High-Performance and Energy-Efficient ...",
			"url": "https://taejunham.github.io/data/graphicionado_micro16.pdf",
			"excerpts": [
			  "asily perform next-line prefetches and get the data into the\naccelerator before they are needed. We extend the *sequential*\n*vertex read* and *edge read* modules (stage P1 and P3 in Fig. 6)\nto prefetch and buffer up to *N* cachelines ( *N* = 4 is used for\nour evaluation) and configure them to continue fetching the\nnext cacheline from memory as long as the buffer is not ful",
			  "**exploits not only data structure-centric datapath specialization,**\n**but also memory subsystem specialization, all the while taking ad-**\n**vantage of the parallelism inherent in this domain. G",
			  "icionado\ncan process graph analytics workloads with reasonable effi-\nciency. However, thus far it is a single pipeline with theoretical\nmaximum throughput limited to one edge per cycle in the\n*Processing* phase and one vertex per cycle in the *Apply*\nphase",
			  "e on-chip storage allowed dramatic\nreduction of the communication latency and bandwidth by con-\nverting the frequent and inefficient random off-chip data com-\nmunication to on-chip, efficient, finer-granularity scratchpad\nmemory accesse",
			  "This paper makes the following contributions:",
			  " A specialized graph analytics processing hardware pipeline\nthat employs datatype and memory subsystem special-\nizations while offering workload-specific reconfigurable\nblocks called Graphicionado",
			  "Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "The Graphicionado\npipeline is carefully designed to overcome inefficiencies in\nexisting general purpose processors by 1) utilizing an on-chip\nscratchpad memory efficiently, 2) balancing pipeline designs\nto achieve higher throughput, and 3) achieving high memory\nlevel parallelism with minimal cost and complexity.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "Graphicionado\nfeatures a pipeline that is inspired by the vertex programming\nparadigm coupled with a few reconfigurable blocks; this\nspecialized-while-flexible pipeline means that graph analytics\napplications (written as a vertex program) will execute well\non Graphicionado.",
			  "= 4 is used for\nour evaluation"
			]
		  },
		  {
			"title": "Graphicionado",
			"url": "https://mrmgroup.cs.princeton.edu/slides2/graphicionado_slide.pdf",
			"excerpts": [
			  "Graphicionado : A high-performance , energy-efficient graph analytics HW\naccelerator which overcomes the limitations of software frameworks while\nretaining the programmability benefit of SW frameworks",
			  "Graphicionado utilizes an on-chip storage to avoid wasting off-chip BW",
			  "Partition a graph before the execution; Then, process each subgraph\nat a time",
			  "Scaling On-Chip Memory Usage",
			  "Only half of the vertices to be stored in on-chip storage at a time"
			]
		  },
		  {
			"title": "adaptive prefetching on POWER7",
			"url": "https://people.ac.upc.edu/fcazorla/articles/vjimenez_pact_2012.pdf",
			"excerpts": [
			  "ardware data prefetch is a well-known technique to help\nalleviate the so-called *memory wall* problem [31]",
			  ", 7, 15] is programmable and al-\nlows the user to set different parameters (knobs) that control\nits behavior: i) *prefetch depth* , how many lines in advance\nto prefetch, ii) *prefetch on stores* , whether to prefetch store\noperations, and iii) *stride-N* , whether to prefetch streams\nwith a stride larger than one cache block.",
			  "The prefetcher is\ncontrolled via the data stream control register (DSCR).",
			  "The\nLinux kernel exposes the register to the user through the sys\nvirtual filesystem [22], allowing the user to set the prefetch\nsetting on a per-thread basis.",
			  "In this paper,\nwe present an adaptive prefetch scheme that dynamically\nadapts the prefetcher configuration to the running work-\nload, aiming to improve performan",
			  "e use the IBM\nPOWER7 [27] as the vehicle for this study, since: (i) this\nrepresents a state-of-the-art high-end processor, with a ma-\nture data prefetch engine that has evolved significantly since\nthe POWER3 time-frame; and (ii) this product provides\nfacilities for accurate measurement of performance metrics\nthrough a user-visible performance counter interface",
			  "POWER7 contains a programmable prefetch engine that\nis able to prefetch consecutive data blocks as well as those\nseparated by a non-unit stride [27].",
			  "The processor system is\nprovided to customers with a default prefetch setting that\nis targeted to improve performance for most applications."
			]
		  },
		  {
			"title": "Making Data Prefetch Smarter: Adaptive Prefetching on POWER7 | Conference Paper | PNNL",
			"url": "https://www.pnnl.gov/publications/making-data-prefetch-smarter-adaptive-prefetching-power7",
			"excerpts": [
			  "Hardware data prefetch engines are integral parts of many general purpose server-class microprocessors in the eld to- day. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default conguration during system bring-up and dynamic reconguration of the prefetch engine is not an autonomic feature of current machines.",
			  "In fact, they may actually de- grade performance due to useless bus bandwidth consump- tion and cache pollution.",
			  "In this paper, we present an adap- tive prefetch scheme that dynamically modies the prefetch settings in order to adapt to the workload requirements.",
			  "We implement and evaluate adaptive prefetching in the con- text of an existing, commercial processor, namely the IBM POWER7.",
			  "Our adaptive prefetch mechanism improves per- formance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively.",
			  "Categories",
			  "*Revised: July 23, 2014 | Published: September 19, 2012*"
			]
		  },
		  {
			"title": "Adaptive prefetching on power7: Improving performance and power consumption for ACM TOPC - IBM Research",
			"url": "https://research.ibm.com/publications/adaptive-prefetching-on-power7-improving-performance-and-power-consumption",
			"excerpts": [
			  "We present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to workloads' requirements.",
			  "Adaptive prefetching is also able to reduce power consumption in some cases.",
			  "showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively.",
			  "First we characterize\"in terms of performance and power consumption\"the prefetcher in that processor using microbenchmarks and SPEC CPU2006.We then present our adaptive prefetch mechanism showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Making data prefetch smarter: Adaptive prefetching on power 7 for PACT 2012 - IBM Research",
			"url": "https://research.ibm.com/publications/making-data-prefetch-smarter-adaptive-prefetching-on-power-7",
			"excerpts": [
			  "ware data prefetch engines are integral parts of many general purpose server-class microprocessors in the field today. Some prefetch engines allow the user to change some of their parameters. The prefetcher, however, is usually enabled in a default configuration during system bring-up and dynamic reconfiguration of the prefetch engine is not an autonomic feature of current machines. Conceptually, however, it is easy to infer that commonly used prefetch algorithms, when applied in a fixed mode will not help performance in many cases. In fact, they may actually degrade performance due to useless bus bandwidth consumption and cache pollution. In this paper, we present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to the workload requirements. We implement and evaluate adaptive prefetc",
			  "Our adaptive prefetch mechanism improves performance with respect to the default prefetch setting up to 2.7X and 30% for single-threaded and multiprogrammed workloads, respectively."
			]
		  },
		  {
			"title": "Skewed-Associative Caches: CS752 Final Project",
			"url": "https://pages.cs.wisc.edu/~chalpin/about/resume/cs752.pdf",
			"excerpts": [
			  "Skewed-associative caches were proposed as a way to decrease the miss rate, while\nnot further increasing the size or associativity.",
			  "In a single level cache system, skewing\nfunctions are useful to help decrease the miss rate, and cause fewer requests to main\nmemory.",
			  "They could be used in the L1 cache, but since they add a small amount of logic\nto the critical path of cache access, they do not meet the criteria of fast L1 cache access\nstated above.",
			  "We will explore the claim by Seznec that a two-way skewed-associative cache\nwill perform as well as a four-way set associative cache.",
			  "The basic requirement is that *A* 1 ** *A* 2 results in values\nappropriate to address the number of blocks in a bank. ",
			  "The shuffle function is simply a reordering of the wires."
			]
		  },
		  {
			"title": "Cache architecture research  ALF",
			"url": "https://team.inria.fr/alf/members/andre-seznec/cache-architecture-research/",
			"excerpts": [
			  "The skewed associative cache is a new organization for multi-bank caches.",
			  "Skewed-associative caches have been shown to have two major advantages over conventional set-associative caches.",
			  "First, at equal associativity degrees, a skewed-associative cache typically exhibits the same hardware complexity as a set-associative cache, but exhibits lower miss ratio.",
			  "This is particularly significant for BTBs and L2 caches for which a significant ratio of conflict misses occurs even on 2-way set-associative caches.",
			  "Second, the behavior of skewed-associative caches is quite insensitive to the precise data placement in memory.",
			  "We also  showed that the skewed associative structure offers a unique opportunity to build TLBs supporting multiple page sizes."
			]
		  },
		  {
			"title": "Minimally-skewed-associative caches",
			"url": "https://ieeexplore.ieee.org/document/1180765/",
			"excerpts": [
			  "Skewed-associativity is a technique that reduces the miss ratios of CPU caches by applying different indexing functions to each way of an associative cache.",
			  "Although several of the new architectures were welcomed by the industry, such as victim caches which are employed in AMD processors, skewed-associativity was not, presumably because implementation of the original scheme is complex and, most probably, involves access-time penal-ties among other costs",
			  ". The purpose of this work is to evaluate a simplified, easy to implement version that we call minimally-skewed-associativity (MSkA).",
			  "Miss ratios are not as good as those for full skewing, but they are still advantageous.",
			  "Minimal-skewing is thus proposed as a way to improve the hit/miss performance of caches, often without producing access-time delays or increases in power consumption as other techniques do (for example, using higher associativities).",
			  "**Published in:** [14th Symposium on Computer Architecture and High Performance Computing, 2002. Proceedings.](/xpl/conhome/8407/proceeding)",
			  "**Date of Conference:** 28-30 October 2002"
			]
		  },
		  {
			"title": "GraphP: Reducing Communication for PIM-based Graph ...",
			"url": "http://alchem.usc.edu/portal/static/download/graphp.pdf",
			"excerpts": [
			  "Martonosi,. Graphicionado: A high-performance and energy-efficient ac- celerator for graph analytics, in Microarchitecture (MICRO),. 2016 49th Annual IEEE ...Read more"
			]
		  },
		  {
			"title": "Seminar on Computer Architecture",
			"url": "https://pdfs.semanticscholar.org/efc1/28130e3b2170ca9102abbeb62f7984d1a81a.pdf",
			"excerpts": [
			  "DDR3-OoO HMC-OoO HMC-MC Tesseract Tesseract. LP. Tesseract. LP + MTP. +56%. +25%. 9.0x. 11.6x. 13.8x. Average Performance. Page 18. Evaluation Results. 18.Read more"
			]
		  },
		  {
			"title": "Programming Strategies for Irregular Algorithms on the ...",
			"url": "https://inria.hal.science/hal-02991204/document",
			"excerpts": [
			  "Our more recent tests have shown that the Emu hardware can achieve up to 1.6 GB/s per node and 12.8 GB/s on 8 nodes for the STREAM benchmark, ...Read more"
			]
		  },
		  {
			"title": "Improving Streaming Graph Processing Performance using ...",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3466752.3480096",
			"excerpts": [
			  "The protocol FSM takes appropriate actions on the MSHR status change: the task is forwarded to a FIFO buffer to the cache controller  and the ...Read more"
			]
		  },
		  {
			"title": "Energy characterization of graph workloads - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S221053792030189X",
			"excerpts": [
			  "For these reasons, there are several ongoing research efforts exploring custom architectures to enhance graph processing. Some solutions include custom processing elements that decouple computation from communication (e.g., Graphicionado [1], Lincoln Lab graph processor [2]), while other designs explore near memory processing (e.g., Tesseract [3], GraphPIM [4], GraphP [5]).",
			  "Some systems, like the Cray XMT [6], [7], exploit multithreading to tolerate, rather than reduce, latencies even at a large scale.",
			  "The EMU system [8] exploits the concept of migrating threads near to the data.",
			  "The DARPA Hierarchical Identify Verify and Exploit (HIVE) program [9] is looking to build a graph analytics processor that can process (streaming) graphs faster and at much lower power than current processing technology.",
			  "A particular focus of HIVE is to optimize both performance and power (i.e., the efficiency), trying to reach 1000 times the TEPS/W (traversed edges per second per Watt) of current designs (such as GPUs and conventional CPUs).",
			  "Ideally, a processor 1000faster in the same power envelope of a current design, or a processor as fast as a current one, but consuming 1/1000th of the power, would both reach the project objective."
			]
		  },
		  {
			"title": "A Scalable Processing-in-Memory Accelerator for Parallel ...",
			"url": "https://users.ece.cmu.edu/~omutlu/pub/tesseract-pim-architecture-for-graph-processing_isca15.pdf",
			"excerpts": [
			  "ign a programmable PIM accelerator for large-scale*\n*graph processing called Tesseract. Tesseract is composed of*\n*(1) a new hardware architecture that fully utilizes the available*\n*memory bandwidth, (2) an efficient method of communication*\n*between different memory partitions, and (3) a programming*\n*interface that reflects and exploits the unique hardware de-*\n*sign. It also includes two hardware prefetchers specialized for*\n*memory access patterns of graph processing, which operate*\n*based on the hints provided by our programming model. Our*\n*comprehensive evaluations using five state-of-the-art graph*\n*processing workloads with large real-world graphs show that*\n*the proposed architecture improves average system perfor-*\n*mance by a factor of ten and achieves 87% average energy*\n*reduction over conventional s",
			  "* We provide case studies of how five graph processing work-\nloads can be mapped to our architecture and how they\ncan benefit from it. Our evaluations show that Tesseract\nachieves 10x average performance improvement and 87%\naverage reduction in energy consumption over a conven-\n ... ",
			  "\nOur prefetching mechanisms, when employed together, en-\nable Tesseract to achieve a 14x average performance improve-\nment over the DDR3-based conventional system, while min-\nimizing the storage overhead to less than 5 KB per core (see\nSection 4.1). Me",
			  "The reason why conventional systems fall behind Tesseract\nis that they are limited by the low off-chip link bandwidth\n( 102.4 GB/s in DDR3-OoO or 640 GB/s in HMC-OoO/-MC)\nwhereas our system utilizes the large internal memory band-\nwidth of HMCs ( 8 TB/s ). 10 "
			]
		  },
		  {
			"title": "Retrospective: A Scalable Processing-in-Memory Accelerator ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/Tesseract_50YearsOfISCA-Retrospective_isca23.pdf",
			"excerpts": [
			  "15 paper [1] provides a new pro-**\n**grammable processing-in-memory (PIM) architecture and system**\n**design that can accelerate key data-intensive applications, with**\n**a focus on graph processing wor",
			  "our accelerator system, Tesseract, using 3D-stacked memories**\n**with logic layers, where each logic layer contains general-purpose**\n**processing cores and cores communicate with each other using a**\n**message-passing programming mod",
			  " the first to completely design**\n**a near-memory accelerator system from scratch such that it is**\n**both generally programmable and specifically customizable to**\n**accelerate important applications, with a case study on major**\n**graph processing workloads. Ensuing work in academia and**\n**industry showed that similar approaches to system design can**\n**greatly benefit both graph processing workloads and other**\n**applications, such as machine learning, for which ideas from**\n**Tesseract seem to have been influential"
			]
		  },
		  {
			"title": "[2306.15577] Retrospective: A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing",
			"url": "https://arxiv.org/abs/2306.15577",
			"excerpts": [
			  ". We built our accelerator system, Tesseract, using 3D-stacked memories with logic layers, where each logic layer contains general-purpose processing cores and cores communicate with each other using a message-passing programming model.",
			  " Our ISCA 2015 paper provides a new programmable processing-in-memory (PIM) architecture and system design that can accelerate key data-intensive applications, with a focus on graph processing workloads. ",
			  "r major idea was to completely rethink the system, including the programming model, data partitioning mechanisms, system support, instruction set architecture, along with near-memory execution units and their communication architecture, su"
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://dl.acm.org/doi/10.1145/2749469.2750386",
			"excerpts": [
			  "The key modern enabler for PIM is the recent advancement of the 3D integration technology that facilitates stacking logic and memory dies in a single package, which was not available when the PIM concept was originally examined.",
			  "Tesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design.",
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems.",
			  "In this work, we argue that the conventional concept of processing-in-memory (PIM) can be a viable solution to achieve such an objective.",
			  "In order to take advantage of such a new technology to enable memory-capacity-proportional performance, we design a programmable PIM accelerator for large-scale graph processing called Tesseract.",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model."
			]
		  },
		  {
			"title": "A scalable processing-in-memory accelerator for parallel ...",
			"url": "https://ieeexplore.ieee.org/document/7284059/",
			"excerpts": [
			  "Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87% average energy reduction over conventional systems.",
			  "It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model."
			]
		  },
		  {
			"title": "An Initial Characterization of the Emu Chick",
			"url": "https://fruitfly1026.github.io/static/files/ipdpsw18-hein.pdf",
			"excerpts": [
			  "The Emu Chick prototype is still in active development. The\ncurrent hardware iteration uses an Arria 10 FPGA on each\nnode card to implement the Gossamer cores, the migration\nengine, and the stationary cores.",
			  ".\nThe Emu is a cache-less system built around nodelets that\neach execute lightweight threads and migrate threads to data\nrather than moving data through a traditional cache hierarchy.\nThi",
			  "e current prototype hardware uses**\n**FPGAs to implement cache-less Gossamer cores for doing**\n**computational work and a stationary core to run basic operating**\n**system functions and migrate threads betw",
			  "Pointer chasing on the Xeon architecture performs poorly\nfor several reasons. For small block sizes, the memory system\nbandwidth is used inefficiently. An entire 64-byte cache line\nmust be transferred from memory, but only 16 bytes will\nbe used. The best performance is achieved with a block size\nbetween 256 and 4096 elements.",
			  "Performance on Emu remains mostly flat regardless of block\nsize. Emus memory access granularity is 8 bytes, so it never\ntransfers unused data in this benchmark. As long as a block\nfits within a single nodelets local memory channel, there is no\npenalty for random access within the block.",
			  "The Emu Chick is a prototype system designed**\n**around the concept of migratory memory-side processing. Rather**\n**than transferring large amounts of data across power-hungry,**\n**high-latency interconnects, the Emu Chick moves lightweight**\n**thread contexts to near-memory cores before the beginning**\n*"
			]
		  },
		  {
			"title": "abseil / Swiss Tables and <code>absl::Hash</code>",
			"url": "https://abseil.io/blog/20180927-swisstables",
			"excerpts": [
			  "Swiss Tables boast improvements to efficiency and provide C++11 codebases early\naccess to APIs from C++17 and C++20.",
			  "\nThese hash tables live within the [Abseil `container` library](",
			  "We are extremely pleased to announce the availability of the new Swiss Table family of hashtables in Abseil and the `absl::Hash` hashing framework that allows easy extensibility for user defined types.",
			  "Last year at CppCon, We presented a [talk](https://www.youtube.com/watch?v=ncHmEUmJZf4&t=3s) on a new hashtable that\nwe were rolling out across Googles codebase. When asked about its release date, we may have been a touch optimistic. But hopefully it will have been worth the wait.",
			  "hese hash tables live within the [Abseil `container` library](https://github.com/abseil/abseil-cpp/tree/master/absl/container) "
			]
		  },
		  {
			"title": "SwissTables: High Performance HashMaps - by Pratik Pandey",
			"url": "https://pratikpandey.substack.com/p/swisstables-high-performance-hashmaps",
			"excerpts": [
			  "he array is broken into logical *groups* of **8 slots** each.",
			  "Alongside the array of key-value slots, Swiss Tables maintain a compact array of \"control bytes\" called Control Word, which is of 64 bits(8 bytes). Each control byte stores metadata about the a corresponding slot in the group, typically:\n**Empty** : The slot is free.\n**Deleted (Tombstone)** : The slot previously held an entry that has been removed. This is important so probing sequences aren't prematurely terminated as discussed in Open Addressing.\n**Full** : The slot contains an active entry. In this case, the control byte also stores the **H2 hash** , which is the lower 7 bits of the full hash of the key stored in that slot.",
			  "The magic of SwissTables** is in the implementation detail of how it determines if the group contains the key or not. Instead of iterating over all the slots in the group, **SwissTables** leverage the Control Word!",
			  "SwissTables** do a byte-by-byte equality comparison within the control word, where we compare each byte in the control word with the H2 hash we compute",
			  "However, instead of doing a byte-by-byte comparison by using multiple instructions, SwissTable implementations use **SIMD (Single Instruction, Multiple Data)** instructions.",
			  "This operation is very powerful, as we have effectively performed 8 steps of a probe sequence at once, in parallel with the help of Control Word and SIMD.",
			  "**Fast Lookups** : When searching for a key, the map first computes the hash and identifies a starting group of slots. It then quickly scans the *control bytes* for that group. The Control Word metadata is designed to be scanned very efficiently using **SIMD** instructions, making lookups extremely fast.",
			  "Swiss Tables, popularised by Google's Abseil C++ library, takes a different approach, primarily using **open addressing** (instead of chaining) with a clever variation of **linear probing** leveraging a dedicated metadata array."
			]
		  },
		  {
			"title": "folly/folly/container/F14.md at main  facebook/folly  GitHub",
			"url": "https://github.com/facebook/folly/blob/main/folly/container/F14.md",
			"excerpts": [
			  "F14 is a 14-way probing hash table that resolves collisions by double\nhashing. Up to 14 keys are stored in a chunk at a single hash table\nposition. Vector instructions (SSE2 on x86_64, NEON on aarch64)\nare used to filter within a chunk; intra-chunk search takes only a\nhandful of instructions. **F14** refers to the fact that the algorithm **F** ilters up to **14** keys at a time. This strategy allows the hash\ntable to be operated at a high maximum load factor (12/14) while still\nkeeping probe chains very short.",
			  "The vector search is coded using SIMD intrinsics, SSE2 on x86_64 and\nNEON on aarch64. These instructions are a non-optional part of those\nplatforms (unlike later SIMD instruction sets like AVX2 or SVE), so no\nspecial compilation flags are required. The exact vector operations\nperformed differs between x86_64 and aarch64 because aarch64 lacks a\nmovemask instruction, but the F14 algorithm is the same.",
			  "F14 computes a secondary hash value for each key, which we call the key's\ntag. Tags are 1 byte: 7 bits of entropy with the top bit set. The 14\ntags are joined with 2 additional bytes of metadata to form a 16-byte\naligned __m128i at the beginning of the chunk. When we're looking for a\nkey we can compare the needle's tag to all 14 tags in a chunk in parallel."
			]
		  },
		  {
			"title": "Open-sourcing F14 for memory-efficient hash tables - Engineering at Meta",
			"url": "https://engineering.fb.com/2019/04/25/developer-tools/f14/",
			"excerpts": [
			  "The core idea of F14 is to use the hash code to map keys to a chunk (a block of slots) instead of to a single slot, then search within the chunk in parallel. The intra-chunk search uses vector instructions (SSE2 or NEON) to filter all the slots of the chunk at the same time. We call our algorithm F14 because it filters 14 slots at once (this chunk size is a good trade-off between cache alignment and collision rate).",
			  "F14 performs collision resolution if a chunk overflows or if two keys both pass the filtering step. The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "The lower bits of the full hash code determine the chunk. The upper bits are used to filter which slots in a chunk might hold the search key.",
			  "Chunking is an effective strategy because the chance that 15 of the tables keys will map to a chunk with 14 slots is much lower than the chance that two keys will map to one slot. For instance, imagine you are in a room with 180 people. The chance that one other person has the same birthday as you is about 50 percent, but the chance that there are 14 people who were born in the same fortnight as you is much lower than 1 percent.",
			  "Below is a plot of the likelihood that an algorithm wont find a search key in the very first place it looks. The happiest place on the graph is the bottom right, where the high load factor saves memory and the lack of collisions means that keys are found quickly with predictable control flow. Youll notice that the plot includes lines for both F14 ideal and F14 with 7-bit tag. The former includes only chunk overflow, while the latter reflects the actual algorithm. Theres a 1/128 chance that two keys have the same 7-bit tag even with a high-quality hash function.",
			  "The two-step search is a bit more work than in a normal hash table algorithm when neither has a collision, but F14 is faster overall because theres a much lower probability that a collision will interfere with instruction pipelining.",
			  "Collisions are the bane of a hash table: Resolving them creates unpredictable control flow and requires extra memory accesses. Modern processors are fast largely because of pipelining  each core has many execution units that allow the actual work of instructions to overlap."
			]
		  },
		  {
			"title": "Database Processing-in-Memory: An Experimental Study",
			"url": "https://pages.cs.wisc.edu/~yxy/cs839-s20/papers/p334-kepe.pdf",
			"excerpts": [
			  "ns. The hash join and\naggregation require the *gather* and *scatter* SIMD memory\ninstructions to load and store multiple entries of hash tables.",
			  "n par-\nticular, we present a new SIMD sorting algorithm that re-\nquires fewer memory instructions compared to the state of\nthe art [21]. For each operator, we gauge the latency and en-\nergy spend to process TPC-H and Zipf distribution dataset",
			  "Finally, the sorting operation and sort-merge join require the\n*min/max* and *shuffle* SIMD instructions.",
			  "**SIMD units** Unified func. units (integer + floating-point) @1 GHz;",
			  "**4.**",
			  "**IMPLEMENTATION DETAILS OF THE**",
			  "**SIMD QUERY OPERATORS**",
			  "In a nutshell, the implementations of the selection and\nprojection operators require SIMD *load* and *store* memory\ninstructions."
			]
		  },
		  {
			"title": "Efficient SIMD and MIMD parallelization of hash-based aggregation by conflict mitigation | Proceedings of the International Conference on Supercomputing",
			"url": "https://dl.acm.org/doi/10.1145/3079079.3079080",
			"excerpts": [
			  "To address this problem, we design a variant of basic bucket hashing and a *bucketized* aggregation procedure that can utilize both SIMD and MIMD parallelism efficiently. Our approach first adds distinct offsets to input rows on different SIMD lanes, which reduces the possibility of different lanes accessing identical slot in the hash table.",
			  "For parallelization across cores, we adopt separate hash tables and optimize with parallel reduction and a hybrid approach.",
			  "Section Title: Efficient SIMD and MIMD parallelization of hash-based aggregation by conflict mitigation > Recommendations",
			  "### SIMD Vectorized Hashing for Grouped Aggregation",
			  "Advances in Databases and Information SystemsAbstractGrouped aggregation is a commonly used analytical function. The common implementation\nof the function using hashing techniques suffers lower throughput rate due to the\ncollision of the insert keys in the hashing techniques. During collision, the ..."
			]
		  },
		  {
			"title": "40x faster hash joiner with vectorized execution",
			"url": "https://www.cockroachlabs.com/blog/vectorized-hash-joiner/",
			"excerpts": [
			  "In most SQL engines, including CockroachDB's current engine, data is processed a row at a time: each component of the plan (e.g. a join or a distinct) asks its input for the next row, does a little bit of work, and prepares a new row for output. This model is called the \"Volcano\" model, based off of a paper by Goetz Graefe.\nB",
			  "By contrast, in the vectorized execution model, each component of the plan processes an entire batch of columnar data at once, instead of just a single row.",
			  "This idea is written about in great detail in the excellent paper [MonetDB/X100: Hyper-Pipelining Query Execution](http://cidrdb.org/cidr2005/papers/P19.pdf) , and it's what we've chosen to use in our new execution engine.",
			  "\nAlso, as you might have guessed from the word \"vectorized\", organizing data in this batched, columnar fashion is the primary prerequisite for using SIMD CPU instructions, which operate on a vector of data at a time. ",
			  "Now that we have a taste of what vectorized execution and hash joiners are, let's take it one step further and combine the two concepts. The challenge is to break down the hash join algorithm into a series of simple loops over a single column, with as few run-time decisions, if statements and jumps as possible. Marcin Zukowski described one such algorithm to implement a many-to-one inner hash join in his paper \" [**Balancing Vectorized Query Execution with Bandwidth-Optimized Storage**](https://dare.uva.nl/search?identifier=5ccbb60a-38b8-4eeb-858a-e7735dd37487) \". This paper laid invaluable groundwork for our vectorized hash join operator."
			]
		  },
		  {
			"title": "Analyzing Vectorized Hash Tables Across CPU Architectures",
			"url": "https://www.vldb.org/pvldb/vol16/p2755-bother.pdf",
			"excerpts": [
			  "**Fingerprints:** For VFP, using 8-bit fingerprints always per-\nforms best.",
			  ".\n**Hash Tables.** Richter et al . [ 66 ] conduct an analysis of scalar\nhashing schemes and derive a decision guide that focuses on the\nworkload at hand. They implement a variant of VLP on AVX2. Poly-\nchroniou et al . [ 65 ] differentiate horizontal and vertical vectoriza-\ntion. Vertical vectorization looks up multiple keys in parallel, which\nrequires scatter/gather operations and bulk inserts/lookups. Hori-\nzontal vectorization serves as a drop-in replacement for scalar hash\ntables. Pietrzyk et al . [ 63 ] implement a conflict detection-aware ver-\nsion of vertical VLP. Behrens et al . [ 13 ] use OpenCL to implement\nvertical VFP. Metas F14 [ 20 , 21 ] and Googles Abseil containers [ 28 ]\nare industry implementations of BBC using SSE/NEON.",
			  "**Implementation Details:** When extracting matches from a\nmovemask, it is beneficial to check whether there has been any\nmatch (TEST). For iterating over multiple matches on ARM and\nPower, which do not natively support movemasks, simulating a\nmovemask instead of working with a native vectorized iterator\nperforms better."
			]
		  },
		  {
			"title": "Faster Go maps with Swiss Tables",
			"url": "https://go.dev/blog/swisstable",
			"excerpts": [
			  "This improvement to probing behavior allowed both the Abseil and Go implementations to increase the maximum load factor of Swiss Table maps ...Read more",
			  "Compute hash(key) and break the hash into two parts: the upper 57-bits (called h1 ) and the lower 7 bits (called h2 ).  The upper bits ( h1 ) ...Read more"
			]
		  },
		  {
			"title": "Deep Dive Into GO 1.24 Swiss Table-Part1 | by Rajesh Samala | Medium",
			"url": "https://medium.com/@samal.rajesh/deep-dive-into-go-1-24-swiss-table-part1-0d96e49630ee",
			"excerpts": [
			  "One of its key optimizations is the use of SIMD hardware to perform parallel operations on multiple control bytes simultaneously.",
			  "Slots where `h2` does match are potential matches, but we must still check the entire key, as there is potential for collisions (1/128 probability of collision with a 7-bit hash, so still quite low).",
			  "It is high performance hash-table, it replace earlier hashmap which is based on open addressing with linear probing. It enhanced map type with open addressing, quadratic probing, SIMD optimizations and cache aligned memory layouts.",
			  "Swiss table achieves the faster lookups, lower memory usage and better scalability.",
			  "Each group contains 8-slots and an 8-byte control words(64bit, 1byte per slots)",
			  "The control word encodes:",
			  "State: Empty(0xff), occupied or deleted",
			  "H2: A 7 bit fragment of 64-bit hash, it used for quick slot filtering.",
			  "from abseil.io",
			  "For example, a 64-bit control word(8-bytes, each byte represent a slot) comparison can be completed in a single SIMD operation, allowing the table to process multiple slots in parallel, significantly improving lookup and insertion performance."
			]
		  },
		  {
			"title": "SwissTable: A High-Performance Hash Table Implementation - DEV Community",
			"url": "https://dev.to/huizhou92/swisstable-a-high-performance-hash-table-implementation-1knc",
			"excerpts": [
			  "SwissTable uses a new metadata control mechanism to significantly reduce unnecessary `key` comparisons and leverages SIMD instructions to boost throughput.",
			  "In `swisstable` , `ctrl` is an array of `metadata` , corresponding to the `group[K, V]` array. Each `group` has 8 `slots` .",
			  "The hash is divided into `57 bits` for H1 to determine the starting `groups` , and the remaining `7 bits` called H2, stored in `metadata` as the hash signature of the current key for subsequent search and filtering.",
			  "The key advantage of `swisstable` over traditional hash tables lies in the metadata called `ctrl` . Control information includes:",
			  "Whether a slot is empty: `0b10000000`",
			  "Whether a slot has been deleted: `0b11111110`",
			  "The key's hash signature (H2) in a slot: `0bh2`",
			  "Multiplying `h2` by `0x0101010101010101` to get a uint64, allowing simultaneous comparison with 8 `ctrl` values.",
			  "The process of adding data in `swisstable` involves several steps:",
			  "Calculate the hash value and split it into `h1` and `h2` . Using `h1` , determine the starting groups.",
			  "Use `metaMatchH2` to check the current group's `metadata` for a matching `h2` . If found, further check for the matching key and update the value if they match.",
			  "If no matching key is found, use `metaMatchEmpty` to check for empty `slots` in the current group. Insert the new key-value pair if an empty slot is found and update the `metadata` and `resident` count.",
			  "If no empty slots are available in the current group, perform linear probing to check the next `groups` ."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases",
			"url": "https://15721.courses.cs.cmu.edu/spring2016/papers/p1493-polychroniou.pdf",
			"excerpts": [
			  "Our\nvectorization principle is to process a different key per SIMD\nlane using gathers to access the hash table.",
			  "Hash tables are used in database systems to execute joins\nand aggregations since they allow constant time key lookups.",
			  "The vectorized implementation of probing a hash table\nusing a linear probing scheme is shown in Algorithm 5."
			]
		  },
		  {
			"title": "Rethinking SIMD Vectorization for In-Memory Databases | Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
			"url": "https://dl.acm.org/doi/10.1145/2723372.2747645",
			"excerpts": [
			  " Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[D",
			  " Section Title: Rethinking SIMD Vectorization for In-Memory Databases > References",
			  "Content:\n[Digital Library](/doi/10.14778/2002938.2002940)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.14778%2F2002938.2002940)\n[23]\nR. Pagh et al. Cuckoo hashing. J. Algorithms, 51(2):122--144, May 2004.\n[Digital Library](/doi/10.1016/j.jalgor.2003.12.002)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016%2Fj.jalgor.2003.12.002)\n[24]\nH. Pirk et al. Accelerating foreign-key joins using asymmetric memory channels. In ADMS, 2011.\n[Google Scholar](https://scholar.google.com/scholar?q=H.+Pirk+et+al.+Accelerating+foreign-key+joins+using+asymmetric+memory+channels.+In+ADMS%2C+2011.)\n[25]\nO. Polychroniou et al. High throughput heavy hitter aggregation for modern SIMD processors. In DaMoN, 2013.\n[Digital Library](/doi/10.1145/2485278.2485284)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2485278.2485284)\n[26]\nO. Polychroniou et al. A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort. In SIGMOD, pages 755--766, 2014.\n[Digital Library](/doi/10.1145/2588555.2610522)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2588555.2610522)\n[27]\nO. Polychroniou et al. Vectorized Bloom filters for advanced SIMD processors. In DaMoN, 2014.\n[Digital Library](/doi/10.1145/2619228.2619234)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F2619228.2619234)\n[28]"
			]
		  },
		  {
			"title": "Library-based Prefetching for Pointer-intensive Applications",
			"url": "http://csl.stanford.edu/~christos/publications/2006.library_prefetch.manuscript.pdf",
			"excerpts": [
			  "In general, compiler-based prefetching has to handle the difficulty of data- structure layout analysis and pointer disambiguation."
			]
		  },
		  {
			"title": "A prefetching indexing scheme for in-memory database ...",
			"url": "https://www.sciencedirect.com/science/article/pii/S0167739X24000840",
			"excerpts": [
			  "There are two typical cache prefetching strategies: reducing the number of cache misses and reducing the impact of cache misses. Generally, a database workload ..."
			]
		  },
		  {
			"title": "Optimal Prefetching in Random Trees",
			"url": "https://inria.hal.science/hal-03361953v2/document",
			"excerpts": [
			  "Prefetching is a basic technique underlying many computer science applications. Its main purpose is to reduce the time needed to access some ..."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms and Data Structures",
			"url": "https://www.cs.cornell.edu/courses/cs612/2005sp/lectures/hitesh.pdf",
			"excerpts": [
			  "Different kinds of layouts. In-order. Breadth-first. Depth-first van Emde Boas van Emde Boas Layout : Main Idea. Store recursive sub-trees in contiguous memory.Read more"
			]
		  },
		  {
			"title": "A speculation-friendly binary search tree",
			"url": "https://onlinelibrary.wiley.com/doi/am-pdf/10.1002/cpe.4883",
			"excerpts": [
			  "We introduce a speculation-friendly tree (s-tree for short) as a tree that transiently breaks its balance structural invariant without hampering the abstraction ...Read more"
			]
		  },
		  {
			"title": "Fractal Prefetching B+-Trees: Optimizing Both Cache and Disk ...",
			"url": "https://www.pdl.cmu.edu/PDL-FTP/Database/CMU-CS-02-115.pdf",
			"excerpts": [
			  "This paper, however, is the first to propose a B+-Tree index structure that effectively optimizes both CPU cache and disk performance on modern processors, for ...Read more"
			]
		  },
		  {
			"title": "Improving index performance through prefetching | ACM SIGMOD Record",
			"url": "https://dl.acm.org/doi/abs/10.1145/376284.375688",
			"excerpts": [
			  "This paper proposes and evaluate *Prefetching B* + - *Trees* (pB + -Trees), which use prefetching to accelerate two important operations on B + -Tree indices: searches and range scans.",
			  "To accelerate searches, pB + -Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.",
			  "These wider nodes reduce the height of the B + -Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node.",
			  "Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B + -Trees.",
			  "In addition, it outperforms and is complementary to Cache-Sensitive B + -Trees.",
			  "To accelerate range scans, pB + -Trees provide arrays of pointers to their leaf nodes.",
			  "These allow the pB + -Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.",
			  "ur results show that this technique yields over a *sixfold* speedup on range scans of 1000+ keys",
			  "Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency."
			]
		  },
		  {
			"title": "Compiler-based prefetching for recursive data structures | Proceedings of the seventh international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/237090.237190",
			"excerpts": [
			  "Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors.",
			  "This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures.",
			  "Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme ( *greedy prefetching* ) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000.",
			  "Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45% for the applications we study."
			]
		  },
		  {
			"title": "Lecture 22 Prefetching Recursive Data Structures",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s16/www/lectures/L22-Prefetching-Pointer-Structures.pdf",
			"excerpts": [
			  "Summary of Prefetching Algorithms\n\nGreedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "**Carnegie Mellon**\n**Lecture 22**\n**Prefetching Recursive Data Structures**\nMaterial from: C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data\nStructures. In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "Eliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45%",
			  "Add new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n\nTrade space & time for better control over prefetching distances",
			  " History-Pointer Prefetching",
			  "\nCreation order equals major traversal order in treeadd & perimeter",
			  " and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in tr",
			  " Data-Linearization Prefetching",
			  "No pointer dereferences are required",
			  "Map nodes close in the traversal to contiguous memory",
			  "hree schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetch",
			  " improves performance significantly for half of Olden",
			  "Greedy prefetching is the most widely applicable algorithm\n fully implemented in SUIF",
			  "call: Example Code with Prefetching Arrays\n15-745: Prefetching Pointer Structures\n3\n**for (i = 0; i < 3; i++)**\n**for (j = 0; j < 100; j++)**\n**A[i][j] = B[j][0] + B[j+1][0];**\nOriginal Code\n**prefetch(&B[0][0]);**",
			  "Applicable because a list structure does not change over time",
			  "Improved accuracy outweighs increased overhead in this case",
			  "**H** = history-pointer prefetching",
			  "Health",
			  "Health",
			  "Performance of Data-Linearization Prefetching",
			  " hence data linearization is done without data restructuring",
			  "9% and 18% speedups over greedy prefetching through:",
			  " 94%->78% in perimeter, 87%->81% in treeadd\n",
			  " while maintaining good coverage factors:",
			  " 100%->80% in perimeter, 100%->93% in treeadd",
			  "**G** = greedy prefetching",
			  "**G** = greedy prefetching",
			  "**D** = data-linearization prefetching",
			  "15-745: Prefetching Pointer Structures",
			  "15-745: Prefetching Pointer Structures",
			  "load stall",
			  "load stall",
			  "load stall",
			  "store stall",
			  "store stall",
			  "store stall",
			  "inst. stall",
			  "inst. stall",
			  "inst. stall",
			  "busy",
			  "busy",
			  "busy",
			  "Three schemes to overcome the pointer-chasing problem:",
			  "Carnegie Mellon",
			  "\n**preorder(treeNode * t){**\n**if (t != NULL){**\n**pf(t->left);**\n**pf(t->right);**\n**process(t->data);**\n**preorder(t->left);**\n**preorder(t->right);**\n**}**\n*",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "**O** = original",
			  "Conclusions",
			  "Automated greedy prefetching in SUIF",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**"
			]
		  },
		  {
			"title": "Automatic compiler-inserted prefetching for pointer-based ...",
			"url": "https://ieeexplore.ieee.org/document/752654/",
			"excerpts": [
			  "As the disparity between processor and memory speeds continues to grow, memory latency is becoming an increasingly important performance bottleneck.",
			  "While software-controlled prefetching is an attractive technique for tolerating this latency, its success has been limited thus far to array-based numeric codes.",
			  "In this paper, we expand the scope of automatic compiler-inserted prefetching to also include the recursive data structures commonly found in pointer-based applications.",
			  "We propose three compiler-based prefetching schemes, and automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler.",
			  "Our experimental results demonstrate that compiler-inserted prefetching can offer significant performance gains on both uniprocessors and large-scale shared-memory multiprocessors."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  " We claim that we can achieve $O(\\log_B N)$ page accesses, but without having to know $B$ ahead of time.",
			  "The data structure were using is a good old balanced Binary Search Tree.",
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page.",
			  "One application of the recursive van Embde Boas layout",
			  "Suppose the page size is $B$. Every time step, the height of atoms halves.",
			  "Were interested in the height of the atoms at the first time step where an entire atom can fit in a page.",
			  "Since the number of vertices in a complete binary tree grows exponentially with height, that happens when atoms have height $\\Theta(\\log B)$. T",
			  ". Then, analysing the layout at this resolution, we can fit any atom (which all have the same height of $\\Theta(\\log B)$) into the cache with 1 page load*.",
			  "Then, now consider what happens in a search. A search basically consists of a path from the root of the BST to some leaf*. This path will spend some time in the first atom, until it reaches the leaf of the atom and goes into the next atom, and so forth.",
			  ". Since the path always start at the root vertex of an atom and ends on a leaf, it will spend $\\Theta(\\log B)$ steps in that atom.",
			  "Since the overall search path is $\\log N$ steps long, well need $O(\\frac{\\log N}{\\log B}) = O(\\log_B N)$ atoms, and thats the number of page accesses we need."
			]
		  },
		  {
			"title": "Lab note #044 Sailing by cache-oblivious data structures",
			"url": "https://interjectedfuture.com/lab-notes-044-sailing-by-cache-oblivious-data-structures/",
			"excerpts": [
			  "Cache-oblivious data structures base their Big-O on the number of cache line transfers between different levels of the memory hierarchy to minimize its growth as the data set gets bigger.",
			  "Through it, I learned about the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "the van Emde Boas Layout. It's a linearization of a tree that's supposed to minimize the number of cache transfers in the ideal memory model by being fractal.",
			  "If cache-oblivious works as advertised, why haven't I heard of any modern databases using it as an index?",
			  "the idea is 25 years old."
			]
		  },
		  {
			"title": "Improving Index Performance through Prefetching",
			"url": "http://pdl.cmu.edu/PDL-FTP/Database/CMU-CS-00-177.pdf",
			"excerpts": [
			  "Luk and Mowry proposed three solutions to the pointer-chasing problem 13, 14 ... We then prefetch the next chunk ahead in the jump-pointer array.Read more"
			]
		  },
		  {
			"title": "[PDF] Automatic Compiler-Inserted Prefetching for Pointer-Based Applications | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Automatic-Compiler-Inserted-Prefetching-for-Luk-Mowry/be36f6c5b363116faf91e6eb62f5585b226656a5",
			"excerpts": [
			  "The scope of automatic compiler-inserted prefetching is expanded to also include the recursive data structures commonly found in pointer-based applications, ..."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Our goal: In this paper, we aim to provide an effective, bandwidth- efficient, and low-cost solution to prefetching linked data structures by 1) overcoming the ...Read more",
			  "is paper proposes a low-cost hardware/software cooperative*\n*technique that enables bandwidth-efficient prefetching of linked data*\n*structure",
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,",
			  "The prefetcher brings cache blocks into the L2 (last-\nlevel) cache, since we use an out-of-order execution machine that can\ntolerate short L1-miss latencies",
			  "far ahead of the demand miss\nstream the prefetcher can send requests is determined by the *Prefetch*\n*Distance* parameter.",
			  "ch relies on the observation that most virtual addresses share com-\n ... \ning and object metadata. In *PLDI* , 2004."
			]
		  },
		  {
			"title": "Lecture 21 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s11/public/lectures/L21-Data-Prefetching.pdf",
			"excerpts": [
			  "\nI.\nPrefetching for Arrays\nII. Prefetching for Recursive Data Structures\nReading: ALSU 11 11 4\n**Carnegie Mellon**\nReading: ALSU 11.11.4\nAdvanced readings (optional):\nT.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm for\nPrefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.\nC.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures. In\nProceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.\nTodd C. Mowry\n15-745: Data Prefetching\n1\nThe Memory Latency Problem\n**Carnegie Mellon**\n\n processor speed >>  memory speed\n\ncaches are not a panacea\nTodd C. Mowry\n15-745: Data Prefetching\n2\nUniprocessor Cache Performance on Scientific Code\n\nApplications from SPEC, SPLASH, and NAS Parallel.\nM\nb\nt\nt\ni\nl f MIPS R4000 (100 MH )\n**Carnegie Mellon**\n\nMemory subsystem typical of MIPS R4000 (100 MHz):\n 8K / 256K direct-mapped caches, 32 byte lines\n miss penalties: 12 / 75 cycles\n\n8 of 13 spend > 50% of time stalled for memory\nTodd C. Mowry\n15-745: Data Prefetching\n3\nPrefetching for Arrays: Overview\n\nTolerating Memory Latency\n\nPrefetching Compiler Algorithm and Results\n\nImplications of These Results\n**Carnegie Mellon**\nTodd C. Mowry\n15-745: Data",
			  "Performance of History-Pointer Prefetching\n**O** = original\n\nApplicable because a list structure does not change over time\n**O**\noriginal\n**G** = greedy prefetching\n**H** = history-pointer prefetching\nHealth\n**Carnegie Mellon**\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)\n\nImproved accuracy outweighs increased overhead in this case",
			  "Performance of Data-Linearization Prefetching\n**O** = original\n**G**\nd\nf t hi\n\nCreation order equals major traversal order in treeadd & perimeter\nhence data linearization is done without data restructuring\n**G** = greedy prefetching\n**D** = data-linearization prefetching\n**Carnegie Mellon**\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n 94%->78% in perimeter, 87%->81% in treeadd\n while maintaining good coverage factors:\n 100%->80% in perimeter, 100%->93% in treeadd",
			  "Performance of History-Pointer Prefetching",
			  "**O** = original",
			  "40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "**Lecture 21**",
			  "**Compiler Algorithms for Prefetching Data**",
			  "**Compiler Algorithms for Prefetching Data**"
			]
		  },
		  {
			"title": "Lectures 26-27 Compiler Algorithms for Prefetching Data",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s15/public/lectures/L26-27-Data-Prefetching.pdf",
			"excerpts": [
			  "Propose 3 schemes to overcome the pointer-chasing problem:",
			  "**Compiler Algorithms for Prefetching Data**",
			  "II. Prefetching for Recursive Data Structures",
			  "f Data-Linearization Prefetching",
			  "59",
			  "Conclusions",
			  "Conclusions",
			  "Propose 3 schemes to overcome the pointer-chasing problem:\n Greedy Prefetching\n History-Pointer Prefetching\n Data-Linearization Prefetching",
			  "Automated greedy prefetching in SUIF",
			  "Automated greedy prefetching in SUIF",
			  "T.C. Mowry, M. S. Lam and A. Gupta. Design and Evaluation of a Compiler Algorithm fo",
			  " memory feedback can further reduce prefetch overhead",
			  "The other 2 schemes can outperform greedy in some situations",
			  "C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures.",
			  "fewer unnecessary prefetches:",
			  "94%->78% in perimeter, 87%->81% in treeadd",
			  "while maintaining good coverage factors:",
			  "100%->80% in perimeter, 100%->93% in treeadd",
			  "**Lectures 26-27**",
			  "Reading: ALSU 11.11.4",
			  "Advanced readings (optional):",
			  "Prefetching. In Proceedings of ASPLOS-V, Oct. 1992, pp. 62-73.",
			  "In Proceedings of ASPLOS-VII, Oct. 1996, pp. 222-233.",
			  "1",
			  "\ncaches are not a panacea",
			  "Uniprocessor Cache Performance on Scientific Code",
			  "Applications from SPEC, SPLASH, and NAS Parallel.\n",
			  "\nMemory subsystem typical of MIPS R4000 (100 MHz):",
			  " 8K / 256K direct-mapped caches, 32 byte lines\n",
			  " miss penalties: 12 / 75 cycles",
			  "8 of 13 spend > 50% of time stalled for memory",
			  "3",
			  "Prefetching for Arrays: Overview",
			  "Tolerating Memory Latency",
			  "Prefetching Compiler Algorithm and Results",
			  "Implications of These Results",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "15-745: Data Prefetching",
			  "4",
			  "2",
			  "2",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "**Carnegie Mellon**",
			  "Coping with Memory Latency",
			  "**Reduce Latency:**",
			  " Locality Optimizations\n",
			  " reorder iterations to improve cache reuse\n",
			  "**Tolerate Latency:**",
			  " move data close to the processor before it is needed\n",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry",
			  "Todd C. Mowry"
			]
		  },
		  {
			"title": "Accelerated Single Ray Tracing for Wide Vector Units",
			"url": "https://www.highperformancegraphics.org/wp-content/uploads/2017/Papers-Session2/HPG2017_AcceleratedSingleRayTracing.pdf",
			"excerpts": [
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "3",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "4",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2"
			]
		  },
		  {
			"title": "Lecture 27 Compiler Algorithms for Prefetching Data",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s12/public/lectures/L27-Data-Prefetching-1up.pdf",
			"excerpts": [
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n\n1\n\n",
			  "reedy Prefetching\n\nOur proposals:\n\n\nuse existing pointer(s) in ni to approximate &ni+d\n\na dd new pointer(s) to ni to a pproximate &ni+d\n\nni\nni+d\n\nn\n\na new p oin",
			  "Greedy Prefetching\n\n\nPrefetch all neighboring nodes (simplified definition)\n only one will be followed by the immediate control flow\n hopefully, we will visit other neighbors later\n",
			  "History-Pointer Prefetching\n\n\nAdd new pointer(s) to each node\n history-pointers are obtained from some recent traversal\n",
			  "Data-Linearization Prefetching\n\n\nNo pointer dereferences are required\n\nMap nodes close in the traversal to contiguous memory\n",
			  "Performance of Compiler-Inserted Greedy Prefetching\n\nload stall\nO = Original\n\nG = Compiler-Inserted Greedy Prefetching\n\nload stall\nstore stall\ninst. stall\nbusy\n\n\nEliminates much of the stall time in programs with large load stall\npenalties\n half achieve speedups of 4% to 45",
			  "Performance of History-Pointer Prefetching\n\nO = original\n\nG = greedy prefetching\n\nH = history-pointer prefetching\n\n\nApplicable because a list structure does not change over time\n\n40% speedup over greedy prefetching through:\n\nHealth\n\n40% speedup over greedy prefetching through:\n better miss coverage (64% -> 100%)\n fewer unnecessary prefetches (41% -> 29%)",
			  "Performance of Data-Linearization Prefetching\n\nO = original\nG = greedy prefetching\n\nD = data-linearization prefetching\n\n\nCreation order equals major traversal order in treeadd & perimeter\n hence data linearization is done without data restructuring\n\n9% and 18% speedups over greedy prefetching through:\n9% and 18% speedups over greedy prefetching through:\n fewer unnecessary prefetches:\n\n 94%->78% in perimeter, 87%->81% in treeadd\n\n while maintaining good coverage factors:\n\n 100% >80% in perimeter  100% >93% in treeadd\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance.",
			  "In the evaluation, the proposed solution achieves an average speedup of 1.37  over a set of memory-intensive benchmarks."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism | ACM SIGOPS Operating Systems Review",
			"url": "https://dl.acm.org/doi/10.1145/635508.605427",
			"excerpts": [
			  "C.-K. Luk and T. Mowry. Compiler-based prefetching for recursive data structures. In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 222-233, Cambridge, Massachusetts, October 1996. ACM.",
			  "D. Joseph and D. Grunwald. Prefetching using markov predictors. In Proceedings of the 24th Annual International Symposium on Computer Architecture, pages 252-263, Denver, Colorado, June 1997. ACM."
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[30] A. Roth, A. Moshovos, and G. S. Sohi. Dependence based prefetching\nfor linked data structures. In *ASPLOS-8* , 1998.",
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown",
			  "Dependence based\nprefetching for linked data structures, *",
			  "indirections through memory as seen in sparse tensor algebra\nand graphs [ 13  15 ], and more generally applications with\npointer chasing [ 10 , 11 ]. The target application influences the\ndata access pattern that the prefetcher tries to identify and\nprefetch for. For example, a common access pattern in sparse\ntensor algebra and graph computations is An [ *...* A1 [ A0 [ i ]] *...* ] .\nCorr",
			  "Yu et al. [ 13 ] (a.k.a. IMP) tries to detect\naccess patterns given by Y [ Z [ i ]] (2-level IMP) and X [ Y [ Z [ i ]]]\n(3-level IMP), for striding loop variable i , and prefetch\ndata by assuming that Y [ Z [ i + ** ]] and X [ Y [ Z [ i + ** ]]] will\nbe needed in the future. Ainsw"
			]
		  },
		  {
			"title": "Dependence Based Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/papers/1998/asplos-prefetch-lds.pdf",
			"excerpts": [
			  "k and Mowry [12] proposed and evaluated a greedy compiler\nalgorithm for scheduling software prefetches for linked data struc-\ntures. They showed this scheme to be effective for certain pro-\ngrams, citing instruction overhead and the generation of useless\nprefetches as performance degradation factors for other",
			  "ry [12] presented a case for history-pointer prefetch-\ning, which augments linked structure nodes with prefetching\npointer fields, and data-linearization, in which LDS are program-\nmatically laid out at runtime to allow sequential prefetch machin-\nery to capture their traversal.",
			  "eir\nalgorithm uses type information to identify recurrent pointer\naccesses, including those accessed via arrays, and may have advan-\ntages in tailoring a prefetch schedule to a particular traversal.",
			  "It collects these loads along with the\ndependence relationships that connect them and constructs a\ndescription of the steps the program has followed to traverse the\nstructure.",
			  "Predicting that the program will continue to follow these\nsame steps, a small prefetch engine takes this description and spec-\nulatively executes it in parallel with the original progra",
			  "Linked data structures (LDS) such as lists and trees are used in\nmany important applications."
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Later Luk and\nMowry proposed a compiler-based prefetching scheme for\nrecursive data structures [22].",
			  "This requires extra storage at\nruntime.",
			  "Jump Pointer [29], which has\nbeen used widely to break the serial LDS (linked data struc-\nture) traversal, stores pointers several iterations ahead in the\nnode currently visite",
			  "In a recent work, Gau-\ntam Doshi et al. [14] discussed the downside of software\nprefetching and exploited the use of rotating registers and\npredication to reduce the instruction overhead",
			  "Profile Guided Software Prefetching",
			  "Software prefetching is ineffective in pointer-based pro-\ngrams. To address this problem, Chi K. Luk et al. presented\na Profile Guided Post-Link Stride Prefetching [23] using a\nstride profile to obtain prefetching guidance for the com-\npil",
			  "[22] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching\nFor Recursive Data Structures. In *ASPLOS-7* , pages 222\n233. ACM Press, 1996",
			  "[23] C.-K. Luk, R. Muth, H. Patil, R. Weiss, P. G. Lowney, and\nR. Cohn. Profile-Guided Post-link Stride Prefetching. In\n*ICS-16* , pages 167178. ACM Press, 2002.",
			  "24] T. C. Mowry, M. S. Lam, and A. Gupta. Design and Evalua-\ntion of A Compiler Algorithm for Prefetching. In *ASPLOS-*\n*5* , pages 6273. ACM Press, ",
			  "[25] T. C. Mowry and C.-K. Luk. Predicting Data Cache Misses\nin Non-Numeric Applications Through Correlation Profil-\ning. In *Micro-30* , pages 314320. IEEE Computer Society\nPress, 199"
			]
		  },
		  {
			"title": "Dynamic Hot Data Stream Prefetching for General-Purpose ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s09/www/papers/prefetch_hds.pdf",
			"excerpts": [
			  "Jump pointers are a software technique for prefetching linked data\nstructures, overcoming the array-and-loop limitation.",
			  "Artificial\njump pointers are extra pointers stored into an object that point to\nan object some distance ahead in the traversal order.",
			  "d. Natural jump pointers are existing pointers in the\ndata\nstructure\nused\nfor\nprefetching.\n",
			  "ng.\nFor\nexample,\ngreedy\nprefetching makes the assumption that when a program uses an\nobject o, it will use the objects that o points to, in the near future,\nand hence prefetches the targets of all pointer fields.",
			  " These\ntechniques were introduced by Luk and Mowry in [22] and refined\nin [5, 18].",
			  "n\ndependence-based prefetching, producer-consumer pairs of loads\nare identified, and a prefetch engine speculatively traverses and\nprefetches them [26].",
			  "\nThe hardware technique that best corresponds to history-pointers is\ncorrelation-based prefetching. As originally proposed, it learns\ndigrams of a key and prefetch addresses: when the key is observed,\nthe prefetch is issued [6]. J"
			]
		  },
		  {
			"title": "2003 Workshop on Duplicating, Deconstructing and ... - PHARM",
			"url": "https://pharm.ece.wisc.edu/wddd/2003/wddd2003_proceedings.pdf",
			"excerpts": [
			  " ware prefetch when bandwidth is limited; with sufficient\nbandwidth software prefetch is the most successful strategy.\nHowever, their research also shows that the combination of\ncache-conscious allocation and software prefetch might not\nlead to further performance improvements, instead it coun-\nteracts changes in bandwidth or latency. Their results are\nsimilar to ours, although we have implemented a different\nsoftware prefetch that does not require any extra memory.\nSeveral researchers have studied hardware prefetch,\nor hybrid schemes, and successfully adapted hardware\nprefetch to pointer-based data structures with irregular ac-\ncess behavior. However, they generally require more hard-\nware than those evaluated in this study. Hardware support\nhas been investigated by the use of lock-up free prefetching,\n[13], and prefetch buffers, [10], and general prefetching in\nhardware is described in [20, 21] together with other cache\nmemory aspects. Karlsson et al., [11], propose a technique\nfor prefetching pointer-based data structures, either in soft-\nware combined with hardware or in software alone, by im-\nplementing prefetch arrays, making it possible to prefetch\nboth short data structures and longer data structures without\nknowing the traversal path. Roth et al. have investigated\nmore adaptable strategies for hybrid prefetch schemes, us-\ning dependence graphs, [18], and jump pointer prefetching,\n[19]. In [19], Roth et al. evaluate a framework for jump-\n ... \n[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999.\n[20] Alan J. Smith. Cache memories. *ACM Computing*\n*Surveys* , 14:3:473530, September 1982.\n[21] Steven P. VanderWiel and David Lilja. Data prefetch\nmechanisms.\n*ACM Computing Surveys* , 32:2:174\n199, June 2000.\n[22] Chengqiang Zhang and Sally A. McKee. Hardware-\nonly stream prefetching and dynamic access order-\ning. In *International Conference on Supercomputing* ,\npages 167175, 2000.\n[23] L. Zhang, S. McKee, W. Hsieh, and J. Carter. Pointer-\nbased prefetching within the impulse adaptable mem-\nory controller: Initial results. In *Proceedings of the*\n*Workshop on Solving the Memory Wall Problem* , June\n2000.\n[24] Craig B. Zilles. Benchmark health considered harm-\nful. *Computer Architecture News* , 29:3, 2001.\n13\n**Comparison of State-Preserving vs. Non-State-Preserving Leakage Control**\n**in Caches**\nDharmesh Parikh\n\n, Yan Zhang\n\n, Karthik Sankaranarayanan\n\n, Kevin Skadron\n\n, Mircea Stan\n\n Dept. ",
			  "[19] Amir Roth and Gurindar S. Sohi.\nEffective jump-\npointer prefetching for linked data structures. In *Pro-*\n*ceedings of the 26th Annual International Symposium*\n*on Computer Architecture* , pages 111121, 1999."
			]
		  }
		],
		"reasoning": "The primary insight from the performance model is that the traversal problem is fundamentally memory-latency bound. This is due to the serialized and data-dependent nature of pointer-chasing, which exacerbates the issue. The constraint of only two memory load slots per cycle further limits the core's execution throughput, but this becomes a bottleneck only if memory latency is perfectly hidden. The core challenge identified is that the available software parallelism (256 walkers) far exceeds the hardware's ability to exploit Memory-Level Parallelism (MLP) on a single CPU core, which is limited by the number of Miss Status Holding Registers (MSHRs). Therefore, the most effective optimizations are those that either reduce the number and cost of memory accesses by improving spatial and temporal locality (e.g., cache-friendly layouts like AoS vs. SoA, treelet-based layouts, compressed BVHs, and van Emde Boas layouts) or restructure the algorithm to create independent work that can hide latency (e.g., batching, Decoupled Access/Execute (DAE), asynchronous operations, and helper threads for prefetching). Techniques like software prefetching, jump pointers, and data linearization are specifically mentioned as ways to mitigate pointer-chasing latency. Additionally, wide BVHs (e.g., BVH4, BVH8) and SIMD instructions are highlighted for their ability to process multiple data elements or nodes in parallel, which is crucial for improving performance on modern architectures.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.0.suitability_for_problem",
		"citations": [
		  {
			"title": "Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/pdf/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the applications dataflow, all without access to the application\nsource code.",
			  "In this paper, we propose an inline software\nprefetching technique that overcomes these restrictions by inserting the\nhelper code into the main thread itself.",
			  "ur approach is complementary\nto and does not interfere with existing hardware prefetchers since we\ntarget only delinquent irregular load instructions (those with no constant\nor striding address patterns). ",
			  "Load instructions in a program can fall into three categories: (a)\nconstant address, (b) striding, and (c) irregular. Constant address\nloads are loads whose virtual address does not change over multiple\ndynamic instances of the load (for e.g., global variables and stack\naccesses). Striding loads are those with successive virtual addresses\nfollowing an arithmetic progression (for e.g., array accesses).\n*Irregular loads* are those which do not fall into either of the\nabove two categories (for e.g., indirect and pointer references)."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code.",
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more"
			]
		  }
		],
		"reasoning": "The most directly relevant material describes an inline software prefetching approach that embeds the prefetcher into the main thread to cope with delinquent irregular loads, which matches the need to optimize 256 independent walkers without incurring thread creation or synchronization overhead. Additional sources describe customizing prefetchers tailored to the applications dataflow, enabling prefetching without access to source code, and doing so specifically for delinquent irregular loads. This aligns with the requirement to mitigate irregular, data-dependent accesses in a batched traversal by software-directed prefetching rather than relying on helper threads, which would be costly for many walkers. Other excerpts discuss the categorization of loads, including irregular loads, reinforcing that irregular access is the core challenge, and note that such approaches can be complementary to existing hardware prefetchers, offering a practical path to improve memory throughput under limited bandwidth. Together, these excerpts support the idea that inline, customized software prefetching is highly suitable for the described problem because it targets delinquent irregular loads, avoids thread-related overhead, and complements hardware mechanisms.",
		"confidence": "high"
	  },
	  {
		"field": "database_and_hashing_optimizations.1.technique",
		"citations": [
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page."
			]
		  }
		],
		"reasoning": "The core idea behind the fine-grained field value is using implicit array-based layouts to eliminate pointer-based navigation and to leverage cache-friendly traversal. Excerpts that explicitly discuss Eytzinger layout describe arranging a binary tree into an array so that root-proximate nodes are accessed earlier and are more cache-friendly, which is the essence of pointer elimination and implicit layouts. One excerpt explains that this layout is inherently cache-friendly for binary search structures and highlights the temporal locality benefits achieved when elements near the root are at the beginning of the array, making them more likely to be fetched from cache. Another excerpt notes that the Eytzinger approach can be used to implement various tree structures, such as heaps or segment trees, in an implicit, pointer-free way, which directly aligns with eliminating pointer-chasing in traversal. Additional excerpts discuss practical aspects like prefetching within an Eytzinger layout, demonstrating how explicit cache-line prefetching can further enhance performance by anticipating future accesses within a batched, structured layout. Collectively, these excerpts provide direct support for the notion of Pointer Elimination & Implicit Layouts via Eytzinger, including its cache-locality advantages, applicability to different tree types, and tangible prefetching strategies. Other excerpts that address related but broader topics (e.g., general memory access patterns, batching, or alternative layouts like van Emde Boas) offer contextual relevance but are less direct in supporting the specific Eytzinger pointer-elimination concept, thus ranking slightly lower in direct relevance.\n",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.4.mechanism",
		"citations": [
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "hing can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching."
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%."
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  }
		],
		"reasoning": "The most directly relevant material defines the core mechanisms referenced in the finegrained field value. Excerpts that describe Delta Correlating Prediction Tables (DCPT) and their combination with Reference Prediction Tables and PC/DC prefetching reflect the exact mechanism described as learning patterns in the memory access stream and using delta sequences to forecast subsequent accesses. Additionally, explicit mentions of the Global History Buffer (GHB) as a store for misses and the linkage to prefetching behavior corroborate the components the field value highlights. Excerpts that discuss the quantitative benefits of prefetching (speedups and performance improvements) provide supportive context but are slightly less central to the core mechanism. Related content about different prefetching schemes or variants (including performance gains from specific benchmarks) still relates to the same broad topic of memory-access-pattern learning and speculative prefetching, but with a broader or alternative focus. Finally, entries about other prefetching or memory-management topics (e.g., local delta data prefetchers) are relevant insofar as they concern memory access prediction and locality, but they do not directly define the same exact mechanism described in the field value.\n\nKey connections:\n- DCPT combines RPT and PC/DC ideas to predict memory accesses using a delta-correlation approach, which matches the idea of learning patterns in the memory access stream to anticipate future misses.\n- The Global History Buffer is described as a structure used to track misses and guide prefetching, aligning with the notion of learning from recent memory-access sequences.\n- The cited improvements (e.g., reported speedups) illustrate the practical impact of such memory-prediction techniques, supporting why these mechanisms would be used in highly parallel, irregular access scenarios like batched walkers traversing a tree with limited bandwidth.\n\nOverall, the strongest support comes from excerpts that explicitly name and describe DCPT and the GHB, followed by excerpts that discuss prefetching benefits and related schemes, with diminishing directness for the remaining items.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.4.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E"
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  }
		],
		"reasoning": "The finegrained field value discusses DCPT (Delta Correlating Prediction Tables) as a prefetching technique and quantifies performance gains (a 42% speedup on SPEC2006 with 4KB storage overhead) along with a GHB-driven improvement (20%). Excerpts that describe the DCPT concept, its predecessors (RPT/PC/DC), and its measured speedups provide direct support for these claims. Statements that DCPT builds on Reference Prediction Tables and PC/DC, and that DCPT reduces prefetching complexity while offering substantial speedups, directly connect to the values claim about low-overhead, high-impact prefetching. Additional excerpts explicitly reporting SPEC2006-level gains (up to 3.7x in some benchmarks and a geometric mean of 42%) further corroborate the magnitude of performance benefits associated with DCPT and related prefetching techniques. The references to GHB-based correlation prefetching performance (about 20% improvement) align with the field values note about GHBs role in correlation prefetching. Taken together, these excerpts support the idea that DCPT can yield meaningful speedups and that GHB-based correlation can contribute, though the exact applicability to the users irregular, data-dependent tree traversal needs benchmarking in the same scenario. The most directly relevant parts describe DCPTs mechanism, its historical context, and quantified gains, which are precisely the aspects highlighted in the field value. Excerpts that discuss performance gains in the same domain (SPEC2006 benchmarks) reinforce the magnitude of potential benefits and help anchor expectations for transfer to the batched, divergent-access tree traversal case.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.1.mechanism",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load."
			]
		  }
		],
		"reasoning": "The finegrained field value describes a mechanism where an idle or spare execution context (a dedicated helper thread, potentially within an SMT or similar setup) runs a pre-computed slice of the original program to determine the address of a future delinquent load and issues a prefetch ahead of the main thread. This mirrors multiple excerpts: one discusses using helper threads to run a customized prefetcher extracted from the dataflow without needing source access, effectively debiasing latency by performing prefetch-related work in a separate context. Another excerpt explicitly frames helper threads as a way to extract and execute a backward slice of instructions that lead to a cache-miss load, enabling pre-execution and warming of the cache. Additional excerpts describe pre-execution via backward slices and the concept of running pre-execution code on spare thread contexts to trigger cache misses ahead of the main thread, thereby hiding latency. Taken together, these excerpts support the idea of a dedicated helper thread computing a slice to predict or determine future memory accesses and issuing prefetches before the main traversal continues, which matches the described mechanism of using idle contexts to prefetch delinquent loads. The relevance is strongest where the text directly ties helper threads or spare contexts to prefetching and to pre-execution slices; supporting lines also describe backward slices that lead to cache-miss loads and the notion of driving pre-execution through program slicing, which aligns with the mechanism in the fine-grained value. The combination of inline software prefetching, helper thread prefetching, backward slices, and pre-execution on spare contexts collectively substantiates the described technique of using an idle context to compute a slice and prefetch ahead of the main thread.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.1.technique",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18].",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load."
			]
		  }
		],
		"reasoning": "The target field value refers to a technique known as Helper-Thread Prefetching implemented as Speculative Precomputation. Several excerpts directly describe helper-thread based prefetching that uses spare threads to run forward or backward slices of intentions to prefetch data ahead of the main thread, thereby hiding memory latency. One excerpt introduces the concept of helper threads executing a backward/back-slice of the delinquent load to prefetch memory into the cache, which is exactly the idea of speculative pre-execution using auxiliary execution contexts. Other excerpts elaborate on inline software prefetching via helper code embedded in the main thread, and on long-range speculative precomputation where a helper thread runs ahead of the main thread to warm the cache for memory accesses that the main thread will perform. Together, these sources support the notion of using helper threads for speculative prefetching to mask irregular memory access patterns in a batched, parallel traversal setting. Additional excerpts discuss backward slices and pre-execution strategies that underpin how such helper-thread approaches identify the essential instructions needed to compute memory addresses, enabling effective prefetching for delinquent or irregular loads, which reinforces the linkage between the field value and practical implementation detail. Collectively, the most directly relevant material centers on helper-thread prefetching and its speculative precomputation nature; surrounding material on backward slices and pre-execution provides mechanistic support for how these techniques operate in practice.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.4.suitability_for_problem",
		"citations": [
		  {
			"title": "(PDF) Storage efficient hardware prefetching using delta correlating prediction tables",
			"url": "https://www.researchgate.net/publication/228641226_Storage_efficient_hardware_prefetching_using_delta_correlating_prediction_tables",
			"excerpts": [
			  "Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " paper, we have presented a new prefetching heuristic called Delta Correlating Pre-\ndiction Tables (DCPT). DCPT builds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [14] and PC/DC prefetching by Nesbit and Smith [17].\nIt combines the table based design of RPT and the delta correlating design of PC/DC with\nsome improvement",
			  " DCPT substantially reduces the complexity of PC/DC\nprefetching by avoiding expensive pointer chasing in the GHB (Global History Buer) and\nrecomputation of the delta buer.\n",
			  "PT prefetching can increase performance by up to 3.7X for\nsingle benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks\nis 42% compared to no prefetching.\n",
			  ". The structure of the GHB is shown in gure 2.Each cache miss or cache hit to a\ntagged (prefetched) cache block is inserted into the GHB in FIFO order.The index table\nstores the address of the load instruction and a pointer into the GHB for the last miss issued\nby that instruction.E",
			  "In 2004, Nesbit and Smith [17] proposed a dierent approach using a Global History Buer\n(GHB). T"
			]
		  },
		  {
			"title": "(PDF) Comparing Hardware Prefetching Schemes on an L2 Cache",
			"url": "https://www.researchgate.net/publication/301564549_Comparing_Hardware_Prefetching_Schemes_on_an_L2_Cache",
			"excerpts": [
			  "DCPT prefetching can increase performance by up to 3.7X for single benchmarks, while the geometric mean of speedups across all SPEC2006 benchmarks is 42% compared to no prefetching."
			]
		  },
		  {
			"title": "Storage Efficient Hardware Prefetching using Delta ...",
			"url": "https://jilp.org/dpc/online/papers/02grannaes.pdf",
			"excerpts": [
			  "called Delta Correlating Prediction Tables (DCPT). DCPT\nbuilds upon two previously proposed techniques, Reference\nPrediction Tables by Chen and Baer [4] and PC/DC prefetch-\ning by Nesbit et al. [6]. It combines the table based design\nof RPT and the delta correlating design of PC/DC, as well\nas improving upon the ideas.\nW",
			  "This is an improvement over PC/DC\nprefetching by 27.2%."
			]
		  },
		  {
			"title": "Enhancing Signature Path Prefetching with Perceptron ...",
			"url": "https://dpc3.compas.cs.stonybrook.edu/pdfs/Enhancing_signature.pdf",
			"excerpts": [
			  "In this paper, we evaluate an implementation of a multi- cache-level prefetching system under the 3rd Data Prefetch- ing Championship framework.Read more"
			]
		  },
		  {
			"title": "Berti: an Accurate Local-Delta Data Prefetcher",
			"url": "https://webs.um.es/aros/papers/pdfs/anavarrotorres-micro22.pdf",
			"excerpts": [
			  "Variable Length Delta Prefetching (VLDP). This spatial data prefetcher uses multiple histories of deltas between successive cache lines observed within an ...Read more"
			]
		  }
		],
		"reasoning": "Excerpts that describe Delta Correlating Prediction Tables (DCPT) and related prefetching approaches illustrate how speculative prefetching reduces memory latency in irregular access patterns. DCPT combines ideas from Reference Prediction Tables and PC/DC prefetching and shows tangible performance gains, which supports the notion that prefetching can be effective for irregular access patterns but may require patterns to be exploitable (e.g., some delta regularity) to maximize benefit. This aligns with the finegrained field values emphasis on moderately suitable techniques: beneficial for irregular patterns but potentially limited for pure pointer-chasing when there are no repeating delta patterns. Other excerpts discuss prefetching can yield substantial speedups on benchmarks, underscoring the potential but also implying that benefits depend on the workload characteristics, such as the presence of data-dependent or batched access patterns. Some content notes how prefetching can avoid expensive pointer chasing or reduce the penalty of cache misses, which is relevant to a batched, parallel tree-walking scenario where many walkers diverge but start from a common root, creating a window where batching or temporal locality might be exploited. The consensus across these excerpts is that speculative or delta-based prefetching can be powerful for irregular patterns and can reduce latency, but there are caveats when the access pattern has high entropy or lacks repeatable deltas, which supports the claim of moderate suitability for the given problem. Overall, the combination of technique descriptions and reported performance benefits provides a coherent, evidence-supported basis for the stated field value, including the nuance that benefits are workload-dependent and that pure pointer-chasing can limit effectiveness without exploitable patterns.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.5.technique",
		"citations": [
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  }
		],
		"reasoning": "The target field value is precisely named Content-Directed Prefetching (CDP). Excerpts that explicitly introduce CDP or clearly describe a content-directed prefetching mechanism directly support this field value. The strongest support comes from an excerpt that explicitly labels the technique as Content-directed prefetching (CDP) and describes it as an attractive technique for prefetching in memory hierarchies. Related excerpts discuss Content-Directed Data Prefetching, a closely named variant, indicating the same core idea of prefetching guided by the application's data access patterns, which aligns with CDPs concept of predicting and fetching likely future addresses. Additional excerpts discuss content-aware or statically-determined prefetching mechanisms that share the same theme of using data structure/pointer access patterns to drive prefetching, reinforcing the context of CDP's relevance to irregular or pointer-heavy access patterns. The less central excerpts still relate by exploring speculative/preconditioning ideas about prefetching for dynamic or pointer-intensive data structures, which are conceptually adjacent to CDP but do not name CDP explicitly. Taken together, these excerpts map a coherent landscape around content-directed strategies for prefetching in irregular memory access scenarios, which directly supports the notion and relevance of CDP in the specified field value.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.1.suitability_for_problem",
		"citations": [
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread."
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18].",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "For each recorded\nload, the optimizer analyzes the hot trace in reverse order,\nbeginning with the delinquent load, to build up a slice of in-\nstructions the load depends on, either directly or indirectly.\nThis is called *back-slicing* .",
			  "The goal of p-slice construction is to extract all instruc-\ntions which are necessary to compute the memory address\nfor a delinquent load, so that we can prefetch the load."
			]
		  }
		],
		"reasoning": "The concept described, where a lightweight prefetching mechanism is embedded into the main thread flow to hide memory latency, directly supports the idea of speculative prefetching as a way to mitigate irregular memory access in a batched, data-dependent traversal. The notion of inline software prefetching that avoids needing additional threads or external contexts addresses the core concern of scarce resources when trying to scale to 256 concurrent walkers. Related excerpts discuss leveraging helper threads to run a backward slice of a delinquent load on spare contexts to prefetch ahead of the main thread, which conceptually aligns with latency-hiding goals but raises questions about feasibility when hundreds of walkers would contend for limited helper-context resources. The literature on speculative precomputation and run-ahead or backward-slice techniques provides concrete mechanisms to prefetch memory by analyzing data dependencies and pre-executing ahead of time, which could be adapted to staggered, early-round work where walkers share a common root before diverging. Discussions of pre-execution using backward slices and program slicing demonstrate how to identify the minimal set of instructions needed to fetch a memory address, enabling targeted prefetches that could reduce cache misses during traversal. Descriptions of pre-execution threads executing a subset of the original program reveal a practical strategy to prefetch while respecting hardware constraints, which is central to evaluating scalability to 256 walkers. Collectively, these sources offer concepts (inline prefetching, helper-thread prefetching, backward slices, and pre-execution) that are conceptually suitable for the problem, but the scalability concerns (limited SMT contexts, overhead from managing hundreds of helper threads) suggest that a naive application may not scale to 256 walkers without additional architectural or scheduling innovations. The best-fitting excerpts emphasize proactive, dependency-aware prefetching and lightweight pre-execution strategies rather than full-blown thread-for-each-walker schemes, aligning with the core question about scalability and practicality in this setting.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.2.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://dl.acm.org/doi/fullHtml/10.1145/3613424.3614288",
			"excerpts": [
			  "Ray traversal is a pointer-chasing operation where a ray travels down a node's child nodes repeatedly, creating a chain of dependent memory accesses where the latency is serialized. With treelet prefetching, while the pointer-chasing nature of ray traversal is still present, subsequent node accesses are confined within a treelet and can be fetched in advance without traversal. This reduces the node access latency during ray traversal as the nodes are already prefetched to the GPU's cache.",
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption.",
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more",
			  "Aila et al. [ [5]() ] proposed to use *treelets* , which are small subtrees of the overall BVH tree to speed up ray traversal. They explored using treelet queues to queue up rays that visit the same treelet and process them together to increase memory reuse."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/abs/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing | Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/3613424.3614288",
			"excerpts": [
			  "Our simulation results show treelet prefetching on average improves performance of the baseline RT Unit in Vulkan-Sim by 32.1% while maintaining the same power consumption."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://people.ece.ubc.ca/~aamodt/publications/papers/chou.micro2023.pdf",
			"excerpts": [
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache. Ray tracing is a pointer-chasing application\nand memory accesses are divergent and hard to predict. With the\ntreelet based traversal algorithm introduced previously, memory\naccesses are now clustered as individual treelets, making it possible\nto prefetch easily.",
			  "Treelet prefetching prefetches BVH nodes in treelet granularity\nto the GPUs cache.",
			  "Ray tracing workloads also exhibit irregular access\npatterns during ray traversal because each ray is used to sample\ndifferent parts of the scene. As a consequence, rays are usually\ndispatched from various locations and cast in different directions."
			]
		  },
		  {
			"title": "Treelet Prefetching For Ray Tracing",
			"url": "https://ieeexplore.ieee.org/document/10411396/",
			"excerpts": [
			  "We propose treelet prefetching to reduce the latency of ray traversal. Treelets are smaller subtrees created by splitting the BVH tree.Read more"
			]
		  }
		],
		"reasoning": "The field value asserts that speculative prefetching of BVH/treelet nodes reduces memory access latency and yields IPC gains, with a notable caveat that many prefetched nodes may be unused. Excerpts that explicitly describe treelet prefetching confining traversal to treelets and enabling prefetching ahead of time provide direct mechanism-level support for the claimed latency reductions. Statements noting improved performance (e.g., IPC uplift or latency reduction) directly tie to the finegrained field value. Excerpts that describe memory access clustering within treelets and the GPU cache alignment further justify why such prefetching would decrease latency in a divergent traversal scenario. Finally, notes about irregular access patterns in ray tracing establish the relevance to a workload with data-dependent indices and divergent paths, reinforcing why treelet-based strategies are beneficial in this context. Collectively, the most supportive excerpts describe treelet prefetching reducing latency and improving throughput, while others provide contextual support about memory access clustering and handling irregular access patterns, which underpin the reported gains. The caveat about unused prefetched data is acknowledged in the field value; excerpts that discuss the general trade-offs or timing of prefetching help explain why some prefetched data might not be utilized, even if they do not state the exact unused percentage.\n",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.1.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "Speculative Pre-Execution: Backward slices of branches that are difficult to predict and loads that miss in the cache often are executed speculatively on different threads. This thread executes ahead of the normal execution, and warms-up the cache and branch predictor for it [14,53,61,47, 20, 19]. Runahead execution [22,9,38,37] uses the same insights, but speculatively executes future independent instructions during long pipeline stalls of the normal thread."
			]
		  },
		  {
			"title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads | Request PDF",
			"url": "https://www.researchgate.net/publication/344038643_Helper_Without_Threads_Customized_Prefetching_for_Delinquent_Irregular_Loads",
			"excerpts": [
			  "Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "[2009.00202] Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads",
			"url": "https://arxiv.org/abs/2009.00202",
			"excerpts": [
			  "In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself.Read more",
			  "For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code."
			]
		  },
		  {
			"title": "Using Program Slicing to Drive Pre-Execution on ... - DRUM",
			"url": "https://drum.lib.umd.edu/bitstreams/8c6b6fd2-9c24-4db7-976e-c533f6a51948/download",
			"excerpts": [
			  "Pre-execution uses helper threads running in spare hardware contexts to trigger cache misses\nin front of the main thread, hence hiding their latency.",
			  " backward slice\nis a sequence of instructions extracted from the original program executable that leads to a cache-\nmissing load.",
			  "Backward slices can be extracted by analyzing either the program\nbinary [17], or a simulator-generated instruction trace [4, 13, 18].",
			  "Like instruction-level backward slices, program slices represent a\nsmaller version of the original program necessary to pre-execute the cache-missing loads.",
			  "However,\ninstead of extracting the pre-execution code through instruction-level analysis, program slicing\nperforms the code extraction directly on the program source using static code analysis techniques,\nresulting in a slice that consists of source code rather than binary code.",
			  "At the heart of pre-execution is the code that runs in the pre-execution threads themselves.",
			  "Amongst existing proposals, the most common approach is for pre-execution threads to run a subset\nof the instructions executed by the original program, called backward slices [18]."
			]
		  }
		],
		"reasoning": "The core field value discusses speculative prefetching and latency hiding as techniques to reduce memory stall time in irregular access patterns, with notes on the need for spare hardware contexts and potential synchronization overhead. Excerpts that describe speculative precomputation and long-range prefetching lay the groundwork for forward-acting approaches: they describe running backward slices of delinquent loads on helper threads so the main thread can be serviced from cache sooner, and they explicitly mention prefetching memory by executing a backward slice ahead of the normal path. This directly connects to the idea of pre-executing or prefetching for irregular access patterns to reduce stalls. Additional excerpts describe general strategies of pre-execution using helper threads and forward/backward slicing to trigger cache misses ahead of the main thread, which aligns with the concept of latency hiding in a multithreaded or SIMD-divergent setting. The included material on customized inline prefetchers and running code on spare contexts further reinforces the practical mechanisms by which speculative prefetching can be implemented, complementing the high-level claim of latency reduction. Taken together, these excerpts support the notion that speculative prefetching and latency-hiding techniques can significantly reduce memory stall time in batched or divergent traversals, while also acknowledging the overheads and complexity introduced by extra threads, pre-execution code paths, and synchronization concerns, which is consistent with the caveat about overhead mentioned in the target field value.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.3.technique",
		"citations": [
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique.",
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique.",
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures"
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown"
			]
		  }
		],
		"reasoning": "The most relevant excerpts directly describe Jump-Pointer Prefetching (JPP) and its capabilities. One excerpt explicitly enumerates four prefetching idioms under JPP (queue, full, chain, and root jumping) and mentions multiple implementation strategies, which solidly supports the existence and scope of JPP. Another excerpt provides a concise description of JPP in the linked data structures context, reinforcing its core idea. A third directly cites a JPP-focused title and corroborates the concept by reiterating its framing within linked data structures. A fourth excerpt reinforces the same technique with a slightly different presentation, contributing to the definition and framing of JPP. Additional excerpts discuss related prefetching approaches for linked data structures, including pointer-chase prefetchers and cooperative hardware/software strategies; while not naming JPP explicitly, they illuminate the broader landscape of speculative and bandwidth-efficient prefetching techniques that underpin or contrast with JPP, thus offering contextual support that helps situate JPP among related methods. Collectively, these excerpts substantiate the existence, scope, and impact of Jump-Pointer Prefetching as a technique relevant to reducing memory stalls in pointer-intensive or irregular access patterns, which aligns with the user's interest in exploiting early-stage batching and temporal locality opportunities.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.5.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems."
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  }
		],
		"reasoning": "The most relevant excerpt directly corroborates the exact quantitative claim: it reports an 11.3% speedup achieved with no additional processor state and notes that reducing cache overhead to less than 0.5% in the L2 cache yields further gains (up to 12.6% in broader real-world tests). This aligns precisely with the fine-grained field value, which specifies an 11.3% speedup and a minimal cache overhead, and mentions hybrid (ECDP) approaches that leverage compiler hints for pointer-rich regionsan explicit example of the class of techniques described in the query. The next most relevant excerpts discuss content-directed data prefetching mechanisms and compiler-directed prefetching, which provide general context about techniques that can yield performance improvements in pointer-intensive or irregular-access patterns but do not state the exact numerical result. These contextual excerpts are supportive but less directly tied to the precise empirical figure. Additional excerpts elaborate on the design and performance characteristics of prefetching systems and their evaluation methodologies, reinforcing the domain relevance (prefetching, memory-access patterns, and performance implications) without contradicting the specific numeric claim. Overall, the reasoning rests on the explicit numerical claim about speedup and cache overhead, with supporting context from related prefetching strategies that could realize such gains, including compiler-aware and content-directed approaches.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.3.suitability_for_problem",
		"citations": [
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more",
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ]."
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique.",
			  "jump-pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999.",
			  "[29] A. Rogers et al. Supporting dynamic data structures on distributed mem-\nory machines. *ACM TOPLAS* , 17(2), Mar. 199"
			]
		  },
		  {
			"title": "Opening Pandora's Box - Stanford Computer Science",
			"url": "https://cs.stanford.edu/people/trippel/pubs/pandora-isca-21.pdf",
			"excerpts": [
			  "*Motivating example: data memory-dependent prefetchers**\n**leak as much privacy as Spectre/Meltdown"
			]
		  }
		],
		"reasoning": "The finegrained field value emphasizes that speculative prefetching and cooperative or hardware-assisted approaches for linked data structures are highly suitable for mitigating pointer-chasing bottlenecks, and that such techniques can be implemented in software, hardware, or a cooperative model, directly breaking long dependency chains. The most relevant excerpts explicitly describe pointer-chase prefetchers and hardware/software cooperative approaches that prefetch linked data structures or traverse pointer-chase patterns ahead of the processor, which aligns with the goal of reducing memory stalls in irregular traversal. Specifically, excerpts describing a pointer-chase prefetcher that enables bandwidth-efficient prefetching of linked data structures, a programmable prefetch engine able to traverse linked data structures ahead of time, and a cooperative hardware/software prefetching scheme provide direct evidence that such techniques are well-suited for the described 256-walker, data-dependent traversal scenario. Additional excerpts discussing helper-thread prefetching for loosely-coupled architectures further support that prefetching can be organized to hide latency and improve throughput in irregular access patterns, including the batched or parallel traversal setting. The remaining excerpts about jump-pointer prefetching, while related to reducing memory stall time for linked data structures, are somewhat less directly tied to the specific context of speculative/predicted traversal of multiple walkers in parallel, but they still corroborate that multiple prefetching idioms (queue, full, chain, root jumping) can be used to improve memory access efficiency in such structures. Overall, the strongest support comes from explicit pointer-chase prefetcher discussions and cooperative prefetching approaches designed for linked data structures, with additional corroboration from helper-thread methods and related prefetching frameworks that address irregular access and latency hiding in similar workloads.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.6.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  }
		],
		"reasoning": "The field value concerns speculative prefetching as a technique to tolerate memory latency, with a note that it can be inefficient or energy-inefficient. Excerpts that describe helper threads extracting backward slices of a delinquent load and running them on a spare thread to prefetch memory directly address the mechanism by which speculative precomputation reduces latency by preloading data that would be required later. Specifically, the excerpt explains that helper threads 'extract the backward slice of a delinquent load and run it on a spare thread context' and that when the latency of the backward slice is less than the original loop, the helper thread runs ahead to prefetch memory into the cache. This directly supports the idea of latency hiding through speculative prefetching. Additional excerpts discuss the broader context of speculative precomputation, prefetching slices, and how these techniques can improve performance by getting memory accesses ahead of the main thread, which aligns with the concept of increasing tolerance to memory latency. The references that describe combining precomputation and inline or other prefetching, and the reported performance improvements from such techniques, further corroborate how speculative prefetching can mitigate latency, though they do not necessarily quantify energy costs. Together, these excerpts collectively support the general notion of speculative prefetching as a latency-hiding technique and illustrate the practical mechanisms (backward slices, helper threads, forward prefetching) by which it operates.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.3.mechanism",
		"citations": [
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999."
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system."
			]
		  }
		],
		"reasoning": "The fine-grained field value centers on augmenting a tree traversal with jump pointers to skip intermediate nodes and using a prefetch mechanism to follow those pointers for ahead-of-time memory fetching. Excerpts that explicitly describe a framework for jump-pointer prefetching and the accompanying prefetching idioms directly substantiate this concept, including descriptions of queue, full, chain, and root jumping as prefetching strategies and the existence of software/hardware or cooperative implementations. Additional excerpts reinforce the idea of prefetching linked data structures by presenting the measured benefits of jump-pointer prefetching and the general approach of prefetch engines that can operate ahead of the processor to initiate data transfers. Related content on bandwidth-efficient prefetching, pointer-chase prefetchers, and helper-thread prefetching provides corroborating context for speculative prefetching in irregular access patterns and for architectures that aim to hide memory latency during traversal by prefetching future or distant nodes. Taken together, these excerpts collectively support the notion of using jump pointers to guide prefetching in a tree traversal to improve bandwidth-limited memory accesses, consistent with the described mechanism and its intended use.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.5.suitability_for_problem",
		"citations": [
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,"
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems."
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  }
		],
		"reasoning": "The target fine-grained field value discusses speculative prefetching for irregular/pointer-chasing memory access and notes a potential downside: mispredictions may cause cache pollution and wasted bandwidth. Excerpts describing content-directed or compiler-directed prefetching for dynamic data structures and stateless/content-directed mechanisms directly address prefetching strategies aimed at pointer-intensive or irregular access patterns, which is precisely the area of interest for optimizing 256 walkers traversing a tree with data-dependent indices. In particular, the most directly relevant parts describe prefetching approaches that do not require extra pointer-tracking state, monitor cache behavior to identify candidate addresses, and apply prefetching to pointer-heavy or irregular data references; these pieces support both the concept (prefetching can help pointer-chasing workloads) and the caveat (mispredictions or inappropriate data interpretation can degrade performance). The included excerpts discuss bandwidth-efficient prefetching, content-directed prefetching, and compiler-directed content-aware prefetching for dynamic data structures, all of which map to the idea of accelerating multiple independent traversals with limited memory bandwidth by predicting and batching memory accesses. While they emphasize potential performance gains, they implicitly acknowledge the downside risk of mispredictions and wasted bandwidth when prefetching decisions are incorrect or data interpreted as pointers diverge from actual pointers. Taken together, these sources substantiate the relevance of speculative/content-directed prefetching techniques for irregular access patterns and the need to balance prediction accuracy with bandwidth utilization in a batched, tree-traversal setting.",
		"confidence": "medium"
	  },
	  {
		"field": "database_and_hashing_optimizations.1.application_to_traversal_problem",
		"citations": [
		  {
			"title": "Eytzinger Binary Search - Algorithmica",
			"url": "https://algorithmica.org/en/eytzinger",
			"excerpts": [
			  "This tutorial is loosely based on a [46-page paper](https://arxiv.org/pdf/1509.05053.pdf) by Paul-Virak Khuong and Pat Morin Array layouts for comparison-based searching and describes one particular way of performing efficient binary search by rearranging elements of a sorted array in a cache-friendly way.",
			  "In computer science** , this enumeration has been widely used for implicit (i. e. pointer-free) implementation of heaps, segment trees, and other binary tree structures, where instead of names it stores underlying array item",
			  "The main problem with binary search over a sorted array is that its memory accesses pattern is neither temporary nor spacially local. For example, element \\(\\lfloor \\frac n 2 \\rfloor\\) is accessed very often (every search) and element \\(\\lfloor \\frac n 2 \\rfloor + 1\\) is not, while they are probably occupying the same cache line. In general, only the first 3-5 reads are temporary local and only the last 3-4 reads are spacially local, and the rest are just random memory accesses.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "This is how this layout will look when applied to binary search:\nYou can immediately see how its temporal locality is better (in fact, theoretically optimal) as the elements closer to the root are closer to the beginning of the array, and thus are more likely to be fetched from cache.",
			  "As we know a bit more about our problem than the compiler does, we can explicitly tell it to prefetch a cache line we need. This is done by `__builtin_prefetch` in GCC:\nHere, `block_size` equals 16, which is precisely how many ints are needed to cover a cache line. When we reference cache line at `b + k * block_size` , we are referencing \\(k\\) s grand-grandson ( `block_size` = \\(2 \\times 2 \\times 2 \\times 2\\) , or 4 left turns) and possibly some of his neighbours in his layer (recall that indexes at the same level are just consecutive numbers).",
			  "The whole point of doing this is that there is a good chance that we will prefetch an element that we will use later on \\((i+4)\\) -th iteration. What chance, exactly? Well, it turns out that it is constant for each iteration.",
			  "Few more things to note:\nIt works best when \\(n\\) is a power of 2 or close to it, because otherwise the branch predictor will have a hard time figuring out whether or not to unroll the \\((\\log n)\\) -th cycle.\nIts performance varies by cache size and array length, but stays >3x even on smaller arrays (<1MB)",
			  "Preprocessing isnt costly. It is around 1% of the cost of firing the same number of queries as the array size."
			]
		  },
		  {
			"title": "Binary search variants and the effects of batching  CuriousCoding",
			"url": "https://curiouscoding.nl/posts/binsearch/",
			"excerpts": [
			  " Section Title: ... > 1 Optimizing Binary Search And Interpolation Search [Link to heading]()\nContent:\nThis blogpost is a preliminary of the [post on static search trees](https://curiouscoding.nl/posts/static-search-tree/) . We will be looking into binary search and how it can be\noptimized using different memory layouts (Eytzinger), branchless\ntechniques and careful use of prefetching. In addition, we will explore\nbatching. Our language of choice will be Rust.\nThe goal of this text is mainly educational, as well mostly be\nreplicating research that has already been done. Looking at performance plots is fun!\nThe source code can be found at https://github.com/RagnarGrootKoerkamp/static-search-tree ."
			]
		  },
		  {
			"title": "Cache Oblivious Algorithms",
			"url": "https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/",
			"excerpts": [
			  "The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page."
			]
		  }
		],
		"reasoning": "The targeted field describes replacing explicit pointers with a contiguous array layout for a binary tree, specifically the Eytzinger mapping where a node at index i has left and right children at 2*i+1 and 2*i+2. Excerpt by excerpt: the first item explicitly frames a cache-friendly array layout for binary-search-type tasks and emphasizes rearranging a sorted array to support efficient access within cache hierarchies, which aligns with moving a tree into a single array for better locality. The second item notes that this layout has been used for implicit, pointer-free implementations of binary-tree structures, underscoring the connection between array storage and pointer elimination. The third item discusses how traditional binary search patterns suffer poor temporal/spatial locality, providing motivation for adopting a layout like Eytzinger to improve locality for traversals. The fourth item reiterates that elements closer to the root are more cache-friendly, which directly supports the idea that a level-order/array-backed layout improves early traversal locality. The fifth item adds practical notes about the performance benefits and the conditions under which the layout excels, reinforcing its cache-efficiency advantages. The sixth item provides a broader example of using Eytzinger alongside similar optimization pitches (batching, static search trees) in a practical context, reinforcing relevance to batched or SIMD-friendly traversal. The seventh item discusses explicit prefetching techniques within such layouts, illustrating concrete ways to exploit the array-based structure for memory timing. The eighth item ties prefetching strategies to this context, illustrating how anticipating future accesses can be beneficial in an irregular traversal. The ninth item introduces van Emde Boas as another cache-oblivious tree layout, which is thematically connected as an alternative approach to improving locality in tree structures. The tenth item supplements with a note on preprocessing costs, which is relevant when considering adopting Eytzinger in a real system. The eleventh item, while about different unrelated topics, still sits in the same vein of discussing cache-aware trees and their performance implications, providing additional context for why Eytzinger-like layouts can be advantageous. Overall, the strongest support comes from explicit references to Eytzinger-like array layouts enabling pointer-free traversal and improving locality, with the rest offering supportive context about locality benefits, prefetching, and alternative layouts.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.6.mechanism",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68].",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache."
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  },
		  {
			"title": "Accelerating and Adapting Precomputation Threads for ...",
			"url": "https://cseweb.ucsd.edu/~calder/papers/HPCA-07-EventBasedPrex.pdf",
			"excerpts": [
			  "Synergy with Inline Prefetching**\nHere we compare and combine the performance of our\nprecomputation thread acceleration and adaptation with in-\nlined prefetching using our previous optimization frame-\nwork [26]. The result is shown in Figure 5. For compari-\nson, the first bar and the third bar in the figure are taken from\nFigure 3 and Figure 4, respectively.\nThe second bar (labeled *inlined prefetching* ) in the graph\nshows the results for the inlined software prefetching from\nthe prior work.\nThis is an aggressive dynamic inline\nprefetching system that takes full advantage of the Trident\nframework, including dynamic detection of delinquent loads,\nstride prediction of pointer loads, and dynamic adaptation of\nthe prefetch distance."
			]
		  }
		],
		"reasoning": "The most relevant passages describe speculative precomputation as a method to prefetch delinquent or long-latency loads by extracting backward slices of instructions and executing them on helper threads. This directly aligns with a hardware technique that checkpoints state and speculatively executes down the miss path to trigger prefetches, even if results are discarded. The cited material explicitly notes that helper threads can run a backward slice ahead of the main thread to prefetch memory accessed by the main thread, and that speculative precomputation can be implemented in hardware or software to generate long-range data prefetches, activating the prefetcher for subsequent misses. Related excerpts discuss prescient/infix slices used to reproduce branch outcomes and pre-execute necessary computations to resolve misses, which supports the broader concept of proactively preparing memory accesses through speculative paths. Additional lines discuss the synergy of precomputation with inline prefetching and how detecting delinquent loads enables dynamic prefetch strategies, reinforcing the idea of hardware- or software-directed pre-execution to hide latency. Taken together, these excerpts corroborate the idea of checkpointing state, speculatively executing along a miss path, and using those results to trigger prefetching behavior that reduces long-latency stalls.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.6.suitability_for_problem",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68].",
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache."
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  }
		],
		"reasoning": "The targeted field describes a general-purpose latency-hiding technique built around speculative precomputation/prefetching to tackle irregular pointer-chasing. Excerpts that explicitly discuss speculative precomputation and long-range prefetching show how backward slices or delinquent-load predictors can generate prefetch work to overlap with computation, which aligns with the notion of using speculative work to hide latency. In particular, the discussion of helper threads extracting the backward slice of a delinquent load and running it on a spare thread to prefetch memory demonstrates a concrete implementation of speculative precomputation to mitigate data-dependent stalls. Additionally, references to long-range prefetching and the potential deployment of speculative/precomputation approaches across hardware or software implementations reinforce the core concept of using speculative work to hide memory latency in irregular access patterns like pointer chasing. These points directly support the field values claim that speculative precomputation is a latency-hiding technique. The surrounding material on prescient/infix slices and pre-execution provides related mechanisms (such as constructing slices to resolve memory addresses or branches) that complement the broader theme of prefetch-driven latency hiding, though they are somewhat tangential to the exact speculative-precomputation framing. Collectively, these excerpts establish the core technique and its practical manifestations, while also offering context on related approaches used to reduce stalls from irregular memory accesses.",
		"confidence": "medium"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.5.mechanism",
		"citations": [
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://users.ece.cmu.edu/~omutlu/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "Content directed prefetching (CDP) [9] is an attractive technique\nfor prefetching LDS because it does not require additional state to\nstore the pointers that form the linkages in an LDS. This mechanism\nmonitors incoming cache blocks at a certain level of the memory hi-\nerarchy, and identifies candidate addresses to prefetch within those\ncache blocks. To do so, it uses a virtual address matching predictor,"
			]
		  },
		  {
			"title": "(PDF) A stateless, content-directed data prefetching mechanism",
			"url": "https://www.researchgate.net/publication/220939021_A_stateless_content-directed_data_prefetching_mechanism",
			"excerpts": [
			  "This technique is modeled after conservative garbage collection, and prefetches \"likely\" virtual addresses observed in memory references.",
			  "This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modem language systems."
			]
		  },
		  {
			"title": "A stateless, content-directed data prefetching mechanism",
			"url": "https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=18-6-cooksey.pdf",
			"excerpts": [
			  ". This prefetching mechanism uses the\nunderlying data of the application, and provides an 11.3% speedup\nusing *no additionalprocessor state. By* adding less than % space\noverhead to the second level cache, performance can be further in-\ncreased to 12.6% across a range of\"real world\" applications.",
			  "he rest of this paper is organized as follows. The simulation\nframework used to examine the feasibility and practicality of the\ncontent prefetcher is presented in Section 2, followed by the de-\nsign and implementation of the prefetcher in Section 3. A perfor-\nmance evaluation of the content prefetcher embodiment is given in\n"
			]
		  },
		  {
			"title": "(PDF) Compiler-directed content-aware prefetching for dynamic data structures",
			"url": "https://www.researchgate.net/publication/4038487_Compiler-directed_content-aware_prefetching_for_dynamic_data_structures",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs.",
			  "compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher.",
			  "The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/291006.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/10.1145/291069.291034",
			"excerpts": [
			  "We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy.",
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  },
		  {
			"title": "Dependence based prefetching for linked data structures | Proceedings of the eighth international conference on Architectural support for programming languages and operating systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/291069.291034",
			"excerpts": [
			  "Dependence-based prefetching achieves speedups of up to 25% on a suite of pointer-intensive programs."
			]
		  }
		],
		"reasoning": "The fine-grained field describes a stateless hardware mechanism that scans cache lines to identify values that look like pointers and then prefetches them, potentially following pointer chains recursively. Excerpts that directly name a stateless, content-directed data prefetching mechanism align most closely with this concept, as they explicitly describe hardware-leaning, content-aware prefetch strategies that rely on memory data patterns rather than software state. Quoted descriptions emphasize statelessness and content-directed behavior, which map directly to scanning fetched data for pointer-like values and initiating prefetches without maintaining additional per-access state. Related excerpts that discuss compiler-directed or software-driven prefetching provide useful context about alternative approaches but do not match the hardware-centric, stateless, content-aware nature as closely. Consequently, the most relevant material consists of items that clearly define a stateless, content-directed prefetching mechanism and demonstrate its application to pointer-like data patterns, followed by items that discuss similar but more generalized or software-oriented prefetching approaches, and finally those focused on infrastructural or broader prefetching concepts that touch on related ideas such as data-driven or dynamic prefetch strategies.",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.3.reported_gain_or_caveat",
		"citations": [
		  {
			"title": "Effective jump-pointer prefetching for linked data structures | Proceedings of the 26th annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.1145/300979.300989",
			"excerpts": [
			  "On a suite of pointer intensive programs, jump pointer prefetching reduces memory stall time by 72% for software, 83% for cooperative and 55% for hardware, producing speedups of 15%, 20% and 22% respectively.",
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective jump-pointer prefetching for linked data structures",
			"url": "http://ieeexplore.ieee.org/document/765944/",
			"excerpts": [
			  "This paper describes a framework for jump-pointer prefetching (JPP) that supports four prefetching idioms: queue, full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique."
			]
		  },
		  {
			"title": "Effective Jump-Pointer Prefetching for Linked Data Structures",
			"url": "https://ftp.cs.wisc.edu/sohi/talks/1999/isca99.pdf",
			"excerpts": [
			  "Effective Jump Pointer Prefetching for Linked Data Structures"
			]
		  },
		  {
			"title": "Techniques for Bandwidth-Efficient Prefetching of Linked ...",
			"url": "https://people.inf.ethz.ch/omutlu/pub/bandwidth_lds_hpca09.pdf",
			"excerpts": [
			  "[31] A. Roth and G. S. Sohi. Effective jump-pointer prefetching for linked data\nstructures. In *ISCA-26* , 1999."
			]
		  },
		  {
			"title": "Pointer-Chase Prefetcher for Linked Data Structures",
			"url": "https://arxiv.org/pdf/1801.08088",
			"excerpts": [
			  "A low-cost hardware/software cooperative technique that\nenables bandwidth-efficient prefetching of linked data structures\nhas also been proposed [ 1 ].",
			  "a dynamic scheme\nwhich captures the access patterns of linked data structures and\nalso predict future accesses with high accuracy has been adopted",
			  "A programmable prefetch engine has been used\nto traverse LDS independently from the processor.",
			  "The prefetch\nengine is able to run ahead of the processor to initiate data transfers\nearlier than the processor.",
			  "a mechanism to implement a prefetcher for linked-\nlist type data structures is explored.",
			  "A pointer-chase mechanism\nalong with compiler hints to prefetch the nodes of linked data struc-\nture is adopte",
			  "In [2] the authors have studied memory-side prefetching technique to hide latency incurred by inherently serial accesses to linked data.Read more"
			]
		  },
		  {
			"title": "Helper Thread Prefetching for Loosely-Coupled ...",
			"url": "https://www.cecs.uci.edu/~papers/ipdps06/pdfs/1568974848-IPDPS-paper-1.pdf",
			"excerpts": [
			  "This paper presents a helper thread prefetching scheme that",
			  "is designed to work on loosely-coupled processors, such as in",
			  "a standard chip multiprocessor (CMP) system or an intelligent",
			  "memory system."
			]
		  }
		],
		"reasoning": "The specific field value cites a cooperative software/hardware prefetching model that reduces memory stall time by 83% and yields a 20% overall speedup, as well as a software-only reduction of 72% with a 15% speedup, and notes a caveat about overhead from creating and storing extra pointers. Direct excerpts describe a framework for jump-pointer prefetching with four prefetching idioms and three implementations including a cooperative software/hardware approach, which aligns with the cooperative model and its performance benefits. Additional excerpts quantify the performance improvements on a pointer-intensive workload, giving explicit stall-time reductions and speedups that match the field value figures. A cited source note reinforces that these results trace back to Roth and Sohi 1999, confirming the provenance of the numbers and the cooperative approach. There is also content on related prefetching techniques (pointer-chase, helper-thread and bandwith-oriented prefetching) that provide contextual support for the broader claim that intelligent prefetching can hide memory latency in irregular access patterns, though the core numbers and caveat align most strongly with the jump-pointer cooperative framework and its reported gains. The most relevant parts, therefore, are the explicit cooperative model description, the exact numerical gains, and the identified caveat about extra pointers, which together substantiate the reported field value. The surrounding related works further corroborate the viability and limitations of prefetching in irregular or pointer-based traversals, reinforcing the overall conclusion that such techniques can substantially reduce stalls but may incur metadata or memory overhead. ",
		"confidence": "high"
	  },
	  {
		"field": "speculative_prefetching_and_latency_hiding.6.technique",
		"citations": [
		  {
			"title": "Speculative precomputation: long-range prefetching of delinquentloads | Request PDF",
			"url": "https://www.researchgate.net/publication/3905663_Speculative_precomputation_long-range_prefetching_of_delinquentloads",
			"excerpts": [
			  "Helper threads [12], [13], [15], [16] , [20], [21], [31], [32], [34], [35], [41], [42], [43], [59], [67], [68], [75] extract the backward slice of a delinquent load and run it on a spare thread context. When the latency of the backward slice is less than that of the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache.",
			  "This technique has the advantage of being flexible enough to be implemented in hardware [12], [13], [15], [16] , [20], [21], [23], [43], [59], [67], [75], or software [31], [32], [34], [35], [41], [42], [68]."
			]
		  },
		  {
			"title": "Hardware Support for Prescient Instruction Prefetch",
			"url": "https://people.ece.ubc.ca/aamodt/publications/papers/hw-support-prescientprefetch.hpca10.pdf",
			"excerpts": [
			  " We begin by investigating a straightforward implemen-\ntation of prescient instruction prefetch we call *direct pre-*\n*execution* . During direct pre-execution, instructions from\nthe main threads postfix region are prefetched into the first-\nlevel I-cache by executing those same instructions on a\nspare SMT thread context.",
			  "1.1. Constructing**\n**precomputation**\n**slices.** For\ndi-\nrect pre-execution to correctly resolve postfix branches,\nthe outcome of the backward slice of each postfix branch\nmust be accurately reproduced. This slice may con-\ntain computations from both the infix and postfix regions.\nThus, as shown in Figure 1(b), direct pre-execution con-\nsists of two phases: The first phase, live-in precomputation,\nreproduces the effect of the code skipped over in the in-\nfix region that relates to the resolution of branches in\nthe postfix region. We refer to these precomputation in-\nstructions as the infix slice.",
			  "Similar to speculative pre-\ncomputation [8, 16], infix slices for direct pre-execution\nhelper threads could be encoded as additional instruc-\ntions embedded in a programs binary image. In the sec-\nond phase, the helper thread executes the remaining slice\n ...",
			  "single-threaded application [5], and later proposed hard-\nware mechanisms for dynamically constructing and spawn-\ning subordinate microthreads to predict difficult-path\nbranches [6]. Zilles and Sohi analyzed the dynamic back-\nward slices of performance degrading instructions [30].\nThey subsequently implemented hand crafted specu-\nlative slices to precompute branch outcomes and data\nprefetch addresses [28]. Roth and Sohi [22] proposed us-\ning *data-driven multi-threading* (DDMT) to dynami-\ncally prioritize sequences of operations leading to branches\nthat mispredict or loads that miss. Moshovos *et al.* pro-\nposed *slice processors,* a hardware mechanism for dy-\nnamically constructing and executing slice computations\nfor generating data prefetches [18]. Balasubramonian pro-\nposed a mechanism for allowing a future thread to ad-\nvance ahead of the main thread when a long latency\nD-cache miss stalls the processor [4]. Annavaram *et al.* pro-\nposed dependence graph precomputation [3]. Luk pro-\nposed software controlled pre-execution [17] as a mech-\nanism to prefetch data by executing a future portion of\nthe program. Collins *et al.* proposed speculative precom-\nputation [8], and later dynamic speculative precomputa-\ntion [7] as techniques to leverage spare SMT resources for\ngenerating long range data prefetches and showed the im-\nportance of chaining helper threads to achieve effective\ndata prefetching. Liao *et al.* extended this work by imple-"
			]
		  }
		],
		"reasoning": "The target fine-grained field value describes Runahead Execution, a technique where speculative work is done ahead of the main thread to hide latency, especially by running auxiliary work (helper threads) in advance to prefetch data. The most directly relevant excerpt explains exactly this pattern: helper threads extract the backward slice of a delinquent load and run it on a spare thread context; when the latency of that backward slice is less than the original loop, the helper thread runs ahead of the main thread and prefetches memory accessed by the main thread into the cache. This directly maps to runahead execution where speculative work is executed ahead of the primary computation to mask memory latency by prefetching data. Related discussions in other excerpts describe speculative precomputation, long-range prefetching, and the use of pre-execution slices to predict and prefetch memory accesses; these collectively show a family of techniques aimed at overlapping computation with memory access by moving work ahead of the main path, which aligns with the concept of runahead execution as a broader strategy to hide latency in irregular access patterns. Collectively, these excerpts support the idea that running speculative slices ahead of the main traversal can improve memory bandwidth usage and throughput when access patterns are data-dependent and irregular, as in batched or parallel tree traversal with divergence. The presence of backward-slice analysis, helper-thread prefetching, and precomputation/sliding windows reinforces that runahead-style execution is a viable approach in this domain.",
		"confidence": "medium"
	  }
	],
	"outputSchema": {
	  "type": "object",
	  "properties": {
		"executive_summary": {
		  "type": "string",
		  "description": "A high-level summary of the most effective strategies for optimizing the parallel tree traversal problem, considering the constraints of scattered memory access and limited load slots."
		},
		"prioritized_recommendations": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the recommended optimization technique."
			  },
			  "priority": {
				"type": "string",
				"description": "The priority of the recommendation (e.g., High, Medium, Low)."
			  },
			  "rationale": {
				"type": "string",
				"description": "The reasoning for the assigned priority, balancing impact and complexity."
			  },
			  "target_platform": {
				"type": "string",
				"description": "The primary platform this technique applies to (e.g., CPU, GPU, Both)."
			  }
			},
			"required": [
			  "technique",
			  "priority",
			  "rationale",
			  "target_platform"
			],
			"additionalProperties": false
		  },
		  "description": "A ranked list of the most promising techniques, prioritized by their estimated performance impact versus implementation complexity for the 256-walker, 16-round scenario."
		},
		"initial_coherence_exploitation_strategy": {
		  "type": "object",
		  "properties": {
			"early_round_algorithm": {
			  "type": "string",
			  "description": "The algorithm for processing the first few coherent rounds, such as level-synchronous batching."
			},
			"shared_cache_staging_method": {
			  "type": "string",
			  "description": "How top-level treelets are staged in fast memory (e.g., L3 cache via CAT, GPU shared memory)."
			},
			"switching_criterion": {
			  "type": "string",
			  "description": "The heuristic used to switch from the batched, coherent mode to a divergent, independent mode."
			},
			"coordinated_prefetch_strategy": {
			  "type": "string",
			  "description": "The prefetching strategy used during the coherent phase, such as batch prefetching for the next level."
			}
		  },
		  "required": [
			"early_round_algorithm",
			"shared_cache_staging_method",
			"switching_criterion",
			"coordinated_prefetch_strategy"
		  ],
		  "additionalProperties": false
		},
		"gpu_optimization_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the GPU optimization technique."
			  },
			  "mechanism_summary": {
				"type": "string",
				"description": "A brief explanation of how the technique works."
			  },
			  "primary_benefit": {
				"type": "string",
				"description": "The main advantage of using this technique, such as maintaining throughput or restoring SIMT efficiency."
			  },
			  "reported_performance_gain": {
				"type": "string",
				"description": "Quantified performance improvements cited in research (e.g., '1.5x-2.2x speedup')."
			  }
			},
			"required": [
			  "technique",
			  "mechanism_summary",
			  "primary_benefit",
			  "reported_performance_gain"
			],
			"additionalProperties": false
		  },
		  "description": "A collection of advanced techniques for maintaining SIMD/SIMT efficiency on GPUs despite divergent traversal paths. Includes details on persistent threads, work queues, dynamic warp formation/compaction, and hybrid packet vs. single-walker traversal."
		},
		"cpu_simd_vectorization_strategies": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "strategy": {
				"type": "string",
				"description": "The name of the CPU SIMD vectorization strategy."
			  },
			  "description": {
				"type": "string",
				"description": "A summary of how the strategy vectorizes the traversal."
			  },
			  "required_simd_support": {
				"type": "string",
				"description": "The specific SIMD instruction set features required for efficient implementation (e.g., AVX-512 for native gather)."
			  },
			  "key_implementation_detail": {
				"type": "string",
				"description": "A critical detail for implementing the strategy, such as using masked gathers or lane compaction."
			  }
			},
			"required": [
			  "strategy",
			  "description",
			  "required_simd_support",
			  "key_implementation_detail"
			],
			"additionalProperties": false
		  },
		  "description": "Strategies for vectorizing the 256 walkers on CPUs using SIMD instruction sets like AVX2/AVX-512. Details cover the use of masked gather/scatter operations, lane compaction, and level-synchronous processing."
		},
		"tree_layout_and_structure_optimizations": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "layout_technique": {
				"type": "string",
				"description": "The name of the tree layout or structural optimization."
			  },
			  "description": {
				"type": "string",
				"description": "A summary of how the layout is constructed and how it improves locality."
			  },
			  "layout_category": {
				"type": "string",
				"description": "The category of the layout, such as Cache-Oblivious, Cache-Aware, or SIMD-Focused."
			  },
			  "primary_benefit": {
				"type": "string",
				"description": "The main advantage of using this layout, such as reducing cache misses or enabling SIMD processing."
			  }
			},
			"required": [
			  "layout_technique",
			  "description",
			  "layout_category",
			  "primary_benefit"
			],
			"additionalProperties": false
		  },
		  "description": "An analysis of tree layout transformations that improve cache and memory locality. Includes van Emde Boas (vEB) layouts, cache-oblivious layouts (COLBVH), wide-node transformations (e.g., BVH4/BVH8), and hybrid multi-level layouts."
		},
		"node_packing_and_compression_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the node packing or compression technique."
			  },
			  "description": {
				"type": "string",
				"description": "A summary of how the technique reduces the memory footprint of tree nodes."
			  },
			  "impact_on_memory_traffic": {
				"type": "string",
				"description": "The effect of the technique on memory bandwidth usage."
			  },
			  "tradeoff_or_consideration": {
				"type": "string",
				"description": "Any associated costs, such as computational overhead for decompression."
			  }
			},
			"required": [
			  "technique",
			  "description",
			  "impact_on_memory_traffic",
			  "tradeoff_or_consideration"
			],
			"additionalProperties": false
		  },
		  "description": "Methods to reduce memory traffic by optimizing the node data representation. Covers sibling co-location, Structure-of-Arrays (SoA) layouts, node quantization/compression, and pointer elimination via implicit indexing."
		},
		"speculative_prefetching_and_latency_hiding": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the prefetching or latency hiding technique."
			  },
			  "mechanism": {
				"type": "string",
				"description": "A summary of how the prefetching is performed (e.g., inline software, helper thread, hardware)."
			  },
			  "suitability_for_problem": {
				"type": "string",
				"description": "How well the technique applies to the irregular, pointer-chasing nature of the traversal problem."
			  },
			  "reported_gain_or_caveat": {
				"type": "string",
				"description": "Quantified performance gains or potential downsides like cache pollution."
			  }
			},
			"required": [
			  "technique",
			  "mechanism",
			  "suitability_for_problem",
			  "reported_gain_or_caveat"
			],
			"additionalProperties": false
		  },
		  "description": "A survey of prefetching techniques for irregular, pointer-chasing access patterns. Includes software prefetching (inline, helper-thread), hardware-inspired approaches (correlation, jump-pointers), and GPU-specific methods (many-thread-aware, treelet prefetching)."
		},
		"execution_and_scheduling_models": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "model": {
				"type": "string",
				"description": "The name of the execution or scheduling model."
			  },
			  "core_concept": {
				"type": "string",
				"description": "The fundamental idea behind the model, such as separating access from execution."
			  },
			  "platform_realization": {
				"type": "string",
				"description": "How the model is typically implemented on different platforms (e.g., DSWP on CPU, Warp Specialization on GPU)."
			  },
			  "primary_benefit": {
				"type": "string",
				"description": "The main advantage of using this model, such as hiding memory latency or improving memory access patterns."
			  }
			},
			"required": [
			  "model",
			  "core_concept",
			  "platform_realization",
			  "primary_benefit"
			],
			"additionalProperties": false
		  },
		  "description": "Advanced execution models to manage the workload. Details Decoupled Access/Execute (DAE) designs, software memory request coalescing, and access scheduling policies to batch node fetches across walkers."
		},
		"database_and_hashing_optimizations": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the technique borrowed from database or hashing research."
			  },
			  "source_domain": {
				"type": "string",
				"description": "The original domain of the technique (e.g., Vectorized B-Trees, High-Performance Hash Tables)."
			  },
			  "application_to_traversal_problem": {
				"type": "string",
				"description": "How the technique can be adapted to solve a part of the tree traversal problem."
			  },
			  "key_insight": {
				"type": "string",
				"description": "The core idea that makes the technique effective, such as using branchless SIMD comparisons."
			  }
			},
			"required": [
			  "technique",
			  "source_domain",
			  "application_to_traversal_problem",
			  "key_insight"
			],
			"additionalProperties": false
		  },
		  "description": "Techniques from high-performance database indexes and hash tables applicable to the traversal. Covers vectorized B-tree concepts and SIMD hash table probing methods (e.g., from SwissTable/F14) for the `hash(current_value ^ node_value)` step."
		},
		"hardware_accelerator_inspired_designs": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "hardware_concept": {
				"type": "string",
				"description": "The architectural concept from a hardware accelerator."
			  },
			  "inspiration_source_accelerator": {
				"type": "string",
				"description": "The specific FPGA or ASIC design that inspired the concept (e.g., Emu Chick, Tesseract)."
			  },
			  "software_analogue_description": {
				"type": "string",
				"description": "A description of how the hardware concept can be emulated in software on commodity hardware."
			  },
			  "expected_benefit_in_software": {
				"type": "string",
				"description": "The anticipated performance improvement from implementing the software analogue."
			  }
			},
			"required": [
			  "hardware_concept",
			  "inspiration_source_accelerator",
			  "software_analogue_description",
			  "expected_benefit_in_software"
			],
			"additionalProperties": false
		  },
		  "description": "Software analogues of techniques found in FPGA/ASIC tree traversal implementations. Includes concepts like software-managed MSHRs, emulating multi-bank caches via memory allocation, and explicit prefetch engines."
		},
		"practitioner_and_community_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique": {
				"type": "string",
				"description": "The name of the practical trick or technique."
			  },
			  "source_community": {
				"type": "string",
				"description": "The community where this knowledge originates (e.g., Game Development, Ray Tracing, Database Engineering)."
			  },
			  "description": {
				"type": "string",
				"description": "A brief explanation of the technique."
			  },
			  "reported_gain_or_caveat": {
				"type": "string",
				"description": "Any reported performance impact or important considerations for implementation."
			  }
			},
			"required": [
			  "technique",
			  "source_community",
			  "description",
			  "reported_gain_or_caveat"
			],
			"additionalProperties": false
		  },
		  "description": "A collection of obscure and practical techniques sourced from game developer blogs, ray tracing forums, and database engineering posts. Includes tricks like node hot/cold splitting, partial materialization, and queue coalescing."
		},
		"system_level_and_numa_controls": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "control": {
				"type": "string",
				"description": "The name of the OS or system-level control."
			  },
			  "mechanism_and_usage": {
				"type": "string",
				"description": "How the control works and how to apply it (e.g., using `numactl`, writing to MSRs)."
			  },
			  "primary_benefit_for_traversal": {
				"type": "string",
				"description": "The main reason this control is beneficial for the scattered traversal workload."
			  },
			  "reported_performance_impact": {
				"type": "string",
				"description": "Quantified performance changes observed in research or practice."
			  }
			},
			"required": [
			  "control",
			  "mechanism_and_usage",
			  "primary_benefit_for_traversal",
			  "reported_performance_impact"
			],
			"additionalProperties": false
		  },
		  "description": "OS and system-level controls to reduce latency for scattered memory access. Covers the impact and usage of huge pages, thread pinning on NUMA nodes, page coloring, and toggling hardware prefetchers via MSRs."
		},
		"quantitative_performance_model": {
		  "type": "object",
		  "properties": {
			"model_type": {
			  "type": "string",
			  "description": "The type of performance model used, such as a Roofline or latency-hiding model."
			},
			"cpu_model_parameters": {
			  "type": "string",
			  "description": "Key parameters for the CPU performance model, such as load/store unit limits and MSHR depth."
			},
			"gpu_model_parameters": {
			  "type": "string",
			  "description": "Key parameters for the GPU performance model, such as warp scheduling for latency hiding and memory bandwidth."
			},
			"key_insight_from_model": {
			  "type": "string",
			  "description": "The primary conclusion drawn from the performance model, identifying the main bottlenecks."
			}
		  },
		  "required": [
			"model_type",
			"cpu_model_parameters",
			"gpu_model_parameters",
			"key_insight_from_model"
		  ],
		  "additionalProperties": false
		},
		"cpu_vs_gpu_adaptation_summary": {
		  "type": "string",
		  "description": "A comparative analysis of how the recommended techniques are implemented differently on CPU versus GPU architectures, highlighting the unique strengths and constraints of each platform for this problem."
		},
		"summary_of_risks_and_tradeoffs": {
		  "type": "string",
		  "description": "An overview of the potential risks, overheads, and implementation complexities associated with the most powerful techniques, such as cache pollution from prefetching, sorting overhead from batching, and synchronization costs."
		}
	  },
	  "required": [
		"executive_summary",
		"prioritized_recommendations",
		"initial_coherence_exploitation_strategy",
		"gpu_optimization_techniques",
		"cpu_simd_vectorization_strategies",
		"tree_layout_and_structure_optimizations",
		"node_packing_and_compression_techniques",
		"speculative_prefetching_and_latency_hiding",
		"execution_and_scheduling_models",
		"database_and_hashing_optimizations",
		"hardware_accelerator_inspired_designs",
		"practitioner_and_community_techniques",
		"system_level_and_numa_controls",
		"quantitative_performance_model",
		"cpu_vs_gpu_adaptation_summary",
		"summary_of_risks_and_tradeoffs"
	  ],
	  "additionalProperties": false
	}
  }