{
	"input": "I'm optimizing code for a custom VLIW architecture with these characteristics:\n  - 12 scalar ALU slots, 6 vector ALU slots, 2 load slots, 2 store slots, 1 flow slot per cycle\n  - All effects commit at cycle end (write-after-read hazards don't exist within a cycle)\n  - Loop-carried dependencies across tree traversal iterations\n\n  I need the most obscure and advanced techniques for VLIW instruction scheduling. Search for:\n\n  1. PhD theses and research papers on VLIW software pipelining, especially for loops with irregular memory access patterns or data-dependent\n  control flow\n\n  2. Obscure compiler research from TI C6x DSP, Qualcomm Hexagon DSP, Intel Itanium (IA-64), Transmeta Crusoe, or any VLIW-specific codegen\n  papers\n\n  3. Forum posts, blog posts, or developer notes from embedded DSP engineers discussing manual VLIW scheduling tricks that compilers miss\n\n  4. Research on \"trace scheduling\", \"superblock scheduling\", \"hyperblock formation\", \"modulo scheduling with irregular memory\", or \"predicated\n   execution for VLIW\"\n\n  5. Any techniques for hiding memory latency in VLIW architectures when you have loop-carried dependencies and can't trivially unroll\n\n  6. Academic papers on optimal instruction packing under heterogeneous execution unit constraints (different counts of different functional\n  units)\n\n  7. Niche optimization techniques from the demoscene, competitive programming, or hardware hacking communities for instruction-level\n  parallelism\n\n  8. Research on \"software-managed caches\" or \"scratchpad memory optimization\" specifically for VLIW architectures\n\n  Look particularly for techniques that help when the critical path involves a long chain of dependent operations (like a hash function) that\n  creates a bottleneck preventing full utilization of parallel execution slots. I need ideas for interleaving independent computations to fill\n  empty slots.",
	"output": {
	  "executive_summary": "For the custom VLIW architecture with 12 scalar, 6 vector, 2 load, 2 store, and 1 flow slot, achieving high performance on irregular workloads requires a multi-faceted strategy that combines advanced compiler techniques with architectural awareness. The most effective approach centers on aggressive software pipelining, augmented by specialized methods to handle control flow, memory latency, and long dependency chains.\n\n**1. Handling Irregular Control and Memory Flow:**\n*   **Control Flow:** The most powerful technique for managing data-dependent branches is **Hyperblock Formation with Predicated Execution**, inspired by the Intel Itanium and Qualcomm Hexagon architectures. By converting conditional branches into predicated instructions, the compiler can merge multiple execution paths into a single, large, branch-free scheduling region (a hyperblock). This is critical for rationing the single flow slot and creating a large pool of instructions to fill the 18 available ALU slots. For architectures without hardware predication, **Enhanced Modulo Scheduling (EMS)** offers a robust software-only alternative that outperforms simpler methods like Hierarchical Reduction.\n*   **Irregular Memory (Pointer-Chasing):** To combat the latency of pointer-chasing in tree traversals, **Decoupled Software Pipelining (DSWP)** is a key paradigm. This involves partitioning the loop into an 'access' stage that traverses the data structure and prefetches data into a queue, and an 'execute' stage that consumes from the queue and performs computations. This effectively hides memory latency by converting it into pipeline depth. For hardware-assisted solutions, **Data Speculation** via an Advanced Load Address Table (ALAT), as seen in Itanium, allows loads to be moved before potentially aliasing stores, breaking dependency chains. For software-managed approaches, using a **Scratchpad Memory (SPM) with DMA-driven double-buffering** is a classic and effective DSP technique.\n\n**2. Tackling Long Dependency Chains (e.g., Hashing):**\nWhen the critical path is a long chain of dependent operations (high Recurrence-MII), the only way to utilize the wide VLIW is through **Multi-Stream Interleaving**. By processing multiple independent data streams concurrently within a single software-pipelined loop, the scheduler can interleave instructions from different streams to fill the numerous ALU slots. For cryptographic workloads, **Bit-Slicing** offers a specialized alternative, transforming the problem into a massively parallel one using bitwise logic that can be mapped to either scalar or vector units and provides side-channel resistance.\n\n**3. Maximizing Slot Utilization and Obscure Techniques:**\n*   **Instruction Packing:** The Qualcomm Hexagon compiler's use of a **DFAPacketizer** driven by target-description files (`.td`) provides a blueprint for managing the heterogeneous 12/6/2/2/1 unit mix within a framework like LLVM. This separates scheduling from bundling, allowing for more intelligent, resource-aware instruction ordering.\n*   **Manual & Niche Optimizations:** Insights from DSP engineers and the demoscene highlight the value of **manual loop unrolling**, using compiler pragmas like TI's `MUST_ITERATE` to guide the scheduler, and alignment assertions (`_nassert`) to enable wider memory operations. Branchless logic using **bitwise 'tricks'** is a powerful way to eliminate control flow and create parallelizable work for the scalar ALUs.\n*   **Repurposing Advanced Research:** An obscure but potent technique involves repurposing concepts from **fault-tolerant VLIW research**. Dynamic instruction replication, originally for error checking, can be adapted to speculatively execute code in idle functional unit slots to hide latency or eliminate inter-cluster communication bottlenecks, with studies showing average IPC gains of 25% for the latter.",
	  "vliw_scheduling_playbook": "This playbook provides a step-by-step decision tree for scheduling code on the custom VLIW architecture (12 scalar, 6 vector, 2 load, 2 store, 1 flow), focusing on maximizing slot utilization for loops with irregular memory access and long dependency chains.\n\n### **Step 1: Loop Analysis & Goal Setting**\n\n1.  **Identify Hot Loops:** Use profiling to find the most performance-critical loops.\n2.  **Analyze Dependencies:** For each hot loop, construct a Data Dependence Graph (DDG) to identify all loop-carried dependencies (recurrences). These define the critical path.\n3.  **Calculate Minimum Initiation Interval (MII):** The MII is the theoretical best-case number of cycles per iteration. It is the maximum of two bounds:\n    *   **Resource-Constrained MII (ResMII):** `max(ceil(N_scalar_ops / 12), ceil(N_vector_ops / 6), ceil(N_load_ops / 2), ceil(N_store_ops / 2), ceil(N_flow_ops / 1))`\n    *   **Recurrence-Constrained MII (RecMII):** The sum of latencies along the longest loop-carried dependency cycle.\n4.  **Set Goal:** The primary goal is to achieve an actual Initiation Interval (II) as close to the MII as possible. The ratio `MII / II` reflects the efficiency of the schedule.\n\n### **Step 2: Control Flow Transformation Strategy**\n\n*   **Question:** Does the loop contain data-dependent conditional branches (`if/else`)?\n    *   **YES (Default Strategy) -> Use Predication & Hyperblock Formation:** This is the most effective strategy to manage the single flow slot.\n        *   **Action:** Apply if-conversion to transform conditional branches into predicated instructions. Group these predicated blocks into a single-entry, multiple-exit **hyperblock**.\n        *   **Benefit:** This creates a large, linear block of code, exposing maximum Instruction-Level Parallelism (ILP) to the scheduler and eliminating branch misprediction penalties.\n    *   **NO (or highly biased branches) -> Use Superblock Scheduling:**\n        *   **Action:** Identify the most frequent execution path (trace). Use **tail duplication** to eliminate side-entries, forming a single-entry **superblock**.\n        *   **Benefit:** Simpler to manage than full trace scheduling, but less powerful than hyperblocks for complex control flow.\n\n### **Step 3: Memory Access & Latency Hiding Strategy**\n\n*   **Question:** What is the loop's memory access pattern?\n    *   **Case A: Irregular Pointer-Chasing (e.g., Tree Traversal, Linked List)**\n        *   **Top-Tier Strategy -> Decoupled Software Pipelining (DSWP):** Partition the loop into two logical stages communicating via a software queue (in scratchpad or registers).\n            *   **Stage 1 (Access):** Traverses the data structure, performs pointer-chasing loads, and pushes data/pointers into the queue. This stage primarily uses the 2 load slots.\n            *   **Stage 2 (Execute):** Consumes from the queue and performs computation using the 18 ALU slots.\n            *   **Benefit:** Decouples memory latency from computation, allowing the access stage to run far ahead and keep the execute stage fed with data.\n        *   **Hardware-Assisted Alternative -> Data Speculation:** If the architecture supports it (like Itanium's ALAT), use speculative loads (`ld.a`) to hoist loads above potentially aliasing stores. Place a check instruction (`chk.a`) at the original load position to validate the speculation and trigger recovery if needed.\n    *   **Case B: Regular Streaming/Strided Access**\n        *   **Action:** Use a **Software-Managed Scratchpad Memory (SPM) with DMA and Double-Buffering**.\n            *   Initiate a DMA transfer to fill `buffer_A` in the SPM.\n            *   While the DMA runs, the CPU processes data in `buffer_B`.\n            *   Once the DMA to `buffer_A` is complete, initiate a new DMA to fill `buffer_B` and have the CPU switch to processing `buffer_A`.\n            *   **Benefit:** Completely overlaps memory transfer latency with computation.\n\n### **Step 4: Core Software Pipelining Algorithm**\n\n*   **Question:** Which modulo scheduling algorithm to use?\n    *   **Primary Choice -> Swing Modulo Scheduling (SMS):** This heuristic algorithm is designed to be sensitive to register lifetimes from the start. It produces near-optimal schedules for both II and register usage, making it the best default choice.\n    *   **Alternative -> Iterative Modulo Scheduling (IMS):** A more robust, backtracking algorithm. Use this if SMS fails or for very complex recurrence constraints. It will find a valid schedule by incrementally increasing the II if necessary.\n\n### **Step 5: Handling Long Dependency Chains (High RecMII)**\n\n*   **Question:** Is the schedule bottlenecked by the critical path (`RecMII > ResMII`)?\n    *   **YES -> Use Multi-Stream Interleaving:** This is the only way to fill the 18 ALU slots when a single stream of work is inherently sequential.\n        *   **Action:** Restructure the problem to process N independent data streams concurrently within a single software-pipelined loop. The scheduler will interleave instructions from all N streams into each VLIW bundle.\n        *   **Interleave Depth Calculation:** To saturate the scalar ALUs, the required interleave depth is `ceil((12 ALUs * RecMII) / N_scalar_ops_per_iteration)`.\n        *   **Specialized Alternative:** For cryptographic workloads, consider **Bit-Slicing**, which processes many streams in parallel using bitwise logic on scalar registers.\n\n### **Step 6: Register Pressure Management**\n\n*   **Question:** Does the generated schedule exceed the available physical registers?\n    *   **YES -> Apply Corrective Measures in Order of Preference:**\n        1.  **Rotating Registers (Hardware):** If the architecture supports it, this is the most efficient solution. It automatically renames registers for each loop iteration, eliminating the need for code changes.\n        2.  **Modulo Variable Expansion (MVE) (Software):** Unroll the steady-state kernel of the pipelined loop and explicitly rename variables to give each live value its own register. This increases code size but is a powerful software-only fix.\n        3.  **Stage Scheduling (Post-Pass):** Apply this post-pass optimization to shift non-critical instructions in the schedule, which can reduce variable lifetimes and register requirements without changing the II.\n        4.  **Increase II (Last Resort):** If all else fails, the scheduler must increase the II. This reduces the number of overlapping iterations, lowering register pressure at the direct cost of throughput.\n\n### **Step 7: Final Checks, Metrics, and Heuristics**\n\n*   **Metrics to Monitor:**\n    *   **Initiation Interval (II):** Compare achieved II to the calculated MII.\n    *   **Slot Occupancy:** Measure the percentage of the 23 available slots used in the loop kernel.\n    *   **Spill Code:** The number of loads/stores added by the register allocator. This should be zero for high performance.\n*   **Key Heuristics (Do's and Don'ts):**\n    *   **DO** use compiler pragmas and keywords to provide more information (e.g., `restrict` for pointer aliasing, `_nassert` for alignment to enable wider loads, `MUST_ITERATE` for loop trip counts, inspired by TI C6x).\n    *   **DO** analyze the compiler's assembly output to understand its scheduling decisions, resource conflicts, and dependency chains.\n    *   **DON'T** place function calls inside a performance-critical loop that you intend to software pipeline.\n    *   **DON'T** ignore compiler warnings or advice about potential performance inhibitors.",
	  "control_flow_optimization_techniques": [
		{
		  "technique_name": "Trace Scheduling",
		  "description": "A global compaction technique that increases Instruction-Level Parallelism (ILP) by optimizing across basic block boundaries. It identifies a 'trace'—a frequent, statically predicted path of execution—and treats it as a single, long sequence of straight-line code for scheduling, allowing instructions to be moved past their original basic block boundaries.",
		  "primary_mechanism": "The compiler identifies a frequent execution path (the trace) and schedules it as a single unit. To maintain program correctness when instructions are moved past control flow splits or joins, the compiler must insert 'compensation code' on the off-trace paths to handle control flow that deviates from the predicted path.",
		  "key_trade_offs": "The primary trade-off is the complexity and potential for code bloat. The compensation code required at every split and join along a trace can be difficult to generate and can lead to potentially exponential code growth. Its effectiveness is also limited to programs where a dominant, predictable path can be identified."
		},
		{
		  "technique_name": "Superblock Scheduling",
		  "description": "A refinement of trace scheduling designed to simplify compiler bookkeeping and code generation. A superblock is a trace with a single entry point but multiple possible exit points, which makes it a more structured and manageable unit for scheduling.",
		  "primary_mechanism": "The core mechanism is 'tail duplication'. To create the single-entry property, all side-entries (joins) into the middle of the trace are eliminated by duplicating the code from the join point to the end of the trace for each side entry. This generates the maximum required compensation code upfront.",
		  "key_trade_offs": "While simpler to manage than trace scheduling, it often results in substantial code bloat due to its 'worst-case bookkeeping' approach of tail duplication. It is less effective for highly irregular control flow with many join points, where the cost of tail duplication becomes prohibitively expensive."
		},
		{
		  "technique_name": "Hyperblock Formation",
		  "description": "An advanced technique for handling irregular control flow by creating a single scheduling region that contains instructions from multiple execution paths. A hyperblock is a single-entry, multiple-exit sequence of predicated instructions, designed to expose a large number of instructions from different control paths to the scheduler simultaneously.",
		  "primary_mechanism": "The primary mechanism is 'if-conversion' using predicated execution. The compiler selects multiple important execution paths and merges them into a single, larger scheduling region by converting the conditional branches that separate them into predicated instructions. This transforms control dependencies into data dependencies.",
		  "key_trade_offs": "The fundamental requirement is explicit hardware support for predicated execution, including a set of predicate registers, making it less portable than software-only techniques. It can increase 'predicate pressure' (the number of live predicates required). While it eliminates branch misprediction penalties, it consumes execution slots and power for instructions on paths that are ultimately nullified."
		},
		{
		  "technique_name": "Predicated Execution (If-Conversion)",
		  "description": "An architectural feature and compiler transformation that allows an instruction's execution to be conditional on a boolean source operand, known as a predicate. This is used to implement 'if-conversion', which transforms control dependencies (from conditional branches) into data dependencies, resulting in straight-line code that is much easier for a VLIW scheduler to optimize.",
		  "primary_mechanism": "A conditional branch is replaced with a compare instruction that sets one or more predicate registers based on the condition. Subsequent instructions that were on the original control paths are then 'guarded' by these predicate registers. The hardware only executes or commits the results of an instruction if its governing predicate is true.",
		  "key_trade_offs": "Requires dedicated hardware support (predicate registers), which impacts portability. It eliminates branch misprediction penalties but can saturate machine resources by executing instructions from multiple paths, even those whose results are discarded. The decision to predicate involves a complex trade-off based on branch predictability, misprediction penalty, and resource availability. Aggressive if-conversion can also lead to high predicate pressure, potentially exceeding the number of available predicate registers."
		}
	  ],
	  "advanced_software_pipelining_methods": [
		{
		  "method_name": "Decoupled Software Pipelining (DSWP)",
		  "core_principle": "DSWP automatically partitions a single sequential loop into multiple pipeline stages that can be executed as separate threads. These stages communicate through explicit producer-consumer queues. The partitioning is typically based on identifying Strongly Connected Components (SCCs) in the loop's dependence graph to break cyclic dependencies.",
		  "primary_use_case": "Handling loops with complex loop-carried dependencies, particularly pointer-chasing recurrences found in irregular applications like linked-list or tree traversals, where traditional loop transformations are ineffective.",
		  "key_benefit": "It effectively hides memory latency by decoupling the memory access part of a loop from the computation part. The 'access' thread can run far ahead of the 'execute' thread, filling the queue with data and ensuring the compute units are never stalled waiting for memory. This converts memory latency into pipeline depth."
		},
		{
		  "method_name": "Coarse-Grained Pipelined Accelerators (CGPA)",
		  "core_principle": "Extends the principles of DSWP, particularly the Parallel-Stage DSWP (PS-DSWP) partitioning algorithm, to the domain of High-Level Synthesis (HLS) for designing hardware accelerators. It focuses on creating coarse-grained pipeline parallelism while being selective about which code sections are replicated to minimize hardware cost and communication overhead.",
		  "primary_use_case": "Designing pipelined hardware accelerators for loops with irregular memory accesses and imperfect loop nests. It is particularly concerned with resource efficiency in hardware.",
		  "key_benefit": "Provides a systematic framework for applying DSWP-like parallelism in hardware design. Its key benefit is resource optimization, as it explicitly avoids duplicating 'heavyweight' sections containing load and multiply instructions in parallel stages, which is critical for minimizing hardware area and power."
		},
		{
		  "method_name": "Enhanced Modulo Scheduling (EMS)",
		  "core_principle": "EMS is a software-only technique that combines the benefits of if-conversion and hierarchical reduction. It first uses if-conversion to transform a loop with conditional branches into a straight-line predicated body, performs modulo scheduling on this body, and then regenerates the explicit control flow in the final schedule.",
		  "primary_use_case": "Software pipelining of loops containing conditional branches on VLIW architectures that do *not* have hardware support for predicated execution.",
		  "key_benefit": "It achieves better performance (approximately 18% better in studies) than simpler techniques like Hierarchical Reduction because it avoids the need for prescheduling conditional constructs and uses a more optimal resource model than a simple union of all paths. It provides the scheduling benefits of if-conversion without the hardware cost."
		},
		{
		  "method_name": "Hierarchical Reduction",
		  "core_principle": "A software-only technique that enables software pipelining of loops with conditionals. It works by recursively reducing an entire control construct (e.g., an if-then-else block) into a single pseudo-operation. This pseudo-operation has a resource usage pattern that is the union of the resources of all its internal paths and a length equal to the maximum length of any path.",
		  "primary_use_case": "A foundational method for applying software pipelining to loops with conditional statements on any VLIW machine, regardless of hardware support for predication.",
		  "key_benefit": "Allows software pipelining to be applied to all innermost loops, not just those with straight-line code, using a purely software-based approach. It is highly portable but often produces suboptimal schedules compared to more advanced techniques like EMS."
		}
	  ],
	  "memory_latency_hiding_techniques": [
		{
		  "technique_name": "Itanium Data Speculation (Advanced Load Address Table - ALAT)",
		  "mechanism_summary": "A hardware-assisted mechanism where the compiler issues a speculative 'advanced load' (`ld.a`) before a potentially aliasing store. The load's address is recorded in a hardware structure called the ALAT. Subsequent stores check the ALAT and invalidate the entry if their address matches. A 'check' instruction (`chk.a` or `ld.c`) later verifies if the entry is still valid. If it was invalidated, recovery code is executed to re-issue the load, otherwise the speculation was successful, effectively hiding the load's latency.",
		  "implementation_type": "Hardware-Software Co-design",
		  "reported_performance": "Yielded performance gains of 1% to 7% on SPEC benchmarks. The cost of a failed speculation on Itanium 2 was significant, ranging from ~8 cycles for a `ld.c` miss to ~18+ cycles for a `chk.a` miss."
		},
		{
		  "technique_name": "Itanium Control Speculation (NaT Bit)",
		  "mechanism_summary": "A hardware-assisted mechanism allowing a load to be moved above a preceding branch. The speculative load (`ld.s`) defers exceptions. If an exception occurs, a special 'Not a Thing' (NaT) bit is set on the destination register. A check instruction (`chk.s`) placed at the original load location later tests this bit and branches to recovery code if it's set. This hides latency by allowing loads to execute on likely-taken paths before the branch resolves.",
		  "implementation_type": "Hardware-Software Co-design",
		  "reported_performance": "Part of the suite of EPIC features that contributed to Itanium 2 achieving an average of 2.63 useful operations per cycle on SPECint2000 benchmarks. The penalty for a `chk.s` failure could be 50+ cycles on the original Itanium."
		},
		{
		  "technique_name": "Decoupled Software Pipelining (DSWP)",
		  "mechanism_summary": "A compiler technique that partitions a loop into multiple pipeline stages (e.g., an 'access' stage and an 'execute' stage) that communicate via software-managed FIFO queues. For pointer-chasing, the access stage traverses the data structure and pushes data/pointers into the queue, while the execute stage consumes from the queue and performs computations. This decouples memory access from execution, allowing the access stage to run far ahead and hide memory latency.",
		  "implementation_type": "Software-only (Compiler Technique)",
		  "reported_performance": "DSWP is a foundational technique for parallelizing irregular loops. Its performance gain is highly dependent on the workload and the depth of the software queue, which determines the amount of latency that can be tolerated."
		},
		{
		  "technique_name": "Prodigy (Data Indirection Graph-guided Prefetcher)",
		  "mechanism_summary": "A hardware-software co-design where the compiler analyzes memory access patterns and generates a Data Indirection Graph (DIG). This DIG is passed to a hardware prefetcher, enabling informed prefetching for complex, irregular workloads, including support for 'ranged indirection' critical for graph algorithms.",
		  "implementation_type": "Hardware-Software Co-design",
		  "reported_performance": "Achieved a 2.6x speedup and 1.6x energy savings over a non-prefetching baseline. It converted 85.1% of prefetchable LLC misses into hits and outperformed other prefetchers like IMP (2.3x faster)."
		},
		{
		  "technique_name": "IMP (Indirect Memory Prefetcher)",
		  "mechanism_summary": "A hardware-based prefetcher that dynamically detects and prefetches for indirect memory patterns like `A[B[i]]`. It identifies both the index stream (accesses to B) and the data stream (accesses to A). It also supports partial cacheline fetching to reduce memory bandwidth pressure.",
		  "implementation_type": "Hardware-assisted",
		  "reported_performance": "On a 64-core system, IMP achieved an average speedup of 56% (up to 2.3x) over a baseline with streaming prefetchers. It increased prefetch coverage from 28% to 86% with over 95% accuracy for many applications."
		},
		{
		  "technique_name": "Software Prefetching (Ainsworth & Jones)",
		  "mechanism_summary": "A pure software approach implemented as a compiler pass. It automatically generates prefetch instructions for indirect memory accesses, often using loop splitting and software pipelining to schedule the prefetches 'k' iterations ahead, where 'k' is calculated to hide the expected memory latency.",
		  "implementation_type": "Software-only (Compiler Technique)",
		  "reported_performance": "Speedups are highly architecture-dependent: 1.3x on Intel Haswell (out-of-order) but up to 2.7x on Intel Xeon Phi (in-order), demonstrating its effectiveness for simpler cores."
		},
		{
		  "technique_name": "SQRL (Specialized Data Structure Fetcher)",
		  "mechanism_summary": "A hardware accelerator specifically designed for hash tables and tree data structures. It acts as a decoupled access engine, integrating a 'Collector' unit with the Last-Level Cache (LLC) that runs ahead of processing elements, gathering data objects and exploiting knowledge of the data structure to find memory-level parallelism.",
		  "implementation_type": "Hardware Accelerator",
		  "reported_performance": "Achieved massive performance improvements of 13x to 121x for the kernel portion of relevant benchmarks."
		},
		{
		  "technique_name": "Transmeta Crusoe Hardware Rollback",
		  "mechanism_summary": "A hardware-assisted recovery mechanism for aggressive speculation. The VLIW core operated on 'working' registers, while 'shadow' registers held the last known-good state. Stores were buffered. On a speculation failure, a `rollback` operation instantly restored the shadow state to the working state and discarded the store buffer, providing a very low-overhead, coarse-grained squash mechanism.",
		  "implementation_type": "Hardware-assisted",
		  "reported_performance": "This mechanism was fundamental to the Code Morphing Software's ability to perform aggressive dynamic translation and optimization of x86 code, enabling high performance on a low-power VLIW core."
		}
	  ],
	  "long_dependency_chain_strategies": [
		{
		  "strategy_name": "Multi-Stream Interleaving",
		  "description": "Instead of processing a single data stream with a long dependency chain, this technique processes multiple independent data streams concurrently. Using software pipelining (modulo scheduling), the VLIW core executes different stages of the computation for different streams in the same cycle. For example, cycle 5 of stream N can execute in parallel with cycle 3 of stream N+1 and a load for stream N+2.",
		  "primary_application": "Hash functions (e.g., SHA-256), ciphers, any dependency-bound algorithm where multiple independent inputs can be processed in parallel.",
		  "impact_on_vliw_units": "This is the most effective strategy for saturating the 12 scalar and 6 vector ALUs. By interleaving enough streams (e.g., 5-6), the scheduler can find sufficient independent ALU operations to fill all available slots each cycle, transforming a latency-bound problem into a throughput-bound one."
		},
		{
		  "strategy_name": "Bit-Slicing",
		  "description": "A data-reorganization technique where data is transposed so that each native register (e.g., 64-bit) holds a single bit from multiple independent variables (e.g., the 5th bit of 64 different data blocks). All subsequent computations are performed using simple, parallel bitwise logical operations (AND, OR, XOR) across these registers.",
		  "primary_application": "Cryptography (e.g., AES, DES), finite field arithmetic, and other algorithms that can be expressed as a boolean circuit.",
		  "impact_on_vliw_units": "Massively increases ILP. A single 64-bit scalar ALU operation can perform the same logical step on 64 independent data streams at once. This allows the 12 scalar ALUs to process 768 streams in parallel per instruction. If the vector units support wide bitwise logic, they can be used for even greater throughput. It also provides constant-time execution, offering side-channel resistance."
		},
		{
		  "strategy_name": "Algebraic Transformations",
		  "description": "Leverages mathematical properties like associativity and commutativity to restructure computations. For example, a sequential reduction `sum = sum + val[i]` has a loop-carried dependency. This can be transformed into a parallel tree-based reduction, where pairs of elements are summed in parallel, then pairs of results are summed, and so on, reducing the dependency chain length from O(N) to O(log N).",
		  "primary_application": "Reductions (summation, min/max), hash combining functions (e.g., BLAKE3), parallel prefix sums.",
		  "impact_on_vliw_units": "This transformation exposes parallelism by creating many independent sub-problems (e.g., the initial pairwise sums) that can be distributed across the 12 scalar or 6 vector ALUs, significantly shortening the critical path."
		},
		{
		  "strategy_name": "SWAR (SIMD Within A Register)",
		  "description": "A micro-optimization technique that packs multiple smaller data items into a single native-width register (e.g., four 8-bit values into a 32-bit register) and performs a single arithmetic or logical operation on all of them simultaneously. This requires careful use of bitmasks to prevent overflow between the packed data items.",
		  "primary_application": "Graphics (pixel processing), population count, image processing, any algorithm operating on small integer data types.",
		  "impact_on_vliw_units": "Generates data parallelism at the sub-register level, which can be exploited by the 12 scalar ALUs. A single scalar instruction effectively performs multiple operations, increasing throughput without needing dedicated vector hardware. It provides a source of independent work to fill idle ALU slots."
		},
		{
		  "strategy_name": "Branchless Control Flow",
		  "description": "Replaces conditional branches (`if-then-else`) with sequences of arithmetic and logical operations that produce the same result. For example, `min = y ^ ((x ^ y) & -(x < y))` computes the minimum of two integers without a branch. This converts control dependencies into data dependencies.",
		  "primary_application": "General purpose code, algorithms with frequent, data-dependent conditional logic.",
		  "impact_on_vliw_units": "Directly addresses the bottleneck of the single flow slot by eliminating branches. The conditional logic is transformed into ALU operations that can be scheduled in parallel on the 12 scalar ALUs, increasing ILP and preventing pipeline stalls from branch mispredictions."
		}
	  ],
	  "register_pressure_management_techniques": [
		{
		  "technique_name": "Rotating Register Files",
		  "type": "Hardware Support",
		  "mechanism": "A hardware mechanism where a subset of architectural registers are designated as 'rotating'. The effective register address is automatically modified each loop iteration by a hardware counter (e.g., Iteration Register). This provides each new overlapping loop iteration with a fresh set of physical registers, automatically renaming variables and breaking false dependencies without compiler intervention. This was a key feature of architectures like the Intel Itanium (IA-64) and Cydra 5.",
		  "main_trade_off": "Requires dedicated and complex architectural support in the processor's register file and instruction set. It is not a software-only solution and must be designed into the hardware from the beginning."
		},
		{
		  "technique_name": "Modulo Variable Expansion (MVE)",
		  "type": "Software-only Compiler Optimization",
		  "mechanism": "A software technique that emulates rotating registers. For a variable whose lifetime spans across loop iterations, the compiler allocates multiple physical registers. The steady-state of the software-pipelined loop is then unrolled by a factor 'k', and in each unrolled instance, a different physical register is used for the variable. This breaks the write-after-read dependencies across iterations, allowing for a smaller Initiation Interval (II).",
		  "main_trade_off": "Increases static code size due to the required unrolling of the loop kernel. The unroll factor 'k' can be large if variable lifetimes are long relative to the II, leading to significant code bloat."
		},
		{
		  "technique_name": "Swing Modulo Scheduling (SMS)",
		  "type": "Scheduling Heuristic",
		  "mechanism": "A lifetime-sensitive scheduling algorithm that considers register pressure during the scheduling process, rather than as a post-pass. It uses sophisticated node-ordering heuristics to prioritize instructions, attempting to schedule dependent operations close together in the schedule. This shortens the live ranges of the values they produce, directly reducing the number of simultaneously live registers.",
		  "main_trade_off": "While highly effective and near-optimal, it is a heuristic and may not find the absolute minimum register requirement in all cases. It is more complex to implement than a simple list scheduler."
		},
		{
		  "technique_name": "Stage Scheduling",
		  "type": "Software-only Compiler Optimization",
		  "mechanism": "A post-processing optimization applied after an initial modulo schedule has been generated. It reduces register requirements by shifting operations that are not on the critical recurrence path by multiples of the Initiation Interval (II). This reassignment to different pipeline stages shortens variable lifetimes without altering the schedule's II or resource usage. It has been shown to achieve significant register requirement reductions (e.g., 24.6% on average).",
		  "main_trade_off": "It is a post-pass optimization, so its effectiveness is limited by the initial schedule it receives. It refines an existing schedule rather than generating one from scratch with register pressure as a primary objective."
		},
		{
		  "technique_name": "Live-Range Splitting",
		  "type": "Software-only Compiler Optimization",
		  "mechanism": "A register allocation technique that divides the live range of a single long-lived variable into multiple, smaller, disjoint subranges. Each subrange can then be assigned to a different physical register or be spilled to memory independently. This can lead to more efficient spilling decisions, as only the necessary subranges are spilled instead of the entire variable's lifetime.",
		  "main_trade_off": "Can increase the complexity of the interference graph used by the register allocator, which can sometimes make the overall allocation problem more difficult. It also introduces register-to-register copy instructions at the split points."
		},
		{
		  "technique_name": "Register-Aware Pre-RA Scheduling",
		  "type": "Scheduling Heuristic",
		  "mechanism": "A scheduling strategy, used in compilers like the one for Qualcomm Hexagon, that runs before register allocation. It uses a simulation of the final instruction bundling (packetization) to get a more accurate estimate of the final cycle count. This foresight allows it to manage register lifetimes more effectively, shaping the instruction schedule to reduce the peak number of live registers and thereby minimize the likelihood of the register allocator needing to insert costly spill code.",
		  "main_trade_off": "This heuristic is more complex than a simple list scheduler as it requires a model of the target's bundling constraints and a more sophisticated cost model that balances schedule length with register pressure."
		},
		{
		  "technique_name": "Increasing the Initiation Interval (II)",
		  "type": "Scheduling Heuristic (Fallback)",
		  "mechanism": "This is a last-resort strategy. If the register pressure for a given schedule at a target Initiation Interval (II) is too high and cannot be resolved by other means, the scheduler will intentionally increase the II. A larger II means fewer loop iterations are active simultaneously, which directly reduces the number of overlapping live variables and thus lowers register pressure.",
		  "main_trade_off": "Directly degrades performance. A larger II means lower throughput, as a new loop iteration is started less frequently. This trades runtime performance for the ability to successfully allocate registers."
		}
	  ],
	  "software_managed_memory_strategies": {
		"core_concept": "The fundamental idea is to use a Scratch-Pad Memory (SPM), which is a small, fast, on-chip SRAM that is mapped into the processor's address space but is not managed by hardware cache controllers. The compiler or programmer takes explicit control over data movement between main memory and the SPM. This provides predictable, low-latency access to critical data and code, bypassing the complexities and unpredictability of hardware caches (e.g., conflict misses, coherence traffic) and consuming less energy.",
		"key_technique": "The primary technique for hiding memory latency with SPM is **Double-Buffering** (also known as ping-pong buffering). This software pipelining method involves allocating two buffers in the SPM ('ping' and 'pong'). A Direct Memory Access (DMA) controller is used to fill one buffer (e.g., 'pong') with the data needed for the next phase of computation, while the VLIW's CPU cores are simultaneously processing the data already present in the other buffer ('ping'). This overlaps the long-latency data transfer from main memory with the computation, ensuring the CPU rarely has to wait for data.",
		"allocation_algorithms": "Deciding which data to place in the SPM is a critical compiler task. Several algorithms exist: \n1. **Static Allocation:** The compiler decides at compile-time which data objects (e.g., frequently used global variables) will reside in the SPM for the entire program execution. This is simple but inflexible. \n2. **Dynamic Allocation:** A more advanced approach where the compiler inserts explicit data transfer instructions (e.g., DMA commands) to move data in and out of the SPM at different program phases. This is more efficient and can adapt to changing data requirements. Research by Udayakumaran and Barua developed a highly predictable, low-overhead dynamic allocation strategy suitable for irregular code. \n3. **Profile-Guided Allocation:** Uses profile data from previous program runs to identify the most frequently accessed ('hot') data, allowing the compiler to make more intelligent dynamic allocation decisions. \n4. **Memory Coloring:** An advanced compiler technique that adapts graph-coloring algorithms (traditionally used for register allocation) to the problem of SPM allocation for arrays and structs, treating the SPM as a 'pseudo register file'.",
		"dma_scheduling_pattern": "The typical pattern involves the CPU offloading bulk data transfers to a DMA engine. The sequence is: \n1. **Initiation:** The CPU configures the DMA controller with the source address (main memory), destination address (SPM buffer), and transfer size, then initiates the transfer. \n2. **Parallel Execution:** The DMA engine takes over the memory bus to perform the transfer in the background. The VLIW core is now free to execute its computational kernel, using its load/store units to access data already in SPM or registers, keeping the scalar and vector ALUs busy. \n3. **Synchronization:** Upon completion, the DMA controller signals the CPU, typically via an interrupt or by setting a status flag that the CPU can poll. For continuous streaming, advanced DMA controllers like the TI C6000's EDMA can use linked parameter sets (PaRAMs) to automatically start the next transfer in a chain without any CPU intervention, maximizing the overlap between computation and data movement."
	  },
	  "instruction_packing_algorithms_and_heuristics": {
		"problem_analogy": "The VLIW instruction packing and scheduling problem is a classic analog of the Resource-Constrained Project Scheduling Problem (RCPSP). In this analogy, the instructions are the 'tasks' to be scheduled, the data and control dependencies form the 'precedence constraints', and the processor's heterogeneous functional units (12 scalar ALUs, 6 vector ALUs, 2 load, 2 store, 1 flow) are the 'renewable resources' with fixed capacities. The goal is to find a schedule that minimizes the total duration (makespan) while respecting all constraints. The problem can also be viewed as a form of Multi-dimensional Bin Packing, where the 'items' are the instructions and the 'bins' are the VLIW instruction words (or clock cycles), with each functional unit type representing a different dimension of the bin's capacity.",
		"exact_methods": "Exact methods guarantee a provably optimal solution but are computationally expensive. The primary methods are: 1) Integer Linear Programming (ILP), which formulates the problem as a set of integer variables (e.g., `x_ic` = 1 if instruction `i` is in cycle `c`), linear constraints for dependencies and resources, and a linear objective function to minimize schedule length. Research has shown this can optimally solve for blocks of up to 1000 instructions. 2) Constraint Programming (CP), which defines variables with domains (e.g., instruction start times) and uses powerful global constraints (like the `cumulative` constraint) to model resource limits. CP has been shown to scale to even larger basic blocks and superblocks (up to 2600 instructions). 3) Satisfiability Modulo Theories (SMT), which encode the scheduling problem as a set of logical formulas and use a solver to find a satisfying assignment, which corresponds to a valid schedule.",
		"heuristic_methods": "Heuristic methods are fast but suboptimal and are used in most production compilers. The most common is Greedy List Scheduling, which creates a priority-ordered list of ready instructions (e.g., based on critical path) and greedily schedules them cycle by cycle. More advanced heuristics include: 1) The DFAPacketizer approach (used in Qualcomm Hexagon's LLVM backend), which uses a Deterministic Finite Automaton (DFA) generated from a target description file to model resource constraints. The scheduler simulates bundling with the DFA to make more intelligent, resource-aware decisions. 2) Machine Learning-based approaches, such as using Deep Reinforcement Learning (DRL) to train an agent to select optimal instruction bundles, or using supervised learning to mimic the behavior of an optimal offline solver. 3) Two-Dimensional Constrained Dynamic Programming (TDCDP), a novel algorithmic method that integrates time-slot and functional-unit assignment to find near-optimal solutions.",
		"compile_time_trade_off": "There is a fundamental trade-off between the time it takes to compile the code and the runtime performance of the resulting executable. Exact methods like ILP and CP can produce significantly faster code (e.g., up to 52% fewer cycles than heuristics) but can have exponential worst-case compile times, making them slow. Heuristic methods like list scheduling are very fast to compile but produce code that may be substantially slower than optimal. This trade-off is often managed with hybrid approaches, where a compiler uses fast heuristics for general code and reserves the slow, optimal solvers for small, performance-critical inner loops. For domains like DSP, embedded systems, and HPC, the high compile-time cost of exact methods is often considered an acceptable price for achieving maximum runtime performance."
	  },
	  "platform_specific_compiler_insights": [
		{
		  "platform_name": "Texas Instruments C6x DSP (e.g., TMS320C6000)",
		  "key_architectural_feature": "An aggressive software pipelining compiler that relies heavily on programmer-provided hints (pragmas, intrinsics) to achieve optimal performance on its partitioned VLIW architecture. The compiler provides extensive diagnostic feedback to guide manual optimization.",
		  "specific_techniques": "1. **Pragmas for Loop Information:** Using `#pragma MUST_ITERATE(min, max, multiple)` to inform the compiler about a loop's trip count. This enables more aggressive loop unrolling, which is critical for balancing resource usage across functional units and reducing the resource-bound initiation interval (II). 2. **Alignment Assertions:** Using the `_nassert((int)ptr % 8 == 0);` intrinsic to guarantee pointer alignment. This allows the compiler to generate wider, more efficient memory operations like `LDDW` (Load Double Word), which can halve the pressure on the load/store units. 3. **Alias Disambiguation:** Employing the C99 `restrict` keyword to guarantee that pointers do not alias. This is crucial for breaking false memory dependencies that would otherwise inflate the loop-carried dependency bound and prevent effective software pipelining. 4. **Detailed Diagnostics:** Using the `--debug_software_pipeline` compiler flag to generate detailed assembly listings that report the final II, the loop-carried dependency bound, the resource bound, and identify the specific instructions on the critical path, guiding targeted optimization efforts. 5. **Software-Managed Memory:** Leveraging the Enhanced DMA (EDMA) controller to perform background data transfers to and from on-chip scratchpad memory, enabling double-buffering patterns that hide main memory latency.",
		  "relevance_to_custom_vliw": "The C6x approach is highly relevant as it provides a model for a compiler-centric VLIW that can be significantly enhanced with programmer guidance. For the custom 12/6/2/2/1 architecture, using similar pragmas and intrinsics would be essential. Asserting alignment would allow the two load/store slots to be used more efficiently (e.g., for wider vector loads). The `restrict` keyword is vital for the user's tree-traversal problem to eliminate false pointer-aliasing dependencies. The diagnostic feedback model is a blueprint for how to debug performance on the custom VLIW by understanding whether the bottleneck is a dependency chain or resource contention."
		},
		{
		  "platform_name": "Qualcomm Hexagon DSP",
		  "key_architectural_feature": "A sophisticated, decoupled compiler backend featuring a two-stage scheduling process (pre-RA and post-RA) and a separate, DFA-based `DFAPacketizer` pass for instruction bundling. This architecture allows for more intelligent trade-offs between ILP and register pressure.",
		  "specific_techniques": "1. **Decoupled Scheduling and Packetization:** The instruction scheduler does not create VLIW packets directly. Instead, it simulates bundling using a model of the hardware to guide its decisions. The final bundling is performed by a late-stage `DFAPacketizer` pass, which uses a Deterministic Finite Automaton (DFA) generated from target-description files (`.td`) to enforce hardware resource constraints. 2. **VLIW-Aware Pre-RA Scheduler:** This scheduler runs before register allocation. It aims to produce a schedule that is already 'shaped' for the packetizer by considering resource limits, thus balancing the goals of a short schedule (high ILP) and full packets, which helps manage register lifetimes and reduce spills. 3. **Software Prefetching:** Using non-blocking prefetch instructions like `l2fetch` to pull data into the L2 cache hundreds of cycles before it is needed, effectively hiding memory latency without stalling the pipeline. 4. **Tightly Coupled Memory (TCM):** Utilizing a software-managed scratchpad memory (TCM) for irregular memory access. Powerful vector gather/scatter instructions can only access the TCM, enforcing a pattern where data is first explicitly staged from main memory into the fast, predictable TCM before processing.",
		  "relevance_to_custom_vliw": "The Hexagon compiler architecture provides a state-of-the-art blueprint for supporting the custom VLIW within a modern compiler framework like LLVM. Defining the 12/6/2/2/1 unit mix in target-description files would allow the `DFAPacketizer` to correctly bundle instructions. The VLIW-aware pre-RA scheduler's strategy of simulating bundling is a key heuristic for managing the trade-off between filling the 23 parallel slots and avoiding register spills. The explicit software prefetching and TCM/gather-scatter patterns are directly applicable strategies for handling the user's irregular memory access challenges."
		},
		{
		  "platform_name": "Intel Itanium (IA-64)",
		  "key_architectural_feature": "An EPIC (Explicitly Parallel Instruction Computing) architecture with extensive hardware support for compiler-driven ILP extraction, including full predication, rotating registers, and advanced data/control speculation.",
		  "specific_techniques": "1. **Full Predication and Hyperblocks:** Nearly all instructions are predicable, controlled by a 64-entry predicate register file. This allows the compiler to use if-conversion to eliminate branches and form 'hyperblocks'—large, single-entry, multiple-exit regions of predicated code that expose massive ILP to the scheduler. 2. **Rotating Registers:** A hardware mechanism that automatically renames a subset of registers for each iteration of a software-pipelined loop. This elegantly solves the problem of managing variable lifetimes across overlapped iterations, reducing register pressure and eliminating the need for code-expanding software workarounds. 3. **Data Speculation:** A powerful latency-hiding technique using an `ld.a` (advanced load) instruction and a hardware `ALAT` (Advanced Load Address Table). The compiler can move a load before a potentially aliasing store. The hardware tracks the access in the ALAT and invalidates it if a conflict occurs, allowing for runtime memory dependency checking. A `chk.a` instruction later verifies the speculation. 4. **Control Speculation:** Using an `ld.s` (speculative load) and `chk.s` (check) pair, the compiler can move a load above a branch. The hardware defers any exceptions (e.g., page fault) until the `chk.s` instruction, hiding latency along the speculative path.",
		  "relevance_to_custom_vliw": "Itanium's features represent a 'gold standard' for a high-performance VLIW. For the custom architecture, implementing predication is essential for utilizing the 18 ALU slots in the presence of data-dependent control flow. Rotating registers would be the most effective solution for managing register pressure caused by aggressive software pipelining. Most importantly, the data speculation mechanism (ALAT) is a proven, powerful solution for the user's core problem of irregular memory access in tree traversals, as it allows the compiler to break ambiguous memory dependencies and hide latency. The documented ~3 IPC limit of Itanium also serves as a crucial lesson: memory latency is a dominant bottleneck, making these advanced features a necessity, not a luxury."
		},
		{
		  "platform_name": "Transmeta Crusoe",
		  "key_architectural_feature": "A unique 'Code Morphing Software' (CMS) that dynamically translated x86 instructions into native VLIW code, enabling extremely aggressive speculation backed by a hardware rollback mechanism.",
		  "specific_techniques": "1. **Dynamic Translation and Optimization:** The CMS acted as a JIT compiler, interpreting x86 code to identify hot execution traces. These traces were then translated into highly optimized VLIW 'molecules' (instruction words). 2. **Aggressive Speculation with Hardware Rollback:** The CMS made optimistic assumptions, such as speculating that memory loads and stores did not alias. This was made possible by a hardware commit-and-rollback mechanism. The VLIW core operated on 'working' registers, and stores were buffered. If an assumption failed, the hardware could instantly discard the speculative work by restoring a 'shadow' copy of the registers, providing a very low-overhead recovery path. 3. **Adaptive Retranslation:** The CMS continuously monitored the performance of its translated code. If a region performed poorly or caused frequent exceptions, it could be re-evaluated and re-translated with different, more conservative assumptions.",
		  "relevance_to_custom_vliw": "While the custom VLIW is static, Crusoe's dynamic approach provides a model for an advanced offline or JIT-assisted toolchain. The core insight is to use runtime information to guide optimization. A Profile-Guided Optimization (PGO) workflow can emulate Crusoe's trace selection. A JIT compiler could adapt schedules to real-world latencies (e.g., frequent cache misses on a particular load) by re-scheduling code to increase the load-to-use distance. The hardware rollback concept highlights the high cost of software-only recovery and reinforces the value of more granular hardware-assisted speculation like Itanium's."
		}
	  ],
	  "manual_and_niche_optimizations": [
		{
		  "optimization_name": "SIMD Within A Register (SWAR)",
		  "source_community": "Demoscene, Competitive Programming",
		  "description": "This technique packs multiple smaller data values into a single native-width register (e.g., four 8-bit values into a 32-bit register) and performs a single operation (e.g., addition, bitwise AND) on all of them simultaneously. It requires careful use of bitmasks and knowledge of value ranges to prevent overflow or crosstalk between the packed data elements.",
		  "vliw_application": "SWAR is a powerful way to generate data parallelism for the 12 scalar ALU slots. A single scalar instruction can effectively perform 4, 8, or more operations at once, dramatically increasing throughput for tasks like image processing (e.g., plasma effects) or bit manipulation (e.g., parallel popcount). This allows the VLIW to achieve SIMD-like speedups using only its scalar units, leaving the vector units free for other tasks."
		},
		{
		  "optimization_name": "Branchless Arithmetic Transforms",
		  "source_community": "Competitive Programming, Hardware Hacking (e.g., 'Bit Twiddling Hacks')",
		  "description": "This involves replacing conditional branches (`if-then-else`) with sequences of arithmetic and bitwise operations that produce the same result. For example, computing `min(x, y)` can be done with `y ^ ((x ^ y) & -(x < y));`. This converts a control dependency into a data dependency.",
		  "vliw_application": "This technique is critical for the custom VLIW because it has only one flow slot, making branches a major bottleneck. By converting conditional logic into a sequence of ALU operations, these transforms free up the flow slot and provide more independent instructions that can be scheduled in parallel on the 12 available scalar ALU slots. This directly increases ILP and allows the compiler to create denser, more efficient VLIW bundles."
		},
		{
		  "optimization_name": "Latency-Aware Instruction Pairing / Interlock Avoidance",
		  "source_community": "DSP Engineers (Qualcomm Hexagon)",
		  "description": "This manual trick involves understanding specific micro-architectural hazards of the processor. For example, on the Hexagon HVX, using a register as an input to certain instructions immediately after it was produced by the previous instruction packet causes a stall. The manual fix is to reorder the code to insert an unrelated, independent instruction between the producer and the consumer, breaking the stall.",
		  "vliw_application": "The custom VLIW architecture will likely have its own subtle, undocumented micro-architectural hazards. This technique represents the expert-level practice of analyzing compiler output for unexpected stalls and then manually re-scheduling instructions to work around them. It allows the programmer to fill potential stall cycles with useful work from the 18 available ALU slots, achieving performance that a generic compiler might miss."
		},
		{
		  "optimization_name": "Manual Loop Unrolling for Multi-Stream Interleaving",
		  "source_community": "DSP Engineers (Texas Instruments C6x)",
		  "description": "While compilers can unroll loops, they are often conservative. Manual unrolling gives the programmer full control to expose more ILP by creating a larger loop body containing instructions from multiple original iterations. This is particularly effective for processing multiple independent data streams concurrently.",
		  "vliw_application": "For a dependency-bound task like a hash function, a single stream cannot utilize the VLIW's parallelism. By manually unrolling the loop and restructuring the code to process 4, 8, or even 12 independent data streams at once, the programmer creates a massive pool of independent instructions. The VLIW scheduler can then pick instructions from these different streams to fill the 12 scalar and 6 vector ALU slots in each cycle, transforming a latency-bound problem into a throughput-bound one and maximizing hardware utilization."
		},
		{
		  "optimization_name": "Vectorization of Mathematical Functions",
		  "source_community": "Demoscene, Graphics Programming",
		  "description": "This technique, popularized by graphics programmer Inigo Quilez, involves replacing expensive and often sequential mathematical functions (like `acos`, `sin`, `atan2`) with equivalent formulations that use a series of vector operations such as dot products, cross products, and normalizations. This is common in 3D graphics for tasks like calculating rotations.",
		  "vliw_application": "This is a perfect match for the custom architecture's 6 vector ALU slots. A complex trigonometric calculation that might occupy a scalar unit for many cycles can be transformed into a short sequence of parallel vector instructions. This dramatically increases throughput for geometric, physics, or signal-processing algorithms, allowing the vector units to handle the heavy math while the scalar units perform other logic."
		}
	  ],
	  "machine_learning_in_scheduling": {
		"primary_approach": "Deep Reinforcement Learning (DRL) combined with Graph Neural Networks (GNNs) is the leading-edge paradigm. This approach moves beyond traditional heuristics by training an agent to learn optimal scheduling policies directly from the code's structure and observed performance.",
		"mechanism": "The instruction scheduling problem is framed as a Markov Decision Process (MDP). The state is represented by the Data Dependency Graph (DAG) of a basic block, which is processed by a GNN to generate rich embeddings that capture complex instruction relationships and dependency chains. An RL agent, powered by a neural network, uses these embeddings to select an action, which consists of choosing the next instruction to schedule. The environment provides a reward based on the quality of the resulting schedule, primarily the reduction in total execution cycles. Training can be done in a fast custom simulator and then fine-tuned with a 'compiler in the loop', or a supervised model can be trained to imitate the output of a provably optimal solver like a Two-Dimensional Constrained Dynamic Programming (TDCDP) model.",
		"key_features": "To effectively schedule for the 12/6/2/2/1 VLIW, the ML model requires a comprehensive set of features: 1) Architectural Features: A vector representing the real-time availability of the 12 scalar ALUs, 6 vector ALUs, 2 load slots, 2 store slots, and 1 flow slot. 2) Programmatic Features: The instruction dependency graph is the primary input, allowing the GNN to implicitly learn critical path lengths and dependency structures. Static features like the number of successors/predecessors for an instruction are also used. 3) Runtime Behavior Features: To handle irregular memory, Profile-Guided Optimization (PGO) data is crucial. This includes runtime information on branch bias, cache miss hotspots, and common memory strides, which can be fed to the model as annotations or features.",
		"safeguards": "Aggressive ML-driven scheduling can have negative side effects that must be managed. The primary safeguard is to design a multi-objective reward function for the RL agent. In addition to rewarding cycle reduction, the function should penalize the agent for generating schedules that increase register pressure above a certain threshold or cause significant code size expansion. Furthermore, the current register pressure can be included as a feature in the state representation, allowing the agent to make pressure-aware decisions. A structural safeguard is to perform scheduling after register allocation (Post-RA scheduling), which forces the scheduler to operate within the constraints of the already-assigned physical registers, inherently preventing it from creating unmanageable register pressure."
	  },
	  "repurposed_fault_tolerance_techniques": {
		"core_idea": "The central concept is to repurpose hardware mechanisms originally designed for reliability (fault tolerance) to achieve performance gains. This is achieved by using the idle functional units, which are abundant in wide-issue VLIW machines, for opportunistic computations that hide latency or improve throughput, rather than for error checking.",
		"original_technique": "The primary fault-tolerance mechanisms being adapted are **dynamic instruction replication** and **space-time scheduling**. Dynamic replication involves hardware that duplicates instructions at runtime to execute on idle functional units. The results from the original and replicated instructions are then compared to detect faults. Space-time scheduling enhances this by allowing the hardware to place these replicas not only in the current cycle's empty slots (space) but also in the next cycle's (time), providing greater flexibility.",
		"performance_application": "The most successful and well-documented performance application is **hiding inter-cluster communication latency**. In a clustered VLIW architecture, when a value produced in one cluster is needed by an instruction in another, it incurs a communication delay. By using a compiler-driven approach to replicate the value-producing instruction in both the source and destination clusters, the need for the cross-cluster data transfer is eliminated entirely. Research on a 12-issue VLIW demonstrated that this technique yielded an average Instructions Per Cycle (IPC) increase of 25%, with peak speedups of 70%.",
		"justification": "This approach is viable due to the high, predictable under-utilization of functional units in wide-issue VLIW machines. Real-world applications often lack sufficient instruction-level parallelism (ILP) to keep all units busy. Empirical data from the Intel Itanium, a VLIW-like processor, showed an average functional unit under-utilization of 28-32% on common benchmarks. The user's even wider 23-slot VLIW is highly likely to exhibit similar or greater levels of idle slots, creating a substantial opportunity to use these otherwise wasted resources for performance-enhancing redundant or speculative computations."
	  },
	  "empirical_case_studies_summary": {
		"platform": "The Intel Itanium 2, a 6-issue EPIC (Explicitly Parallel Instruction Computing) processor. EPIC is a VLIW-like philosophy where the compiler is responsible for explicitly encoding instruction-level parallelism into the machine code.",
		"workload": "The SPECint2000 benchmark suite, a collection of general-purpose integer programs that are representative of non-scientific code and often feature irregular control flow and complex memory access patterns.",
		"key_finding": "An empirical study using the research-focused IMPACT compiler demonstrated an average of **2.63 useful operations per cycle (IPC)**. This represents a **43.8% slot utilization** on the 6-issue machine. This level of performance was achieved through the aggressive use of ILP transformations, most notably **hyperblock formation** (using predication to merge multiple control paths into a single schedulable block) and hardware-assisted **data and control speculation** to hide memory latency. Data speculation alone yielded performance gains of 1% to 7%.",
		"implication": "The Itanium 2 study provides a crucial, sobering data point. Despite its advanced architecture and a highly sophisticated research compiler, it struggled to utilize even half of its available execution slots on irregular integer code. This strongly implies that the user's much wider 23-slot VLIW will face a severe scarcity of instruction-level parallelism. To achieve high utilization, especially of the 18 available ALU slots, a single thread of execution is insufficient. The key implication is that **multi-stream interleaving**—structuring the software to process multiple independent data streams (e.g., parallel tree traversals) concurrently—is not just an optimization but a fundamental necessity to keep the hardware busy."
	  },
	  "key_architectural_feature_recommendations": {
		"feature_name": "Rotating Registers",
		"description": "A hardware mechanism where a subset of the architectural register file (e.g., general-purpose, floating-point, and predicate registers) is designated as 'rotating'. When a special loop-back branch instruction is executed, the hardware automatically renames these registers by incrementing a hardware-managed Iteration Register (IR). An instruction referencing a rotating register `rN` will effectively access the physical register at `(rN + IR) mod rotating_set_size`. This provides a new, unique physical register for a variable in each new loop iteration, managed entirely by the hardware.",
		"benefit_for_vliw": "Rotating registers provide a direct, low-overhead hardware assist for **software pipelining (modulo scheduling)**, the most critical optimization for VLIW loops. They solve the problem of managing variable lifetimes across overlapped loop iterations without requiring the compiler to perform complex and code-size-increasing transformations like **Modulo Variable Expansion (MVE)**, which involves unrolling the loop kernel and inserting explicit register-to-register copy instructions. This results in smaller, more efficient, and higher-performance code for pipelined loops, as it simplifies one of the most difficult tasks for the VLIW compiler.",
		"relevance_to_user_problem": "The user's core challenge is handling \"loop-carried dependencies across tree traversal iterations.\" This is precisely the problem that rotating registers are designed to solve. By automatically providing a fresh set of registers for each new iteration, they break the false dependencies (Write-After-Read hazards across iterations) that would otherwise prevent aggressive overlapping of loop iterations. This allows the compiler to achieve a much lower Initiation Interval (II) and higher throughput, directly addressing the user's need to hide latency and maximize ILP in loops with dependencies, without the significant code bloat associated with software-only solutions."
	  }
	},
	"outputBasis": [
	  {
		"field": "manual_and_niche_optimizations",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "The user is asking for advanced and niche optimization techniques applicable to a custom VLIW architecture with specific characteristics, including 12 scalar ALU slots and 6 vector ALU slots. The provided excerpts offer relevant information on several such techniques:\n\n**SIMD Within A Register (SWAR)**: This technique is described as a powerful way to generate data parallelism for scalar ALU slots, effectively achieving SIMD-like speedups using scalar units. This directly addresses the user's need to increase throughput and utilize the 12 scalar ALU slots.\n\n**Branchless Arithmetic Transforms**: These transforms replace conditional branches with arithmetic and bitwise operations, converting control dependencies into data dependencies. This is crucial for the custom VLIW architecture because it has a single flow slot, and branches can be a bottleneck. By eliminating branches, more independent instructions can be scheduled in parallel on the ALU slots, increasing Instruction-Level Parallelism (ILP).\n\n**Latency-Aware Instruction Pairing / Interlock Avoidance**: This manual trick involves understanding micro-architectural hazards and reordering instructions to avoid stalls. It allows for filling stall cycles with useful work from the ALU slots, which is directly relevant to maximizing hardware utilization.\n\n**Manual Loop Unrolling for Multi-Stream Interleaving**: This technique allows for exposing more Instruction-Level Parallelism (ILP) by creating larger loop bodies that process multiple independent data streams concurrently. This is particularly useful for the custom VLIW architecture, as it can transform a latency-bound problem into a throughput-bound one by filling the ALU slots with instructions from different streams.\n\n**Vectorization of Mathematical Functions**: This technique replaces complex mathematical functions with sequences of vector operations. This is a perfect match for the custom architecture's 6 vector ALU slots, as it can significantly increase throughput for tasks involving heavy mathematical computations, allowing the vector units to be utilized effectively.",
		"confidence": "high"
	  },
	  {
		"field": "manual_and_niche_optimizations.0",
		"citations": [
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "The most closely related material discusses Iterative Modulo Scheduling, which is a classic technique for exposing ILP in loops with potential irregularities, reordering or overlapping iterations to better utilize VLIW slots. This directly informs how to keep the pipeline filled when dependencies exist across loop-carried iterations. Closely tied to this is the concept of modulo variable expansion, which extends the idea of overlap across iterations by effectively providing per-iteration versions of a variable, enabling more independent scheduling decisions and thus better occupancy of the available scalar and vector units. Additional discussion of general software pipelining for VLIW machines covers the broader strategy of arranging instructions to tolerate hazards and maintain throughput, which is relevant when attempting to interleave independent computations to hide latency and keep the diverse execution units busy. Taken together, these sources offer a toolbox of scheduling strategies (modulo scheduling, prologue/epilogue handling, loop unrolling variants) that can be leveraged to improve utilization of the 12 scalar ALU slots and other units in a complex VLIW, even if they do not mention SWAR packing directly. While SWAR as a topic is not explicitly covered in these excerpts, the emphasis on interleaving and overlapping work across iterations aligns with the overarching goal of filling processing slots efficiently in the presence of long dependency chains.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.1",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  }
		],
		"reasoning": "The target fine-grained field describes a Branchless Arithmetic Transform intended to convert a conditional control dependency into a data dependency, thereby freeing the flow slot of a VLIW that has a single flow slot and enabling denser instruction packing across the 12 scalar ALU slots. The excerpts discuss advanced VLIW scheduling techniques that are closely related to that goal. Specifically, software pipelining and modulo scheduling address how to hide latencies and restructure loops so that independent computations can be overlapped, which directly supports interleaving work to fill parallel slots when a single control flow hazard would otherwise bottleneck execution. Statements describing applying software-pipelining concepts to VLIW machines, as well as modulo variable expansion to expose independence between iterations, provide a concrete basis for interleaving independent computations and increasing instruction-level parallelism in a constrained VLIW layout. The referenced work on iterative modulo scheduling demonstrates how to rotate and reuse registers to sustain a steady pipeline, which complements the idea of removing data/control dependencies by transforming the loop body into a schedule that can be executed more densely across available ALUs. Collectively, these excerpts illustrate practical scheduling techniques (software pipelining, modulo scheduling, and variant expansions) that align with converting conditional logic into data-friendly sequences and better utilizing heterogeneous or uniform execution resources in a VLIW context. This supports the existence of strategies to mitigate long dependent chains (like a hash function) by interleaving independent computations to keep slots busy, which is exactly the aim of branchless or data-driven transforms in a VLIW workflow.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.2",
		"citations": [
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "Latency-aware interlock avoidance is a practical embodiment of software-pipelining concepts. Iterative modulo scheduling presents techniques to rotate registers and schedule iterations of a loop so that independent computations can overlap across iterations, directly supporting the goal of hiding latency in the presence of loop-carried dependencies. The notion of modulo variable expansion introduces a form of addressing that allows per-iteration independence by effectively widening variables across iterations, enabling the compiler (or programmer) to reap parallelism across the long dependency chain. These strategies collectively provide a framework for restructuring instruction streams to fill otherwise idle parallel slots with independent work, which is precisely the manual interleaving approach described in the field value to break stalls caused by a producer-consumer dependency within a packet. Additionally, classic software-pipelining research and scheduling techniques (with emphasis on dealing with hazards and incorporating profile information) illustrate how to adapt scheduling to hardware constraints and irregular control flow, reinforcing the rationale for latency-aware pairing and interlock avoidance. The inclusion of techniques for irregular memory access patterns and loop behavior in the cited works further supports applying these ideas to your custom VLIW with heterogeneous functional units and a long dependency chain, by guiding how to interleave computations to keep all slots busy and reduce stalls. Therefore, these excerpts collectively substantiate the feasibility and methodology of latency-aware instruction pairing in VLIW contexts, even if not every claim maps to the exact hardware specifics of Hexagon, the underlying principles remain applicable across advanced VLIW scenarios.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.3",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "The most relevant content directly notes an optimization where a loop variable that normally changes each iteration is transformed so that each iteration refers to a distinct location, thereby making iterations independent and enabling loop-level parallelism. This matches the idea of expanding a loop to interleave multiple data streams and expose more independent work for the VLIW scheduler to exploit across the available scalar and vector units. A related optimization is described as modulo variable expansion, which implements part of the same concept by widening the loop’s view to accommodate multiple iterations in flight, effectively enabling the hardware to schedule across several iterations within a single cycle. This aligns well with manual interleaving aims to keep the VLIW pipeline populated when relying on diverse execution units and when facing irregular or dependent work patterns. Additional excerpts discuss software pipelining techniques for VLIW and iterative modulo scheduling, which provide broader context on how compilers and schedulers handle hazards and irregular loops, further supporting the idea that advanced loop transformations (including manual interleaving) are a key part of achieving high utilization on VLIW architectures. Taken together, the cited materials substantiate that loop-interleaving via expansion (and its modulo variant) is a viable manual optimization strategy to maximize ILP on DSP-oriented VLIW systems, especially when the pipeline includes many execution slots and when dependencies constrain straightforward unrolling.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.4",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "The central claim is that expensive or sequential computations can be restructured to fit a vector-enabled, highly parallel execution model by making loop iterations effectively independent. The strongest support is found in language describing a technique that expands loop variables into higher-dimension forms so each iteration can reference a distinct location, thereby enabling loop-level vectorization and better parallelization across multiple iterations. This matches the goal of converting scalar-heavy mathematical routines into shorter, parallel vector sequences that can be dispatched to vector ALUs, which is exactly the essence of vectorizing trig-like functions for VLIW hardware. A related excerpt reinforces this idea by describing an optimization that implements part of this functionality through modulo-variable expansion, aimed at achieving similar performance by enabling more parallelism in the presence of traditional hardware constraints. Surrounding discussions of software pipelining and modulo scheduling provide broader context about scheduling trials and hazard management on VLIW machines, illustrating why such vectorization-oriented strategies are valuable, especially when memory access patterns and dependencies complicate straightforward unrolling. Taken together, the excerpts support the premise that transforming a long, dependent computation into parallelizable, vector-friendly constructs is a viable path to hide latency and improve throughput on a VLIW-like architecture with multiple vector units.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.0.optimization_name",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "SWAR refers to packing multiple data elements into a single register and performing parallel operations within that register. Excerpts that discuss vectorization techniques, such as expanding a loop variable into higher-dimension variables to make iterations independent and enable vector-like usage, are most relevant as they touch on intra-register parallelism goals similar to SWAR. The discussion of modulo variable expansion and its relation to vectorizing or exposing more parallelism inside registers aligns with the core idea of SWAR, where independent computations are interleaved or packed within a register to improve throughput. The references that focus on software pipelining and handling hazards describe broader scheduling strategies for VLIW machines, which are foundational to exploiting any SWAR-like packing indirectly, but they do not explicitly claim SWAR. Consequently, the most relevant material supports the general principle of increasing parallelism within register boundaries (through vectorization-like expansions and intra-iteration independence), whereas the more general software-pipelining discussions provide context for mitigating hazards in a VLIW with heterogeneous slots but do not directly address SWAR. Based on this, the strongest connections are to the discussions of vectorization through variable expansion and related scheduling refinements; the software-pipelining discussions provide indirect support and broader context.",
		"confidence": "low"
	  },
	  {
		"field": "manual_and_niche_optimizations.2.optimization_name",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n"
			]
		  }
		],
		"reasoning": "The most directly relevant content centers on software pipelining and handling hazards in VLIW contexts. One excerpt presents a compilation technique for software pipelining of loops with conditional jumps, which explicitly addresses how to organize iterations to hide latencies and manage hazards, aligning with the goal of latency-aware pairing to avoid interlocks. Another excerpt discusses hardware support and scheduling adjustments to cope with hazards when leveraging profile information, which informs strategies to avoid stalls due to interdependencies—core to interlock avoidance. Additional excerpts describe modulo scheduling and related optimizations (such as rotating register allocation and variable expansion) that enable overlapping iterations or redefining loop-carried variables to expose independent iterations, thereby increasing instruction-level parallelism and reducing critical-path pressure. Together, these sources provide a spectrum of techniques—from concrete software-pipelined loop scheduling under irregular control flow to loop-variable transformations—that support latency-aware pairing and interlock avoidance in VLIW architectures. The remaining excerpts offer broader context on scheduling heuristics and classic approaches but contribute less directly to the explicit latency-aware interlock avoidance topic compared to the prioritized items above.",
		"confidence": "high"
	  },
	  {
		"field": "manual_and_niche_optimizations.2.source_community",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "The most relevant excerpts discuss concrete scheduling techniques for VLIW machines and include concepts like modulo variable expansion and loop transformation strategies that are commonly explored in DSP compiler literature and toolchains. These excerpts are directly about how to optimize VLIW scheduling, which is central to DSP-oriented code generation and performance tuning done by DSP engineers. The following excerpts expand on similar themes: implementing modulo variable expansion to mimic specialized hardware capabilities and expanding loop variables to enable per-iteration independence, which aligns with DSP-focused optimization practices in VLIW contexts. Other excerpts provide broad software-pipelining discussions and historical compilations techniques that, while foundational and widely applicable to DSP compiler work, do not tie explicitly to DSP-specific vendors or architectures in the given text. Nevertheless, they remain relevant to the general knowledge space a DSP engineer would draw from when considering manual VLIW optimizations and potential compiler gaps. Therefore, they are included as supporting context, but are ranked after the more directly applicable VLIW scheduling discussions tied to DSP-oriented codegen concepts.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.1.description",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  }
		],
		"reasoning": "The field value describes replacing a conditional branch with a sequence of arithmetic/bitwise operations to yield the same result, effectively turning a control dependency into a data dependency. The primary relevance comes from sources that discuss software pipelining and loop scheduling techniques designed to expose parallelism in VLIW architectures despite conditional or loop-carried control flow. The most directly relevant materials describe how to structure and schedule loop bodies to keep multiple slots busy (software pipelining) and how to manage iteration boundaries (modulo scheduling, variable expansion) so that independent data paths fill the available ALU and memory slots, mitigating the impact of control-flow hazards. These works often cover transforming or avoiding conditional branches within tight loops by restructuring loops, using predication-like ideas, or expanding loop state to decouple iterations, all of which support the goal of converting control dependencies into data dependencies to maintain high utilization. Therefore, the cited excerpts collectively support the approach of replacing or hiding branches through scheduling and loop transformation techniques that increase data-parallelism on a VLIW with many execution slots and complex unit constraints.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.3.vliw_application",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "The finegrained field value centers on turning a latency-bound, dependency-heavy loop (such as a hash function) into a throughput-bound one by introducing multiple independent streams that can be scheduled in parallel across the abundant VLIW slots. The most directly supportive content discusses transforming loop-carried dependencies into within-iteration or cross-iteration independence, notably via a technique that expands a variable across iterations so each iteration references a different location, effectively decoupling iterations and enabling vectorization-like parallelism. This aligns with the idea of unrolling and restructuring to create several independent data streams that the VLIW scheduler can interleave to keep all ALU slots busy. Closely related are discussions of modulo-variable expansion and software-pipelining approaches which implement similar independence and scheduling strategies, enabling more aggressive, hazard-aware packaging of instructions. The references to iterative modulo scheduling, rotating register allocation, and software-pipelining provide additional mechanisms and historical context for achieving the same goal: breaking deep dependency chains into scheduling-friendly units that the hardware can exploit across multiple functional units and per-cycle slot utilization. Collectively, these excerpts support the claim that deliberate loop restructuring and multi-stream independence are effective for maximizing VLIW utilization under dependency constraints, by enabling the scheduler to fill the 12 scalar ALUs and 6 vector ALUs in each cycle with instructions drawn from separate data streams.",
		"confidence": "high"
	  },
	  {
		"field": "manual_and_niche_optimizations.3.description",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "The core idea in the field value is to manually increase the amount of work per loop iteration by incorporating instructions from multiple iterations, thereby creating more independent data streams to fill the VLIW slots. The most directly relevant excerpt describes a technique called variable expansion, which identifies variables redefined at the beginning of every loop iteration and expands them into a higher-dimensional form so that each iteration can refer to a different location; this makes per-iteration usage independent and enables loop vectorization, which aligns with the concept of manually unrolling to expose more ILP across iterations. Related excerpts discuss modulo-related scheduling and software pipelining, which are established methods to interleave work from successive iterations to better utilize parallel units and hide latency, further supporting the strategy of increasing ILP exposure through expanded or interleaved iterations. Additional context on iterative modulo scheduling and its scheduling considerations reinforces the general approach of reordering or interleaving operations to keep heterogeneous execution units occupied, which complements manual unrolling as a technique to push more parallelism when a loop’s natural iteration granularity would otherwise underutilize resources. Taken together, these excerpts illustrate concrete mechanisms (iteration-independent expansion, software pipelining, modulo scheduling) that conceptually underpin manually unrolling to create multiple independent data streams and improve ILP utilization in a VLIW setting.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.0.vliw_application",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "The strongest alignment with the fine-grained field value comes from excerpts that describe transforming or expanding how data is represented across iterations to achieve parallelism and independence. One excerpt explains that a loop variable can be expanded into a higher-dimensional form so that each iteration references a different location, making iterations effectively independent and enabling vector-like behavior within a loop, which is a core idea behind SWAR-style data parallelism in a VLIW context. Another excerpt discusses modulo variable expansion, tying the technique to enabling similar performance benefits to specialized hardware aimed at exploiting parallelism, which mirrors the motivation of using SWAR to pack more work into scalar units. Together, these two excerpts directly address enabling data-parallel behavior within a VLIW by manipulating data dependencies and iteration structure to increase available parallel work slots. Additional excerpts focus on software pipelining techniques, hazard handling, and profile-driven scheduling, which are relevant as complementary methods for keeping VLIW execution units busy and managing long dependency chains, thereby supporting a broader theme of filling idle slots and achieving higher throughput in the presence of irregularities or dependencies. While these latter excerpts do not explicitly name SWAR, they provide the scheduling and architectural context necessary to realize SWAR-like throughput gains on a VLIW with multiple scalar and vector units. The oldest work on software pipelining demonstrates foundational methods for arranging computations to keep pipelines full despite conditional branches or unpredictable iteration counts, which underpins any advanced data-parallel packing strategy on VLIW architectures.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.2.vliw_application",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n"
			]
		  }
		],
		"reasoning": "The most relevant excerpt describes software pipelining as an effective scheduling technique for VLIW machines and discusses how to handle loops with conditional jumps, which is directly applicable to manually re-scheduling instructions to hide stalls caused by control flow or long dependency chains. The next most relevant excerpt touches on software-pipelining concepts that include hardware/compiler support for hazards arising from aggressive use of profile information, which informs how to avoid stalls through careful scheduling and hazard management. Following that, the remaining excerpts discuss specific VLIW scheduling optimizations (like modulo scheduling, and the technique of variable expansion to expose loop-carried independence), which are foundational ideas a practitioner would leverage to interleave independent work and fill idle slots in a complex, heterogenous VLIW with many functional units. Collectively, these excerpts provide concrete methods and rationale for inspecting compiler output, identifying unintended stalls, and re-ordering or restructuring instructions to improve utilization of the 18 available ALU slots, aligning with expert-level manual optimization practices for VLIW architectures. The content does not claim to fully solve all hazards in every context, but it furnishes substantial, concrete techniques and considerations that a practitioner would apply to the described scenario.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.1.vliw_application",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  }
		],
		"reasoning": "To support the need for reducing flow-slot bottlenecks by turning conditional logic into sequences of ALU operations, the most directly relevant material discusses software pipelining techniques for VLIW loops, including handling conditional jumps and unpredictable iteration behavior. The first excerpt describes a compilation algorithm for efficient software pipelining of general inner loops where iteration time and length may be unpredictable, emphasizing how to schedule around conditionals and control flow to keep pipelines filled. This directly aligns with transforming conditional logic into parallelizable ALU operations to alleviate flow-slot pressure. The third excerpt introduces a optimization known as modulo variable expansion, which expands a loop variable across iterations so that each iteration refers to a different location, effectively enabling loop-carried independence and enabling vector-like scheduling across iterations, which is highly relevant for increasing ILP under limited flow-slot capacity. The fifth excerpt also discusses modulo scheduling advancements and improvements to scheduling heuristics in the VLIW context, reinforcing methods to pack instructions more densely by exposing more independent operations. The second excerpt, Iterative Modulo Scheduling, reinforces the rotation-style register allocation in the kernel for software-pipelined loops, which supports keeping more independent instructions in flight and helps manage dependencies across iterations. The second excerpt adds context about handling hazards and the importance of scheduling decisions when dealing with kernel-level loops, which complements the main strategy of converting conditional logic into parallelizable work. Lastly, the second excerpt (the hazard-aware scheduling discussion) provides broader background on necessary support for aggressive scheduling, profiling, and hazard mitigation which is relevant when pushing more work into each cycle, even if not addressing the conditional-to-ALU transformation directly. Overall, the gathered themes—software pipelining of conditional loops, modulo/variable expansion for loop independence, iterative modulo scheduling, and hazard-aware scheduling—collectively support the requested technique of converting branches into ALU-rich sequences to maximize ILP on a constrained VLIW.",
		"confidence": "high"
	  },
	  {
		"field": "manual_and_niche_optimizations.2.description",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "The most relevant items address the core idea of mitigating hazards and improving instruction-level parallelism through scheduling techniques that create or exploit independence between operations. One discusses software pipelining specifically as a technique for loops with potentially unpredictable behavior and hazardous dependencies, which is directly related to reorganizing instruction order to avoid stalls caused by data dependencies or control flow. Another discusses handling hazards arising from aggressive use of profile information, which aligns with the concept of anticipating and mitigating stalls through careful scheduling choices and hazard-aware code motion. A third excerpt describes expanding loop-carried variables to create independence across iterations, which is a concrete method to transform dependent patterns into independent ones, enabling more effective overlap of independent computations and better utilization of parallel slots. A fourth excerpt covers modulo scheduling techniques that aim to expose and exploit parallelism in VLIW architectures, which underpins the general approach of reordering and interleaving operations to fill execution slots. The remaining excerpt discusses register allocation strategies and their role in software scheduling, which provides background on how to manage data for temporal independence, though it is less directly tied to the specific manual trick of inserting unrelated instructions to break stalls. Collectively, these excerpts support the field value by illustrating practical scheduling and transformation techniques to hide latency and break critical paths through reordering and independence, which can be applied to the Hexagon HVX stall scenario described in the field value.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.4.optimization_name",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  }
		],
		"reasoning": "To support the notion of vectorizing mathematical computations in VLIW contexts, the most directly relevant points describe transforming loop iterations so that successive iterations can be handled independently, enabling vector-like behavior across iterations. One excerpt explains expanding a variable that is redefined at the start of every iteration into a higher-dimensional variable so each iteration references a distinct location, effectively enabling loop-level vectorization. This directly supports the idea of distributing work for mathematical function calculations across iterations in a vectorized or parallel fashion. Another excerpt discusses modulo variable expansion as implementing functionality akin to specialized hardware concepts, which is a concrete technique to expose parallelism and improve utilization of VLIW slots when dealing with irregular access patterns or data-dependent paths, thereby facilitating vectorization-like throughput gains for mathematical kernels. Additional excerpts describe software pipelining and modular scheduling approaches that optimize the overlap of instruction stages across iterations, addressing hazards, profile information usage, and the general machinery needed to keep the long dependency chains (such as those found in math-heavy computations) from stalling the pipeline. Collectively, these excerpts map closely to strategies for interleaving independent computations, hiding latency, and packing instructions effectively in the presence of heterogeneous execution units, all of which are essential for vectorization of mathematical functions on a VLIW architecture. The progressive discussion from loop variable expansion to modulo scheduling and software pipelining provides a coherent set of techniques that mirror the goals of vectorizing complex mathematical kernels, especially when facing irregular access or data-dependent control flow.",
		"confidence": "high"
	  },
	  {
		"field": "manual_and_niche_optimizations.3.optimization_name",
		"citations": [
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  }
		],
		"reasoning": "The explicit field value points to a manual unrolling approach aimed at interleaving multiple streams to better utilize the diverse functional units of a VLIW processor. The most directly related content discusses iterative modulo scheduling, which is a loop-centered technique that manages the timing of instructions across iterations and can be adapted to interleave independent computations to fill parallel slots when facing long dependency chains. Related material on software pipelining provides strategies for arranging loops with irregular behavior and conditional control flow, which informs how one might manually unroll or interleave computations to maintain high unit utilization despite dependencies. Additionally, techniques that expand loop variables to create per-iteration independence offer a conceptual bridge to interleaving multiple streams, since making iterations independent is a prerequisite for effectively overlapping work from different iterations. While none of the excerpts explicitly label the exact technique as “Manual Loop Unrolling for Multi-Stream Interleaving,” they collectively describe the foundational scheduling and interleaving concepts that would support such a manual strategy in a VLIW context.\n",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.4.vliw_application",
		"citations": [
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  }
		],
		"reasoning": "The finegrained field value discusses leveraging vector units (specifically a fixed count of vector ALU slots) to offload heavy math (like a complex trigonometric calculation) from scalar units, enabling the vector units to absorb the heavy computation while scalars handle ancillary logic. Several excerpts directly speak to turning loop bodies or computations into parallel, vector-like forms or into overlapping, pipelined schedules to improve throughput on VLIW-like architectures. The most directly relevant excerpt describes an optimization that expands a loop variable so that each iteration can refer to a different location, effectively making iterative work independent and allowing vectorization to occur across iterations. This aligns with the idea of reshaping work so vector units can be saturated with parallel operations rather than serialized scalar work. A closely related excerpt discusses an optimization that implements part of the functionality of specialized hardware to achieve similar performance, i.e., using advanced scheduling ideas to reach better parallelism, which complements the notion of exploiting the vector units more aggressively. Another excerpt covers iterative modulo scheduling, a well-known technique for balancing instruction packing and pipeline utilization in loops, which is highly relevant for filling a fixed set of heterogeneous execution slots (vector and scalar) without stalling. Additional excerpts elaborate on software pipelining of loops with potential conditional branches and the hardware/compiler support required to handle hazards when data-dependent flow is involved, further reinforcing how careful scheduling can keep all units busy despite irregular control flow. Taken together, these excerpts provide a spectrum of strategies—from variable expansion enabling vectorization to modulo scheduling and software pipelining—that are directly applicable to maximizing use of a fixed vector ALU slot count while hiding latency and maintaining correctness in the presence of loop-carried dependencies. They collectively support the feasibility and rationale of transforming a heavy trig computation into a sequence of vector operations to sustain high throughput on the vector units, with scalars handling non-vectorizable logic.",
		"confidence": "medium"
	  },
	  {
		"field": "manual_and_niche_optimizations.4.description",
		"citations": [
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.\nThis op\ntimization is a variation of the variable expansion technique\nused in vectorizing compilers [18]. The variable expansion\ntransformation identifies those variables that are redefmed at\nthe beginning of every iteration of a loop, and expands the\nvariable into a higher dimension variable, so that each itera-\ntion can refer to a different location. Consequently, the use of\nthe variable in different iterations is thus independent, and the\nloop can be vector&d.\n",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc"
			]
		  }
		],
		"reasoning": "The requested fine-grained field value concerns replacing sequential, latency-bound computations with vector-like formulations to parallelize work and hide latency in a graphics-focused context. While none of the excerpts name the exact trig-approximation technique, they cover core strategies for exploiting VLIW parallelism under loop-carried dependencies and irregular access patterns. The most directly relevant excerpt discusses Iterative Modulo Scheduling, which directly targets how to schedule instructions across iterations to maximize parallelism while managing dependencies. Closely following that, another excerpt describes software pipelining approaches for inner loops, including handling unpredictable iteration times and dependencies, which is central to keeping multiple slots busy despite long dependency chains. Additional excerpts discuss enhancements to scheduling heuristics and handling hazards with profile information, which are pertinent to refining how instructions are packed in a VLIW context with diverse functional units. Earlier excerpts introduce the concept of variable expansion to decouple iteration-local variables and enable loop vectorization, providing concrete methods to increase per-iteration independence and slot utilization. Collectively, these excerpts map a landscape of advanced VLIW scheduling techniques that would be valuable for interleaving independent computations to fill slots when the critical path is long. They do not confirm the exact trig-elimination technique, but they strongly support the broader goal of maximizing instruction packing and latency hiding in a VLIW setting.",
		"confidence": "medium"
	  },
	  {
		"field": "control_flow_optimization_techniques",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5).",
			  "Trace scheduling can be viewed as a method for dealing with the difficulties presented by branches.",
			  " limitation of trace scheduling is that it cannot in general migrate instructions above\nconditional branches. However, such code motions are possible on machines that provide support\nfor **speculative execution",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or",
			  "Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates.\nFor example, the branch in an if-then-else structure can be replaced with an instruction which\ndefines",
			  "Trace scheduling was introduced by Joseph Fisher in [Fisher81]. Originally conceived as an\nalgorithm for compacting microcode, it is a very general technique that can produce highly\noptimized code for any VLIW, scalar or superscaler architecture.",
			  "The motivation behind trace\nscheduling is to be able to optimize across basic block boundaries. We will begin our discussion of\ntrace scheduling by reviewing **list scheduling** , a technique for scheduling instructions within a\nsingle basic block. We then show how this algorithm is extended to schedule traces that span\nmultiple blocks, we explain how loops are handled, and we outline some related work.",
			  "To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or a join.",
			  "A key observation is that we can solve this problem by adding off-trace compensation code\nwhich restores the original semantics of program execution.",
			  "uppression of Compensation Code**\nOne of the drawbacks of trace scheduling is that the bookkeeping phase can cause exponential code\ngrowth ([Isoda93], [Lah83], [Linn83]). A restricted version of trace scheduling is presented in\n[Freuden94] which reduces code growth by avoiding and suppressing compensation co",
			  "Compensation code is avoided in two ways: by disallowing downward code motion past splits\n(figure 3c,d), and by disallowing upward code motion past joins with fan-in greater than four.",
			  "ted by branches.\nAn alternate approach is to attempt to eliminate branches altogether. **Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates",
			  "In section 5 we present **Delayed Issue** , a novel technique which allows instructions to be\nexecuted out-of-order without the hardware complexity of dynamic out-of-order issue.",
			  "Instructions within a queue are issued in order; out of order execution results from different\ninstructions being inserted into the queues at various delays.",
			  "delayed issue reduces code expansion due to trace scheduling and software pipelining.",
			  "While trace scheduling, speculative execution and software pipelining are effective techniques,\nthey require that all parallelism be explicitly discovered by the compiler.",
			  "Trace scheduling allows programs to be\noptimized across basic block boundaries, and is particularly effective when profiling information\nreveals commonly executed traces.",
			  "We have seen that the compiler\ntechnology to support these processors is well established; furthermore advances in processing\ntechnology, notably the on-die integration of logic and DRAM, provide hardware support for the\nhigh instruction bandwidth requirements of VLIW.",
			  "[Lam88]\nMonica Lam, “Software Pipelining: An Effective Scheduling Technique for VLIW\nMachines”, Proc. SIGPLAN ’88 Conference on Programming Language Design and\nImplementation, Atlanta, Georgia, June 22-24, 1988, pp. 318-328.",
			  "onica S. Lam, A Systolic Array Optimizing Compiler, Kluwer Academic Publishers, 1989,\n200p",
			  "VLIW architectures offer high performance at a much lower cost than dynamic out-of-order\nsuperscalar processors. By allowing the compiler to directly schedule machine resource usage, the\nneed for expensive instruction issue logic is obviated.",
			  "ssue*** *, a novel instruction issuing mechanism which*\n*allows a VLIW machine to perform out-of-order execution at a fraction of the cost of typical out-of-*\n*order issue hardware",
			  "**\nSuppose that when the instructions are scheduled, there is a value which is defined at some point\nand used *n* time steps later. This would seem to impose the restriction II = *n* , as otherwise the next\ninstance of the instruction defining the value II cycles later would overwrite the value before it could\nbe used. We can solve this problem by using *k* different registers to store the variable, where on\niteration *m* we use the ( *m* mod *k* ) th register. In this case, the value is not overwritten for *k* ·II\ntimesteps, so as long as *k* ·II = *n* the program will execute correctly.\nThe integer *n* is called the *lifetime* of the value. In order to use a different register for *k*\nconsecutive iterations we need to unroll the loop *k* times (since a single copy of an instruction\ncannot access different registers on different cycles without hardware support). The minimum\n",
			  "dware support for software pipelining is typically used to eliminate some or all of the code\nexpansion normally associated with this technique. One way to accomplish this is to provide\nsupport for modulo variable expansion so that the same effect can be achieved without multiple\ncopies of the loop code. Specifically, a mechanism must be provided for accessing different pieces\nof data using the same register name so that a single copy of the code can be used for each loop\niteration. In [Su90] a horrible architectural structure is proposed in which data is physically shifted\nbetween registers arranged in an array. In [Ugurdag93] an equally distasteful mechanism is described\nin which functional units are connected to a set of register files through large reconfigurable\ncrossbars. These are two unfortunate examples of what can happen when designers fail to realize\nthat “It’s the wires, stupid” [Knight97].\nA more practical solution found in both the Cydra 5 and the (ridiculously named) Itanium is\n**register rotation** ([Dehnert89], [Doshi99]). A certain set of registers are specified as rotating\nregisters. When an instruction references one of these registers, a hardware index is added to the\nregister number specified in the instruction (modulo the number of rotating registers). The index is\nincremented at the start of each new iteration. Thus, the same instruction will refer to different\nregisters on each cycl",
			  "Rotating registers obviate the need to duplicate loop kernels to perform modulo variable\nexpansion. Shifting predicates and delayed issue can both be used to eliminate prologue and\nepilogue code.",
			  "Rotating registers obviate the need to duplicate loop kernels to perform modulo variable\nexpansion.",
			  " some of the more interesting ideas which are based on the algorithm in [Lam88]. The majority of\nthese seek to improve the performance of software pipelining in the presence of conditional\nbranches.",
			  "**Fractional II**",
			  " II**\nThe lower bounds on II described in section 4.2 are not in general integers. Thus, even when it is\nreported that a schedule is found with the “optimal” II, better performance may be possible if II is\nsomehow allowed to be a fractional value. This can in effect be accomplished by unrolling the loop\na number of times before applying software pipelining. If the loop is unrolled *k* times and an\ninitiation interval of II’ is found for the unrolled loop, then in a sense we have II = II’/ *k* as *k*\niterations are initiated every II’ cycles. In [Lavery95] it is shown that this technique can produce\nsurprisingly large speedups, particularly when the theoretical lower bounds on II are less than 1.\n**4.7.2**\n**Pre",
			  "**Predicated Execution and Enhanced Modulo Scheduling**",
			  "**\nOne of the objections to hierarchical reduction of conditional constructs is its rigidity. Since\nconditionals are scheduled separately without any regard to the resource requirements of the rest of\nthe loop, the resulting reduced nodes may have complex resource usages that create artificial\nconflicts and may prevent a schedule from being found. In [Warter93a] another approach is\nsuggested wherein conditional branches are replaced by predicated execution, as described in section\n"
			]
		  },
		  {
			"title": "Program decision logic optimization using predication and control ...",
			"url": "https://liberty.princeton.edu/Publications/ieeepro89_decision.pdf",
			"excerpts": [
			  "paths are selected for inclusion, the code after if-conversion con- sists of only a single block of predicated instructions, a hyper- block [9], as shown in ..."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol1.pdf",
			"excerpts": [
			  "1.",
			  "2.",
			  "3.",
			  "4.",
			  "Predication is the conditional execution of instructions. Conditional execution is implemented\nthrough branches in traditional architectures. IA-64 implements this function through the use of\npredicated instructions. Predication removes branches used for conditional execution resulting in\nlarger basic blocks and the elimination of associated mispredict penalties.",
			  "\nThe IA-64 pseudo-code shown below implements the C expressions without branches:\ncmp.ne p1,p2=a,r0\n// p1 <- a != 0\ncmp.ne p3,p4=e,r0 ;;// p3 <- e != 0\n(p1)add\nb=c,d\n ... ",
			  "Predication is the conditional execution of an instruction based on a qualifying predicate. A\nqualifying predicate is a predicate register whose value determines whether the processor commits\nthe results computed by an instruction.",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction. The check instruction must specify the same register number as the\ncorresponding advanced load.",
			  "When a check instruction is executed, the ALAT is searched for an entry with the same target\nphysical register number and type. If an entry is found, execution continues normally with the next\ninstruction.",
			  "cating and Checking ALAT Entries**\nAdvanced loads perform the following actions:\n1.\nThe ALAT register tag for the advanced load is computed. (For ldfp.a , a tag is computed\nonly for the first target register.)\n2.\nIf an entry with a matching ALAT register tag exists, it is removed.\n3.\nA new entry is allocated in the ALAT which contains the new ALAT register tag, the load\naccess size, and a tag derived from the physical memory address.\n4.\nThe value at the address specified in the advanced load is loaded into the target register and,\nif specified, the base register is updated and an implicit prefetch is performed.",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and floating-point pair\n( ldfp.a ) forms.",
			  "Data speculation allows loads to be moved above possibly conflicting memory references.",
			  ") forms. When an advanced load is executed, it allocates an entry in a structure called the\nAdvanced Load Address Table (ALAT).",
			  "Later, when a corresponding check instruction is executed,\nthe presence of an entry indicates that the data speculation succeeded; otherwise, the speculation\nfailed and one of two kinds of compiler-generated recovery is performed:",
			  "The check instruction must specify the same register number as the\ncorresponding advanced load.",
			  "The advanced load check ( chk.a ) is used when an advanced load and several instructions\nthat depend on the loaded value are scheduled before a store that is ambiguous relative to the\nadvanced load.",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks to operate\ncorrectly.",
			  ". In both control and data speculation, the\ncompiler exposes ILP by issuing an operation early and removing the latency of this operation from\ncritical path. T",
			  "The compiler will issue an operation speculatively if it is reasonably sure that the\nspeculation will be beneficial.",
			  "To be beneficial two conditions should hold: it must be\n(1) statistically frequent enough that the probability it will require recovery is small, and (2) issuing\nthe operation early should expose further ILP-enhancing optimization.",
			  "Speculation is one of the\nprimary mechanisms for the compiler to exploit statistical ILP by overlapping, and therefore\n ..",
			  "Special mechanisms are provided to allow for compiler-directed speculation. This speculation takes\ntwo forms, control speculation and data speculation, with a separate mechanism to support each.",
			  "If no matching entry is found, the speculative results need to be recomputed:",
			  "ata speculation in IA-64 uses a special load instruction ( ld.a ) called an *advanced load*\ninstruction and an associated check instruction ( chk.a or ld.c ) to validate data-specul",
			  "When the ld.a instruction is executed, an entry is allocated in a hardware structure called the\nAdvanced Load Address Table (ALAT).",
			  "The ALAT is indexed by physical register number and\nrecords the load address, the type of the load, and the size of the load.",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction.",
			  "The check detects a deferred exception or a conflict with an intervening store and provides a\nmechanism to recover from failed speculation.",
			  "A new entry is allocated in the ALAT which contains the new ALAT register tag, the load\naccess size, and a tag derived from the physical memory address.",
			  "The value at the address specified in the advanced load is loaded into the target register and,\nif specified, the base register is updated and an implicit prefetch is performed.",
			  "Support for this is enabled by special NaT bits that are attached to integer registers and by special\nNatVal values for floating-point registers.",
			  "When a speculative load causes an exception, it is not\nimmediately raised.",
			  "Instead, the NaT bit is set on the destination register (or NatVal is written into\n ..",
			  "Data speculation in IA-64 uses a special load instruction ( ld.a ) called an *advanced load*\ninstruction and an associated check instruction ( chk.a or ld.c ) to validate data-speculated\nresult"
			]
		  },
		  {
			"title": "chapter 15 - the ia-64 architecture",
			"url": "https://www.cs.helsinki.fi/u/kerola/tikra/ch15.pdf",
			"excerpts": [
			  "e PFS application register contains the following fields:\n• **pfm:** Previous frame marker; contains all of the fields of the CFM\n• **pec:** Previous epilog count\n• **ppl:** Previous privilege leve",
			  "With **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (true).\nThis enables the processor to speculatively execute both branches of an if\nstatement and only commit after the condition is determined.\n�",
			  " With **control speculation** , a load instruction is moved earlier in the program\nand its original position replaced by a check instruction. The early load saves\ncycle time; if the load produces an exception, the exception is not activated\nuntil the check instruction determines if the load should have been taken.\n",
			  " With **data speculation** , a load is moved before a store instruction that might\n ",
			  "**CHAPTER 15 / THE IA-64 ARCHITECTURE**\n**KEY POINTS**\n� The IA-64 instruction set architecture is a new approach to providing hard-\nware support for instruction-level parallelism and is significantly different that\nthe approach taken in superscalar architectures.\n� The most noteworthy features of the IA-64 architecture are hardware support\nfor predicated execution, control speculation, data speculation, and software\npipelining.\n� With **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (true).\nThis enables the processor to speculatively execute both branches of an if\nstatement and only commit after the condition is determined.\n�",
			  "**Figure 15.3**\nIA-64 Predication and Speculative Loading\n**552**\n**CHAPTER 15 / THE IA-64 ARCHITECTURE**",
			  "\nThe basic concepts underlying IA-64 are as follows:\n• Instruction-level parallelism that is explicit in the machine instructions rather\nthan being determined at run time by the processor\n• Long or very long instruction words (LIW/VLIW)\n• Branch predication (not the same thing as branch prediction)\n• Speculative loading\nIntel and HP refer to this combination of concepts as explicitly parallel instruc-\ntion computing (EPIC). ",
			  "oftware pipelining** is a technique in which instructions from multiple itera-\ntions of a loop are enabled to execute in paralle",
			  "ntel and HP refer to this combination of concepts as explicitly parallel instruc-\ntion computing (EPIC).",
			  "Itanium organization blends superscalar features with support for\nthe unique EPIC-related IA-64 features.",
			  "he Ita-\nnium includes nine execution units: two integer, two floating-point, two memory,\nand three branch execution units",
			  "ith **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (tru",
			  "ith **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (tru",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier; advanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nld8.c r6 = [r8]\n// Cycle 0; check load\nadd r5 = r6, r7 ;;\n// Cycle 0\nst8 [r18] = r5\n// Cycle 1\nHere we have moved the ld instruction earlier and converted it into an\nadvanced load. In addition to performing the specified load, the ld8.a instruction\nwrites its source address (address contained in r8) to a hardware data structure\nknown as the Advanced Load Address Table (ALAT). Each IA-64 store instruc-\ntion checks the ALAT for entries that overlap with its target address; if a match is\nfound, the ALAT entry is removed. When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c. When the check load is executed, it checks the ALAT\nfor a matching address. If one is found, no store instruction between the advanced\nload and the check load has altered the source address of the load, and no action is\ntaken. However, if the check load instruction does not find a matching ALAT entry,\n ...",
			  "A-64 overcomes this problem with the use of a tech-\nnique known as advanced load",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier; advanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0",
			  "ld8.c r6 = [r8]\n// Cycle 0; check load",
			  "add r5 = r6, r7 ;;\n// Cycle 0",
			  "st8 [r18] = r5\n// Cycle 1",
			  "When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c. When the check load is executed, it checks the ALAT\nfor a matching address. If one is found, no store instruction between the advanced\nload and the check load has altered the source address of the load, and no action is\ntaken. However, if the check load instruction does not find a matching ALAT entry,"
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier;\nadvanced load",
			  "// other instructions\nst8 [r4] = r12",
			  "advanced load",
			  "When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c.",
			  "// Cycle 0; check load",
			  "If one is found, no store instruction between the\nadvanced load and the check load has altered the source address of the load, and\nno action is taken.",
			  "owever, if the check load instruction does not find a match-\ning ALAT entry, then the load operation is performed again to assure the correct\nresult",
			  "Here we use a chk.a instruction rather than an ld8.c instruction to validate\nthe advanced load.",
			  "Recover:",
			  "ld8 r6 = [r8] ;;",
			  "// reload r6 from [r8]",
			  "add r5 = r6, r7 ;;",
			  "add r5 = r6, r7 ;;",
			  "// re-execute the add",
			  "ld8.c r6 = [r8]\n// Cycle 0; check load",
			  "With **predicated execution** , every IA-64 instruction includes a refer-\nence to a 1-bit predicate register, and only executes if the predicate\nvalue is 1 (true). This enables the processor to speculatively execute\nboth branches of an if statement and only commit after the condition\nis determined.",
			  "The IA-64 instruction set architecture is a new approach to providing\nhardware support for instruction-level parallelism and is significantly\ndifferent than the approach taken in superscalar architectures.",
			  "The most noteworthy features of the IA-64 architecture are hardware\nsupport for predicated execution, control speculation, data speculation,\nand software pipelining.",
			  "ith **data speculation** , a load is moved before a store instruction that\nmight alter the memory location that is the source of the load.A subse-\nquent check is made to assure that the load receives the proper memory\nval",
			  "A speculative load (ld.s) executes the memory fetch, performs exception de-\ntection, but does not deliver the exception (call the OS routine that handles\nthe exception).This ld.s instruction is hoisted to an appropriate point earlier in\nthe program.",
			  "A checking instruction (chk.s) remains in the place of the original load and de-\nlivers exceptions. This chk.s instruction may be predicated so that it will only\nexecute if the predicate is true.",
			  "If the ld.s detects an exception, it sets a token bit associated with the target\nregister, known as the *Not a Thing* (NaT) bit.",
			  "The term *hoist*\nis used to refer to the movement of a load instruction to a point earlier in the\ninstruction stream.",
			  "Control Speculation",
			  "Another key innovation in IA-64 is control speculation, also known as speculative\nloading. This enables the processor to load data from memory before the program\nneeds it, to avoid memory latency delays.",
			  "IA-64 guarantees that if a branch is\ntaken, later instructions, even in the same instruction group, are not executed.",
			  "With data speculation, a load is moved before a store instruction that might alter the memory location that is the source of the load. A subse- quent check is ...Read more",
			  "With the Pentium 4, the microprocessor family that began with the 8086 and that\nhas been the most successful computer product line ever appears to have come\nto an end. Intel has teamed up with Hewlett-Packard (HP) to develop a new\n64-bit architecture, called IA-64. IA-64 is not a 64-bit extension of Intel’s 32-bit\nx86 architecture, nor is it an adaptation of Hewlett-Packard’s 64-bit PA-RISC\narchitecture. Instead, IA-64 is a new architecture that builds on years of research\nat the two companies and at universities. The architecture exploits the vast cir-\ncuitry and high speeds available on the newest generations of microchips by a\nsystematic use of parallelism. IA-64 architecture represents a significant depar-\nture from the trend to superscalar schemes that have dominated recent proces-\nsor development.",
			  "Let us look at a simple example, taken from [INTE00a,Volume 1]. Here is the\noriginal program:\n(p1) br some_label\n// Cycle 0\nld8 r1 = [r5] ;;\n// Cycle 1\nadd r2 = r1, r3\n// Cycle 3\nThe first instruction branches if predicate p1 is true (register p1 has value 1). Note\nthat the branch and load instructions are in the same instruction group,even though the\nload should not execute if the branch is taken. IA-64 guarantees that if a branch is\ntaken, later instructions, even in the same instruction group, are not executed. IA-64\nimplementations may use branch prediction to try to improve efficiency but must as-\nsure against incorrect results. Finally, note that the add instruction is delayed by at least",
			  "4 defines a 128-bit **bundle** that contains three instructions, called **syllables** , and\na template f",
			  "The processor can fetch instructions one or more\nbundles at a time; each bundle fetch brings in three instructions.",
			  "predicated execution** , every IA-64 instruction includes a refer-\nence to a 1-bit predicate register, and only executes if the predicate\nvalue is 1 (true).",
			  "ontrol Speculation**\nAnother key innovation in IA-64 is control speculation, also known as speculative\nloading. This enables the processor to load data from memory before the program\nneeds it, to avoid memory latency delays.",
			  "Intel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC).",
			  "he CFM register describes the state of the current general register stack frame, as-\nsociated with the currently active procedure. It includes the following fields",
			  "sof:** size of stack frame",
			  "sol:** size of locals portion of stack frame",
			  "sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining",
			  "The Ita-\nnium includes nine execution units: two integer, two floating-point, four memory,\nand three branch execution unit",
			  "ersal, the processor views at most two\ninstruction bundles at a time.The processor can issue a maximum of six instructions\nper clock cycle.\n",
			  "tanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.",
			  "EPIC related hardware includes support for predicated execution,\ncontrol and data speculation, and software pipelining.",
			  " architecture. Next, we look at the general organization to support the architecture.We\nthen examine in some detail the key features of the IA-64 architecture that promote\ninstruction-level parallelism. Finally, we look at the IA-64 instruction set architecture\nand the Itanium organization.\n**21.1 / MOTIVATION** **21-3**\n**21.1 MOTIVATION**\nThe basic concepts underlying IA-64 are:\n• Instruction-level parallelism that is explicit in the machine instructions rather\nthan being determined at run time by the processor\n• Long or very long instruction words (LIW/VLIW)\n• Branch predication (not the same thing as branch prediction)\n• Speculative loading\nIntel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC). Intel and HP use the term **EPIC** to refer to the tech-\nnology, or collection of techniques. **IA-64** is an actual instruction set architecture\nthat is intended for implementation using the EPIC technology.The first Intel prod-\nuct based on this architecture is referred to as **Itanium** . Other products will follow,\nbased on the same IA-64 architecture.\nTable 21.1 summarizes key differences between IA-64 and a traditional super-\nscalar approach.\nFor Intel,the move to a new architecture that is not hardware compatible with the\nx86 instruction architecture was a momentous decision.But it was driven by the dictates\nof the technology.When the x86 family began, back in the late 1970s, the processor chip\n ... \nbe restored through successive returns. This is a function of the alloc instruction,\nwhich designates a general register to save the current value of the PFS fields before\nthey are overwritten from the CFM fields.\n**Current Frame Marker and Previous Function State**\nThe CFM register describes the state of the current general register stack frame, as-\nsociated with the currently active procedure. It includes the following fields:\n• **sof:** size of stack frame\n• **sol:** size of locals portion of stack frame\n• **sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining\n• **register rename base values:** Values used in performing register rotation gen-\neral, floating-point and predicate registers\nThe PFS application register contains the following fields:\n• **pfm:** Previous frame marker; contains all of the fields of the CFM\n• **pec:** Previous epilog count\n• **ppl:** Previous privilege level\n**21.5 ITANIUM ORGANIZATION**\nIntel’s Itanium processor is the first implementation of the IA-64 instruction set ar-\nchitecture.The first version of this implementation, known as Itanium, was released\nin 2001, followed in 2002 by the Itanium 2. The Itanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.Among the\nsuperscalar features are a six-wide, ten-stage-deep hardware pipeline, dynamic\nprefetch, branch prediction, and a register scoreboard to optimize for compile time\nnondeterminism. EPIC related hardware includes support for predicated execution,\ncontrol and data speculation, and software pipelining.\nFigure 21.10 is a general block diagram of the Itanium organization. The Ita-\nnium includes nine execution units: two integer, two floating-point, four memory,\nand three branch execution units. Instructions are fetched through an L1 instruction\ncache and fed into a buffer that holds up to eight bundles of instructions. When de-\nciding on functional units for instruction dispersal, the processor views at most two\ninstruction bundles at a time.The processor can issue a maximum of six instructions\nper clock cycle.\nThe organization is in some ways simpler than a conventional contemporary\nsuperscalar organization. The Itanium does not use reservation stations, reorder\nbuffers, and memory ordering buffers, all replaced by simpler hardware for specula-\ntion.The register remapping hardware is simpler than the register aliasing typical of\nsuperscalar machines. Register dependency-detection logic is absent, replaced by\nexplicit parallelism directives precomputed by the software.\n**21.5 / ITANIUM ORGANIZATION** **21-29**\nUsing branch prediction, the fetch/prefetch engine can speculatively load\nan L1 instruction cache to minimize cache misses on instruction fetches. The\nfetched code is fed into a decoupling buffer that can hold up to eight ",
			  "Speculative loading",
			  "A-64** is an actual instruction set architecture\nthat is intended for implementation using the EPIC technology",
			  "With predicated execution, every IA-64 instruction includes a refer- ence to a 1-bit predicate register, and only executes if the predicate value is 1 (true). ...",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier;\nadvanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nld8.c r6 = [r8]\n// Cycle 0; check load\nadd r5 = r6, r7 ;;\n// Cycle 0\nst8 [r18] = r5\n// Cycle 1\nHere we have moved the ld instruction earlier and converted it into an ad-\nvanced load. In addition to performing the specified load, the ld8.a instruction\nwrites its source address (address contained in r8) to a hardware data structure\nknown as the Advanced Load Address Table (ALAT). Each IA-64 store instruc-\ntion checks the ALAT for entries that overlap with its target address; if a match is\nfound, the ALAT entry is removed.",
			  "We may also want to speculatively execute instructions that are data depen-\ndent on a load instruction, together with the load itself. Starting with the same\noriginal program, suppose we move up both the load and the subsequent add\ninstruction:\nld8.a r6 = [r8] ;;\n// Cycle -3 or earlier;\nadvanced load\n// other instructions\nadd r5 = r6, r7\n// Cycle -1; add that uses r6\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nchk.a r6, recover\n// Cycle 0; check\nback:\n// return point from jump to\nrecover\nst8 [r18] = r5\n// Cycle 0\nHere we use a chk.a instruction rather than an ld8.c instruction to validate\nthe advanced load. If the chk.a instruction determines that the load has failed, it\ncannot simply reexecute the load; instead, it branches to a recovery routine to\nclean up:\nRecover:\nld8 r6 = [r8] ;;\n// reload r6 from [r8]\nadd r5 = r6, r7 ;;\n// re-execute the add\nbr back\n// jump back to main code"
			]
		  },
		  {
			"title": "An Automated OpenCL FPGA Compilation Framework ...",
			"url": "https://repository.lboro.ac.uk/articles/An_automated_OpenCL_FPGA_compilation_framework_targeting_a_configurable_VLIW_chip_multiprocessor/9539657/files/17169290.pdf",
			"excerpts": [
			  "Predicated execution was used to reduce the negative effect of ... Code size can affect the performance of the instruction cache as a larger instruction.Read more"
			]
		  },
		  {
			"title": "Optimal vs. Heuristic Methods in a Production Compiler",
			"url": "https://www.cs.utexas.edu/~less/reading/spring00/ruttenberg.pdf",
			"excerpts": [
			  "a, if-conversion to convert loops with internal branches to a form using ... These two fundamental heuristics are modified in one or both of two ways to derive ..."
			]
		  },
		  {
			"title": "METHODOLOGIES FOR MANY-INPUT FEEDBACK-DIRECTED ...",
			"url": "https://central.bac-lac.gc.ca/.item?id=NR89287&op=pdf&app=Library&oclc_number=1019490294",
			"excerpts": [
			  "When a BB is selected for inclusion in the hyperblock, if-conversion is applied to remove the side exit and merge the block into the hyperblock. Tail ..."
			]
		  },
		  {
			"title": "Hyperblock formation of seemingly incompatible paths with...",
			"url": "https://www.researchgate.net/figure/Hyperblock-formation-of-seemingly-incompatible-paths-with-positive-results-due-to-code_fig1_3728133",
			"excerpts": [
			  "Hyperblock formation of seemingly incompatible paths with positive results due to code transformations. The T and F annotations in (a) indicate the taken ..."
			]
		  },
		  {
			"title": "CCIS 95 - Contemporary Computing - Springer Link",
			"url": "https://link.springer.com/content/pdf/10.1007/978-3-642-14825-5.pdf",
			"excerpts": [
			  "The control path relationship is also obvious if the control flow graph for the hyperblock before if-conversion is examined. In fig 2(b), it is clear that ..."
			]
		  },
		  {
			"title": "Hyperblock formation of seemingly compatible paths with...",
			"url": "https://www.researchgate.net/figure/Hyperblock-formation-of-seemingly-compatible-paths-with-negative-results_fig2_220091347",
			"excerpts": [
			  "In this paper we present simulation of optimization of branch prediction in parallel register sharing architecture to show high degree of ILP. The main idea ..."
			]
		  },
		  {
			"title": "Predicate Analysis and If-Conversion in an Itanium Link- ...",
			"url": "https://www2.cs.arizona.edu/~debray/Publications/ifcvt.pdf",
			"excerpts": [
			  "he Itanium contains multiple functional units and uses pro-\ngrammer specified instruction-level parallelism. Moreover, every\ninstruction is *predicated* : It specifies a one-bit predicate register,\nand if the value of that register is true (1), then the instruction is\nexecuted; otherwise, the instruction usually has no effect",
			  "The Ita-\nnium has 64 predicate registers; register p0 has constant value true\n(assignments to it are ignored",
			  "Instructions that specify a predicate register other\nthan p0 are said to be *guarded* ",
			  "Predicate registers are set by compare instructions.",
			  "A predicated instruction is guarded\nby a Boolean source operand; the instruction is executed only if this\nguard evaluates to true.",
			  " order to make effective use of the capabilities of such archi-\ntectures, a compiler must selectively eliminate conditional branches\nin favor of predicated instructions that are conditionally executed.",
			  "process, known as *if-conversion* , must be carried out judi-\nciously: if it is too aggressive, it leads to contention for system\nresources and a concomitant degradation in performance; if it is not\naggressive enough, it results in insufficient instruction-level paral-\nlelism, which also leads to a loss in performanc"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "The Effects of Predicated Execution on Branch Prediction",
			  "Gary Scott Tyson",
			  "Department of Computer Science",
			  "University of California, Davis",
			  "Davis, CA 95616",
			  "tyson@cs.ucdavis.edu",
			  "**\nHigh performance architectures have always had to\ndeal with the performance-limiting impact of branch\noperations. Microprocessor designs are going to have to\ndeal with this problem as well, as they move towards\ndeeper pipelines and support for multiple instruction\nissue. Branch prediction schemes are often used to allevi-\nate the negative impact of branch operations by allowing\nthe\nspeculative\nexecution\nof\ninstructions\nafter\nan\nunresolved branch.\n",
			  " form.\nThis paper analyzes a variety of existing predica-\ntion models for eliminating branch operations, and the\neffect that this elimination has on the branch prediction\nschemes in existing processors, including single issue\narchitectures with simple prediction mechanisms, to the\nnewer multi-issue designs with correspondingly more\nsophisticated branch predictors.\n",
			  "The effect on branch\nprediction accuracy, branch penalty and basic block size\nis studied.",
			  "Predicated Execution Models",
			  "entioned in the introduction, predicated execu-\ntion refers to the conditional execution (or completion) of\ninstructions based on the result of a boolean condition.\nSeveral different approaches to providing predicated exe-\ncution have been proposed and implemented. They fall\ninto two broad categories referred to as **restricted** and\n**unrestricted** .",
			  "\nIn the restricted model a limited number of new\npredicate instructions are introduced which are used to\nexplicitly delay the effect of executing a statement on\nprogram variables. This is achieved by moving the state-\nment in question up to before the branch, modifying it to\nwrite to a free register instead of a program variable, and\nthen using one of the special predicate operations to con-\nditionally update the active program variables. This is the\napproach used in the Alpha and HP-RISC processors.\nIn t",
			  "In the unrestricted predication model, *all* instruc-\ntions can be predicated. This can be accomplished in a\nnumber of ways. One way is to include an additional\noperand field for each instruction, as was done in the\nCydra 5. Another way is to introduce a special instruction\n�\n�\nwhich controls the conditional execution of following\n(non-predicated)\ninstructions.\nAn\nexample\nof\nthis\napproach is seen in the *guarded execution model* pro-\nposed by Pnevmatikatos and Sohi [PnSo94], which\nincludes special instructions whose execution specify\nwhether following instructions should or should not be",
			  "This section will present 4 different existing predi-\ncation models, those used in the Alpha processor, the\nHP-RISC, guarded execution model and the Cydra ",
			  " Alpha processor supports the restricted model\nof predication via a *conditional move* instruction. A stan-\ndard move instruction only requires 2 operands (source\nand destination), leaving one field free to specify the con-\nditional value in the conditional move. If the condition is\nsatisfied then the register movement is allowed, otherwise\na state change is prevented",
			  "e Alpha compiler uses the conditional move\ninstruction in the following way: An expression calcula-\ntion is moved up to before a conditional branch, and is\nmodified to write to a free register instead of a live pro-\ngram variable. A conditional move is then used in place\nof the original branch instruction to transfer the temporary\nvalue into the live register. If the condition is satisfied,\nthe original destination of the expression will contain the\nresult; if the condition is not satisfied, then the conditional\nmove operation is not performed, and the active state of\nthe application being executed is unchange",
			  "There are several restrictions on this form of predi-\ncation. First, the compiler must ensure that no exceptions\nwill be generated by boosting the expression code (e.g.\ndivision by zero must be excluded). Other instructions\nthat cannot be predicated include some memory access\ninstructions and flow control operation",
			  "Second, the\ncompiler must allocate free registers to store any results\nprior to the conditional move instruction(s). This may not\nbe feasible if it results in register spilling.",
			  "The HP Precision Architecture uses *instruction*\n*nullification* to provide a less restricted form of predica-\ntion. In this architecture, control flow (branch) and arith-\nmetic instructions can specify whether the *following*\ninstruction should execute. The ability of arithmetic\ninstructions to nullify the following instruction allows the\ncompiler to remove branch instructions. A transformation\nsimilar to that of the Alpha’s can be performed with the\nadded capacity to include additional *skip* instructions over\nexception producing instructi",
			  "Pnevmatikatos and Sohi propose the use of a **guard**\ninstruction to control the execution of a sequence of\ninstructions. A guard instruction specifies two things - a\ncondition register and a mask value to indicate which of\nthe following instructions are dependent on the contents\nof that condition register. The processor hardware then\nuses this information to create a dynamic *scalar mask*\nwhich is used to determine whether a given instruction\nshould be allowed to modify the state of the processor.",
			  "5.3. Effects of Predication on Basic Block Size",
			  "ntrol dependencies restrict the ability of multi-\nissue architectures to fill instruction slots. Branch predic-\ntion schemes are used to help alleviate this problem by\nproviding a set of candidate instructions with a high pro-\nbability of execution that can be used to fill vacant slots.",
			  "Unfortunately, many compiler transformation are still\nprecluded.",
			  "y removing the control dependency entirely via\npredication, the compiler has more flexibility in reorder-\ning the code sequence. This can achieve a more efficient\ncode schedule. An average of 30% of branch instructions\nare short forward branches and are good candidates for\npredicate transformation. The removal of these branches\nleads to a significant increase in basic block size, and thus\nan increase in the efficiency of the scheduler",
			  "e 7 shows the results of the *Aggressive*\ntransformation on the integer SPEC benchmarks. This\nfigure shows that as the predication distance increases, the\nbasic block size increases as well, and that a branch dis-\ntance of 12 provides a 40% increase in basic block size.\nThis is almost half of the total increase achievable if **all**\nforward branches were removed. The *Restricted* predica-\ntion scheme provides a much more limited improvement\nin basic block size (4% to 6% increas",
			  "It is also possible to use predication in conjunction\nwith other transformations to remove even a wider range\nof branches. For example, a loop unrolling transforma-\ntion removes backward branches by duplicating the body\nof the loop and iterating (and therefore branching) fewer\ntimes. If the number of iterations can be determined at\ncompile time, then the unrolling is a simple duplicatio",
			  "6. Conclusions",
			  "Supporting the conditional execution of instructions\nis a technique that can have a significant impact on the\nperformance of most high-performance architectures. It\n ... \ninstruction slots lost due to branch misprediction. How-\never, aggressive predication also increases average basic\nblock size by up to 45%, providing the code scheduler\nwith more instruction to fill the execution pipeline and\nsignificantly reducing pipeline sta",
			  "In addition to the aggressive predication scheme,\nwe examined two other more restrictive approaches.",
			  "Results indicate that current predication schemes that\nallow for expression boosting but not full instruction set\npredication show dramatically less ability to reduce\nbranch misprediction penalty and increase basic block\nsize than the more aggressive approach.",
			  "While these\nschemes can be useful in certain applications (e.g. tuned\nOS routines), they provide only a limited\nbenefit in\nimproving more general applications.",
			  "e feel adding architectural support for the condi-\ntional execution of instructions is going to continue to\ngrow in importance for the following reasons",
			  "Branch misprediction penalties will continue rising as\npipeline depths and issue widths increase",
			  "The predicate cost is in terms of instructions, and\ntherefore not dependent on issue width. As issue\nwidths increase, the performance penalty for branch\nprediction becomes greater, where the predication\ncost remains constant.",
			  "Branches transformed by predication have worse than\naverage prediction accuracy",
			  "Predication allows for a significant increase in basic\nblock size.",
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for",
			  "This paper analyzes a variety of existing predica-\ntion models for eliminating branch operations, and the\neffect that this elimination has on the branch prediction\nschemes in existing processors, including single issue\narchitectures with simple prediction mechanisms, to the\nnewer multi-issue designs with correspondingly more\nsophisticated branch predictor",
			  "...",
			  "Abstract"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "The main purpose of predication is to avoid jumps over very small sections of program code, increasing the effectiveness of [pipelined](/wiki/Pipeline_(computing) \"Pipeline (computing)\") execution and avoiding problems with the [cache](/wiki/CPU_cache \"CPU cache\") . It also has a number of more subtle benefits:",
			  "Functions that are traditionally computed using simple arithmetic and [bitwise operations](/wiki/Bitwise_operation \"Bitwise operation\") may be quicker to compute using predicated instructions.",
			  "Predicated instructions with different predicates can be mixed with each other and with unconditional code, allowing better [instruction scheduling](/wiki/Instruction_scheduling \"Instruction scheduling\") and so even better performance.",
			  "Elimination of unnecessary branch instructions can make the execution of necessary branches, such as those that make up loops, faster by lessening the load on [branch prediction](/wiki/Branch_prediction \"Branch prediction\") mechanisms.",
			  "Elimination of the cost of a branch misprediction which can be high on deeply pipelined architectures.",
			  "Predication's primary drawback is in increased encoding space. In typical implementations, every instruction reserves a bitfield for the predicate specifying under what conditions that instruction should have an effect. When available memory is limited, as on [embedded devices](/wiki/Embedded_system \"Embedded system\") , this space cost can be prohibitive. However, some architectures such as [Thumb-2](/wiki/Thumb-2 \"Thumb-2\") are able to avoid this issue (see below).",
			  "Other detriments are the following: [[ 3 ]]()\nPredication complicates the hardware by adding levels of [logic](/wiki/Control_unit \"Control unit\") to critical [paths](/wiki/Datapath \"Datapath\") and potentially degrades clock speed.",
			  "A predicated block includes cycles for all operations, so shorter [paths](/wiki/Control-flow_graph \"Control-flow graph\") may take longer and be penalized.",
			  "An extra register read is required. A non-predicated ADD would read two registers from a register file, where a Predicated ADD would need to also read the predicate register file. This increases Hazards in [Out-of-order execution](/wiki/Out-of-order_execution \"Out-of-order execution\") .",
			  "Predication is not usually speculated and causes a longer dependency chain. For ordered data this translates to a performance loss compared to a predictable branch. [[ 4 ]]()",
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true.",
			  "The use of predication is essential in IA-64's implementation of [software pipelining](/wiki/Software_pipelining \"Software pipelining\") because it avoids the need for writing separated code for prologs and epilogs.",
			  "The use of predication is essential in IA-64's implementation of software pipelining because it avoids the need for writing separated code for prologs and ...Read more"
			]
		  },
		  {
			"title": "How do predicated architectures (ARMv7, Itanium, etc.) ...",
			"url": "https://www.reddit.com/r/cpudesign/comments/157e52n/how_do_predicated_architectures_armv7_itanium_etc/",
			"excerpts": [
			  "Predication is not usually speculated and causes a longer dependency chain.",
			  "[This answer](https://stackoverflow.com/questions/50959808/how-does-out-of-order-execution-work-with-conditional-instructions-ex-cmovcc-i) by Peter Cordes indicates that the flags/status register itself is treated as an additional dependency, which makes sense.",
			  "Section Title: How do predicated architectures (ARMv7, Itanium, etc.) manage dynamic execution?",
			  "Is it, quite simply, as Wikipedia puts it - predicated instructions are issued in-order?"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://pages.cs.wisc.edu/~fischer/cs701/meta-optimization.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, the IM-\nPACT predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock. A hyperblock is a predi-\ncated single-entry, multiple-exit regi",
			  "Though predication effectively exposes ILP, simply pred-\nicating everything will diminish performance by saturating\nmachine resources with useless instruction",
			  "Figure 3 highlights the difference between control-flow\nand predicated execution.",
			  "Here, p2 and p3 are mutually exclusive predicates that are\nset according to the branch condition in part (a).",
			  " predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [1",
			  "h frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively affect performance.",
			  "Path ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels o",
			  "r of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc-\ntion in a hyperblock executes, long paths effectively\ndelay the time to completion of short paths. The cost\nof misprediction is relatively high for short paths.",
			  "mber of branches in path** :\nPaths of control\nthrough several branches have a greater chance of mis-\npredicting. Therefore, it may be worthwhile to predi-\ncate such paths. On the other hand, including several\nsuch paths may produce large hyperblocks that satu-\nrate resou",
			  "ompiler optimization considerations** : Paths that\ncontain hazard conditions ( *i.e.* , pointer dereferences\nand procedure calls) limit the effectiveness of many\ncompiler optimizations. In the presence of hazards, a\ncompiler must make conservative assump",
			  "chine-specific considerations** : A heuristic should\naccount for machine characteristics. For instance, the\nbranch delay penalty is a decisive factor",
			  "learly, there is much to consider when designing a heuris-\ntic for hyperblock selection",
			  "MPACT compiler begins by transforming the code\nso that it is more amenable to hyperblock formation [15].\nIMPACT’s algorithm then identifies acyclic paths of con-\ntrol that are suitable for hyperblock inclusio",
			  "A pri-\nority function— which is the critical calculation in the predi-\ncation decision process— assigns a value to each of the paths\nbased on characteristics such as the ones just described [15].\nSome of these characteristics come from runtime profiling.\nIMPACT uses the priority function shown below:\n*h* *i* =\n�\n0 *.* 25\n:\nif *path* *i* contains a hazard.\n1\n:\nif *path* *i* is hazard free.\n*d ratio* *i* =\n*dep height* *i*\nmax *j* =1 *→* *N* *dep height* *j*\n*o ratio* *i* =\n*num ops* *i*\nmax *j* =1 *→* *N* *num ops* *j*\n*priority* *i* = *exec ratio* *i* *·* *h* *i* *·* (2 *.* 1 *−* *d ratio* *i* *"
			]
		  },
		  {
			"title": "Optimize Options (Using the GNU Compiler Collection (GCC))",
			"url": "https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html",
			"excerpts": [
			  "RTL if-conversion tries to remove conditional branches around a block and",
			  "replace them with conditionally executed instructions.",
			  "These parameters\ngive the maximum permissible cost for the sequence that would be generated\nby if-conversion depending on whether the branch is statically determined\nto be predictable or not.",
			  "lso use other heuristics to decide whether if-conversion is likely to be\nprofitable.",
			  "The tracer-dynamic-coverage-feedback parameter\nis used only when profile\nfeedback is available.",
			  "The real profiles (as opposed to statically estimated\nones) are much less balanced allowing the threshold to be larger value.",
			  "`tracer-max-code-growth`",
			  "Stop tail duplication once code growth has reached given percentage.",
			  "he maximum number of instructions in a block which should be\nconsidered for if-conversion. ",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "Set to 0\nif either vectorization ( -ftree-vectorize ) or if-conversion\n( -ftree-loop-if-convert ) is disabled.",
			  "`case-values-threshold`",
			  "The smallest number of different values for which it is best to use a\njump-table instead of a tree of conditional branches."
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region.",
			  ". The processor\nnullifies all instructions in the incorrect path. I",
			  "redication can effectively expose ILP, simply predi-\ncating everything will diminish performance by saturating machine resources with useless\ninstructions",
			  "f p2 is true all of the time for instance, the instructions guarded by p3 con-\nsume valuable execution slots"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.eecg.utoronto.ca/~steffan/carg/readings/metaopt_talk.pdf",
			"excerpts": [
			  "***Hyperblock Formation***\nUses If-Conversion to convert control dependences into data\ndependences.\nSingle entry multiple exit block with predicated instructions.",
			  "There is a limit to the effectiveness of predicating instructions\nand forming large hyperblocks.",
			  "Path predictability - unpredictable branches",
			  "Path frequency - frequent paths",
			  "**Page 13**\n***Hyperblock Formation (2)***\nNumber of instructions in path - shorter paths\nNumber of branches in path - more branches but not too many\nCompiler optimization considerations (e.g. exceptions)\nMachine-specific considerations (e.g. branch delay penalty)",
			  "Experiment performed with Trimaran's IMPACT compiler\napproximating the Intel Itanium architecture (EPIC).",
			  "Tested with selections from SpecInt, SpecFP, Mediabench and\nother miscellaneous benchmark programs.",
			  "**Page 14**\n***Hyperblock Formation (3)***\n**Trimaran's Heuristic**\n**GP-generated Heuristic**",
			  "**Page 15**\n***Hyperblock Formation (4)***",
			  "***Hyperblock Formation (5)***",
			  "Quantitative Results:\nSingle benchmark results are very impressive: (1.54/1.23)\nMultiple benchmark results: (1.44/1.25)"
			]
		  },
		  {
			"title": "(PDF) A novel heuristic for selection of hyperblock in If-conversion",
			"url": "https://www.researchgate.net/publication/261459597_A_novel_heuristic_for_selection_of_hyperblock_in_If-conversion",
			"excerpts": [
			  "Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow.",
			  "Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions.",
			  "In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied.",
			  "On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution.",
			  "This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time",
			  "A hyperblock [10] is predicted region of code that contains straight-line sequence of instruction with a single entry point and possibly multiple exit points.",
			  "The navigation bandwidth of prediction mechanism depends upon the degree of ILP. It can be increased by increasing control flow prediction in procedural languages at compile time.",
			  "The main idea behind this concept is to use a step beyond the prediction of common branch and permitting the architecture to have the information about the CFG (Control Flow Graph) components of the program to have better branch decision for ILP.",
			  "on the ItaniumTM processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ].",
			  "We therefore implement the first formally verified implementation of a general if-conversion algorithm, with support for heuristics for branch prediction."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Most of these\nschemes group sequential basic blocks into larger entities, thereby\nreducing control dependencies, increasing the fetch rate and allow-\ning more opportunities for optimizatio",
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "imilar to\nsuperblocks, successive sets of instructions are combined and op-\ntimized to form a single hyperblock, however, hyperblocks may\ninclude multiple paths if the bias cannot be determined",
			  "predicated execution using hyper-\nblocks [1"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "Very Long Instruction Word Architectures and the ELI-512",
			"url": "https://courses.cs.washington.edu/courses/cse548/16wi/Fisher-VLIW.pdf",
			"excerpts": [
			  "TRACE SCIIEDULING. The VLIW compiler we have built uses a recent global compaction technique called trace scheduling Fisher 811. This technique ww originally ...Read more"
			]
		  },
		  {
			"title": "583 Lecture Notes",
			"url": "https://web.eecs.umich.edu/~mahlke/courses/583f11/lectures/583L5.ppt",
			"excerpts": [
			  "Mahlke et al., MICRO-25, 1992. \"Control CPR: A Branch Height Reduction Optimization for EPIC Processors\", M. Schlansker et al., PLDI-99, 1999. Material for ..."
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations.",
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "We also give a very simple proof that the trace scheduling algorithm always terminates."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant).",
			  "The performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%)."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "**In addition,** we provide a certified checker for path du-\nplications, which we use to prove tail duplication and loop\nunrolling optimizations. These optimizations increase sched-\nuling opportunities (at the price of code size increase). Our\nchecker of path duplications illustrates the interest of trans-\nlation validation designs: a simple formally verified checker\nenables to certify the correctness of an important class of\ntransformations, modulo small hints provided by oracles.",
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling.",
			  "**In addition,** we provide a certified checker for path du-\nplications, which we use to prove tail duplication and loop\nunrolling optimization"
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.classes.cs.uchicago.edu/archive/2017/fall/32001-1/papers/meta-optimization.pdf",
			"excerpts": [
			  "perblock for-\nmation. Here we introduce the optimization, and then we\ndiscuss factors that might be important when creating a pri-\nority function for it. We conclude the section by presenting\nexperimental results for hyperblock formatio",
			  "chitects have proposed two noteworthy methods for de-\ncreasing the costs associated with control transfers 3 : im-\nproved branch prediction, and predication. Improved branch\nprediction algorithms would obviously increase processor uti-\nlization. Unfortunately, some branches are inherently unpre-\ndictable, and hence, even the most sophisticated algorithm\nwould fail. For such branches, predication may be a fruitful\nalternative.",
			  "In the following list we give a brief overview of several cri-\nteria that are useful to consider when forming hyperblocks.\nSuch criteria are often referred to as features. In the list,\na path refers to a path of control flow ( *i.e.* , a sequence of\nbasic blocks that are connected by edges in the control flow\ngraph):",
			  "*Path predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [15]",
			  "ath frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively a ff ect performance",
			  "th ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels of ILP.",
			  "*Number of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc"
			]
		  },
		  {
			"title": "An Overview of IA-64 Architectural Features and Compiler ...",
			"url": "https://cseweb.ucsd.edu/classes/fa99/cse240/lee_ucsd_ia64.PDF",
			"excerpts": [
			  "Setting and propagation of deferred exceptions. – Speculative loads (ld.s) set the NaT bit or NaTVal if a deferrable exception occurs.Read more",
			  "❒ Parallel compares allow compound conditionals to be executed in a single instruction group. – Permit WAW in an instruction group. ❒ Example: if ( A && B && C ) ...Read more",
			  "ALAT - Advanced load address table",
			  "– HW structure containing information about outstanding advanced\nload addresses",
			  "– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "– ld.a - advanced load",
			  "– ld.c - check load",
			  "– ld.sa - combined control and data speculation.",
			  "Architectural Features",
			  "Compiler Optimization",
			  "Control and data speculation,\nPredication,\nMulti-way branch",
			  "Global scheduling with\ncontrol and data speculation,\nPredicate promotio",
			  "Modulo scheduling",
			  "cality hints,\nPrefetch,\nPost-increment",
			  "Memory hierarchy control",
			  "The compiler should take full advantage of IA-64 architectural\nfeatures to generate optimized code",
			  "O riginal\nR e schedule\nuse\nload\ncheck addressing conflict\nTransform\nstore\nuse\nstore\nload ignoring conflict\ncheck addressing conflict\nuse\nstore\nload ignoring conflict\naliased",
			  "chitectural Support for**\n**Data Speculati",
			  "ALAT - Advanced load address table\n– HW structure containing information about outstanding advanced\nload addresses\n– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "Compiler Directed Data Speculation",
			  "Separate load behavior from overlap detection\n– ld8.a can be scheduled passing aliased stores\n– chk.a detects conflict\n",
			  "ld8.a r1 = r2 ;;",
			  "add r3 = r1,r4",
			  "ld8.a r1 = r2",
			  "Data Speculation",
			  "chk.a r1, recovery",
			  "chk.a r1, recovery",
			  "Instructions",
			  "– chk.a - advance load checks"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.0 User's Guide ( ...",
			"url": "https://www.ti.com/litv/pdf/spru187q",
			"excerpts": [
			  "Memory Alias Disambiguation ... speculative execution of loads with bounded address ranges. --speculate_unknown_loads. Allows speculative execution of loads ...Read more",
			  "currently specified by the --speculate_loads option. You must increase the --speculate_loads threshold as recommended in the software-pipeline feedback ...",
			  "--speculate_unknown_loads. Allows speculative execution of loads with ... When the --speculate_loads=n option is used, the speculative threshold is ...Read more",
			  "**The MUST_ITERATE Pragma Syntax**",
			  "The syntax of the pragma for C and C++ is:\n**\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);\n",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple.",
			  "\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  "The arguments *min* and *max* are programmer-guaranteed minimum and maximum trip counts. The trip\ncount is the number of times a loop iterates.",
			  "The trip count of the loop must be evenly divisible by *multiple* .",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);",
			  "However, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple. Also, results of the program are undefined if the trip count is\nless than the minimum or greater than the maximum specified.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. You guarantee that\nthese properties are always true.",
			  "Through the use of the MUST_ITERATE pragma, you can guarantee that a loop executes a certain\nnumber of times.",
			  "Anytime the UNROLL pragma is applied to a loop,\nMUST_ITERATE should be applied to the same loop.",
			  "For loops the MUST_ITERATE pragma's third\nargument, multiple, is the most important and should always be specified.",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size."
			]
		  },
		  {
			"title": "Architecture Software Developer's Manual",
			"url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/itanium-architecture-software-developer-rev-2-3-vol-1-manual.pdf",
			"excerpts": [
			  "hen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly.",
			  "The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags.",
			  "The Not A Thing (NaT) bit is an extra bit on each of the general registers.",
			  "e check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Data Speculation and Instructions",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and\nfloating-point pair ( ldfp.a ) forms.",
			  "Data speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated result",
			  "When the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Support for this is enabled by special NaT bits that are attached to integer\nregisters and by special NatVal values for floating-point registers.",
			  " When a speculative\nload causes an exception, it is not immediately raised. Instead, the NaT bit is set on the\ndestination register (or NatVal is written into the floating-point register). Subsequent\nspeculative instructions that use a register with a set NaT bit propagate the setting until\na non-speculative instruction checks for or raises the deferred exception.",
			  "The advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "The advanced load check works like the\n**Figure 4-",
			  "Recovery code may use either a normal or advanced load to obtain the correct value for\nthe failed advanced load.",
			  "An advanced load is used only when it is advantageous to\nhave an ALAT entry reallocated after a failed speculation.",
			  " an advanced load is executed, it allocates an\nentry in a structure called the Advanced Load Address Table (ALAT). Later, when a\ncorresponding check instruction is executed, the presence of an entry indicates that the\ndata speculation succeeded; otherwise, the speculation failed and one of two kinds of\ncompiler-generated recovery is perfo",
			  "1.\nThe check load instruction ( ld.c , ldf.c , or ldfp.c ) is used for recovery when\nthe only instruction scheduled before a store that is ambiguous relative to the\nadvanced load is the advanced load itself.",
			  "2.\nThe advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "g (NaT) bit is an extra bit on each of the general registers. A register\nNaT bit indicates whether the content of a register is valid. If the NaT bit is set to one,\nthe register contains a deferred exception token due to an earlier speculation fault.",
			  "**Architecture**\nData speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated results.\nW",
			  "\nWhen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT). The ALAT is indexed by physical\nregister number and records the load address, the type of the load, and the size of the\nload.\n",
			  "\nA check instruction must be executed before the result of an advanced load can be used\nby any non-speculative instruction. The check instruction must specify the same\nregister number as the corresponding advanced load.\n",
			  "\nWhen a check instruction is executed, the ALAT is searched for an entry with the same\ntarget physical register number and type. If an entry is found, execution continues\nnormally with the next instruction.\n",
			  "\nIf no matching entry is found, the speculative results need to be recomputed:\n• Use a chk.a if a load and some of its uses are speculated. The chk.a jumps to\ncompiler-generated recovery code to re-execute the load and dependent\ninstructions.\n• Use a ld.c if no uses of the load are speculated. The ld.c reissues the load.\nEntries are removed from the ALAT due to:\n• Stores that write to addresses overlapping with ALAT entries.\n• Other advanced loads that target the same physical registers as ALAT entries.\n• Implementation-defined hardware or operating system conditions needed to\nmaintain correctness.\n• Limitations of the capacity, associativity, and matching algorithm used for a given\nimplementation of the ALAT.\n",
			  "\nThe ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly. The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags. An ALAT register tag is a unique number derived from the\nphysical target register number and type in conjunction with other\nimplementation-specific state. Implementation-specific state might include register\nstack wraparound information to distinguish one instance of a physical register that\nmay have been spilled by the RSE from the current instance of that register, thus\navoiding the need to purge the ALAT on all register stack wraparounds.\n",
			  "The Itanium architecture provides extensive support for software-pipelined loops,\nincluding register rotation, special loop branches, and application registers. When\ncombined with predication and support for speculation, these features help to reduce\ncode expansion, path length, and branch mispredictions for loops that can be software\npipelined."
			]
		  },
		  {
			"title": "IA64 Architecture and Compilers",
			"url": "https://old.hotchips.org/wp-content/uploads/hc_archives/hc11/1_Sun/hc99.t2.s1.IA64tut.pdf",
			"excerpts": [
			  "Original:",
			  "(p1)",
			  "br.cond",
			  "ld8.s r1 = [ r2 ]",
			  "chk.s r1, recovery",
			  "Data Speculation",
			  "original",
			  "ld8 r1 = [ r2 ]",
			  "ld8 r1 = [ r2 ]",
			  "Transformed:",
			  "Transformed:",
			  "ld8.a r1 = [ r2 ]",
			  "st4 [ r3 ] = r7",
			  "st4 [ r3 ] = r7",
			  "chk.a r1, recovery",
			  "Separates loads into 2 parts: speculative\nloading of data and detection of conflicts/faults .",
			  "Hoisting Uses",
			  "Uses can be hoisted, but then chk.a needed for recovery",
			  "ld.c r1=",
			  "ld8.a r1=",
			  "ld8.a r1=",
			  "instr 1",
			  "uses =r1",
			  "uses =r1",
			  "instr 2",
			  "instr 2",
			  "st8",
			  "st8",
			  "chk.a r1,rec",
			  "br home",
			  "Recovery code",
			  "No hoisted uses",
			  "With hoisted uses",
			  "Hot Chips 1999",
			  "Architectural Support for Data Speculation",
			  "ALAT - HW structure containing information about\noutstanding advanced loads",
			  "Instructions",
			  "– ld.a - advanced loads",
			  "– ld.c - check loads",
			  "– chk.a - advance load checks",
			  "Speculative Advanced loads - ld.sa - is an control\nspeculative advanced load with fault deferral\n(combines ld.a and ld.s)",
			  "Almost all instructions can have a qualifying predicate (qp). – Ex: (p1) add r1 = r2, r3. – if qp is true, instruction executed normally. – if qp is false ...Read more",
			  "Two kinds of instructions in IA-64 programs. – Non-Speculative Instructions -- known to be useful/needed. • would have been executed in the original program.Read more",
			  "...",
			  "...",
			  "Control Speculation"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 6.1 User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/spru187o/spru187o.pdf",
			"excerpts": [
			  "**--speculate_loads** = *n*\nSpecifies speculative load byte count threshold. Allows speculative\nexecution of loads with bounded addresses. See Section 3.2.3.1 .",
			  "Specifies speculative load byte count threshold. Allows speculative",
			  ".com\n*Optimizing Software Pipelining*\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "**--speculate_unknown_loads**",
			  "**--speculate_unknown_loads**\nAllows speculative execution of loads with bounded addresses.",
			  "Alias disambiguation is a technique that determines when two pointer expressions cannot point to the\nsame location, allowing the compiler to freely optimize such expressions.",
			  ";*\nMinimum required memory pad\n: 8 bytes",
			  "**3.2.3.2**",
			  "When you are writing your linear assembly, your code does *not* need to indicate the following:\n–\nPipeline latency\n–\nRegister usage\n–\nWhich unit is being used",
			  "e size reduction optimizations.\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.\nThe software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running. Example 3-1 illustrates the\ninformation that is generated for each loop.\nT",
			  "\nThe macro works regardless of which C6x device you build for, or if you port the code to another target.\nA more rare case is to pass the address of an offset from an array, for example:\nf(&buffer[3]);\nThis code passes an unaligned address to ptr, thus violating the presumption coded in the _nassert().\nThere is no direct remedy for this case. Avoid this practice whenever possible.",
			  "**Selecting the Best Threshold Value**",
			  "When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop",
			  "Collapsing can usually reduce the minimum safe trip count.",
			  "If the minimum known trip count is less than\nthe minimum safe trip count, a redundant loop is required.",
			  "Minimum required memory pad",
			  ": 8 bytes",
			  "Known Minimum Trip Count",
			  "Known Maximum Trip Count",
			  "Loop Carried Dependency Bound(^)",
			  "Unpartitioned Resource Bound",
			  "Partitioned Resource Bound(*)",
			  "5",
			  "Resource Partition:",
			  "A-side",
			  ".L units",
			  ".S units",
			  ".D units",
			  ".M units",
			  ".X cross paths",
			  ".T address paths",
			  "Long read paths",
			  "Long write paths",
			  "The\n--disable_software_pipelining option affects both compiled C/C++ code and assembly optimized code.",
			  "**Software Pipelining May Increase Code Size**",
			  "**Turn Off Software Pipelining (--disable_software_pipelining Option)**",
			  "If you enter comments on instructions in your linear assembly input file, the compiler moves the comments\nto the output file along with additional information. It attaches a 2-tuple <x, y> to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the software pipeline.",
			  "Figure 3-2. Software-Pipelined Loop*",
			  "This chapter provides an overview of these tools and introduces the features of the optimizing C/C++\ncompiler.",
			  "**3.2.3.1**",
			  "**Speculative Execution**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops.",
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "4",
			  "4",
			  "4",
			  "Optimizing Software Pipelining",
			  "0",
			  "0",
			  "0",
			  "2",
			  "2",
			  "2",
			  "1",
			  "1",
			  "1"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/sprui04b",
			"excerpts": [
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous.",
			  "Memory alias disambiguation is the process of determining when such\nambiguity is not possible. When you cannot determine whether two memory references are ambiguous,\nyou presume they are ambiguous. This is the same as saying the two instructions have a memory\ndependence between them.",
			  "The assembly optimizer assumes memory references are aliased, unless it can prove otherwise.",
			  "ne side** . The C6000 has predicate, or conditional, registers available\nfor use with conditional instructions. There are six predicate registers. There are three on the A side\nand three on the B side. ",
			  "peculative threshold exceeded** . It would be necessary to speculatively load beyond the threshold\ncurrently specified by the --speculate_loads option. You must increase the --speculate_loads threshold\nas recommended in the software-pipeline feedback located in the assembly file",
			  "The resource breakdown\ndisplayed in the software pipelining information in the asm file, is computed after the compiler has\npartitioned instructions to either the A-side or the B-side. If the resources are imbalanced (i.e.; some\nresources on one side are used more than resources on the other) software pipelining is resource-bound,\nand the loop cannot be efficiently scheduled. If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining. You can give loop trip-\ncount information to the compiler using the \"MUST_ITERATE\" pragma",
			  "advice #30011: Consider adding assertions to indicate n-byte alignment\nof variables input1, input2, output if they are actually n-byte\naligned: _nassert((int)(input1) % 8 == 0).\nMost loops have memory access instructions. The compiler attempts to use wider load instructions, and\naligned memory accesses instead of non-aligned memory accesses to reduce/balance out resources used",
			  "**LDDW**",
			  "**STDW**",
			  "Allows aligned loads and stores of 8 bytes to memory. The pointer\nmust be aligned to an eight-byte boundary. This is defined as a\nmacro. You must include c6x.h. (2)(1)",
			  "Allows aligned loads of 8 bytes from memory. The pointer must be\naligned to an eight-byte boundary.",
			  "*Example 2",
			  "_nassert (((int)(x) & 0x3) == 0);",
			  "\\ MUST_ITERATE(20, , 4);",
			  "The C66x DSP generation comprises floating-point devices in the C6000 DSP platform. SPRUFE8— TMS320C674x CPU and Instruction Set Reference Guide. Describes ...Read more",
			  ".2.2***\n***Software Pipelining Information***\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "**4.2.3.1**\n**Speculative Execution**\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "collapsing which can be performed.\nWhen the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop. If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited. To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.\nCo",
			  "**4.3**\n**Redundant Loops**\nEvery loop iterates some number of times before the loop terminates. The number of iterations is called\nthe *trip count* . The variable used to count each iteration is the *trip counter* . When the trip counter reaches\na limit equal to the trip count, the loop terminates. The C6000 tools use the trip count to determine\nwhether or not a loop can be pipelined. The structure of a software pipelined loop requires the execution\nof a minimum number of loop iterations (a minimum trip count) in order to fill or prime the pipeline.",
			  "*4.4**\n**Utilizing the Loop Buffer Using SPLOOP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory",
			  "ized code.\n**Software Pipelining May Increase Code Size**\n**NOTE:**\nSoftware pipelining without the use of SPLOOP can lead to significant increases in code\nsize. To control code size for loops that get software pipelined, the --opt_for_space option is\nrecommended over the --disable_software_pipelining option. The --opt_for_space option is\ncapable of disabling non-SPLOOP software pipelining if necessary to achieve code size\nsavings, but it does not affect the SPLOOP capability (). SPLOOP does not significantly\nincrease code size, but can greatly speed up a loop. Using the --disable_software_pipelining\noption disables all software pipelining including SPLOOP.\n"
			]
		  },
		  {
			"title": "Register Allocation on the Intel® Itanium® Architecture",
			"url": "https://ssw.jku.at/Teaching/PhDTheses/Hoflehner/Diss_GerolfHoflehner_2010.pdf",
			"excerpts": [
			  "7.2 Data Speculation. IA-64 provides an advanced load (ld.a) and two advanced load check (chk.a, ld.c) instructions for data speculation. This enables the ...Read more",
			  "On IA-64 there are 64 predicate registers. Therefore the encoding of the qualifying predicate consumes 6 bits, which is one of the reasons why operations. ( ...Read more"
			]
		  },
		  {
			"title": "\n            BibTeX bibliography intel-ia-64.bib\n        ",
			"url": "http://ftp.math.utah.edu/pub/tex/bib/intel-ia-64.html",
			"excerpts": [
			  "\", URL = \"http://ieeexplore.ieee.org/iel5/6577/17552 ... data dependence speculation, and predication have individually been shown to be very effective.Read more"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531901.pdf",
			"excerpts": [
			  "The result of a control- or data-speculative calculation is checked for success or failure. If the check\nfails, a branch to *target* *25* is taken.",
			  "In the control_form, success is determined by a NaT indication for the source register. If the\nNaT bit corresponding to GR *r* *2* is 1 (in the gr_form), or FR *f* *2* contains a NaTVal (in the fr_form),\nthe check fails.",
			  "An entry is added to the ALAT. This allows later\ninstructions to check for colliding stores. If the referenced\ndata page has a non-speculative attribute, the target\nregister and NaT bit is cleared, and the processor\nensures that no ALAT entry exists for the target register.",
			  "The ALAT is queried using the general\nregister specifier *r* *1* (in the gr_form), or the floating-point register specifier *f* *1* (in the fr_form). If no\n ..",
			  "Regardless of ALAT hit or miss, base register updates\nare performed, if specified. An implementation may\noptionally cause the ALAT lookup to fail independent of\nwhether an ALAT entry matches. If not found, a load is\nperformed, and an entry is added to the ALAT (unless the\nreferenced data page has a non-speculative attribute, in\nwhich case no ALAT entry is allocated).",
			  "chk.a. *aclr r* *1* , *target* *25*",
			  "chk.a. *aclr f* *1* , *target* *25",
			  "**Table 2-30. sz Completers**",
			  "**sz Completer**",
			  "**Bytes Accessed**",
			  "1 byte",
			  "2 bytes",
			  "4 bytes",
			  "8",
			  "8 bytes",
			  "**Table 2-31. Load Types**",
			  "**ldtype**",
			  "**Completer**",
			  "**Interpretation**",
			  "**Special Load Operation**",
			  "none",
			  "Normal load",
			  "S",
			  "Certain exceptions may be deferred rather than\ngenerating a fault. Deferral causes the target register’s\nNaT bit to be set. The NaT bit is later used to detect\ndeferral.",
			  "For the non-speculative load types, if NaT bit associated with GR *r* *3* is 1, a Register NaT\nConsumption fault is taken. For speculative and speculative advanced loads, no fault is raised, and\nthe exception is deferred. For the base-update calculation, if the NaT bit associated with GR *r* *2* is 1,\nthe NaT bit associated with GR *r* *3* is set to 1 and no fault is raised.",
			  "The value of the *ldhint* completer specifies the locality of the memory access.",
			  "speculative",
			  "4",
			  "load",
			  "**ld**",
			  "2",
			  "1"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "ISCA 2004Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including\npredication and explicitcontrol speculation, intended to enhance the compiler'sability\nto expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface. This study presents an empirical evaluation of the use of the ALAT and data ...",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs",
			  "An Empirical Study of Data Speculation Use on the Intel Itanium 2 Processor",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface.",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.Our work shows a 1.13 average speedup(up to 1.50) due to these compilation techniques, relativeto traditionally-optimized code at the same inlining andpointer analysis levels, and a 1.55 speedup (up to 2.30) relativeto GNU GCC, a solid traditional compiler.",
			  "We also demonstratethe increasing significance of runtime effects, such as datacache and TLB, in determining end performance and theinteraction of these effects with control speculation.",
			  "ALAT"
			]
		  },
		  {
			"title": "THE INTEL IA-64 COMPILER CODE GENERATOR",
			"url": "https://www.cs.virginia.edu/~skadron/cs854_uproc_survey/spring_2001/cs854/m5044.pdf",
			"excerpts": [
			  "The ECG consists of multiple phases occur-\nring in the order shown in Figure 1. The first\nphase, translation, converts the optimizer’s\nintermediate representation (IL0) of the pro-\ngram into the ECG IR. Predicate region for-\nmation, if conversion, and compare\ngeneration occur in the predication ",
			  "he ECG contains two schedulers: the soft-\nware pipeliner for targeted cyclic regions and\nthe global code scheduler for all remaining\nregions. Both schedulers make use of control\nand data speculation",
			  "e ECG’s register allocator must handle\nseveral IA-64 specific issues. These include\nnot-a-thing (NaT) bit maintenance during\nspill/fill, advanced load address table (ALAT)\nawareness for data-speculative registers, cor-\nrect rotating register allocation for the soft-\nware pipeliner, and predicate awarenes",
			  "aT\nbits are associated with a control speculation\nmechanism. The ALAT is the hardware mech-\nanism enabling data speculation. 1\nThe predicator\nPredication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features",
			  "Predication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features.",
			  "Data spec-\nulation allows unlikely memory dependencies\nto be broke",
			  "GCS selects the best candidate from all the\nspeculative and nonspeculative candidates based\non a cost-benefit analysis.",
			  "ulation check instruc-\ntions and recovery code 8 are generated as a\nbyproduct of speculation; load safety informa-\ntion 9 avoids unnecessary control speculat",
			  "Figure 6. Comparing data speculation usage:\nwithout (a) and with data speculation (b)."
			]
		  },
		  {
			"title": "Instruction Level Parallelism - the static scheduling approach",
			"url": "http://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/2007-08/Lectures/pdfs/ACA-Ch05-EPICandThreaded-CurrentVersion-WithoutHiddenSlides-4up.pdf",
			"excerpts": [
			  "IA64: Speculative, Non-Faulting Load",
			  ".s** fetches speculatively from memory",
			  "e. any exception due to **ld.s** is suppress",
			  "NOP, else a branch is taken to some compensation code",
			  "ld.a** starts the monitoring of any store to the sa",
			  "address as the advanced load",
			  "no aliasing has occurred since **ld.a, ld.c** is a NO",
			  "aliasing has occurred, **ld.c** re-loads from memor"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v7.4 - User's Guide",
			"url": "https://www.ti.com/lit/pdf/spru187",
			"excerpts": [
			  "Memory Alias Disambiguation*\n***4.5.4***\n***Memory Bank Conflict Algorithm***\nThe assembly optimizer uses the following process to determine if two memory access instructions might\nhave a memory bank conflict:",
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous. Memory alias disambiguation is the process of determining when such\nambiguity is not possible. When you cannot determine whether two memory references are ambiguous,\nyou presume they are ambiguous. This is the same as saying the two instructions have a memory\ndependence between them.",
			  "Every loop iterates some number of times before the loop terminates. The number of iterations is called",
			  "the *trip count* . The variable used to count each iteration is the *trip counter* . When the trip counter reaches",
			  "a limit equal to the trip count, the loop terminates. The C6000 tools use the trip count to determine",
			  "**Redundant Loops**",
			  "**Redundant Loops**",
			  "Beginning with v7.4.0 of the C6000 Code Generation Tools, the compiler and linker can provide automatic\nload speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load\noptimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the\ncompiler can prove to be allocated.\nIn",
			  "However, you can also specify the speculative loads threshold explicitly via the *--speculate_loads=n*",
			  "option, where *n* is at least the minimum required pad (as explained earlier), but you also need to consider",
			  "whether a larger threshold value would facilitate additional collapsing. This information is also provided, if",
			  "applicable. For example, in the above comment block, a threshold value of 14 might facilitate further",
			  "collapsing. If you choose the *auto* argument to --speculate_loads, the compiler will consider the larger",
			  "threshold value automatically.",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing",
			  "loads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler",
			  "cannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the",
			  "cannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the",
			  "compiler can predicate these loads to prevent over execution. However, this can increase register",
			  "compiler can predicate these loads to prevent over execution. However, this can increase register",
			  "pressure and might decrease the total amount collapsing which can be performed.",
			  "pressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0",
			  "to *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory",
			  "to *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory",
			  "location it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "location it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "**3.2.3.1**",
			  "**3.2.3.1**",
			  "Beginning with v7.4.0 of the C6000 Code Generation Tools, the compiler and linker can provide automatic",
			  "load speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load",
			  "load speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load",
			  "optimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the",
			  "optimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the",
			  "compiler can prove to be allocated.",
			  "compiler can prove to be allocated.",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required",
			  "pre- and post-padding:",
			  "pre- and post-padding:",
			  "If the symbol of the speculatively loaded buffer is known during compile time, the linker will ensure the",
			  "If the symbol of the speculatively loaded buffer is known during compile time, the linker will ensure the",
			  "object pointed to by the symbol has the required padding to let the speculative load access legal",
			  "object pointed to by the symbol has the required padding to let the speculative load access legal",
			  "memory.",
			  "memory.",
			  "•",
			  "•",
			  "•",
			  "•",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of",
			  "data sections will allow legal accessing beyond the boundaries of the data sections.",
			  "data sections will allow legal accessing beyond the boundaries of the data sections.",
			  "The syntax of the pragma in C is:\n**\\ DATA_ALIGN (** *symbol* **,** *constant* **);**\n",
			  "The DATA_ALIGN pragma aligns the *symbol* in C, or the next symbol declared in C++, to an alignment\nboundary. The alignment boundary is the maximum of the symbol's default alignment value or the value of\nthe *constant* in bytes. The constant must be a power of 2.",
			  "count is the number of times a loop iterates. The trip count of the loop must be evenly divisible by *multiple* .",
			  "When compiling for C6600 devices, such an array is automatically aligned to a 16-byte boundary. When\ncompiling for C6400, C6400+, C6740, and C6600 devices, such an array is automatically aligned to an 8-\nbyte boundary. When compiling for C6200 or C6700, such an array is automatically aligned to 4-byte\nboundary, or, if the base type requires it, an 8-byte boundary",
			  "argument list as follows:",
			  "\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 16 == 0)",
			  "However, if the trip count could be any nonzero multiple of 5, the pragma would look like this:",
			  "\\ defined(_TMS320C6200) || defined(_TMS320C6700)\n\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 4 == 0)\n\\",
			  "If multiple\nMUST_ITERATE pragmas are specified for the same loop, the smallest max and largest min are used.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. You guarantee that",
			  "these properties are always true. Through the use of the MUST_ITERATE pragma, you can guarantee",
			  "that a loop executes a specific number of times.",
			  ")\n\\ defined(_TMS320C6400)\n\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 8 == 0)",
			  "for(i = 0; i < trip_count; i++)",
			  "{ ...",
			  "This example tells the compiler that the loop executes between 8 and 48 times and that the trip_count",
			  "variable is\na multiple of 8 (8, 16, 24, 32, 40, 48). The compiler can now use all this information to generate the best",
			  "loop possible by unrolling better even when the --interrupt_threshold *n* option is used to specify that",
			  "interrupts do occur every *n* cycles.",
			  "the MUST_ITERATE pragma's third\nargument, multiple, is the most important and should always be specified.",
			  "9.20.2**\n**Using MUST_ITERATE to Expand Compiler Knowledge of Loo",
			  "\\ ALIGNED_ARRAY(ptr) /* empty */\n",
			  "MUST_ITERATE can specify a range for the trip count as well as a factor of the trip count.",
			  "**Using MUST_ITERATE and _nassert to Enable SIMD and Expand Compiler***\n***Knowledge of Loo",
			  "Through the use of MUST_ITERATE and _nassert, you can guarantee that a loop executes a certain\nnumber of times.",
			  "Through the use of MUST_ITERATE and _nassert, you can guarantee that a loop executes a certain\nnumber of times.",
			  "This example tells the compiler that the loop is guaranteed to run exactly 10 times:",
			  "This example tells the compiler that the loop is guaranteed to run exactly 10 times:",
			  "\\ MUST_ITERATE(10,10);",
			  "for (I = 0; I < trip_count; I++) { ...",
			  "for (I = 0; I < trip_count; I++) { ...",
			  "MUST_ITERATE can also be used to specify a range for the trip count as well as a factor of the trip count.",
			  "MUST_ITERATE can also be used to specify a range for the trip count as well as a factor of the trip count.",
			  "For example:",
			  "For example:",
			  "For example:",
			  "\\ MUST_ITERATE(8,48,8);",
			  "\\ MUST_ITERATE(8,48,8);",
			  "for (I = 0; I < trip; I++) { ...",
			  "for (I = 0; I < trip; I++) { ...",
			  "This example tells the compiler that the loop executes between 8 and 48 times and that the trip variable is\na multiple of 8 (8, 16, 24, 32, 40, 48).",
			  "The compiler can now use all this information to generate the best\nloop possible by unrolling better even when the --interrupt_threshold *n* option is used to specify that\ninterrupts do occur every *n* cycles.",
			  "The *TMS320C6000 Programmer's Guide* states that one of the ways to refine C/C++ code is to use word\naccesses to operate on 16-bit data stored in the high and low parts of a 32-bit register. Examples using\ncasts to int pointers are shown with the use of intrinsics to use certain instructions like _mpyh. This can be\nautomated by using the _nassert(); intrinsic to specify that 16-bit short arrays are aligned on a 32-bit\n(word) boundary.",
			  "The following examples generate the same assembly code:",
			  "The following examples generate the same assembly code:",
			  "The C6400+, C6740, and C6600 ISAs have a loop buffer which improves performance and reduces code\nsize for software pipelined loops. The loop buffer provides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.",
			  "u can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C6400/C6400+ CPU*\n ... \nway that improves performance while maintaining the semantics of the original order. Instruction\nscheduling is used to improve instruction parallelism and hide pipeline latencies. It can also be used to\nreduce code size.",
			  "The assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code",
			  "Like the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\n ... \nope",
			  "An LD(B/BU)(H/HU)(W) or ST(B/H/W) operation in linear assembly can have memory bank information\nassociated with it implicitly, by using the .mptr directive. The .mptr directive associates a register with the\ninformation that allows the assembly optimizer to determine automatically whether two memory operations\nhave a bank conflict. If the assembly optimizer determines that two memory operations have a memory\nbank conflict, then it does not schedule them in parallel within a software pipelined loop.",
			  "For example:\n.mptr a_0,a+0,16\n.mptr a_4,a+4,16\nLDW *a_0++[4], val1\n; base=a, offset=0, stride=16\nLDW *a_4++[4], val2\n; base=a, offset=4, stride=16\n.mptr dptr,D+0,8\nLDH *dptr++, d0\n; base=D, offset=0, stride=8\nLDH *dptr++, d1\n; base=D, offset=2, stride=8\nLDH *dptr++, d2\n; base=D, offset=4, stride=8",
			  "The goal is to find a software pipeline in which the following instructions are in parallel:\nLDW *a0++[2],val1 ; load a[0-1]\nbankx\n||\nLDW *a2++[2],val2 ; load a[2-3]\nbankx+2\nLDW *b0++[2],val1 ; load b[0-1]\nbanky\n||\nLDW *b2++[2],val2 ; load b[2-3]\nbanky+2",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute\nin parallel. See Section 3.2 for more information.",
			  "The assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in",
			  "There are five predicate registers on the C6200 and C6700, and\nsix predicate registers on the C6400, C6400+, and C6700+.",
			  "NOTE:**\nRefer to the *TMS320C6000 Programmer’s Guide* for details on the information and\nmessages that can appear in the Software Pipelining Information comment block before each\nloo",
			  "Example 3-1. Software Pipelining Information",
			  "Known Minimum Trip Count\n: 2",
			  "Known Maximum Trip Count\n: 2",
			  "Known Max Trip Count Factor\n: 2",
			  "Loop Carried Dependency Bound(^) : 4",
			  "Unpartitioned Resource Bound\n: 4",
			  "Partitioned Resource Bound(*)\n: 5",
			  "Resource Partition:\n;*\nA-side\nB-s",
			  "ii = 5",
			  "Register is live too long",
			  "ii = 6",
			  "Did not find schedule",
			  "ii = 7",
			  "Schedule found with 3 iterations in parallel",
			  "Prolog not removed",
			  "Collapsed prolog stages\n: 0",
			  "Minimum required memory pad : 2 bytes",
			  "Minimum safe trip count\n: 2",
			  "Turn Off Software Pipelining (--disable_software_pipelining Option)",
			  "TE:**\nSpecifying any --opt_for_space option turns off redundant loops.",
			  "3.4",
			  "Utilizing the Loop Buffer Using SPLOOP on C6400+, C6740, and C6600",
			  "The C6400+, C6740, and C6600 ISAs have a loop buffer which improves performance and reduces code\nsize for software pipelined loops. The loop buffer provides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.",
			  "nterrupt latency. Loops executing out of the loop buffer are interruptible.",
			  "You can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C6400/C6400+ CPU*",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "Turning Off Redundant Loops",
			  "Improves performance for loops with unknown trip counts and eliminates redundant loops.",
			  "Reduces or eliminates the need for speculated loads.",
			  "Reduces power usage.",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/sprui04b/sprui04b.pdf",
			"excerpts": [
			  "**Speculative Execution**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "Collapsing can usually reduce the minimum safe trip count. If the minimum known trip count is less than\nthe minimum safe trip count, a redundant loop is required.",
			  "he compiler and linker can provide automatic load speculation via the *auto* argument to the --\nspeculate_loads option (i.e. *--speculate_loads=auto* or *-mh=auto* ). Use of the *auto* argument makes it\neasier to use and benefit from speculative load optimizations. This option can generate speculative loads\nof up to 256 bytes beyond memory that the compiler can prove to be allocated.",
			  "If the symbol of the speculatively loaded buffer is known at compile time, the linker ensures the object\npointed to by the symbol has the required padding to let the speculative load access legal memory.",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of\ndata sections will allow legal accessing beyond the boundaries of the data sections.",
			  "d.\nHowever, you can also specify the speculative loads threshold explicitly via the *--speculate_loads=n*\noption, where *n* is at least the minimum required pad (as explained earlier)",
			  "**Memory Alias Disambiguation**",
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous. Memory alias disambiguation is the process of determining when such\nambiguity is not possible.",
			  "Dependencies between instructions constrain the instruction schedule, including the software pipeline\nschedule. In general, the fewer the Dependencies, the greater freedom you have in choosing a schedule\nand the better the final schedule performs.",
			  "***5.6.1***",
			  "he Assembly Optimizer Handles Memory References (Default)***\nThe assembly optimizer assumes memory references are aliased, unless it can prove otherwis",
			  "Because alias analysis is very limited in the assembly optimizer, this presumption is often overly\nconservative.",
			  "In such cases, the extra instruction Dependencies, due to the presumed memory aliases,",
			  "The syntax of the pragma for C and C++ is:\n**\\ MUST_ITERATE (** *min* , *max* , *multiple* **)",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5)",
			  "***Advice #30011 Improve Loop; Add _nasssert()***",
			  "\nYou may be able to help the compiler avoid producing redundant loops with the use of --\nprogram_level_compile --opt_level=3 (see Section 4.7 ) or the use of the MUST_ITERATE pragma (see\nSection 7.9.21 ).\n",
			  "The loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.\nYou can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C64x/C64x+ CPU and*\n*Instruction Set Reference Guide* for information on SPLOOP.",
			  "Most loops have memory access instructions. The compiler attempts to use wider load instructions, and\naligned memory accesses instead of non-aligned memory accesses to reduce/balance out resources used",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5) /* Note the blank field for max */\n",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5) /* Note the blank field for max */\n",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. When you use this\npragma, you are guaranteeing to the compiler that a loop executes a specific number of times or a\nnumber of times within a specified range.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. When you use this\npragma, you are guaranteeing to the compiler that a loop executes a specific number of times or a\nnumber of times within a specified range.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "For loops the MUST_ITERATE pragma's third argument, multiple, is the most important and should\nalways be specified.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "***4.14.17***",
			  "Turning Off Redundant Loops",
			  "OTE:**\nSpecifying any --opt_for_space option turns off redundant loops",
			  "Utilizing the Loop Buffer Using SPLOOP",
			  "OP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.\nYou can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C64x/C64x+ CPU and*\n*Instruction Set Reference Guide* for information on SPLOOP.\nWhen",
			  "When the --opt_for_space option is not used, the compiler will not use the loop buffer if it can find a faster\nsoftware pipelined loop without it. When using the --opt_for_space option, the compiler will use the loop\nbuffer when it can.",
			  "he compiler does not generate code for the loop buffer (SPLOOP/D/W) when any of the following occur:\n•\nii (initiation interval) > 14 cycle",
			  "About the Assembly Optimizer",
			  "**\nIf you are not satisfied with the performance of your C/C++ code after you have used all of the C/C++\noptimizations that are available, you can use the assembly optimizer to make it easier to write assembly\ncode for the C6000.\nThe assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\nparallel. The code generation tools attempt to software pipeline your code with inputs from you and with\n ... \nBa",
			  "\n*Avoiding Memory Bank Conflicts With the Assembly Optimizer",
			  "The internal memory of the C6000 family varies from device to device. See the appropriate device data\nsheet to determine the memory spaces in your particular device. This section discusses how to write code\nto avoid memory bank conflicts.\nMost C6000 devices use an interleaved memory bank scheme, as shown in Figure 5-1 . Each number in\nthe diagram represents a byte address. A load byte (LDB) instruction from address 0 loads byte 0 in\nbank 0. A load halfword (LDH) from address 0 loads the halfword value in bytes 0 and 1, which are also in\nbank 0. A load word (LDW) from address 0 loads bytes 0 through 3 in banks 0 and 1.\nBecause each bank is single-ported memory, only one access to each bank is allowed per cycle. Two\naccesses to a single bank in a given cycle result in a memory stall that halts all pipeline operation for one",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop. If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited. To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "The following messages appear if the loop is completely disqualified for software pipelining:",
			  "ad loop structure** . This error is very rare and can stem from the following",
			  "An asm statement inserted in the C code inner loop",
			  "Parallel instructions being used as input to the Linear Assembly Optimizer",
			  "Complex control flow such as GOTO statements and breaks",
			  "op contains a call** . Sometimes the compiler may not be able to inline a function call that is in a\nloop.",
			  "Because the compiler could not inline the function call, the loop could not be software pipelined.",
			  "oo many instructions** . There are too many instructions in the loop to software pipeline",
			  "tware pipelining disabled** . Software pipelining has been disabled by a command-line option, such\nas when using the --disable_software_pipelining option, not using the --opt_level=2 (or -O2) or --\nopt_level=3 (or -O3) option, or using the --opt_for_space=2 or --opt_for_space=3 option.",
			  "Specifying any --opt_for_space option turns off redundant loops.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "hen the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "**Redundant Loops**\nEvery loop iterates some number of times before the loop terminates. The number of iterations is called\nthe *trip count",
			  "The C6000 tools use the trip count to determine\nwhether or not a loop can be pipelined.",
			  "The structure of a software pipelined loop requires the execution\nof a minimum number of loop iterations (a minimum trip count) in order to fill or prime the pipeline.",
			  "*Utilizing the Loop Buffer Using SPLOOP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. ",
			  "The loop buffer\nprovides the following benefits:",
			  "Code size. A single iteration of the loop is stored in program memory.",
			  "Interrupt latency. Loops executing out of the loop buffer are interruptible.",
			  "Improves performance for loops with unknown trip counts and eliminates redundant loops.",
			  "Reduces or eliminates the need for speculated loads.",
			  "Reduces power usage.",
			  "SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end.",
			  "When the --opt_for_space option is not used, the compiler will not use the loop buffer if it can find a faster\nsoftware pipelined loop without it.",
			  "When using the --opt_for_space option, the compiler will use the loop\nbuffer when it can.",
			  "The compiler does not generate code for the loop buffer (SPLOOP/D/W) when any of the following occur:",
			  "ii (initiation interval) > 14 cycles",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "The minimum value of *n* needed to achieve this software pipeline schedule and level of collapsing",
			  "Suggestion for a larger value of *n* to use which might allow additional collapsing",
			  "This information shows up in the comment block as follows:",
			  "Minimum required memory pad : 5 bytes",
			  "Minimum threshold value\n: --speculate_loads=7",
			  "For further improvement on this loop, try option --speculate_loads=14",
			  " you choose the *auto* argument to --speculate_loads, the compiler will consider the larger\nthreshold value automatically.",
			  "he compiler and linker can provide automatic load speculation via the *auto* argument to the --\nspeculate_loads option (i.e. *--speculate_loads=auto* or *-mh=a",
			  "e of the *auto* argument makes it\neasier to use and benefit from speculative load optimizations.",
			  "This option can generate speculative loads\nof up to 256 bytes beyond memory that the compiler can prove to be allocated.",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required\npre- and post-padding:",
			  "It attaches a 2-tuple <x, y> to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the software pipeline.",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops.",
			  "Software-pipelined\nloops are sometimes difficult to debug because the code is not presented serially.",
			  "TE:**\nSoftware pipelining without the use of SPLOOP can lead to significant increases in code\nsize. To control code size for loops that get software pipelined, the --opt_for_space option is",
			  "An asm statement inserted in a C code loop will disqualify the loop for software pipelining. Software-\npipelining is a key optimization for achieving good performance. You may see reduced performance\nwithout software pipelining",
			  "Replace the asm() statement with native C, or an intrinsic function call to prevent this from happening.",
			  "he assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\nparallel. The code generation tools attempt to software pipeline your code with inputs from you and wit",
			  "he --\ndisable_software_pipelining option affects both compiled C/C++ code and assembly optimized code.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute\nin parallel. See Section 4.2 for more information.",
			  "The C6000 architecture is partitioned into two nearly symmetric halves. The resource breakdown\ndisplayed in the software pipelining information in the asm file, is computed after the compiler has\npartitioned instructions to either the A-side or the B-side.",
			  "If the resources are imbalanced (i.e.; some\nresources on one side are used more than resources on the other) software pipelining is resource-bound,\nand the loop cannot be efficiently scheduled.",
			  "You can give loop trip-\ncount information to the compiler using the \"MUST_ITERATE\" pragma",
			  "*Advice #30010 Improve Loop; Add MUST_ITERATE pragma(2)",
			  "advice #30010: If you know that this loop will always execute at least\n<2> times, try adding \" MUST_ITERATE(2)\" just before the loop.",
			  "Advice #30011 Improve Loop; Add _nasssert()*",
			  "advice #30011: Consider adding assertions to indicate n-byte alignment\nof variables input1, input2, output if they are actually n-byte\naligned: _nassert((int)(input1) % 8 == 0).",
			  "The loop buffer improves performance and reduces code size for software pipelined loops.",
			  "If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining.",
			  "If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining.",
			  "***Turn Off Software Pipelining (--disable_software_pipelining Option)***",
			  "Software pipelining schedules instructions from a loop so that multiple iterations of the loop execute in\nparallel.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "Speculative Execution"
			]
		  },
		  {
			"title": "Itanium™ Processor Microarchitecture Reference",
			"url": "https://parallel.ru/sites/default/files/ftp/computers/intel/ia64/itanium.pdf",
			"excerpts": [
			  "When a compiler uses IA-64 control speculation, it is responsible for inserting chk.s instructions to check if recovery code needs to be executed. When a chk.s.Read more"
			]
		  },
		  {
			"title": "Rapid Development of a Flexible Validated Processor Model",
			"url": "https://liberty.princeton.edu/Publications/tech04-03_i2.pdf",
			"excerpts": [
			  "Since IA-64 is an EPIC ISA, data dependences are explicitly marked. In IA-64 this is done via stop bits that appear in a bundle. The encoding supports ..."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.3 - User's Guide",
			"url": "https://coecsl.ece.illinois.edu/se423/datasheets/OMAP138Ref_Guides/C6000CCompiler7.3.pdf",
			"excerpts": [
			  "Optimizing Software Pipelining ... For more information about software pipelining, see the TMS320C6000 Programmer's Guide.Read more",
			  "Software pipelining schedules instructions from a loop so that multiple iterations of the loop execute in\nparallel.",
			  "At optimization levels --opt_level=2 (or -O2) and --opt_level=3 (or -O3), the compiler usually\nattempts to software pipeline your loops.",
			  "The --opt_for_space option also affects the compiler's decision to\nattempt to software pipeline loops.",
			  "In general, code size and performance are better when you use the\n--opt_level=2 or --opt_level=3 options.",
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "Example 3-1 illustrates the\ninformation that is generated for each loop.",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "er to the *TMS320C6000 Programmer* ’ *s Guide* for details on the information and\nmessages that can appear in the Software Pipelining Information comment block before each\nloop",
			  "The terms defined below appear in the software pipelining information. For more information on each\nterm, see the *TMS320C6000 Programmer's Guide* ",
			  "**Loop unroll factor.** The number of times the loop was unrolled specifically to increase performance\nbased on the resource bound constraint in a software pipelined loop.",
			  "**Known minimum trip count.** The minimum number of times the loop will be executed.",
			  "**Known maximum trip count.** The maximum number of times the loop will be executed.",
			  "**Known max trip count factor.** Factor that would always evenly divide the loops trip count. This\ninformation can be used to possibly unroll the loop.",
			  "**Loop carried dependency bound.** The distance of the largest loop carry path. A loop carry path\noccurs when one iteration of a loop writes a value that must be read in a future iteration.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "The --debug_software_pipeline option places additional software pipeline feedback in the generated\nassembly file. This information includes a single scheduled iteration view of the software pipelined loop.",
			  "The number of times the loop was unrolled specifically to increase performance\nbased on the resource bound constraint in a software pipelined loop.",
			  "The minimum number of times the loop will be executed.",
			  "The maximum number of times the loop will be executed.",
			  "The maximum number of times the loop will be executed.",
			  "**3.2.3.1**\n**Speculative Execution**\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.\nWhen the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* "
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.1.x",
			"url": "https://www.ti.com/lit/ug/sprui04a/sprui04a.pdf",
			"excerpts": [
			  "3. Using the C/C++ Compiler ................................................................................................... 20. 3.1. About the Compiler.Read more"
			]
		  },
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "When an advanced load is executed, it allocates an entry in a structure called the\nAdvanced Load Address Table (ALAT).",
			  "If no matching entry is found, the speculative results need to be recomputed:\n• Use a chk.a if a load and some of its uses are speculated. The chk.a jumps to\ncompiler-generated recovery code to re-execute the load and dependent instructions.\n",
			  "• Use a ld.c if no uses of the load are speculated. The ld.c reissues the load.\n",
			  "\nEntries are removed from the ALAT due to:\n• Stores that write to addresses overlapping with ALAT entries.\n• Other advanced loads that target the same physical registers as ALAT entries.\n• Implementation-defined hardware or operating system conditions needed to maintain\ncorrectness.\n•",
			  "Control speculation and data speculation are not mutually exclusive; a given load may be both\ncontrol and data speculative.",
			  "Predication is the conditional execution of an instruction based on a qualifying predicate.",
			  "4**\n**Combining Control and Data Speculation**\nControl speculation and data speculation are not mutually exclusive; a given load may be both\ncontrol and data speculative. Both control speculative ( ld.sa , ldf.sa , ldfp.sa ) and non-control\nspeculative ( ld.a , ldf.a , ldfp.a ) variants of advanced loads are defined for general and\nfloating-point registers",
			  "2.4.4**\n**Data Speculation**\nData speculation allows loads to be moved above possibly conflicting memory references.\n*Advanced loads* exclusively refer to data speculative load",
			  "The Itanium architecture provides extensive support for software-pipelined loops, including register\nrotation, special loop branches, and application registers.",
			  "The Itanium architecture enables the compiler to exploit\nspeculative information without sacrificing the correct execution of an application",
			  "The Itanium architecture allows issuing of independent instructions in bundles (three instructions\nper bundle) for parallel execution",
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register",
			  "The br.cloop instruction does not have a branch predicate. Instead, the branching\ndecision is based on the value of the LC register.",
			  "the Itanium architecture explicitly supports software\npipelining, large register files, and compiler-controlled speculation.",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and floating-point pair\n( ldfp.a ) forms.",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks to operate\ncorrectly.",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction."
			]
		  },
		  {
			"title": "The Itanium (I64) family",
			"url": "https://users.nik.uni-obuda.hu/sima/letoltes/Processor_families_Knowledge_Base_2019/Intels_Itanium_(IA64)_family_2018_10_29.pdf",
			"excerpts": [
			  "ontrol speculation\n[ 13\n]\n� IA\n- 64 provides a new group of loads, called\nspeculative loads (\nld.s\n)\nwhich can\nsafely be scheduled before one or more prior branches",
			  "In the block where the programer originally placed the load, the compiler\nschedules a\nspeculation check (\nchk.s\n),\nas shown in the next Figure.",
			  " runtime, if a speculative load results in an exception, the exception is defered\nand a\ntoken (\nNaT\n)\nis written to the target register.",
			  "he\nchk.s\ninstruction checks then the target register for a\nNaT\ntoken\n, and if\na token is present the execution branches to a special",
			  "execute the load non\n- speculatively.",
			  "ta speculation substitutes a conventional load by a\nadvanced load (\nld.a\n)\nand a\ndata\n-\nverifying load (\nld.",
			  "The compiler can move a data\n- speculative load above potentialy aliasing stores\nto alow the load an early schedule",
			  "The compiler schedules the data verifying load in place of the conventional load.",
			  "he\nprocesor uses hardware to detect whether an alias has ocured.\nWhen no alias has ocured, the data\n- speculative load has already loaded the\ncorect value.",
			  "the oposite case, the data\n- verifying load re\n- executes the load and stals the\nprocesor to ensure that\nthe corect data returns\nin time\nfor subsequent uses\nof the load",
			  "ALAT\nis\nthe Advanced Load Adress Table in Itanium architectures.",
			  "It\nis implemented with an asociative\nmemory and\nis used to store information\nrelated\nto advance load instructions,\nto implement data speculation [\n20\n].",
			  "There are two types of speculation; control speculation and data speculation, as\nindicated below and will be discussed subsequently.",
			  "Control speculation",
			  "Data speculation",
			  "Example for control speculation"
			]
		  },
		  {
			"title": "Itanium processor microarchitecture",
			"url": "https://www.cs.ucr.edu/~bhuyan/cs162/LECTURE7b.pdf",
			"excerpts": [
			  "he advanced\nload address table (ALAT) to support data\nspeculation and, with hardware, to manage\nthe deferral of exceptions on speculative exe-\ncution",
			  "In the common case, there’s no match, the\nALAT state is unchanged, and the advanced\nload result is used normally.",
			  "In the case of an\noverlap, all address-matching advanced loads\nin the ALAT are invalidated.",
			  "hen only the load and no depen-\ndent instructions were boosted, a load-check\n(ld.c) instruction is used, and the load\ninstruction is reissued down the pipeline, this\ntime retrieving the updated memory data",
			  "he ld.c\ninstruction can be issued in parallel with\ninstructions dependent on the load result\ndata. ",
			  "e advanced load and its\ndependent uses were boosted, an advanced\ncheck-load (chk.a) instruction traps to a user-\nspecified handler for a special fix-up code that\nreissues the load instruction and the opera-\ntions dependent on t",
			  "The hardware control for speculation\nis quite simple: adding an extra bit to the data\npath supports deferred exception tokens.",
			  "The hardware control for speculation\nis quite simple: adding an extra bit to the data\npath supports deferred exception tokens.",
			  "32-entry ALAT",
			  "The execution control is augmented with a bookkeeping structure called the advanced load address table (ALAT) to support data speculation and, with hardware, to ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "http://bitsavers.informatik.uni-stuttgart.de/components/ti/TMS320xx/320C6000/SPRU198D_TMS320C6000_Programmers_Guide_Mar2000.pdf",
			"excerpts": [
			  "Use –pm program level optimization to reduce memory pointer aliasing.",
			  "Add restrict declarations to all pointers passed to a function whose objects do\nnot overlap.",
			  "Use the .mdep and .no_mdep assembly optimizer directives.",
			  "The compiler does a much better job of alias disambiguation than the assem-\nbly optimizer. C source provides much more informat",
			  "The command line options:\ncl6x –pm –o3\nhave several effects, of which improved alias disambiguation is only one.",
			  "e\ndefault presumption on aliases which cannot be disambiguated is the pessi-\nmistic one: they are aliases.",
			  "C/C++ Compiler and Alias Disambiguation",
			  "rrect use of the const and restrict keywords helps the alias disambi-\nguation problem. The const keyword tells the compiler that the data object\nwill not be modified, even by an alias. So, any const qualified memory read\ncannot possibly be an alias to a memory write.",
			  "he restrict keyword tells the compiler that\nwithin the scope of a particular pointer, only the object pointed to can be ac-\ncessed by that pointer",
			  "Memory Alias Disambiguation versus Memory Bank Conflict Detection",
			  "f a ’C6000 execute packet (a set of instructions which execute in parallel) in-\ncludes two references to memory, and both of those references are to the\nsame memory bank, because each bank is single-ported memory, the pipeline\nstalls for one cycle while the second memory word is accessed. This is called\na bank conflict, and it is obviously worth avoiding",
			  "he assembly optimizer pro-\nvides a directive called .mptr for the purpose of solving this problem. See the\nTMS320C6000 Optimizing C/C++ Compiler User’s Guide for all the details",
			  "Memory Alias Disambiguation",
			  "Memory alias disambiguation analyzes whether a dependence, through a\nmemory location, exists between a given pair of instructions. Dependences\nbetween instructions are then used to determine the fastest legal schedule of\nthose instructions.",
			  "wo final sections discuss how the C/C++ compiler handles memory alias dis-\nambiguation, and the differences between memory alias disambiguation and\nmemory bank conflict detection",
			  "The compiler does a much better job of alias disambiguation than the assem-\nbly optimizer. C source provides much more information to work wit",
			  "cl6x -ma",
			  "he command line option:\ncl6x -mt\nmeans something different to the compiler than it does to the assembly opti-\nmizer. As presented already, this option reverses the assembly optimizer’s\npessimistic assumption that memory references it cannot disambiguate must\nbe aliases. To the compiler, this same option means several specific instances\nof memory aliases do not occur in your C/C++ code"
			]
		  },
		  {
			"title": "(PDF) EPIC: An Architecture for Instruction-Level Parallel Processors",
			"url": "https://www.researchgate.net/publication/213894488_EPIC_An_Architecture_for_Instruction-Level_Parallel_Processors",
			"excerpts": [
			  "itecture,\nVLIW architecture,\ninstruction - level\npa rallelism, MultiOp, non -\nunit assumed latencies,\nNUAL, rotating register\nfiles, unbundled branches,\ncontrol speculation,\nspeculative opcodes,\nexception tag,\npredicated execution,\nfully - resolved predicates,\nwired - OR and wired - AND\ncompare opcodes,\nprioriti zed loads and\nstores, data speculation,\ncache specifiers,\nprecise interrupts,\nNUAL - freeze and NUAL -\ndrain semantics, delay\nbuffers, replay buffers,\nEQ and LEQ semantics,\nlatency stalling,\nMultiOp - P and MultiOp -\nS\nsemantics,\ndynamic translatio n,\nMultiTemplate and\nVariOp instruction\nformats, history of "
			]
		  },
		  {
			"title": "Integrated Predicated and Speculative Execution in ... - IMPACT",
			"url": "http://impact.crhc.illinois.edu/shared/papers/isca-98-epic.pdf",
			"excerpts": [
			  "\nThe IMPACT EPIC Architecture exposes instruction-level par-\nallelism through predicated execution and compiler-directed con-\ntrol and data dependence speculation. This section of the paper\npresents the architectural features and semantics that enable these\ntechnologies. As",
			  " by additions to operation encodings and to the register file\nand by addition of the Memory Conflict Buffer, a device which\nchecks speculated loads for conflicts with subsequent stores.\n",
			  "sion**\nPredication, control speculation, and data speculation in a co-\nherent architecture can be effectively used by the compiler to en-\nhance the performance of wide-issue processors. Predication al-\nlows the compiler to rewrite the program control structure and to\noverlap separate execution paths. Control and data speculation al-\nlow the compiler to break dependences to reduce the dependence\nheight of computation. This paper shows that these techniques\ncomplement each other enabling the compiler to achieve additive\nperformance benefits.\nThe IMPACT"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol2.pdf",
			"excerpts": [
			  "Unaligned IA-64 semaphore references ( cmpxchg, xchg , fetchadd ) result in an Unaligned Data\nReference fault regardless of the state of PSR.ac.",
			  "When PSR.ac is 0, IA-64 data memory references that are not aligned may or may not result in an\nUnaligned Data Reference fault based on the implementation. The level of unaligned memory\nsupport is implementation specific. However, all implementations will raise an Unaligned Data\n**Table 4-16. ALAT Behavior on Non-faulting Advanced/Check Loads**",
			  "**Memory**",
			  "**Attribute**",
			  "**ld.sa**",
			  "**ld.a**",
			  "**ld.c.clr,**",
			  "**ld.c.clr.acq,**",
			  "**ldf.c.clr**",
			  "**ld.c.nc,**",
			  "**ldf.c.nc**",
			  "**Response**",
			  "**Response**",
			  "**Response**",
			  "**Response**",
			  "**no NaT**",
			  "**NaT**",
			  "**hit**",
			  "**hit**",
			  "**ALAT**",
			  "**ALAT**",
			  "**ALAT**",
			  "**ALAT**",
			  "**miss**",
			  "**miss**",
			  "speculative\nalloc\nremove\nalloc\nremove\nnop\nunchanged a\na. May allocate a new ALAT entry if size and/or address are different than the corresponding ld.a or ld.sa whose\nALAT entry was matched.",
			  "alloc\nnon-speculative\nn/a\nremove\nremove\nundefined\nnop\nundefined\nmust not\nalloc\nlimited speculation\nn/a\nremove\nalloc\nremove\nnop\nunchanged a\nalloc\n*IA-64 Addressing and Protection*",
			  "4-39",
			  "Reference fault if the IA-64 datum spans a 4K aligned boundary, and many implementations will\nraise an Unaligned Data Reference fault if the IA-64 datum spans a cache line. Implementations\nmay also raise an Unaligned Data Reference fault for any other unaligned IA-64 memory reference.",
			  "\n*About the IA-64 System Programmer* *’* *s Guide*",
			  "Chapter 15, “ Context Management ” describes how operating systems need to preserve IA-64\nregister contents. In addition to spilling and filling a register ’ s data value, IA-64 also requires\nsoftware to preserve control and data speculative state associated with that register, i.e. its NaT bit\nand ALAT state. This chapter also discusses IA-64 system architecture mechanisms that allow an\noperating system to significantly reduce the number of registers that need to be spilled/filled on\ninterruptions, system calls, and context switches. These optimizations improve the performance of\nan IA-64 operating system by reducing the amount of required memory traffic. This chapter is\nuseful for IA-64 operating system programmers.",
			  "Chapter 16, “ Memory Management ” introduces various IA-64 memory management strategies.",
			  "The IA-64 region register model, protection keys, and the virtual hash page table usage models are\ndescribed. This chapter is of interest to IA-64 virtual memory management software developers.",
			  "Chapter 17, “ Runtime Support for Control and Data Speculation ” describes the operating system\nsupport that is required for control and data speculation. This chapter describes various speculation\nsoftware models and their associated operating system implications. This chapter is of interest to\nIA-64 operating system developers and IA-64 compiler writers.",
			  "To support advanced loads in the presence of a virtual alias, the processor ensures that the\nAdvanced Load Address Table (ALAT) is resolved using physical addresses and is coherent with\nphysical memory.",
			  "Reference fault if the IA-64 datum spans a 4K aligned boundary, and many implementations will\nraise an Unaligned Data Reference fault if the IA-64 datum spans a cache line.",
			  "Table 4-16. ALAT Behavior on Non-faulting Advanced/Check Loads",
			  "memory",
			  "Attribute",
			  "ld.sa",
			  "ld.a",
			  "ld.c.clr,",
			  "ld.c.clr.acq,",
			  "ldf.c.clr",
			  "ld.c.nc,",
			  "ldf.c.nc",
			  "Response",
			  "Response",
			  "Response",
			  "Response",
			  "no NaT",
			  "NaT",
			  "hit",
			  "hit",
			  "ALAT",
			  "ALAT",
			  "ALAT",
			  "ALAT",
			  "miss",
			  "miss",
			  "speculative",
			  "a. May allocate a new ALAT entry if size and/or address are different than the corresponding ld.a or ld.sa whose\nALAT entry was matched.",
			  "non-speculative",
			  "undefined",
			  "undefined",
			  "must not",
			  "limited speculation",
			  "n/a",
			  "n/a",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "nop",
			  "nop",
			  "nop",
			  "unchanged a",
			  "unchanged a",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "*IA-64 Addressing and Protection*",
			  "*IA-64 Addressing and Protection*",
			  "..."
			]
		  },
		  {
			"title": "HPL-PD Architecture Specification:Version 1.1 - shiftleft.com",
			"url": "https://shiftleft.com/mirrors/www.hpl.hp.com/techreports/93/HPL-93-80R1.pdf",
			"excerpts": [
			  "HPL-PD defines a new philosophy of ILP computing, called Explicitly\nParallel Instruction Computing (EPIC) [2-4], which represents an evolution of the VLIW\narchitecture 2 .",
			  "The\narchitecture\nadmits\nEPIC,\nVLIW\nand\nsuperscalar\nimplementations so as to provide a basis for understanding the\nmerits\nand\ndemerits\nof\nthese\ndifferent\nstyles\nof\nimplementation.",
			  "This report describes those parts of the architecture that are common to all machines in the\nfamily. It introduces basic concepts such as the structure of an instruction, instruction execution\nsemantics, the types of register files, etc. and describes the semantics of the operation repertoire."
			]
		  },
		  {
			"title": "Effective Compiler Support for Predicated Execution Using ...",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=144965.144998.pdf",
			"excerpts": [
			  "If-conversion can eliminate all non- loop backward branches from a program. Predicated execution support has been used effectively for scheduling both numeric ...Read more"
			]
		  },
		  {
			"title": "Modulo Scheduling Aware Cluster Assignment",
			"url": "https://clei.org/proceedings_data/CLEI2008/Anales/pdf/CLEI/CLEI_2008_067.pdf",
			"excerpts": [
			  "The recurrence bound. (RecMII) is determined by loop-carried data dependences. The MII is thus determined as MAX(ResMII, RecMII). Methods for finding the ...Read more",
			  "VLIW compilers use\nsoftware pipelining to schedule loop body instructions such that multiple loop\niterations execute in parallel.",
			  "In this paper, we\ndescribe modulo scheduling, which is a method for implementing software\npipelining.",
			  "A critical phase in a VLIW compiler is cluster\nassignment for software pipelined loop body instructions.",
			  "Very long instruction word (VLIW) processors are well suited for\ntoday’s high performance processing applications, which are characterized by\nmathematically oriented loop kernels and abundant instruction level\nparallelism. VLIW processors have multiple functional units, which are often\npartitioned into clusters with local register files.",
			  "Cluster assignment is the\nprocess of partitioning instructions to clusters with the goals of maximizing\nparallelism and minimizing communication overhead.",
			  "We present a cluster assignment algorithm that is designed to run\nbefore modulo scheduling.",
			  "Finally, we present experimental results that\ndemonstrate the effectiveness of the cluster assignment algorithm for the Texas\nInstruments TMS320C6x 2-cluster VLIW processor."
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "Lecture Outline",
			  "Previous Lecture",
			  "1 Introduction to instruction scheduling",
			  "2 Representation of data dependences and resource constraints",
			  "3 Acyclic scheduling: list scheduling",
			  "Loop scheduling: definition of software pipelining",
			  "Parameters of software-pipelined schedules",
			  "Heuristics: *modulo scheduling*",
			  "Modulo Scheduling",
			  "\nSchedule a single iteration such as it is valid repeted every *II*\ncycle\n",
			  "Need to fix *II* before scheduling one iteration",
			  "If not possible, then increase the value of targeted *II*",
			  "Iterative Modulo Scheduling (IMS)",
			  "Principle",
			  "Principle",
			  "Seminal work on modulo scheduling by Bob Rau, MICRO-27\n(1994)",
			  "Extension of list scheduling to loop\nNotion of budget\nCompiler\nImplemented in Intel’s compiler ICC",
			  "Lecture Overview\nIntroduction\nModulo Scheduling\nHardware Support\nConclusion",
			  "Itanium 2 Architecture",
			  "EPIC (Explicitly Parallel Instruction Computing)",
			  "VLIW-like architecture except for memory operations\n(out-of-order memory accesses)\nIPC up to 6 (Instructions Per Cycle)",
			  "Compiler has to expose parallelism (+ constraints on\ninstruction grouping: *bundling* )\nProvides fully support for software-pipelined schedules:\nrotating register files and predication",
			  "Conclusion\nInstruction Scheduling\nInput: set of instructions\nOutput: order on instructions\nMust respect both data dependences and resource constraints\nNeed a model to represent such constraints (DDG, reservation\ntables, automaton, . . . )\nSchedule Types\nBasic block: List scheduling\nLoop:\nList scheduling on the body ⇒ do not benefit from\nintra-iteration parallelism\nSoftware pipelining ⇒ Modulo scheduling heuristics (IMS,\nSMS)",
			  "Software: Modulo variable expansion. Hardware: Rotating register file. Predication. Small irregular control ⇒ If-conversion. Kernel-only loop.Read more",
			  "Provides fully support for software-pipelined schedules:",
			  "VLIW-like architecture except for memory operations",
			  "Instruction is commited iff the predicate is true",
			  "Add one bit (predicate) per instruction",
			  "Convert control dependence into data dependence",
			  "rotating register files and predication",
			  "rotating register files and predication",
			  "\nItanium 2 Architecture",
			  "Software-pipelined schedule with *II* = 1:",
			  "Predication"
			]
		  },
		  {
			"title": "Probabilistic predicate-aware modulo scheduling",
			"url": "https://ieeexplore.ieee.org/document/1281671/",
			"excerpts": [
			  "Predicated execution enables the removal of branches by converting segments of branching code into sequences of conditional operations.",
			  "However, a resource is only put to productive use when the predicate associated with an operation evaluates to True.",
			  "To reduce this superfluous commitment of resources, we propose probabilistic predicate-aware scheduling to assign multiple operations to the same resource at the same time, thereby over-subscribing its use.",
			  "Assignment is performed in a probabilistic manner using a combination of predicate profile information and predicate analysis aimed at maximizing the benefits of over-subscription in view of the expected degree of conflict.",
			  "Conflicts occur when two or more operations assigned to the same resource have their predicates evaluate to True.",
			  "A predicate-aware VLIW processor pipeline detects such conflicts, recovers, and correctly executes the conflicting operations.",
			  "By increasing the effective throughput of a fixed set of resources, probabilistic predicate-aware scheduling provided an average of 20% performance gain in our evaluations on a 4-issue processor, and 8% gain on a 6-issue processor.",
			  "**Published in:** [International Symposium on Code Generation and Optimization, 2004. CGO 2004.](/xpl/conhome/9012/proceeding)"
			]
		  },
		  {
			"title": "Probabilistic Predicate-Aware Modulo Scheduling - CCCP",
			"url": "https://cccp.eecs.umich.edu/papers/msmelyan-cgo04.pdf",
			"excerpts": [
			  "he scheduler chooses its\ninitial II to be the maximum of two lower bounds. The resource-\nconstrained lower bound ( **ResMII** ) is equal to the number of cycles\nthat the most heavily used resource is busy during a single iteration\nof the",
			  "The recurrence-constrained lower bound ( **RecMII** ) is\ndetermined by longest cycle in the dependence ",
			  "Iterative Modulo Scheduling (IMS) [16] is a software pipelining\ntechnique that interleaves successive iterations of a loop.",
			  " IMS is to find a valid schedule for an innermost loop that can be\noverlapped with itself multiple times so that a constant interval be-\ntween successive iterations ( **Initiation Interval** ( **II** )) is minimize",
			  "he II-cycle code region that achieves the maximum overlap be-\ntween iterations is called the **kern",
			  "MS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time",
			  "signment is performed in a probabilistic manner using a\ncombination of predicate profile information and predicate analy-\nsis aimed at maximizing the benefits of over-subscription in view\nof the expected degree of conflict.",
			  "nflicts occur when two or\nmore operations assigned to the same resource have their predicates\nevaluate to True. A predicate-aware VLIW processor pipeline de-\ntects such conflicts, recovers, and correctly executes the conflicting\noperations.",
			  "predicate-aware VLIW processor pipeline de-\ntects such conflicts, recovers, and correctly executes the conflicting\noperations.",
			  "In this work we apply this technique to modulo\nscheduled loop regions which are generally resource constrained.",
			  " DPAMS\nschedule is shown in Figure 2(b). Disjoint operations *A2* and *A5*\nare scheduled in the same slot of the M",
			  "s a result, the ALU is\nalways utilized at Time 8, and the achieved schedule length is 9 cy-\ncles, an 11% performance improvement over the 10 cycle baselin",
			  "PAMS is\na generalization of deterministic predicate-aware modulo schedul-\ning (DPAMS) [23]. DPAMS allows operations to share the same\nresource when their predicates are provably disjoint, i.e. at most\none will evaluate to True at run-time",
			  "Thus, PPAMS significantly increases the utilization of may-use re-\n ... \n",
			  "predicates",
			  "operation",
			  "Activation",
			  "execution",
			  "Frequencies Frequencies",
			  "p1:",
			  "0.2",
			  "A1: 1.0",
			  "p2:",
			  "0.8",
			  "M1: 1.0",
			  "p3:",
			  "0.09",
			  "C1: 1.0",
			  "p4:",
			  "0.01",
			  "C2: 1.0",
			  "p5:",
			  "0.11",
			  "C3: 1.0",
			  "A2: 0.2",
			  "A3: 0.09",
			  "A4: 0.01",
			  "C4: 1.0",
			  "M2: 0.11",
			  "B:",
			  "1.0",
			  "DPAMS guarantees that no conflicts will occur by combining\nonly disjoint operations to share the same resource [23].",
			  "DPAMS guarantees that no conflicts will occur by combining\nonly disjoint operations to share the same resource [23].",
			  "AMS\nschedule is shown in Figure 2(b). Disjoint operations *A2* and *A5*\nare scheduled in the same slot of the MRT. As a result, the ALU is\nalways utilized at Time 8, and the achieved schedule length is 9 cy-\ncles, an 11% performance improvement over the 10 cycle baseli",
			  "central idea of PPAMS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time.",
			  "central idea of PPAMS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time.",
			  "s a consequence, it is possible for dy-\nnamic over-subscription of resources to take place so that two or\nmore resource-sharing operations will have their predicates evalu-\nate to True at runtime, resulting in a resource confli",
			  "PAMS is\na generalization of deterministic predicate-aware modulo schedul-\ning (DPAMS) [23]"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://www.researchgate.net/publication/305064866_Iterative_Modulo_Scheduling",
			"excerpts": [
			  "This paper presents iterative modulo scheduling, a practical algorithm that is capable of dealing with realistic machine models. The paper also characterizes ...Read more",
			  "Iterative modulo scheduling: An algorithm for software pipelining loops. December 1994. B. Ramakrishna Rau. Module scheduling is a framework within which a wide ...Read more"
			]
		  },
		  {
			"title": "Compiler Optimization-Space Exploration",
			"url": "https://liberty.princeton.edu/Publications/cgo03_ose.pdf",
			"excerpts": [
			  "Hyperblock formation and corresponding heuristics have been proposed to determine when and how to predicate code [16]. However, even with these techniques ...Read more"
			]
		  },
		  {
			"title": "An Implementation of Swing Modulo Scheduling with ...",
			"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
			"excerpts": [
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.\nE",
			  "\nEMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.\n",
			  "\nIf-conversion is performed to transform the loop body into straight line predicated code using\nthe RK algorithm [32].\n",
			  "While If-conversion does allow loops with conditional statements to be software pipelined, the\ndownside is that both execution path’s resources must be summed when determining the resource\nconstraints for the loop. This can lead to a pipelined loop that does not have an optimal II.\n3",
			  "Eichenberger and Edward S. Davidson. Stage scheduling: A technique to reduce the register requirements of a modulo schedule. In MICRO 28: Proceedings of the ...Read more",
			  "This thesis details the implementation of Swing Modulo Scheduling, a Software Pipelining tech- nique, that is both effective and efficient in terms of ...",
			  "Software Pipelining , reconstructs the loop\nsuch that each iteration of the loop is executed at a constant interval, producing an optimal schedule.",
			  "Modulo Scheduling is a family of Software Pipelining\ntechniques that uses a “modulo” technique (instead of maximal unrolling) to place instructions in\n ... ",
			  "Modulo Scheduling algorithms aim to create a schedule with\nan Initiation Interval (II) equal to MII, which is the smallest II possible and results in the most\noptimal schedule. The lower the II, the greater the parallelism.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,. Modulo Scheduling the loop, applying modulo variable expansion, ...Read more",
			  "For complex architectures, the iterative techniques (IMS and IRIS) both",
			  "The second group, Modulo Scheduling [27, 33, 12, 21, 4, 16, 28], aims to create a schedule with no\nresource or dependence conflicts that can be repeated at a constant interval. Since Swing Modulo\nScheduling (SMS) falls into the second category, this thesis will briefly describe a few of the other\nwell known algorithms in this category.",
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.",
			  "The best type of hardware support is one that provides Predicated Execution , such as IA64 [20].\nPredicated Execution allows instructions to be nullified during their execution in the pipeline. So if\nan instruction is speculatively moved above a branch and the branch is taken, even if the instruction\nis already in the processor’s pipeline, the instruction can be nullified",
			  "If-conversion replaces conditional branches with a compare instruction that sets a flag. Instruc-\ntions that were dependent upon the conditional branch are now instructions that only execute if\nthe flag is set. If-conversion typically requires hardware support, but some algorithms [41] have\nmade slight modifications to avoid this. Hardware support such as predicated execution, common\non VLIW and EPIC architectures (such as IA64) set a conditional flag per instruction and allow\ninstructions to execute only when the conditional flag is tru",
			  "tware Pipelining [9] is a group of techniques that aim to exploit Instruction Level Parallelism\n(ILP) by overlapping successive iterations of a loop.\nOver the years, two main approaches to\nSoftware Pipelining have developed: Move-then-Schedule , and Schedule-then-Move . The Move-\nthen-Schedule techniques [16, 30, 14, 22], which will not be discussed in this thesis, move instructions\nacross the back-edge of the loop in order to achieve a pipelined loop. The Schedule-then-Move\nalgorithms attempt to create a schedule that maximizes performance and constructs a new pipelined\nloop composed of instructions from current and previous iterations.",
			  "The second main contribution of this thesis is to extend the Swing Modulo Scheduling algorithm\nto handle more than single basic block loops. We have modified the algorithm to handle superblock\nloops (single-entry, multiple-exit, multiple basic block loops). These extensions allow the algorithm\nto be applied at various stages of compilation:\n• Statically: Just as the original Swing Modulo Scheduling algorithm was done statically, the\nextensions simply expand the number of loops acceptable for this transformation.\n• Offline: Using profile information, hot paths (subsets of the loop in the form of single entry,\nmultiple exit loops), can be recognized and be transformed by our extensions.\n• Runtime: Similar to the profile driven optimization, hot paths are found during program\n .."
			]
		  },
		  {
			"title": "Effective Cluster Assignment for Modulo Scheduling",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-98-modulo.pdf",
			"excerpts": [
			  "Llosa, et al., 'Swing Modulo Scheduling: A lifetime-. Sensitive Approach,' IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques,.Read more"
			]
		  },
		  {
			"title": "GCC Swing Modulo Scheduling | PDF",
			"url": "https://www.scribd.com/document/695649826/Swing-Modulo-Scheduling",
			"excerpts": [
			  "SMS is a technique for software pipelining that aims to reduce register pressure. The key steps are: 1. Calculate a Minimum Initiation Interval (MII) for the ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://www.sigmicro.org/media/tot/retrospective2016.pdf",
			"excerpts": [
			  "Ra- makrishna (Bob) Rau's seminal paper entitled “Iterative. Modulo Scheduling: An Algorithm for Software Pipelining Loops,” which appeared at MICRO. 1994, 22 ..."
			]
		  },
		  {
			"title": "Topic 12: Cyclic Instruction Scheduling - cs.Princeton",
			"url": "https://www.cs.princeton.edu/courses/archive/spring15/cos320/lectures/12-CyclicScheduling.pdf",
			"excerpts": [
			  "• Each iteration is scheduled to start some fixed number of cycles later than the ... • Modulo reservation table solves this problem. • To schedule an op at time ...Read more"
			]
		  },
		  {
			"title": "[PDF] Lifetime-sensitive modulo scheduling",
			"url": "https://www.semanticscholar.org/paper/922b8e03182a201d36102f30924e419c8a176bfa",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time, and empirical results ..."
			]
		  },
		  {
			"title": "Lifetime-sensitive modulo scheduling",
			"url": "https://dl.acm.org/doi/10.1145/173262.155115",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time."
			]
		  },
		  {
			"title": "Lifetime-Sensitive Modulo Scheduling",
			"url": "https://iss.oden.utexas.edu/Publications/Papers/PLDI1993b.pdf",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for mini- mal register pressure without sacrificing the loop's minimum execution time.Read more"
			]
		  },
		  {
			"title": "Figure 5 from Register allocation for software pipelined loops",
			"url": "https://www.semanticscholar.org/paper/Register-allocation-for-software-pipelined-loops-Rau-Lee/0568c568401a88fc03ff93bc1deb483b7e5294bb/figure/7",
			"excerpts": [
			  "Proceedings of MICRO-27. The 27th Annual IEEE/ACM… 1994. TLDR. A Register-Pressure-Sensitive (RPS) scheduling technique is developed and three algorithms-RPS ...Read more"
			]
		  },
		  {
			"title": "Learning basic block scheduling heuristics from optimal data | Proceedings of the 2005 conference of the Centre for Advanced Studies on Collaborative research",
			"url": "https://dl.acm.org/doi/10.5555/1105634.1105652",
			"excerpts": [
			  "The basic block instruction scheduling problem is to find a minimum length schedule ... Govindarajan. Instruction scheduling. In Y. N. Srikant and ...Read more"
			]
		  },
		  {
			"title": "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory | International Journal of Parallel Programming | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1023/A:1007564126785",
			"excerpts": [
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory",
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory > REFERENCES",
			  "Section Title: Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory > Author information > Authors and Affiliations",
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory. *International Journal of Parallel Programming* **28** , 1–46 (2"
			]
		  },
		  {
			"title": "(PDF) Theory of Modulo-Scheduled Pipelines",
			"url": "https://www.researchgate.net/publication/2287790_Theory_of_Modulo-Scheduled_Pipelines",
			"excerpts": [
			  "In this paper w e propose CO-Scheduling, a framework f o r simultaneous design of hardware pipelines struc-tures and software-pipelined schedules. T w o important components of t h e Co-Scheduling framework are: (1) An extension t o t h e analysis of hardware pipeline design t h a t m e e t s t h e needs of periodic (or software pipelined) schedules. Reservation tables, forbidden la-tencies, collision vectors, and state diagrams f r o m classical pipeline theory are revisited and extended to solve t h e n e w problems. (2) An efficient method, based o n t h e above extension of pipeline analysis, t o p e r f o r m (a) software pipeline scheduling and (b) hardware pipeline reconfiguration which are mutually \"compatible \". T h e proposed method has been implemented and pre-liminary experimental results f o r 1008 kernel loops are reported. Co-scheduling successfully obtains a sched-ule f o r 95% of these loops. T h e median t i m e t o obtain these schedules i s 0.25 seconds o n a Sparc-20.",
			  "Reservation tables, forbidden la-tencies, collision vectors, and state diagrams f r o m classical pipeline theory are revisited and extended to solve t h e n e w problems.",
			  "h e proposed method has been implemented and pre-liminary experimental results f o r 1008 kernel loops are reported. Co-scheduling successfully obtains a sched-ule f o r 95% of these loops."
			]
		  },
		  {
			"title": "(PDF) Modulo scheduling without overlapped lifetimes",
			"url": "https://www.academia.edu/125636329/Modulo_scheduling_without_overlapped_lifetimes",
			"excerpts": [
			  "M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proc. of the ACM SIGPLAN '88 Conference on Programming Language Design and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for ...",
			"url": "https://dl.acm.org/doi/10.1145/989393.989420",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in ...Read more",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without complicated hardware support. Our ...Read more"
			]
		  },
		  {
			"title": "TOWARD A SOFTWARE PIPELINING FRAMEWORK FOR ... - CAPSL",
			"url": "https://www.capsl.udel.edu/pub/doc/theses/master/Ributzka-master.pdf",
			"excerpts": [
			  "rotating registers, modulo variable expansion (MVE) has to be applied to guarantee the correctness of the schedule. The following example shows why MVE is ..."
			]
		  },
		  {
			"title": "Software pipelining, VLIW, EPIC, instruction-set support",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch06-StaticSchedulingForILP.pdf",
			"excerpts": [
			  "Rotating register frame. Software-assisted branch prediction. Software-assisted memory hierarchy. Job creation scheme for compiler engineers. We will look at ...Read more",
			  "Monica S. Lam, Robert P. Wilson. 19th ISCA, May 1992, pages 19-21. Limits of ... To help trace scheduling and software pipelining, the Itanium.Read more",
			  "IA-64",
			  "EPIC = “2nd generation VLIW”?",
			  "VLIW: Very Large Instruction Word"
			]
		  },
		  {
			"title": "Space-Time Scheduling of Instruction-Level Parallelism on a ...",
			"url": "https://groups.csail.mit.edu/cag/raw/documents/Lee:ASPLOS:1998.pdf",
			"excerpts": [
			  "Cur- rently, the event scheduler runs before register allocation; it has no register consumption information and does not consider regis- ter pressure when ...Read more"
			]
		  },
		  {
			"title": "Register Pressure-Based Modulo Scheduling for Clustered ...",
			"url": "https://ece.northeastern.edu/groups/nucar/publications/jornadas.pdf",
			"excerpts": [
			  "We propose a new modulo scheduling algorithm for clustered architectures with distributed register file and functional units. A critical step of the ...Read more"
			]
		  },
		  {
			"title": "Modulo Scheduling with Reduced Register Pressure",
			"url": "https://upcommons.upc.edu/bitstreams/9de3a477-f5f8-4307-88c5-16ab35f9e150/download",
			"excerpts": [
			  "Software pipelining is a scheduling technique that is used by some product compilers in order to expose more instruction\nlevel parallelism out of innermost loops. Modulo scheduling refers to a class of algorithms for software pipelining.",
			  "The register pressure increases as\nthe instruction level parallelism increases.",
			  ". Section 3\ndescribes the proposed modulo scheduling algorithm that\nis called HRMS. ",
			  "er\nto [1].\n**2.1 Overview of Modulo Scheduling**\nIn a software pipelined loop, the schedule for an iteration is\ndivided into stages so that the execution of consecutive it-\nerations that are in distinct stages is overlapped. The num-\nber of stages in one iteration is termed **stage count** ( *SC* ). The\nnumber of cycles per stage is *II* .",
			  "Lam, “Software Pipelining: An Effective Scheduling Tech-\nnique for VLIW Machines,” *Proc. SIGPLAN ’88 Conf. Programming*\n*Language Design, and Implementation* , pp. 318-328, June 1988.",
			  "Software pipelining is a scheduling technique that is used by some product compilers in order to expose more instruction\nlevel parallelism out of innermost loops. Modulo scheduling refers to a class of algorithms for software pipelining. Most previous\nresearch on modulo scheduling has focused on reducing the number of cycles between the initiation of consecutive iterations (which\nis termed II ) but has not considered the effect of the register pressure of the produced schedules. The register pressure increases as\nthe instruction level parallelism increases. When the register requirements of a schedule are higher than the available number of\nregisters, the loop must be rescheduled perhaps with a higher II . Therefore, the register pressure has an important impact on the\nperformance of a schedule. This paper presents a novel heuristic modulo scheduling strategy that tries to generate schedules with\nthe lowest II , and, from all the possible schedules with such II , it tries to select that with the lowest register requirements.",
			  "e Scheduling: A Tech-\nnique to Reduce the Register Requirements of a Modulo Sched-\nule,” *Proc. 28th Ann. Int’l Symp. Microarchitecture (MICRO-28)* ,\npp. 338-349, Nov. 1"
			]
		  },
		  {
			"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
			"url": "https://upcommons.upc.edu/bitstreams/a90677c0-9f85-4a0b-8573-a170b09f0555/download",
			"excerpts": [
			  "Swing Modulo Scheduling* (SMS",
			  "generates schedules that are near optimal in terms of\ninitiation interval, register requirements and stage count.",
			  "g* is an heuristic approach that has\na low computational cost.",
			  "For a set of small loops, SMS obtained the\noptimum initiation interval in all the cases and its schedules\nrequired only **5%** more registers and a 1% higher stage\ncount than the optimum.",
			  "technique that exploits instruction level parallelism out of\nloops by overlapping successive iterations of the loop and\nexecuting them in parallel.",
			  "The key idea is to find a pattern\nof operations (named the kernel code) so that when\nrepeatedly iterating over this pattern, it produces the effect\nthat an iteration is initiated before the previous ones have\ncompleted.",
			  "SMS is also\ncompared with an integer linear programming approach\nthat generates optimum schedules but with a huge\ncomputational cost, which makes it feasible only for very\nsmall loops.",
			  "The addition of spill code can degrade performance [16]\ndue to additional cycles in the schedule, or due to memory\ninterferences.",
			  "Some research groups have targeted their work towards\nexact methods based on integer linear programming, For\ninstance, the proposal in [ 111 search the entire scheduling\nspace to find the optimal resource-constrained schedule\nwith minimum buffer requirements, while the proposals in\n[10,6] find schedules with the actual minimum register\nrequirements.",
			  "Keywords:**\nFine\nGrain\nParallelism,\nInstruction\nScheduling, Loop Scheduling, Software Pipelining,\nRegister\nRequirements,\nVLIW\nand\nSuperscalar\nArchitectures.",
			  "load 2 cycles",
			  "**store:** 1 cycle",
			  "**Latencies:**",
			  "For loop-variants, a value is generated in each iteration\nof the loop and, therefore, there is a different lifetime\ncorresponding to each iteration.",
			  "Because of the nature of\nsoftware pipelining, lifetimes of values defined in an\niteration can overlap with lifetimes of values defined in\nsubsequent iterations.",
			  "This is the main reason why the\nregister requirements are increased.",
			  "In addition, for values\nwith a lifetime larger than the **ZZ** new values are generated\nbefore the previous ones are u",
			  "To fix this problem,\neither software solutions (modulo variable expansion [ 151)\nand hardware solutions (rotating register files [7]) have\nbeen proposed.",
			  "Some of the software pipelining approaches can be\nregarded as the sequencing of two independent steps: node\nordering and node scheduling.",
			  "These two steps are\nperformed assuming MZZ as the initial value for **Z",
			  "it is\nnot possible to obtain a schedule with this **ZZ,** the schedulin"
			]
		  },
		  {
			"title": "Modulo scheduling with reduced register pressure",
			"url": "https://ieeexplore.ieee.org/document/689643/",
			"excerpts": [
			  "Section Title: Modulo scheduling with reduced register pressure > Abstract:\nContent:\nIn addition, the proposed method is compared with some other existing methods. The results indicate that the proposed method performs better than other heuristic methods and almost as well as linear programming methods, which obtain optimal solutions but are impractical for product compilers because their computing cost grows exponentially with the number of operations in the loop body.",
			  "Section Title: Modulo scheduling with reduced register pressure > Metrics\nContent:\n[](https://innovate.ieee.org/interested-in-a-subscription-for-your-organization/?LT=XPLLG_XPL_2020_SUB_Journals300x250_Sub-NFT)"
			]
		  },
		  {
			"title": "C6000 Compiler: Tuning Software Pipelined Loops - TI E2E",
			"url": "https://e2e.ti.com/cfs-file/__key/communityserver-discussions-components-files/791/C6000-Compiler_5F00_-Tuning-Software-Pipelined-Loops-_2D00_-Texas-Instruments-Wiki.pdf",
			"excerpts": [
			  "the C6000\narchitecture is partitioned into two nearly symmetric halves. The resource breakdown displayed in the software pipelining information is computed after the compiler has\npartitioned instructions to either the A-side or the B-side.",
			  "The software-pipelined loop information includes the source lines from which the loop originates, a description of the resource and latency requirements for the loop, and whether\nthe loop was unrolled (among other information).",
			  "g factor is now the number of functional units as indicated by the resource bound:\n;* Loop Carried Dependency Bound(^) : 0\n;* Unpartitioned Resource Bound : 2\n;* Partitioned Resource Bound(*) : 2\n",
			  "Exploiting Wide Loads and Stores (SIMD)",
			  "The C64x and C64x+ processors support both aligned and non-aligned double-word loads and stores. (Recall that the C67x and C67x+ support only aligned double-word loads and\nno double-word stores. The C62x supports only word-wide loads and stores.) If it is known that the function parameters are double-word aligned, switching to aligned, double-\nword memory accesses saves both D units and T address paths",
			  "To tell the compiler that the memory accesses are aligned on double-word (64-bit) boundaries, use _nasserts() inside the function *just prior* to the loop of interest:",
			  ":\n_nassert((int) input1 % 8 == 0); // input1 is 64-bit aligned",
			  "d\n_nassert((int) input2 % 8 == 0); // input2 is 64-bit aligned",
			  "_nassert((int) output % 8 == 0); // output is 64-bit aligned\nT",
			  ";* ii = 2 Schedule found with 4 iterations in parallel",
			  ";* Loop Unroll Multiple : 4x",
			  ";* Known Minimum Trip Count : 1",
			  ";* Known Max Trip Count Factor : 1",
			  "...",
			  "...",
			  "\n}\n}\nThe software-pipelining information for the resulting assembly code, which yields one result every 0.75 cycles (or 4 results every 3 cycles), is as follows:\n;* SOF",
			  " is the number of cycles between starting iteration i and iteration i + 1. ii is equivalent to the\ncycle count for the software-pipelined loop body. s is the number of cycles for the first iteration to complete, or equivalently, the length of a *single scheduled iteration* of the\nsoftware-pipelined loop.",
			  "In a software-pipelined loop, even though a single loop iteration might take s cycles to complete, a new iteration is initiated every ii cycles.",
			  "Software\npipelining is only enabled at –o2 or –o3. On the C6000 variants C62x, C67x, and C64x, software pipelining is completely disabled when code size flags –ms2 and –ms3 (see C6000\nCompiler: Recommended Compiler Options) are used. On C64x + , C674x, and C66x cores, software pipelining is enabled if and only if the loop buffer (see section 2.1 of Hand\nTuning Loops and Control Code on the TMS320C6000) can be used.",
			  "The initiation interval (ii) is 7. This means that in the steady state, a result (equivalently, an original loop iteration) is computed every 7 CPU cycles. Therefore, the baseline CPU\nperformance is 7 cycles/result.",
			  "After adding the restrict keyword, rebuild the function. Observe that the loop-carried dependency bound has disappeared. This means that each iteration is independent. New\niterations are now initiated as soon as resources are available.",
			  "The edges from the load instructions to the add instructions are straightforward. The destination registers for the loads are the source registers for the add instruction. A load\ninstruction takes 5 cycles to populate its destination register.",
			  "The edge from the add to the store is also straightforward since the destination of the add is the source of the store. The result of the add is available after 1 cycle. Consequently, the\nedge between the add and the store is annotated with a 1.",
			  "he edges from the store back to the loads are less obvious. How does one know to put them in? Why are they there? The answer to these questions is determined by process of\nelimination. Since there is no register dependence, there is most likely a memory dependence. In this case, the compiler does not know whether the input arrays could reference the\n ...",
			  "Look for the machine resources with a (*) after them. Notice which ones are most congested. In this case, the bottleneck is on the D unit and the T address path.",
			  ";* SINGLE SCHEDULED ITERATION",
			  ";* C26:\n;* 0 LDW .D1T1 *A5++,A4 ; |9|",
			  ";* 1 LDW .D2T2 *B4++,B5 ; |9|",
			  ";* 2 [ B0] BDEC .S2 C26,B0 ; |8|",
			  ";* 3 NOP 3",
			  ";* 6 ADD .L1X B5,A4,A3 ; |9|",
			  ";* 7 STW .D1T1 A3,*A6++ ; |9|",
			  "_nassert((int) output % 8 == 0); // output is 64-bit aligned",
			  "The following diagrams show the concept of unrolling the loop for better (more balanced) utilization of the critical D unit resource (the situation would be analogous for the critical\nT address path). On the left side, four loop iterations are shown, as indicated by the double-arrows, producing eight results in four cycles. One D unit is unused every other cycle.\nThe right side shows performance after unrolling the loop by 2x. Both D units are executing useful instructions in every cycle. Of course, the order of the loads and stores must be\nrearranged, but the compiler takes care of this.",
			  ";* ii = 3 Schedule found with 4 iterations in parallel\n...\n;* SINGLE SCHEDULED ITERATION\n;*\n;* C26:\n;* 0 LDW .D2T2 *B5++(8),B6 ; |10|\n;* 1 NOP 1\n;* 2 LDW .D1T1 *A6++(8),A3 ; |10|\n;* || LDW .D2T2 *-B5(4),B4 ; |10|\n;* 3 LDW .D1T1 *-A6(4),A3 ; |10|\n;* 4 NOP 1\n;* 5 [ B0] BDEC .S2 C26,B0 ; |8|\n;* 6 NOP 1\n;* 7 ADD .S1X B6,A3,A4 ; |10|\n;* 8 ADD .L1X B4,A3,A5 ; |10|\n;* 9 NOP 1\n;* 10 STNDW .D1T1 A5:A4,*A7++(8) ; |10|\n;* 11 ; BRANCHCC OCCURS {C26} ; |8|",
			  "The loop has been sped up 9.3x compared to the original source code with no modifications other than the addition of restrict qualifiers, MUST_ITERATE pragmas and\n_nasserts().",
			  "void BasicLoop(int *restrict output,\nint *restrict input1,\nint *restrict input2,\nint n)\n{\nint i",
			  "Software pipelining is only enabled at –o2 or –o3. On the C6000 variants C62x, C67x, and C64x, software pipelining is completely disabled ...Read more"
			]
		  },
		  {
			"title": "Introduction to TMS320C6000 DSP Optimization",
			"url": "https://www.ti.com/lit/pdf/sprabf2",
			"excerpts": [
			  "– For example, the MUST_ITERATE pragma, when placed right before a loop operation, tells the compiler the minimum iteration count of the loop, ...",
			  "Software pipelining is a technique to schedule instructions from a loop so that multiple iterations of the\nloop can execute in parallel and the pipeline can be utilized as fully as possible.",
			  "In most cases, the C compiler can automatically utilize the SPLOOP buffer while performing software\npipelining.",
			  "However, the SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets",
			  "When the compiler option, – *o3* , is used, the C6000 C compiler is able to\nautomatically perform software pipelining on the loop code. F",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies.",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies.",
			  "the SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets\n(An execute packet consists of up to eight instructions that can be executed in parallel).",
			  "Therefore, it is recommended to avoid implementing long or\ncomplex loops in C.",
			  "In stage 3, based on the knowledge it has gained in the previous stages, the compiler tries to optimize the\nloop by finding the minimal value of ii that it can use to schedule a software pipelined loop.",
			  "One major drawback of software pipelining is that it increases the size of the generated assembly file and\ncan affect interruptibility.",
			  "ure 1 shows that the family consists of fixed point DSP cores such as C64x ™\nand C64x+ ™ , early floating/fixed point DSP cores such as C67x ™ and C67x+, and the newest\nfloating/fixed point DSP cores such as C674x and C66x.",
			  "All the DSP cores share the same high-level\narchitecture, with hardware enhancements made and additional instructions supported on the newer\ndevices.",
			  "Moreover, applications developed on older devices are 100% upward compatible.",
			  "The C674x core is capable to operate at 1.25 GHz, and perform\n10,000 million complex multiply-and-accumulate operations; the C66x core is capable to operate at 1.25",
			  "on C66x core is an\nenhanced version of the C674x core. ",
			  "The latest generation C66x core is an\nenhanced version of the C674x core.",
			  "The C6000 DSPs have many architectural advantages that make them ideal for computation-intensive\nreal-time applications.",
			  "The C6000 DSPs have many architectural advantages that make them ideal for computation-intensive\nreal-time applications.",
			  "Section 2.3 introduces the concept of software pipelining, which is an instruction scheduling method to ensure the full utilization of the ...Read more"
			]
		  },
		  {
			"title": "Hand-Tuning Loops and Control Code on the TMS320C6000",
			"url": "https://www.ti.com/lit/pdf/spra666",
			"excerpts": [
			  "A technique called software pipelining contributes the biggest boost to improving looped code performance. Software pipelining is only enabled at –o2 or –o3. On ...Read more",
			  "The C6000 architecture is an eight-way enhanced VLIW architecture. The C64x, C64x+, C67x+\ncontain 64 registers. The C62x and C67x devices contain 32 registers.",
			  "**M unit.** Multiplication",
			  " an efficient software-pipelined loop, where ii<s, ii is called the *initiation interval* ; it is the\nnumber of cycles between starting iteration i and iteration i+1.",
			  "**S unit.** Shifts, branches and compares",
			  "is called the *initiation interval* ; it is the\nnumber of cycles between starting iteration i and iteration i+1. ii is equivalent to the cycle count\nfor the software-pipelined loop body. s is the number of cycles for the first iteration to complete,\nor equivalently, the length of a *single scheduled iteration* of the software-pipelined loop",
			  "A particular area where the C6000 processor family shines is its ability to speed through looped\ncode. This is quite advantageous in digital signal processing, image processing and other\nmathematical routines that tend to be loop-centric. A technique called *software pipelining*\ncontributes the biggest boost to improving looped code performance. Software pipelining is only\nenabled at –o2 or –o3. On all C6000 variants except the C64x+, software pipelining is\ncompletely disabled when code size flags –ms2 and –ms3 (Section 3.1) are used. On the\nC64x+, software pipelining is enabled if and only if the loop buffer (Section 2.1) can be use",
			  "**X cross-path.** Path between functional units and opposite-side register file",
			  "In a software-pipelined loop, even though a single loop iteration might take s cycles to complete,\na new iteration is initiated every ii cycles.",
			  "Some operations can be done on more than one functional unit. For example, add instructions\ncan execute on the L, S, or D units. Several other operations can be done on either the L or S\nunits.",
			  "*SPRA666*",
			  "4",
			  "*Hand-Tuning Loops and Control Code on the TMS320C6000*",
			  "The C64x, C64x+, C67x+\ncontain 64 registers.",
			  "ithout software pipelining, loops are scheduled so that loop iteration i completes before\niteration i+1 begins. Software pipelining allows iterations to be overlapped. Thus, as long as\ncorrectness can be preserved, iteration i+1 can start before iteration i finishes. This generally\npermits a much higher utilization of the machine’s resources than might be achieved from non-\nsoftware-pipelined scheduling techniques.",
			  "ii cycles",
			  "s cycles",
			  "iteration i",
			  "iteration i+1",
			  "iteration i+2",
			  "Figure 2.",
			  "software pipelining",
			  ";* ii = 7 Schedule found with 1 iterations in parallel",
			  "Loop Unrolling",
			  "One possibility for achieving this is to unroll the loop manually:\nvoid BasicLoop(int *restrict output,\nint *restrict input1,\nint *restrict input2,\nint n)\n{\nint i;\n\\ MUST_ITERATE(1)\n**for (i=0; i<n; i+=2) {**\n**output[i] = input1[i] + input2[i];**\n**output[i+1] = input1[i+1] + input2[i+1];**\n}\n}\nRebuilding yields the following results:",
			  "When the loop is unrolled 2x, each iteration takes longer, but the loop now generates two results\nper iteration instead of one. Thus, the unrolled loop requires 1.5 cycles/result in the steady state,\nwhereas the non-unrolled version requires 2 cycles/result.",
			  "_nassert((int) input1 % 8 == 0); // *input1 is 8-byte aligned*",
			  "_nassert((int) input2 % 8 == 0); // *input2 is 8-byte aligned*",
			  "_nassert((int) output % 8 == 0); // *output is 8-byte aligned*",
			  "**\\ MUST_ITERATE(4,,4)** // *n >= 4, n % 4 = 0*",
			  "for (i=0; i<n; i++) {\noutput[i] = input1[i] + input2[i];\n}",
			  "The software-pipelining information for the resulting assembly code, which yields one result\nevery 0.75 cycles (or 4 results every 3 cycles), is as follows:",
			  "**ii = 3** Schedule found with 3 iterations in parallel\n",
			  "The T address paths are now balanced. There is one less D unit than expected because the\ncompiler chose to use a non-aligned double word store instead of two aligned single-word\nstores. Recall, non-aligned memory accesses use both T address paths but only one D unit."
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "https://spinlab.wpi.edu/courses/ece4703_2008/spru198g.pdf",
			"excerpts": [
			  "C6000 compiler also performs software pipelining on various MIPS intensive loops. This feature is important for any pipelined VLIW machine to perform. In.",
			  "In addition to performing many common DSP optimizations, the. C6000 compiler also performs software pipelining on various MIPS intensive loops. This feature ...",
			  "TMS320C6000 Optimizing C Compiler User's Guide (literature number. SPRU187) describes the 'C6000 C compiler and the assembly optimizer. This C compiler ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C Compiler Tutorial",
			"url": "https://www.ti.com/lit/pdf/spru425",
			"excerpts": [
			  "Through a technique called software pipelin- ing, the 'C6000 code generation tools use the multiple resources of the Veloci-. TI architecture efficiently and ...Read more",
			  "We can see that the Unpartioned Resource. Bound is 4 while the Partitioned Resource Bound is 5. When the Partitioned Resource Bound is higher, this usually ...Read more",
			  "Each time a particular iteration interval fails, the ii is increased and retried. This continues until the ii is equal to the length of a list scheduled loop ( ...Read more",
			  "One thing the compiler does when software pipelining is to allow reordering\nof all loads and stores occurring from the same array or pointer. This allows\nfor maximum flexibility in scheduling.",
			  "The C6000 has predicate, or conditional, registers available for use with condi-\ntional instructions. There are 5 predicate registers on the ’C62x and ’C67x, and\n6 predicate registers on the ’C64x. There are two or three on the A side and\nthree on the B sid"
			]
		  },
		  {
			"title": "Introduction to TMS320C6000 DSP Optimization",
			"url": "http://docs.ampnuts.ru/ti.com.datasheet/66AK2E05/Application_note_SPRABF2.PDF",
			"excerpts": [
			  "When the compiler option, –o3, is used, the C6000 C compiler is able to automatically perform software pipelining on the loop code. For more ...Read more",
			  "Section 2.3 introduces the concept of software pipelining, which is an instruction scheduling method to ensure the full utilization of the ...Read more",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies. The hardware selectively overlays copies of the single scheduled iteration in\na software pipelined manner to construct an optimized execution of the loop.",
			  "One major drawback of software pipelining is that it increases the size of the generated assembly file and\ncan affect interruptibility. The SPLOOP buffer is implemented on the C64x+, C674x, and C66x cores to\naddress these issues.",
			  "he SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets\n(An execute packet consists of up to eight instructions that can be executed in parallel). ",
			  "When the situation becomes too complex, the\ncompiler might not be able to pipeline at all. Therefore, it is recommended to avoid implementing long or\ncomplex loops in C.",
			  "t is recommended to develop the application in C for its simplicity to develop and\nupdate, and use the C6000 code generation tools to generate highly optimized assembly source files ",
			  "Manual Unroll",
			  "As shown in Example 10 , loop unrolling can be accomplished manually. The number of iterations is\nhalved, and two of the previous single loop implementation is carried out in the new loop.",
			  "he MUST_ITERATE pragma gives the compiler the flexibility to\nautomatically unroll when it is deemed profitable, and the compiler often can perform this in an ideal way.\n",
			  "Two of the most frequently used SIMD instructions are the LDDW and STDW instructions, which perform\naligned 64-bit load and 64-bit store, respectively.",
			  "The LDDW and STDW instructions require that data be aligned on 8-byte boundary. If data\nis not aligned, the non-aligned version of these instructions (LDNDW and STNDW) will be\nused instead.",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the correct timing dependencies. The ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler User's Guide",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/DSPs/Texas_Instrument_TMS320Cxx/DSP%20C6000/spru187.pdf",
			"excerpts": [
			  "The assembly optimizer also software pipelines loops. For more information\nabout the assembly optimizer, see Chapter 4. For more information about soft-\nware-pipelining, see the TMS320C6000 Programmer’s Guide.",
			  "n Off Software Pipelining (–mu Option)**\nAt optimization levels –o2 and –o3, the compiler attempts to software pipeline\nyour loops. You might not want your loops to be software-pipelined for debug-\nging reasons. Software-pipelined loops are sometimes difficult to debug be-\ncause the code is not presented serially. The –mu option affects both compiled\nC/C++ code and assembly optimized co",
			  "The terms defined below appear in the software pipelining information. For\nmore information on each term, see the TMS320C6000 Programmer’s Guide.\n�\n**Loop unroll factor.** The number of times the loop was unrolled specifi-\ncally to increase performance based on the resource bound constraint in\na software pipelined loop.\n�\n**Known minimum trip count.** The minimum number of times the loop will\nbe executed.\n�\n**Known maximum trip count.** The maximum number of times the loop will\nbe executed.\n�\n**Known max trip count factor.** Factor that would always evenly divide the\nloops trip count. This information can be used to possibly unroll the loop.\n�\n**Loop label.** The label you specified for the loop in the linear assembly\ninput file. This field is not present for C/C++ code.\n�\n**Loop carried dependency bound.** The distance of the largest loop carry\npath. A loop carry path occurs when one iteration of a loop writes a value\nthat must be read in a future iteration. Instructions that are part of the loop\ncarry bound are marked with the ^ symbol.\n�",
			  "Optimizing Software Pipelining",
			  "3-11",
			  "Optimizing Your Code",
			  "Speculative Execution",
			  "When prologs and epilogs are collapsed, instructions might be speculatively\nexecuted, thereby causing loads to addresses beyond either end of the range\nexplicitly read within the loop.",
			  "When the –mhn option is used, the speculative threshold is increased from the\ndefault of 0 to n.",
			  "When the threshold is n, the compiler can allow a load to be\nspeculatively executed as the memory location it reads will be no more than",
			  "e assembly optimizer also software pipelines loops. For more information\nabout the assembly optimizer, see Chapter 4. For more information about soft-\nware-pipelining, see the TMS320C6000 Programmer’s Guide.",
			  "By default, the compiler cannot speculate loads\nbecause this could cause an illegal memory location to be read.",
			  "Sometimes,\nthe compiler can predicate these loads to prevent over execution.",
			  "owever,\nthis can increase register pressure and might decrease the total amount col-\nlapsing which can be performed",
			  "e prolog and epilog of a software-pipelined loop consists of up to p–1 stages\nof length ii, where p is the number of iterations that are executed in parallel dur-\ning the steady state and ii is the cycle time for the pipelined loop body."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C Compiler User's Guide",
			"url": "https://www.rose-hulman.edu/class/ee/yoder/ece581/ECE581%20documents/Old/spru187e.pdf",
			"excerpts": [
			  "- Unpartitioned resource bound. The best possible resource bound val- ues before the instructions in the loop are partitioned to a particular side ...Read more",
			  "**Speculative Execution**\nAn instruction is speculatively executed if it is executed before it is known\nwhether the result will be used. If the result is unused, it is discarded. To specu-\nlatively execute an instruction, the compiler moves it past a branch. This has\nthe same effect as moving a statement out of the body of an if statement.\nAn instruction can be legally speculatively executed only if the discarded result\ndoes not change the program semantics. In a speculatively executed instruc-\ntion, not only is the result discarded, but also the previous value in that location.",
			  "Load instructions might be able to be speculatively executed, but it is possible\nthat the address register is invalid if the result will not be used. This means\nthat a load instruction might access an address that is past the beginning or\nend of valid memory, causing a fault.",
			  "The 'C6000 C compiler is a full-featured optimizing compiler that translates standard ANSI C programs into 'C6000 assembly language source. The fol- lowing ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide (Rev. K)",
			"url": "https://www.ti.com/lit/pdf/spru198",
			"excerpts": [
			  "The _nassert intrinsic can convey information about the alignment of pointers\nand arrays.",
			  "_nassert(((int) a & 0x3) == 0); /* a is word aligned */",
			  "For C64x/C64x+ _amemd corresponds to\ndifferent assembly instructions than when used\nwith other C6000 devices; see Table 2 − 6.",
			  "Having determined that the minimum iteration interval is 1, you can initiate a\nnew iteration every cycle. You can schedule LDW (or LDDW) and MPY (or\nMPYSP) instructions on every cycle.",
			  "\nTable 5 − 7 shows a fully pipelined schedule for the fixed-point dot product",
			  "The rightmost column in Table 5 − 7 is a single-cycle loop that contains the\nentire loop. Cycles 0 − 6 are loop setup code, or loop prolog.",
			  "Typically, these loops achieve an optimal usage of the 8 functional units\navailable on the C6000 DSP by scheduling multiple iterations of the loop in\nparallel.",
			  "The SPLOOP mechanism includes an SPLOOP buffer large\nenough to store 14 execute packets and several dedicated registers.",
			  "The C64x+ improves code size and interruptibility of software-pipelined loops\nwith the hardware SPLOOP buffer and software support instructions.",
			  "ew packed data manipulation\ninstructions allow for improved reordering of packed data and register pairs.\n",
			  "The following books describe the TMS320C6000 devices and related support tools. To obtain a copy of any of these TI documents, call the Texas Instru-.Read more"
			]
		  },
		  {
			"title": "Example 1. Software Pipelining Information",
			"url": "https://downloads.ti.com/docs/esd/SPRUI04/software-pipelining-information-stdz0577313.html",
			"excerpts": [
			  "Initiation interval (ii). The number of cycles between the initiation of successive iterations of the loop. The smaller the initiation interval, the fewer ...Read more"
			]
		  },
		  {
			"title": "Global Instruction Scheduling for LLVM",
			"url": "https://llvm.org/devmtg/2014-10/Slides/Larin-GlobalInstructionScheduling.pdf",
			"excerpts": [
			  "Cyclic/Acyclic cases are handled rather differently. – Cyclic scheduling is currently performed rather “early”. • When bundling is not yet available.Read more",
			  "Global Instruction\nScheduling for LLVM",
			  "Sergei Larin , Qualcomm Innovation Center",
			  "Aditya Kumar , Qualcomm Innovation Center",
			  "A case study - Hexagon Global Scheduler",
			  "ce critical path is established we are free to utilize “holes” around it and shorten",
			  "It does not have to worry about increase in register pressure",
			  "Global Scheduling done after packetization (bundling)",
			  "without lengthening the critical path",
			  "Uses the DFA based Hexagon VLIW packetizer\n– To manage resource constraints in bundles",
			  " To manage resource constraints in bundles",
			  " Performing global instruction movement on *actual machine instructions*",
			  "and *allocated physical registers* offers several benefits",
			  "it when possible",
			  "It can perform very target specific optimizations",
			  "Like custom peephole opt and peculiar code layout or compaction",
			  "Power consumption related options etc.",
			  "\nYou have nothing to lose but your chains ",
			  "Cons include",
			  "Restricted scheduling region formation",
			  "Limited flexibility in code motion",
			  "Complex (if any) register scavenging",
			  "Multi-way branching",
			  "• Major goal is to fill in the gaps in the bundles by “pulling\nup” instructions from successor basic blocks\n– …without lengthening the critical path",
			  "Utilizes a specialized post-RA predicate-aware liveness\nanalysis pass",
			  "Performs speculative and predicative global scheduling of\ninstructions in a superblock",
			  "1.4% improvement on SPEC 2K Int"
			]
		  },
		  {
			"title": "How to get started with VLIW scheduler? - Project Infrastructure / LLVM Dev List Archives - LLVM Discussion Forums",
			"url": "https://discourse.llvm.org/t/how-to-get-started-with-vliw-scheduler/58139",
			"excerpts": [
			  "The way we do it for Hexagon is that we have our own specialization of the MachineScheduler pass, followed by a packetizer pass.",
			  "Actually, we run two schedulers: machine scheduler, and post-RA scheduler, but that doesn’t change much conceptually.",
			  "In the scheduler passes we don’t form instruction packets, at least not explicitly.",
			  "We do some sort of a simulation of “what if we were forming packets right now” type of thing, maximizing packetization opportunities ahead of time, before the packetization itself, hoping that enough of it will prevail.",
			  "Finally, near the end of the post-RA passes we do run the DFAPacketizer pass which forms the actual packets (i.e. “bundles” in LLVM terminology).",
			  "The DFA packetizer relies on tables that are generated out of your target’s td files describing your processor’s resources (some of which the standard scheduler uses as well).",
			  "If you’re starting from scratch, I’d get the packetizer working first."
			]
		  },
		  {
			"title": "[llvm-dev] RFC: Generalizing Hexagon's VLIWMachineScheduler",
			"url": "https://groups.google.com/g/llvm-dev/c/IXxgPZ5KdZI",
			"excerpts": [
			  "The VLIW-aware pre-allocation scheduler used by Hexagon is a mostly-generic alternative to the default MachineScheduler implementation that attempts to order instructions in such a way as to optimize the number of instructions issued per\ncycle. It does this by tracking available resources (DFAPacketizer) and balancing register pressure. This is a departure from the default list scheduler, which has no resource tracking and does not attempt to model instructions executing in parallel, instead\noptimizing the straight-line path length of the DAG."
			]
		  },
		  {
			"title": "Random Testing of Code Generation in Compilers",
			"url": "https://robcasloz.github.io/teaching/BevinHansson_2015.pdf",
			"excerpts": [
			  "As Hexagon is a VLIW architecture, the compiler is responsible for scheduling\nand bundling instructions to achieve the most efficient execution.",
			  "exagon instructions are\ncollected into groups called *bundles* during compilation. Each bundle can con-\ntain up to four instructions",
			  "The architecture possesses a 32-bit and 64-bit register set, as well as\na set of 1-bit predicate registers for conditional jumps and predicated instruc-\ntions [36]",
			  "In LLVM,\nthe slot requirements are encoded in the form of available resources, which\ninstructions must reserve in order to be bundled together.",
			  "If an instruction is\nto be added to a bundle but its resource has already been taken, it is not a\nviable candidate for bundling.",
			  "LLVM’s packetizer uses a state machine to determine if instructions can be\nbundled with each other.",
			  "ch state is a possible combination of reserved re-\nsources and each transition is a reservation of a certain resource.",
			  "e resource\nrequirements (called *itineraries* ) are encoded into the Hexagon instruction def-\ninitions in LLVM.",
			  "Figure 5.4 shows a toy example with two resources: *S* 0 and\n*S* 1; and three types of instructions: those which must use *S* 0, those which\nmust use *S* 1 and those which can use either.",
			  "e example demonstrates re-\nsource reservation; there is no transition from state *b* on *S* 0, as that resource\nhas already been taken by a previously bundled instruction.",
			  "VM’s packetizer uses a state machine to determine if instructions can be\nbundled with each other. Each state is a possible combination of reserved re-\nsources and each transition is a reservation of a certain resource."
			]
		  },
		  {
			"title": "New Release: Deeper DSP Customization with Qualcomm Hexagon SDK 2.0 (Part 2 of 2) | Qualcomm",
			"url": "https://www.qualcomm.com/news/onq/2015/04/new-release-deeper-dsp-customization-qualcomm-hexagon-sdk-20-part-2-2",
			"excerpts": [
			  "Hexagon is one of the first digital signal processors to support code generation using a low-level virtual machine (LLVM) compiler, and in this release of the SDK we’re expanding its accessibility beyond OEMs to developers.",
			  "The LLVM compiler is part of the effort by Qualcomm Technologies, Inc. to develop a simpler but more powerful code development approach for mobile, with more integrated tools to program the heterogeneous Snapdragon platform.",
			  "Hexagon LLVM compiler converts high-level-language code into Hexagon assembly code when the back end takes the intermediate language and generates Hexagon-specific assembly code.",
			  "The result is more flexible than a monolithic compiler because it decouples the compiler‘s front-end language support from the processor-specific, back-end target code generation. That way, the front-end compiler work can benefit all processors for that language, while the back-end, processor-specific code generator team can incorporate processor-specific optimizations."
			]
		  },
		  {
			"title": "[LLVMdev] VLIW code generation for LLVM backend",
			"url": "https://groups.google.com/g/llvm-dev/c/HLbNkuCFgmE",
			"excerpts": [
			  "\nHi,\nIt seems the only one VLIW target Hexagon in LLVM 3.2 devel uses a\nstraightforward way to emit its VLIW-style asm codes.\nIt uses a list scheduler to schedule on DAG and a simple packetizer to\nwrap the emitted asm instructions.\nBoth scheduling and packetizing work on basic blocks.\nso,",
			  "There is work currently underway to add SW pipelining and some sort of\nglobal scheduling to Hexagon, but if there is some interest to it from other\ntargets, it would be helpful to know. What is your involvement with this?\nSergei Larin\n--\nQualcomm Innovation Center, Inc. is a member of Code Aurora Forum.\n"
			]
		  },
		  {
			"title": "Qualcomm Corporate Template",
			"url": "https://llvm.org/devmtg/2017-02-04/Halide-for-Hexagon-DSP-with-Hexagon-Vector-eXtensions-HVX-using-LLVM.pdf",
			"excerpts": [
			  "Halide provides two execution environments for HVX.",
			  "Hardware model or the offload model.",
			  "Transparently dispatches Halide pipeline from the host CPU to the\nHexagon™ processor.\n",
			  "Very easy to use as a developer.",
			  "HVX supports unaligned loads, but they are less efficient than aligned loads.",
			  "Halide provides an abstraction to specify assumptions about the alignment of\nexternal memory buffers.",
			  "Halide also provides a scheduling directive to prefetch data into the L2 cache.",
			  "For example, “ my_func.prefetch (y, 2)” will prefetch into the L2 cache,\n2 iterations worth of data needed in the ‘y’ loop.\n",
			  " Halide for Hexagon™ DSP with\nHexagon Vector eXtensions (HVX)\nusing LLVM",
			  "ide programs / pipelines consist of two major components\n•\nAlgorithm\n•\nSchedul",
			  "Halide on Hexagon with HVX",
			  "\nCompiler targets include x86/SSE, ARM v7/NEON, CUDA, Hexagon™/HVX\nand OpenCL.",
			  "Halide enables rapid authoring and evaluation of optimized pipelines\nby separating the algorithm from the computational organization of the different\nstages of the pipeline (schedule).",
			  "Halide provides two execution environments for HVX.\n•\nHardware model or the offload mode",
			  "Halide provides two execution environments for HVX.\n•\nHardware model or the offload mode",
			  "\nHalide::Func f;\n// horizontal blur – Algorithm.\nf(x, y) = (input(x-1, y) + input(x, y) + input(x+1, y))/3;"
			]
		  },
		  {
			"title": "llvm/lib/Target/Hexagon/Hexagon.td at master · llvm-mirror/llvm · GitHub",
			"url": "https://github.com/llvm-mirror/llvm/blob/master/lib/Target/Hexagon/Hexagon.td",
			"excerpts": [
			  "HexagonVLIWPacketizer.cpp",
			  "HexagonSchedule.td",
			  "HexagonScheduleV66.td"
			]
		  },
		  {
			"title": "LLVM: lib/CodeGen/DFAPacketizer.cpp Source File",
			"url": "https://llvm.org/doxygen/DFAPacketizer_8cpp_source.html",
			"excerpts": [
			  " This class implements a deterministic finite automaton (DFA) based\n9 // packetizing mechanism for VLIW architectures. It provides APIs to\n10 // determine whether there exists a legal mapping of instructions to\n11 // functional unit assignments in a packet. The DFA is auto-generated from\n12 // the target's Schedule.td ",
			  "sists of 3 major elements: states, inputs, and transitions. For\n15 // the packetizing mechanism, the input is the set of instruction classes for\n16 // a target. The state models all possible combinations of functional unit\n17 // consumption for a given set of instructions in a packet. A transition\n18 // models the addition of an instruction to a packet. In the DFA constructed\n19 // by this class, if an instruction can be added to a packet, then a valid\n20 // transition exists from the corresponding state."
			]
		  },
		  {
			"title": "TableGen BackEnds — LLVM 22.0.0git documentation",
			"url": "https://llvm.org/docs/TableGen/BackEnds.html",
			"excerpts": [
			  "**Purpose** : This class parses the Schedule.td file and produces an API that\ncan be used to reason about whether an instruction can be added to a packet\non a VLIW architecture. The class internally generates a deterministic finite\nautomaton (DFA) that models all possible mappings of machine instructions\nto functional units as instructions are added to a packet.",
			  "**Output** : Scheduling tables for GPU back-ends (Hexagon, AMD).",
			  "**Usage** : Included directly on `<Target>InstrInfo.cpp` ."
			]
		  },
		  {
			"title": "The LLVM Target-Independent Code Generator — LLVM 23.0.0git documentation",
			"url": "https://llvm.org/docs/CodeGenerator.html",
			"excerpts": [
			  "In a Very Long Instruction Word (VLIW) architecture, the compiler is responsible\nfor mapping instructions to functional-units available on the architecture. To\nthat end, the compiler creates groups of instructions called *packets* or *bundles* . The VLIW packetizer in LLVM is a target-independent mechanism to\nenable the packetization of machine instructions.",
			  "The packetizer reads instruction classes from a target’s itineraries and creates\na deterministic finite automaton (DFA) to represent the state of a packet. A DFA\nconsists of three major elements: inputs, states, and transitions. The set of\ninputs for the generated DFA represents the instruction being added to a\npacket. The states represent the possible consumption of functional units by\ninstructions in a packet. In the DFA, transitions from one state to another\noccur on the addition of an instruction to an existing packet. If there is a\nlegal mapping of functional units to instructions, then the DFA contains a\ncorresponding transition.",
			  "The absence of a transition indicates that a legal\nmapping does not exist and that the instruction cannot be added to the packet.",
			  "To generate tables for a VLIW target, add *Target* GenDFAPacketizer.inc as a\ntarget to the Makefile in the target directory. The exported API provides three\nfunctions: `DFAPacketizer::clearResources()` , `DFAPacketizer::reserveResources(MachineInstr *MI)` , and `DFAPacketizer::canReserveResources(MachineInstr *MI)` . These functions allow\na target packetizer to add an instruction to an existing packet and to check\nwhether an instruction can be added to a packet. See `llvm/CodeGen/DFAPacketizer.h` for more information.",
			  "To generate tables for a VLIW target, add *Target* GenDFAPacketizer.inc as a\ntarget to the Makefile in the target directory.",
			  "The VLIW packetizer in LLVM is a target-independent mechanism to\nenable the packetization of machine instructions."
			]
		  },
		  {
			"title": "LLVM: llvm::VLIWPacketizerList Class Reference",
			"url": "https://llvm.org/doxygen/classllvm_1_1VLIWPacketizerList.html",
			"excerpts": [
			  "eimplemented in [llvm::HexagonPacketizerList](classllvm_1_1HexagonPacketizerList.html) .",
			  "llvm::VLIWPacketizerList Class Reference",
			  "` \" [llvm/CodeGen/DFAPacketizer.h](DFAPacketizer_8h_source.html) \"`",
			  "Definition at line [136](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Section Title: Detailed Description > [◆]() initPacketizerState()",
			  "Definition at line [182](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Section Title: Detailed Description > [◆]() PacketizeMIs()",
			  "Definition at line [153](DFAPacketizer_8cpp_source.html) of file [DFAPacketizer.cpp](DFAPacketizer_8cpp_source.html) ",
			  "Section Title: Detailed Description > [◆]() ResourceTracker",
			  "Definition at line [165](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Definition at line [165](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) ."
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.h Source File",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8h_source.html",
			"excerpts": [
			  "3// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions. 4// See https://llvm.org/LICENSE.txt for license information."
			]
		  },
		  {
			"title": "Is there a way to convert between SchedMachineModel and ...",
			"url": "https://discourse.llvm.org/t/is-there-a-way-to-convert-between-schedmachinemodel-and-itineraries/38837",
			"excerpts": [
			  "I was trying to write a very simple VLIW packetizer (Hexagon was my starting point). It turns out that current DFAPacketizer is using itineraries, but my ..."
			]
		  },
		  {
			"title": "SchedMachineModel: Adding and Optimizing a Subtarget",
			"url": "https://llvm.org/devmtg/2014-10/Slides/Estes-MISchedulerTutorial.pdf",
			"excerpts": [
			  "Enable additional analysis / heuristics with -misched-cluster,. -misched-cyclicpath, -misched-regpressure, and -misched- fusion. Set scheduler (strategy) with ...Read more"
			]
		  },
		  {
			"title": "Scheduling Model in LLVM - Part I | Min Hsu's Homepage",
			"url": "https://myhsu.xyz/llvm-sched-model-1/",
			"excerpts": [
			  "Scheduling models are part of the target definition. They are associated with one or more target processors. Multiple processors can also share ...Read more"
			]
		  },
		  {
			"title": "LLVM: include/llvm/CodeGen/DFAPacketizer.h Source File",
			"url": "https://llvm.org/doxygen/DFAPacketizer_8h_source.html",
			"excerpts": [
			  " a deterministic finite automaton (DFA) based\n9 // packetizing mechanism for VLIW architectures. It provides APIs to\n10 // determine whether there exists a legal mapping of instructions to\n11 // functional unit assignments in a packet. The DFA is auto-generated from\n12 // the target's Schedule.td fil",
			  "of 3 major elements: states, inputs, and transitions. For\n15 // the packetizing mechanism, the input is the set of instruction classes for\n16 // a target. The state models all possible combinations of functional unit\n17 // consumption for a given set of instructions in a packet. A transition\n18 // models the addition of an instruction to "
			]
		  },
		  {
			"title": "LLVM: llvm::DFAPacketizer Class Reference",
			"url": "https://llvm.org/doxygen/classllvm_1_1DFAPacketizer.html",
			"excerpts": [
			  "llvm::DFAPacketizer Class Reference",
			  "|void llvm::DFAPacketizer::clearResources |( |) |\n",
			  "\n|void |[reserveResources]() ( [MachineInstr](classllvm_1_1MachineInstr.html) & [MI](IRTranslator_8cpp.html) ) |",
			  "l) |[canReserveResources]() ( [MachineInstr](classllvm_1_1MachineInstr.html) & [MI](IRTranslator_8cpp.html) ) |\n",
			  "ml) |[canReserveResources]() ( [const](AArch64PromoteConstant_8cpp.html) [MCInstrDesc](classllvm_1_1MCInstrDesc.html) *MID) |",
			  "void |[reserveResources]() ( [const](AArch64PromoteConstant_8cpp.html) [MCInstrDesc](classllvm_1_1MCInstrDesc.html) *MID) |\n|"
			]
		  },
		  {
			"title": "[llvm-dev] How to get started with VLIW scheduler?",
			"url": "https://groups.google.com/g/llvm-dev/c/U4JNhxz0IHI/m/atKaNI0dAgAJ",
			"excerpts": [
			  "The way we do it for Hexagon is that we have our own specialization of the MachineScheduler pass, followed by a packetizer pass.  Actually, we run two schedulers: machine scheduler, and post-RA scheduler, but that doesn’t change much conceptually.",
			  "In the scheduler passes we don’t form instruction packets, at least not explicitly.  We do some sort of a simulation of “what if we were forming packets right now” type of thing, maximizing packetization opportunities ahead of time, before the packetization\nitself, hoping that enough of it will prevail.",
			  "Finally, near the end of the post-RA passes we do run the DFAPacketizer pass which forms the actual packets (i.e. “bundles” in LLVM terminology).  The DFA packetizer relies on tables that are generated out of your target’s td files describing\nyour processor’s resources (some of which the standard scheduler uses as well).",
			  "If you’re starting from scratch, I’d get the packetizer working first."
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.cpp File Reference",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8cpp.html",
			"excerpts": [
			  "Gets the predicate register of a predicated instruction.",
			  "Returns true if an instruction is predicated on p0 and false if it's predicated on !p0."
			]
		  },
		  {
			"title": "RFC: Generalizing Hexagon's VLIWMachineScheduler - Project Infrastructure / LLVM Dev List Archives - LLVM Discussion Forums",
			"url": "https://discourse.llvm.org/t/rfc-generalizing-hexagons-vliwmachinescheduler/59178",
			"excerpts": [
			  "The VLIW-aware pre-allocation scheduler used by Hexagon is a mostly-generic alternative to the default MachineScheduler implementation that attempts to order instructions in such a way as to optimize the number of instructions issued per cycle. It does this by tracking available resources (DFAPacketizer) and balancing register pressure.",
			  "This is a departure from the default list scheduler, which has no resource tracking and does not attempt to model instructions executing in parallel, instead optimizing the straight-line path length of the DAG.",
			  "\nI intend to generalize these data structures…\nVLIWResourceModel\n",
			  "VLIWMachineScheduler"
			]
		  },
		  {
			"title": "llvm/include/llvm/Target/TargetSchedule.td at master · llvm-mirror/llvm · GitHub",
			"url": "https://github.com/llvm-mirror/llvm/blob/master/include/llvm/Target/TargetSchedule.td",
			"excerpts": [
			  "class SchedMachineModel {",
			  "int IssueWidth = -1; // Max micro-ops that may be scheduled per cycle.",
			  "int MicroOpBufferSize = -1; // Max micro-ops that can be buffered.",
			  "int LoopMicroOpBufferSize = -1; // Max micro-ops that can be buffered for",
			  "// optimized loop dispatch/execution.",
			  "int LoadLatency = -1; // Cycles for loads to access the cache.",
			  "int HighLatency = -1; // Approximation of cycles for \"high latency\" ops.",
			  "int MispredictPenalty = -1; // Extra cycles for a mispredicted branch.",
			  "// Per-cycle resources tables.",
			  "ProcessorItineraries Itineraries = NoItineraries;",
			  "bit PostRAScheduler = 0; // Enable Post RegAlloc Scheduler pass.",
			  "// Subtargets that define a model for only a subset of instructions",
			  "// that have a scheduling class (itinerary class or SchedRW list)",
			  "// and may actually be generated for that subtarget must clear this",
			  "// bit. Otherwise, the scheduler considers an unmodelled opcode to",
			  "// be an error. This should only be set during initial bringup,",
			  "// or there will be no way to catch simple errors in the model",
			  "// resulting from changes to the instruction definitions.",
			  "def NoSchedModel : SchedMachineModel {",
			  "let NoModel = 1;",
			  "let CompleteModel = 0;",
			  "// Define a kind of processor resource that may be common across",
			  "// similar subtargets.",
			  "class ProcResourceKind;",
			  "// Define a number of interchangeable processor resources. NumUnits",
			  "// determines the throughput of instructions that require the resource.",
			  "// An optional Super resource may be given to model these resources as",
			  "// a subset of the more general super resources. Using one of these",
			  "// resources implies using one of the super resources.",
			  "//",
			  "//",
			  "// ProcResourceUnits normally model a few buffered resources within an",
			  "// out-of-order engine. Buffered resources may be held for multiple",
			  "// clock cycles, but the scheduler does not pin them to a particular",
			  "// clock cycle relative to instruction dispatch.",
			  "class ProcResourceUnits<ProcResourceKind kind, int num> {",
			  "ProcResourceKind Kind = kind;",
			  "int NumUnits = num;",
			  "ProcResourceKind Super = ?;",
			  "int BufferSize = -1;",
			  "SchedMachineModel SchedModel = ?;",
			  "}",
			  "}",
			  "}"
			]
		  },
		  {
			"title": "1 TableGen Programmer's Reference",
			"url": "https://llvm.org/docs/TableGen/ProgRef.html",
			"excerpts": [
			  "This document describes the LLVM TableGen facility in detail. It is intended for the programmer who is using TableGen to produce code for a project.Read more"
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.cpp Source File",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8cpp_source.html",
			"excerpts": [
			  "plements a simple VLIW packetizer using DFA. The packetizer works on\n10 // machine basic blocks. For each instruction I in BB, the packetizer consults\n11 // the DFA to see if machine resources are available to execute I. If so, the\n12 // packetizer checks if I depends on any instruction J in the current packet.\n13 // If no dependency is found, I is added to current packet and machine resource\n14 // is marked as taken. If any dependency is found, a target API call is made t",
			  "HexagonPacketizerList(MachineFunction &MF, MachineLoopInfo &MLI, AAResults *AA, const MachineBranchProbabilityInfo *MBPI, bool Minimal)"
			]
		  },
		  {
			"title": "Allocating Rotating Registers by Scheduling",
			"url": "https://microarch.org/micro46/files/paper5a3_slides.pdf",
			"excerpts": [
			  "Rotating Register Allocation ≡ Scheduling ! • It is a software pipelining problem. – A register allocation is a modulo schedule of lifetimes ... • Extended to ...Read more"
			]
		  },
		  {
			"title": "Lecture 12 - Computer Science - Harvard University",
			"url": "https://www.eecs.harvard.edu/cs146-246/cs146-lecture12.pdf",
			"excerpts": [
			  "Nearly all instructions are predicated",
			  " 64 1-bit predict registers\n",
			  "IA-64 instructions are encoded in bundles , which are 128 bits wide.",
			  "Each bundle consists of a 5-bit template field and 3 instructions, each 41 bits\nin length",
			  "5-bit template field",
			  "Compiler must explicitly indicate the boundary between one instruction\ngroup and another by placing a stop between 2 instructions that belong to\ndifferent groups",
			  "Stop bits",
			  "The integer registers designed to assist procedure calls using\na register stack",
			  "Register rotation eases the task of allocating\nregisters in software pipelined loops",
			  "All INT registers have a 1-bit NaT (Not A Thing)",
			  "Deferred exceptions",
			  "Chk.s instructions can detect and branch to recovery\ncode",
			  "Advanced Loads* allow speculative memory\nreference",
			  "LAT* table is allocated with register destination +\nmemory address",
			  "Stores associatively lookup the table when they execute",
			  "Before using the value of the advanced load\n– Explicit check is needed to see if ALAT entry is valid",
			  "Predicated execution (select 1 out of 64 1-bit flags)",
			  "Predicated execution (select 1 out of 64 1-bit flags)"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol4.pdf",
			"excerpts": [
			  "Profile-guided compilers that use advanced IA-64 architectural features such as predication\nand speculation benefit from run-time profile information to optimize instruction schedules.",
			  "architectural features such as speculation, predication, and more.",
			  "On the Itanium processor, WC loads are performed directly from memory and not from coalescing buffers. It has a separate 2-entry, 64-byte Write Coalesce Buffer ...Read more",
			  "IA-64 architecture features a revolutionary 64-bit instruction set architecture (ISA) which applies a\nnew processor architecture technology called EPIC, or Explicitly Parallel Instruction Computing.",
			  "Chapter 11, “ Software Pipelining and Loop Support ” provides a detailed discussion on optimizing\nloops through use of software pipelinin",
			  "Chapter 10, “ Predication, Control Flow, and Instruction Stream ” describes optimization features\nrelated to predication, control flow, and branch hint",
			  "The Itanium processor cycle accounting monitors account for all major single and multi-cycle stall\nand flush conditions.",
			  "This chapter outlines the targeted performance monitor usage\nmodels, defines the software interface and programming model, and lists the set of monitored\nevents.",
			  "The Itanium processor does not support a non-temporal buffer; instead, non-temporal accesses are\nallocated in L2 cache with biased replacement.",
			  "r 7, “ Performance Monitor Events ” summarizes the Itanium processor events and describes\nhow to compute commonly used performance metrics for Itanium processor event"
			]
		  },
		  {
			"title": "Intel® Itanium™ Processor Reference Manual for Software ...",
			"url": "https://people.freebsd.org/~marcel/refs/ia64/itanium/24532003.pdf",
			"excerpts": [
			  "On the Itanium processor, WC loads are performed directly from memory and not from coalescing buffers. It has a separate 2-entry, 64-byte Write Coalesce Buffer ...Read more"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "Itanium architecture incorporates features which enable high sustained performance\nand remove barriers to further performance increases. The Itanium architecture is\nbased on the following principles:",
			  "predication** section describes if-conversion, predicate usage, and code\nscheduling to reduce the affects of branching.\n•",
			  "• Mechanisms for synergy between the compiler and the processor",
			  "Memory latency is a major factor in determining the performance of integer\napplications. In order to help reduce the effects of memory latency, the Itanium\narchitecture explicitly supports software pipelining, large register files, and\ncompiler-controlled speculation.",
			  "• 128 integer and floating-point registers, 64 1-bit predicate registers, 8 branch\nregisters",
			  "• Support for many execution units and memory ports",
			  "• Features that enhance instruction level parallelism",
			  "• Speculation (which minimizes memory latency impact).",
			  "• Predication (which removes branches).",
			  "The Itanium architecture enables procedures to communicate register usage to\nthe processor. This allows the processor to schedule procedure register operations even\nwhen there is a low degree of ILP.",
			  "The Itanium architecture allows issuing of independent instructions in\nbundles (three instructions per bundle) for parallel execution and can issue multiple\nbundles per clock.",
			  "Supported by a large number of parallel resources such as large\nregister files and multiple execution units, the Itanium architecture enables the\ncompiler to manage work in progress and schedule simultaneous threads of\ncomputation.",
			  "The Itanium architecture\nallows the renaming of registers which provide every iteration with its own set of\nregisters, avoiding the need for unrolling. This kind of register renaming is called\nregister rotation.",
			  "Modulo scheduling of a loop is analogous to hardware pipelining of a functional unit\nsince the next iteration of the loop starts before the previous iteration has finished.",
			  "The Itanium\narchitecture enables the compiler to exploit speculative information without sacrificing\nthe correct execution of an application (see “Speculation” on page 1:16 ).",
			  "ting-point registers 32 through 127 are termed the **rotating floating-point**\n**registers** . These registers can be programmatically renamed to accelerate lo",
			  "The Itanium architecture provides mechanisms, such as instruction templates, branch\nhints, and cache hints to enable the compiler to communicate compile-time information\nto the processor.",
			  "Every memory load and store in the Itanium architecture has a 2-bit cache hint field in\nwhich the compiler encodes its prediction of the spatial and/or temporal locality of the\nmemory area being accessed.",
			  "The Itanium architecture provides extensive support for software-pipelined loops,\nincluding register rotation, special loop branches, and application registers.",
			  "When\ncombined with predication and support for speculation, these features help to reduce\ncode expansion, path length, and branch mispredictions for loops that can be software\npipelined.",
			  "Register Stack Engine",
			  "A key feature of the Itanium architecture is IA-32 instruction set compatibility. The Intel® Itanium® Architecture Software Developer's Manual ...Read more"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  "**Architectural Support for Control Speculation**",
			  "• 65th bit (NaT bit) on each GR indicates if an exception\nhas occurred",
			  " Special speculative loads that set the NaT bit if a\ndeferrable exception occurs\n",
			  " Special chk.s instruction that checks the NaT bit and\nbranches to recovery, if set\n",
			  " Computational instructions propagate NaTs like IEEE\nNaN’s\n•",
			  "Compare operations propagate “false” when writing\npredicates",
			  "*Predication Concepts**",
			  "Predication Concepts",
			  " Branching causes difficulty to handle effects\n",
			  "IA-64 provides predication\n",
			  "� Allows some branches to be removed",
			  "� Allows some types of safe code motion beyond\nbranches",
			  "� Basis for branch architecture and conditional\nexecution",
			  "**Architectural Support for Data Speculation**",
			  "• ALAT - HW structure containing information about\noutstanding loads advanced across stores",
			  "• Compare and test instructions write predicates with results\nof comparison/test",
			  " ld.a - advanced loads\n",
			  "� ld.c - check loads",
			  "� chk.a - advance load checks",
			  " aliasing st invalidating entries in ALAT\n",
			  "• Speculative advanced loads - ld.sa - is a control\nspeculative advanced load with fault deferral\n(combines ld.a and ld.s)",
			  "**Predication Concepts**",
			  "• Move complexity of resource allocation, scheduling,\nand parallel execution to compiler\n",
			  "IA-64 provides predication",
			  "\n• Provide features that enable the compiler to\nreschedule programs using advanced features\n(predication, speculation)",
			  "• Enable wide execution by providing processor\nimplementations that the compiler can take\nadvantage of\n",
			  "Basis for branch architecture and conditional\nexecution",
			  "**Overview of IA-64 Compiler Technology**",
			  "predication",
			  "• If-conversion",
			  " Uses regular, unconditional and parallel compares\n",
			  " Software pipelining\n",
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches",
			  " Global instruction scheduling\n",
			  " Uses control and data speculation, predication, post-\nincrements, multi-way branches",
			  "• Global register allocation",
			  "• Uses register stack",
			  "**IA-64 Strategies**",
			  "**Instruction Formats: Bundles**",
			  "• Instruction types",
			  "� M: Memory",
			  " I: Shifts, MM\n",
			  "� B: Branch",
			  "� F: Floating point",
			  "� L+X: Long",
			  "**Instruction 2**",
			  "**Instruction 1**",
			  "**Instruction 0**",
			  "**41 bits**",
			  "**41 bits**",
			  "**41 bits**",
			  "**template**",
			  "**5 bits**",
			  "**128 bits**",
			  "• Template types",
			  "� Regular: MII, MLX, MMI, MFI, MMF",
			  " Stop: MI_I M_MI\n",
			  " Instructions\n",
			  "**Architectural Support**",
			  "• 64 1-bit predicate registers (true/false)",
			  "� p0 - p63",
			  "� most compare/test write result and complement",
			  "� Ex: cmp.eq p1,p2 = r1,0",
			  "• Almost all instructions can have a qualifying predicate\n(qp)",
			  " Ex: (p1) add r1 = r2, r3\n",
			  "� if qp is true, instruction executed normally",
			  "� if qp is false, instruction is squashed",
			  " Allocate registers for predicated code\n"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration",
			  "tation provides a “new register” for**\n**each loop iterati",
			  "each loop iteration",
			  "General, predicate, FP registers rotate",
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "The Initiation Interval (ii) is essentially the length of one pipeline stage -\nthe number of clock cycles it takes to execute one pass through the\nloop instructions or the number of clock cycles between starting\niteration i and starting iteration i+1.",
			  "Modulo Loop Scheduling is just one software pipelining technique.",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.",
			  "The IA-64 architecture has special support for software pipelining.",
			  "ation provides a “new register” for**\n**each loop iteratio",
			  "Enabled with -O2 switch",
			  "Typical SWP",
			  "Disqualifiers",
			  "Support for software pipelining is included in the Intel C/C++ compiler.",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count regist",
			  "Predication",
			  "One predicate assigned to each stage",
			  "Modulo Loop Scheduling involves developing a schedule for one loop\niteration such that when the schedule is repeated at regular intervals,\nno intra- or inter-iteration dependency is violated, and no resource\nusage conflict arises.",
			  "Initiation interval (ii)",
			  "Constant interval between iterations"
			]
		  },
		  {
			"title": "ItaniumR Architecture for Programmers. Understanding 64-Bit ...",
			"url": "https://amp.flylib.com/books/en/2.376.1.124/1",
			"excerpts": [
			  "In order to make specific assignments of rotating registers, we must consider the number of stages throughout which every quantity is \"live.\" First, notice that ..."
			]
		  },
		  {
			"title": "\nProjects to improve performance on IA-64\n- GNU Project",
			"url": "https://www.gnu.org/software/gcc/projects/ia64.html",
			"excerpts": [
			  "Profile information could be used for if-conversion to decide which side of the branch should be predicated. Profile information is used for predication and ..."
			]
		  },
		  {
			"title": "IA-64 Architecture Innovations",
			"url": "https://heco.estue.nl/courses/ACA/IA64/dale.ppt",
			"excerpts": [
			  "Separate Rotating Register Base for each ... EC counts pipeline stages for drain. Software Pipelining. Overlapping execution of different loop iterations.Read more"
			]
		  },
		  {
			"title": "Constant Propagation on Predicated Code",
			"url": "https://www.jucs.org/jucs_9_8/constant_propagation_on_predicated/Knoop_J.pdf",
			"excerpts": [
			  "For architectures such as the IA-64, the importance of switching by means of predi- cated execution from (smaller) basic blocks to (larger) hyperblocks is that ...Read more"
			]
		  },
		  {
			"title": "(PDF) The Transmeta Code Morphing/spl trade/ Software: using speculation, recovery, and adaptive retranslation to address real-life challenges",
			"url": "https://www.academia.edu/28434567/The_Transmeta_Code_Morphing_spl_trade_Software_using_speculation_recovery_and_adaptive_retranslation_to_address_real_life_challenges",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, systemlevel implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation."
			]
		  },
		  {
			"title": "The Technology Behind Crusoe™ Processors",
			"url": "https://classes.engineering.wustl.edu/cse362/images/c/c7/Paper_aklaiber_19jan00.pdf",
			"excerpts": [
			  "The software layer is called Code Morphing™ software because it dynamically “morphs” x86 instructions into VLIW instructions. The Code Morphing software ...Read more"
			]
		  },
		  {
			"title": "Read a paper: The Transmeta Code Morphing Software",
			"url": "https://www.youtube.com/watch?v=FcFR2of_wsQ",
			"excerpts": [
			  "Transmeta Code Morphing™ Software: using speculation, recovery ... (CGO '03). IEEE Computer Society, USA, 15–24. Link to paper: https ..."
			]
		  },
		  {
			"title": "CS 6120: The Transmeta Code Morphing Software",
			"url": "https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/transmeta/",
			"excerpts": [
			  "The Transmeta CMS system is a compiler solution to a hardware problem. While implemnting a just-in-time compiler for x86 is subtle and difficult ...Read more",
			  "Today we are reading the 2003 paper on Transmeta CMS (Code Morphing SoftwareTM). The CMS layer ran x86 programs on the Transmeta ...Read more",
			  "In a VLIW instruction set like Crusoe's, each instruction is really several smaller instructions which are issued in parallel. In the ...Read more"
			]
		  },
		  {
			"title": "Two-pass MRET trace selection for dynamic optimization",
			"url": "https://patents.google.com/patent/US7694281B2/en",
			"excerpts": [
			  "When the counter exceeds a certain threshold, the candidate trace head becomes a hot trace head. Then the hot trace is simply selected as the execution path ..."
			]
		  },
		  {
			"title": "Binary Compatibility 1 The Transmetta Code Morphing ...",
			"url": "https://users.ece.utexas.edu/~merez/EE382V/lect03.pdf",
			"excerpts": [
			  "The paper discusses the software system and the hardware support for implementing x86, specifically exceptions and interrupts,. I/O, and self- ...Read more",
			  "Transmetta’s Crusoe microprocessor is a VLIW micro-\nprocessor that along with a software layer, the Code Morphing Software (CMS), provides\na full implementation of the x86 architecture."
			]
		  },
		  {
			"title": "The Transmeta Code Morphing™ Software | Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization",
			"url": "https://dl.acm.org/doi/10.5555/776261.776263",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the **Code Morphing Software** (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "In this paper we discuss some of the challenges raised by these issues, and present the techniques developed in Crusoe and CMS to meet those challenges.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects.",
			  "The wide range of PC workloads that CMS must handle gracefully in real-life operation, plus the need for full system-level x86 compatibility, expose several issues that have received little or no attention in previous literature, such as exceptions and interrupts, I/O, DMA, and self-modifying code."
			]
		  },
		  {
			"title": "(PDF) The Transmeta Code Morphing Software: Using Speculation, Recovery, and Adaptive Retranslation to Address Real-Life Challenges",
			"url": "https://www.researchgate.net/publication/220799118_The_Transmeta_Code_Morphing_Software_Using_Speculation_Recovery_and_Adaptive_Retranslation_to_Address_Real-Life_Challenges",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "S must ef ficiently handle memor y-m apped I/O and\nother system-level operations,without penalizing\nnormal (non-I/O) memory references",
			  "elf-modifying code. Similar\nproblems resultfrom pages containing bothcode and\ndata, common in Windows/ 9X device drivers, BIOSs,\nand embedded systems running a real-time operating\nsystem such as QNX.\n",
			  "S mu st faith fully reproduce the precise exception\nbehavior of the x86 target, without overlyconstraining\nthe scheduling of its translations.",
			  "MS mu st respond to int errupts at precise x86\ninstruction boundaries,wherethe system possesses a\nconsistent target state.",
			  "The Transmeta native VLIW processors provide\nhardware assistance for various kinds of speculation and\nsubsequent recovery; we describe thismechanism in\nsubsection 3.1.",
			  "Normal atoms only update theworking copy of the\nregister.",
			  "If execution reachesthe end of a translation, a\nspecial commit operation copies all work ing registers in to\ntheir corresponding shadow registers, committingthe\nwork done in the translation",
			  "On the other hand, if any\nexceptional condition, such as the failure of one of CMS ’ s\ntranslation assumptions, occurs inside the translation, the\nruntimesystem undoes the effects of allmolecules\nexecuted since the last commit via a rollback operation\nthat copies the shadowregister values (committed at the\nend of the previous translation) back into the working\nregisters",
			  "Store data are held in a gated store buffer , fromwhich\nthey are onlyreleased to the memory systemat the time\nof a commit.",
			  "On a rollback, stores notyet committed can\nsimplybe dropped from thestore buffer.",
			  "Commit and rollback also apply to memory operations.",
			  "the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  ".g., memory conflict buffers,\nGallagher et al. [14], or the Intel IA-64 ",
			  "CMS monitors recurring\nfailures andgenerates a more conservativetranslation\nwhen it deemsthe rate of failure to be excessive.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects."
			]
		  },
		  {
			"title": "The Transmeta Code Morphing Software",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2018/lib/exe/fetch.php?media=dehnert_transmeta_code_morphing_software.pdf",
			"excerpts": [
			  "CMS is structured like many other dynamic translation\nsystems. Initially, an interpreter decodes and executes\nx86 instructions sequentially, with careful attention to\nmemory access ordering and precise reproduction of\nfaults, while collecting data on execution frequency,\nbranch directions, and memory-mapped I/O operations.\nWhen the number of executions of a section of x86 code\nreaches a certain threshold, its address is passed to the\ntranslator.",
			  "The translator selects a region including that address,\nproduces native code to implement the x86 code from the\nregion identified, and stores the translation with various\nrelated information in the translation cache . From then\non, until something invalidates the translation cache entry,\nCMS executes the translation when the x86 flow of\ncontrol reaches the translated x86 code region.",
			  "Initially, the exits of a translation branch to a lookup\nroutine (the “ no chain ” path in Figure 1) that transfers\ncontrol either to an existing translation for the next\n ... \ninclude up to 200 x86 instructions. This provides an\nextended scope for optimization. The optimizer performs\na number of traditional and Crusoe-specific optimizations\non the region, and schedules the final native VLIW code\nas a set of single-entry, multiple-exit traces.",
			  "The wide range\nof PC workloads that CMS must handle gracefully in\nreal-life operation, plus the need for full system-level x86\ncompatibility, expose several issues that have received\nlittle or no attention in previous literature, such as excep-\ntions and interrupts, I/O, DMA, and self-modifying code.",
			  "Speculation in this context refers to making and\nexploiting assumptions — unproven at translation time —\nabout the code being translated. (For example, the\ntranslator might assume that two specific load and store\ninstructions reference non-overlapping memory.) This\ntype of speculation enables generation of much more\nefficient translations, but should one or more assumptions\nprove to be false, incorrect results may be produced.\nHence, the assumptions must somehow be verified at\nruntime, with appropriate action taken when a violation is\ndetected.",
			  "The requirement for CMS to deliver an utterly faithful\nyet high-performance implementation of a legacy\ncommercial\nmicroprocessor\narchitecture\nposes\na\nsignificant challenge. A key paradigm that allows us to\naddress many technical obstacles is that of speculation,\nrecovery, and adaptive retranslation.",
			  "CMS guarantees correct operation by careful\nscheduling,\ninserting\nno-ops\nif\nnecessary.\nOnly\nunpredictably long-latency operations such as loads that\nmiss in the caches have their additional latency handled\nautomatically by the hardware.",
			  "In fact, the current TM5000 family evolved\nsignificantly from the first TM3000 family Crusoe\nprocessors, adding atoms to more efficiently implement\nx86 segmentation, 16-bit operations, and indirect\nbranches, all without a change in the target ISA.",
			  "CMS can make no assumptions about the operating\nsystem running on the processor and cannot depend on\ninformation or other assistance from the system. It is\na system-level implementation, not application-level,\nand even executes the BIOS code.",
			  "The choice of translation regions is beyond the scope\nof this paper, but they may be fairly large and complex,\ncontain long traces, IF statements, and nested loops, and\ninclude up to 200 x86 instructions.",
			  "The\nparadigm of speculating aggressively, rolling back to a\nconsistent state for recovery when exceptions are\ndetected, and adaptively retranslating to deal with\nrecurring exceptions is a powerful part of the CMS\nsolution.",
			  "the Intel IA-64 ALAT [18]",
			  "to detect overlapping memory operations\nscheduled out of sequence, and to branch to the\ncompensation code (e.g., memory conflict buffers,\nGallagher et al. [14], or the Intel IA-64 ALAT [18]).",
			  "All registers\nholding x86 state are shadowed ; that is, there exist two\ncopies of each register, a working copy and a shadow\ncopy."
			]
		  },
		  {
			"title": "Dynamo: A Transparent Dynamic Optimization System",
			"url": "https://cseweb.ucsd.edu/classes/sp00/cse231/dynamopldi.pdf",
			"excerpts": [
			  "Dynamo\nfirst selects a “hot” trace, then optimizes it, and finally emits\nrelocatable code for it into the fragment cache.",
			  "The emitted\nrelocatable code is contiguous in the fragment cache memory, and\nbranches that exit this code jump to corresponding exit stubs at the\nbottom of the code. This code is referred to as a *fragment* . The\ntrace is a unit of the application’s dynamic instruction stream (i.e.,\na sequence of application instructions whose addresses are\napplication binary addresses) whereas the fragment is a Dynamo\ninternal unit, addressed by fragment cache addresses. The\nfollowing subsections outline the trace selection, trace optimization\nand fragment code generation mechanisms of Dynamo.",
			  "No profiling is done on the code generated into Dynamo’s\nfragment cache. This allows the cached code to run directly on the\nprocessor at full native speed without any Dynamo introduced\noverheads. ",
			  "the fragment cache is designed to tolerate periodic\nflushes. Periodically flushing some of the traces in the fragment\ncache helps remove unused traces, and also forces re-selection of\nactive traces.",
			  "e selected hot trace is prepared for optimization by\nconverting it into a low-level intermediate representation (IR) that\nis very close to the underlying machine instruction s",
			  "The first task of trace optimization is to transform the\nbranches on the trace so that their fall-through direction remains on\nthe trace. Loops are only allowed if the loop-back branch targets\nthe start-of-trace. Otherwise the loop-back branch is treated as a\ntrace exit. Unconditional direct branches are redundant on the trace\nand can be removed."
			]
		  },
		  {
			"title": "Dynamo TR",
			"url": "https://homes.cs.washington.edu/~bodik/ucb/cs703-2002/papers/dynamo-full.pdf",
			"excerpts": [
			  "**3.3 Innovative features**",
			  "e first, and probably the most important, is Dynamo’s hot trace selection scheme. It is\nhighly speculative in nature, and uses very minimal profiling. The essence of the idea is that when\na program address gets hot, the sequence of addresses executed immediately following it are\nlikely to be a hot trace. Thus, instead of using complicated path profiling, Dynamo only\nassociates counters with certain special program addresses (such as targets of backward taken\nbranches). When such an address gets hot, the interpreter emits a copy of the interpreted native\ninstructions that immediately follow the one at the hot add",
			  "The second innovation is the use of a dynamic trace as the granularity of runtime\noptimization. The instructions comprising a dynamic trace are often non-contiguous in memory,\nand thus may not have been part of the static program scope that was available to the generator of\nthe program binary. A dynamic trace is therefore likely to expose new optimization opportunities,\neven in a statically optimized program binary.",
			  "e traces can only be entered at the top,\n14\nand they have no control join points within them. This simplicity of control flow allowed us to\ndesign a very fast, non-iterative, trace optimizer, that turned out to be highly effective. Dynamo\nonly optimizes each trace once, at the time it is emitted into the software code cache, and does not\n .",
			  "**7 Trace Selector**",
			  "Dynamic optimization is based on identifying hot spots in the executing program. The Trace\nSelector extracts hot code regions around the detected hot spots and passes them to the Fragment\nOptimizer for optimization and eventual placement in the Fragment Cache. The arrangement of\nthe extracted hot code regions in the Fragment Cache leads to a new code layout, which has direct\nimpact on future performance of the hot spot code.",
			  "Thus, the goal of trace selection\nis twofold: to improve the *code layout* and to expose *dynamic re-optimization* opportunities.",
			  "he code region of choice in Dynamo is a *partial execution trace* (trace for short). Traces\nfall into the dynamic scope of the native program and directly represent dynamic control flow\ninformation. A trace is a dynamic sequence of executed basic blocks. The sequence may not be\ncontiguous in the program memory; it may even be *interprocedural* , spanning one or several\nprocedure boundaries, including dynamically linked modules.",
			  "Some profiling is necessary to differentiate hot code region from cold ones. Profile\ninformation is collected as a side effect of interpretation, and trace selection is triggered only\nwhen a hot program address is interpreted. After selecting a hot trace, the Fragment Optimizer\ntransforms the trace into a location-independent, contiguous code sequence called a *fragment* ,\nwhich is later placed in the Fragment Cache for execution",
			  "Trace\nSelector extracts hot code regions around the detected hot spots and passes them to the Fragment\nOptimizer for optimization and eventual placement in the Fragment Cache.",
			  "The Trace Selector attempts to reduce or mitigate taken-branch penalties by converting taken\nbranches on a trace into fall through branches."
			]
		  },
		  {
			"title": "US20120260067A1 - Microprocessor that performs x86 isa and arm isa machine language program instructions by hardware translation into microinstructions executed by common execution pipeline \n        - Google Patents",
			"url": "https://patents.google.com/patent/US20120260067A1/en",
			"excerpts": [
			  "Transmeta Corporation of Santa Clara, Calif., coupled VLIW core hardware and “a pure software-based instruction translator [referred to as “Code Morphing ...Read more"
			]
		  },
		  {
			"title": "Transmeta - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Transmeta",
			"excerpts": [
			  "Code Morphing Software (CMS) consisted of an interpreter, a runtime system and a dynamic binary translator. x86 instructions were first interpreted one ...Read more"
			]
		  },
		  {
			"title": "YETI: a graduallY Extensible Trace Interpreter",
			"url": "http://www.cs.toronto.edu/~matz/dissertation/matzThesisProposal.pdf",
			"excerpts": [
			  "2.3.1 HP Dynamo. HP Dynamo [2, 16, 1] is a system for trace-based runtime optimization of statically optimized binary code. Dynamo initially interprets a ...Read more"
			]
		  },
		  {
			"title": "Top secret chip less secret now",
			"url": "https://www.cnet.com/tech/tech-industry/top-secret-chip-less-secret-now/",
			"excerpts": [
			  "\"Code morphing\" software, assisted by hardware on the chip itself, translates the instructions \"on the fly,\" then stores the translated instructions so they can ...Read more"
			]
		  },
		  {
			"title": "Dynamic Optimization of IA-32 Applications Under DynamoRIO",
			"url": "https://groups.csail.mit.edu/cag/rio/tim-meng-thesis.pdf",
			"excerpts": [
			  "Only code in the fragment cache is allowed to execute, never code in the original image of the binary. As the number of fragments increases, performance is ...Read more"
			]
		  },
		  {
			"title": "Transmeta Awarded Another Patent",
			"url": "https://tech.slashdot.org/story/99/09/28/1531230/transmeta-awarded-another-patent",
			"excerpts": [
			  "This is a hardware method of rolling back stores to get the exact state at the time of exception without tracking state on an instruction by ...Read more"
			]
		  },
		  {
			"title": "The Transmeta Code Morphing/spl trade/ Software: using ...",
			"url": "https://ieeexplore.ieee.org/document/1191529/",
			"excerpts": [
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and run-time system.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects.",
			  "The wide range of PC workloads that CMS must handle gracefully in real-life operation, plus the need for full system-level x86 compatibility, expose several issues that have received little or no attention in previous literature, such as exceptions and interrupts, I/O, DMA, and self-modifying code."
			]
		  },
		  {
			"title": "US7111096B1 - Fast look-up of indirect branch destination in a dynamic translation system \n        - Google Patents",
			"url": "https://patents.google.com/patent/US7111096",
			"excerpts": [
			  "The morph host processor executes the code morphing software which translates the target application programs dynamically into host processor instructions that are able to accomplish the purpose of the original software. A",
			  ", the code morphing software carries out a significant portion of the functions of digital processors in software, reducing the hardware required for processing, and, hence, reducing power consumption.",
			  " As the instructions are translated, they are stored in a translation buffer where they may be subsequently accessed and executed, as needed, during continued program execution without further translation.\n",
			  "anslation memory **7** , sometimes referred to as the translation buffer, holds all of the translations produced during the run of the target application program.",
			  "e method makes use of a cache **5** that is reserved (defined) in the memory of the host processor by the code morphing software for storage of a fraction of the translations of target addresses and the respective accompanying tags for each respective translation."
			]
		  },
		  {
			"title": "US6594821B1 - Translation consistency checking for modified target instructions by comparing to original copy \n        - Google Patents",
			"url": "https://patents.google.com/patent/US6594821",
			"excerpts": [
			  "The morph host processor includes a number of hardware enhancements which allow sequences of target instructions spanning known states of the target processor to be translated into host instructions, stored for further use in the translation buffer, and tested to determine if the translated instructions will execute correctly.",
			  "Memory stores and target processor state are updated upon successful execution in a process referred as “committing.” These hardware enhancements allow the rapid and accurate handling of exceptions which occur during the execution of the sequences of host instructions by returning execution to the beginning of a sequence of instructions at which known state of the target processor exists. Returning the operations to a point in execution at which target state is known is called “rollback.”"
			]
		  },
		  {
			"title": "US5832205A - Memory controller for a microprocessor for detecting a failure of speculation on the physical nature of a component being addressed \n        - Google Patents",
			"url": "https://patents.google.com/patent/US5832205A/en",
			"excerpts": [
			  "the code morphing software combined with the enhanced morph host translates target instructions into instructions for the morph host on the fly and caches those host instructions in a memory data structure (referred to in this specification as a \"translation buffer\").",
			  "a translation buffer to hold translated instructions allows instructions to be recalled without rerunning the lengthy process of determining which primitive instructions are required to implement each target instruction, addressing each primitive instruction, fetching each primitive instruction, optimizing the sequence of primitive instructions, allocating assets to each primitive instruction, reordering the primitive instructions, and executing each step of each sequence of primitive instructions involved each time each target instruction is executed. O",
			  "Once a target instruction has been translated, it may be recalled from the translation buffer and executed without the need for any of these myriad of steps.",
			  "the translator In addition to simply translating the instructions, optimizing, reordering, rescheduling, caching, and executing each translation so that it may be rerun whenever that set of instructions needs to be executed, the translator also links the different translations to eliminate in almost all cases a return to the main loop of the translation process.",
			  "the translator portion of the code morphing software in accomplishing this linking process. It will be understood by those skilled in the art that this linking operation essentially eliminates the return to the main loop for most translations of instructions, which eliminates this overhead.",
			  "Prior art emulators have also struggled with what is generally referred to as self modifying code. Should a target program write to the memory that contains target instructions, this will cause translations that exist for these target instructions to become \"stale\" and no longer valid. It is necessary to detect these stores as they occur dynamically.",
			  "The present invention deals with this problem by another enhancement to the morph host. A translation bit (T bit) which may also be stored in the translation look-aside buffer is used to indicate target memory pages for which translations exist. The T bit thus possibly indicates that particular pages of target memory contain target instructions for which host translations exist which would become stale if those target instructions were to be overwritten."
			]
		  },
		  {
			"title": "Design and Implementation of a Lightweight Dynamic ...",
			"url": "https://jilp.org/vol6/v6paper5.pdf",
			"excerpts": [
			  "amic optimization has been presented in the past in frameworks such as Dynamo [1]\nand Continuous Profiling and Optimization (CPO) [2]. Dynamo is a transparent dynamic\nnative-to-native optimization system. Dynamo starts running a statically compiled exe-\ncutable by interpretation, waiting for hot traces to show up. Once hot traces are detected,\nDynamo stops the program and generates code fragments for these traces. Subsequent ex-\necution on the same trace will be redirected to the newly optimized code in the fragment\ncache. Since interpretation is expensive, Dynamo tries to avoid it by translating as many\nhot traces as possible to the fragment cache. To achieve this goal, it uses a small threshold\nto quickly determine whether a trace is hot. This approach often ends up with generating\ntoo much code and less effective traces. In the recent work Dynamo RIO [6], this feature\nhas been changed",
			  "[2] presents a model closer to traditional PBO where the original code is instru-\nmented, and the profile information is used to compile optimized versions of code. In CPO,\nprofiled information is used to drive PBO while the program is running and the compiled\nresult is hot-swapped into the program. The advantage of this scheme is that the IR infor-\nmation makes application of many optimizations easier.",
			  "There are a lot of additional research topics on dynamic optimization. Many of them\nare often seen in dynamic translation [8],[9],[12] and binary transformation [15].",
			  "In this paper we present a lightweight runtime optimizer implemented on a real system\nwith existing hardware and software support. We discuss much detail of the design and\nimplementation including profiling, hot trace selection, phase detection, optimization and\ncode patching. The overhead of this system is very low due to the use of hardware per-"
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Dynamo starts running a statically\ncompiled executable by interpretation, waiting for hot\ntraces to show up.",
			  "Once hot traces are detected, Dynamo\nstops the program and generates code fragments for these\ntraces.",
			  "bsequent execution on the same trace will be redi-\nrected to the newly optimized code in the fragment cache.",
			  " *trace* is a single entry, multi-exit code\nsequence. ",
			  " Dynamo, a trace is selected once the reference\ncount of the target block of a backwards taken branch ex-\nceeds a threshold.",
			  "s implemented as a shared library on Linux\nfor IA64 that can be automatically linked to the applica-\ntion at startup. It is also a runtime trace optimizer like Dy-\nnamo [3], but ADORE relies on the Itanium hardware per-\nformance monitor (HPM) to identify hotspots instead of us-\ning interpretation"
			]
		  },
		  {
			"title": "Improving Region Selection in Dynamic Optimization Systems | Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2005.22",
			"excerpts": [
			  "The performance of a dynamic optimization system depends heavily on the code it selects to optimize.",
			  "Many current systems follow the design of HP Dynamo and select a single interprocedural path, or trace, as the unit of code optimization and code caching.",
			  "In particular, we propose two new region-selection algorithms and evaluate them against Dynamo's selection mechanism, Next-Executing Tail (NET).",
			  "Our first algorithm, Last-Executed Iteration (LEI), identifies cyclic paths of execution better than NET, improving locality of execution while reducing the size of the code cache.",
			  "Our second algorithm allows overlapping traces of similar execution frequency to be combined into a single large region.",
			  "This second technique can be applied to both NET and LEI, and we find that it significantly improves metrics of locality and memory overhead for each."
			]
		  },
		  {
			"title": "Dynamo: a transparent dynamic optimization system",
			"url": "https://dl.acm.org/doi/10.1145/358438.349303",
			"excerpts": [
			  "Dynamo: a transparent dynamic optimization system. PLDI '00: Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation.Read more"
			]
		  },
		  {
			"title": "[PDF] The Transmeta Code Morphing/spl trade/ Software: using speculation, recovery, and adaptive retranslation to address real-life challenges | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/The-Transmeta-Code-Morphing-spl-trade-Software%3A-and-Dehnert-Grant/5c17b90a86ab37488215f6a060ecd3350a994ebf",
			"excerpts": [
			  "The Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation are presented."
			]
		  },
		  {
			"title": "EP1240582B1 - Fine grain translation discrimination \n        - Google Patents",
			"url": "https://patents.google.com/patent/EP1240582B1",
			"excerpts": [
			  "the morph host processor includes a number of hardware enhancements which allow sequences of target instructions spanning known states of the target processor to be translated into host instructions, stored for further use in the translation buffer, and tested to determine if the translated instructions will execute correctly.",
			  "T bit is stored with a physical page address in a translation lookaside buffer (TLB).",
			  "The exception handler invalidates these translations by turning off the T bit protection for the TLB entry.",
			  "the method is referred to as a \"T bit exception.”",
			  "One problem which can arise with the new processor is that it is possible with some operating systems and applications for a target processor to write to target instructions stored in memory. If this happens, the host instructions which are translations of the target instructions which have been overwritten are no longer valid.",
			  "fine-grain T bit protection eliminates a significant portion of the T bit exceptions generated by the new processor.",
			  "Consequently, a new process has been devised by which the code morphing software may determine whether an attempted write to a memory page is to translated instructions or to data and allow writes to data without invalidating translations of instructions on the page."
			]
		  },
		  {
			"title": "WO2001027743A1 - Fine grain translation discrimination \n        - Google Patents",
			"url": "https://patents.google.com/patent/WO2001027743A1/ja",
			"excerpts": [
			  "Translation consistency checking for modified target instructions by comparing to original copy",
			  "Fast look-up of indirect branch destination in a dynamic translation system"
			]
		  },
		  {
			"title": "United States Patent (19) 11 Patent Number - Googleapis.com",
			"url": "https://patentimages.storage.googleapis.com/99/a4/3b/ea425b14e99f3f/US5832205.pdf",
			"excerpts": [
			  "The code morphing Software of the present invention\nprovides an additional operation which greatly enhances the\nSpeed of processing programs which are being translated.",
			  "\ntranslation buffer is used to store the host instructions which\nembody each completed translation of the target instruc\ntions. A",
			  "ated. If the target instruction\nhas not been translated, it and Subsequent target instructions\nare fetched, decoded, translated, and then (possibly)\noptimized, reordered, and rescheduled into a new host\ntranslation, and Stored in the translation buffer by the trans\nlator. AS will be",
			  "The T bit thus possibly indicates that particular\npages of target memory contain target instructions for which\nhost translations exist which would become stale if those\ntarget instructions were to be overwritten.",
			  "A translation bit (T bit)\nwhich may also be stored in the translation look-aside buffer\nis used to indicate target memory pages for which transla\ntions exist",
			  "he morph host includes additional hardware exception\ndetection mechanisms that in conjunction with the rollback\nand retranslate method described above allow further opti\nmization.",
			  "he present invention deals with this problem by another\nenhancement to the morph host. A translation bit (T bit)\nwhich may also be stored in the translation look-aside buffer\nis used to indicate target memory pages for which transla\ntions exist."
			]
		  },
		  {
			"title": "Dynamo: A Transparent Dynamic Optimization System",
			"url": "https://people.cs.umass.edu/~emery/classes/cmpsci691s-fall2004/papers/bala00dynamo.pdf",
			"excerpts": [
			  "f Dynamo, a\nsoftware dynamic optimization system that is capable of\ntransparently improving the performance of a native instruction\nstream as it executes on the processor.",
			  "Dynamo starts out by interpreting the input native instruction\nstream until a taken branch is encountered ( A ). If the branch target\naddress corresponds to the entry point of a fragment already in the\nfragment cache ( B ), control jumps to the top of that fragment,\neffectively suspending Dynamo, and causing execution of the\ncached fragments to occur directly on the underlying processor ( F ).",
			  "Otherwise, if the branch target satisfies a “start-of-trace” condition\n( C ), a counter associated with the target address is incremented ( D ).",
			  "r current prototype defines start-of-trace as targets of backward-\ntaken branches (likely loop headers) and fragment cache exit\nbranches (exits from previously identified hot traces). ",
			  "If the\ncounter value exceeds a preset hot threshold ( E ), the interpreter\ntoggles state and goes into “code generation mode” ( G ).",
			  "When\ninterpreting in this mode, the native instruction sequence being\ninterpreted is recorded in a hot trace buffer, until an “end-of-trace”\ncondition is reached ( H ).",
			  "At that point the hot trace buffer is\nprocessed by a fast, lightweight optimizer ( I ) to create an\noptimized single-entry, multi-exit, contiguous sequence of\ninstructions called the *fragment",
			  "Our current prototype defines\nend-of-trace as backward taken branches or taken branches whose\ntargets correspond to fragment entry points in the fragment cache\n(i.e., fragment cache hits).",
			  " optimizer is emitted into the fragment cache by a\nlinker ( J ), which also connects fragment exit branches to other\nfragments in the fragment cache if possible.",
			  " execution proceeds, the application’s working set\ngradually materializes in the fragment cache, and the Dynamo\n*overhead* (time spent in the Dynamo interpretive loop / time spent\nexecuting in the fragment cache) begins to drop.",
			  "On the PA-8000 however, breakpoint traps are very expensive, and\nit was more efficient to implement the interpreter by using\nemulation.",
			  "The higher the interpretive overhead, the earlier\nDynamo has to predict the hot trace in order to keep the overheads\nlow.",
			  "ynamo uses a speculative scheme we refer to as MRET (for\n*most recently executed tail* ) to pick hot traces without doing any\npath or branch profilin",
			  "If\nthe counter associated with a certain start-of-trace address exceeds\na preset threshold value, Dynamo switches its interpreter to a mode\nwhere the sequence of interpreted instructions is recorded as they\nare being interpreted.",
			  "The insight behind MRET is that when an instruction\nbecomes hot, it is statistically likely that the very next sequence of\nexecuted instructions that follow it is also hot.",
			  "Besides its\nsimplicity and ease of engineering, MRET has the advantage of\nrequiring much smaller counter storage than traditional branch or\npath profiling techniques.",
			  "Counters are only maintained for\npotential loop headers. Furthermore, once a hot trace has been\nselected and emitted into the fragment cache, the counter\nassociated with its start-of-trace address can be recycled.",
			  "No profiling is done on the code generated into Dynamo’s\nfragment cache. This allows the cached code to run directly on the\nprocessor at full native speed without any Dynamo introduced\noverheads.",
			  "A significant portion of the performance gains come\nfrom the act of selecting a trace and forming a fragment out of it,\nthat is, from the implied partial procedure inlining and improved\ncode layout in the fragment cache.",
			  "Fragment optimization accounts\nfor approximately 3% of the total gains on average, and one-third\nof this is due to conservative (signal and volatile-memory safe)\n2 Our experim",
			  "The target of a backward taken branch is very likely to be a loop\nheader, and thus the head of several hot traces in the loop body."
			]
		  },
		  {
			"title": "(PDF) Dynamo: A Transparent Dynamic Optimization System",
			"url": "https://www.researchgate.net/publication/2560481_Dynamo_A_Transparent_Dynamic_Optimization_System",
			"excerpts": [
			  "Dynamo uses a speculative scheme we refer to as MRET (for\nmost recently executed tail ) to pick hot traces without doi ng any\npath or branch profiling.",
			  "The MRETstrategy works as follows.",
			  "The target of a backward taken branch is very likely to be a loop\nheader, and thus the head of several hot traces in the loop body.",
			  "If\nthe counter associated witha certain start-of-trace address exceeds\na preset threshold value, Dynamo switches its interpreter to a mode\nwhere the sequence of interpreted instructions is recorded as they\nare being interpreted.",
			  "Eventually, when an end-of-trace condition\nis reached,the recordedsequence of instructions (the most recently\nexecuted tail starting from the hotstart-of-trace) is selected as a hot\ntrace.",
			  "Dynamo associates a counter with certain selected start-of-trace\npoints such as the target addresses of backward taken branches.",
			  "the more speculative the trace prediction scheme,\nthe larger we needto size the fragment cache, to compensate for\nthe larger number of tracespicked as a result.",
			  "ream to observe runtime\nexecution behavior. Interpretation is expensive but it prevents the\nApplicationcrt0code",
			  "Dynamo has to predict the hot trace in order to keep the overheads\nlow."
			]
		  },
		  {
			"title": "VLIW Architectures",
			"url": "http://web.cecs.pdx.edu/~mperkows/CLASS_VHDL_99/VLIW.pdf",
			"excerpts": [
			  "Because the C6000 is highly parallel and flexible, the task of\nscheduling code in an efficient way is best completed by the TI\ncode generation tools (compiler and assembly optimizer) and not\nby hand. This results in a convenient C framework to maintain\ncode development on current products as well as reuse the same\ncode on future products (C code, C with TI C6000 Language\nExtensions, and the Linear Assembly source can all be reused on\nfuture TI C6000 DSPs).",
			  "The final territory of programming is the hand-scheduled assembly\nlanguage. At this level, the programmer is literally scheduling the\nassembly code to the DSP pipeline. Depending on the\nprogrammer's ability, she may achieve results similar to those\nachieved by the C6000 tools; however, she risks “man-made”\npipeline scheduling errors that can cause functional errors and\nperformance degradation. In addition, scheduled code may not be\nreusable on future C6000 family members, unlike the three levels\nof C6000 source code. Consequently, avoid the hand-coded\nassembly level “Limited Reuse” programming if at all possible."
			]
		  },
		  {
			"title": "Hand-Optimizing VLIW Assembly Language as a Game | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24525372",
			"excerpts": [
			  "VLIW directly exposes hardware parallelism to the programmer, making more throughput available without requiring complex scheduling machinery (save die area + power). This works well for code targeting a single architecture (like embedded) where timing is predictable (like DSP code).",
			  "Essentially VLIW machine code embeds a bunch of very specific assumptions about processor's internal design.",
			  "There's no reason for the VLIW machine not to get wider with time. The fact that Itanium didn't speaks more of its economic unhealthiness than of anything else.",
			  "Summarizing what I learned when I looked into this:\nVLIW directly exposes hardware parallelism to the programmer, making more throughput available without requiring complex scheduling machinery (save die area + power). This works well for code targeting a single architecture (like embedded) where timing is predictable (like DSP code). Essentially VLIW machine code embeds a bunch of very specific assumptions about processor's internal design."
			]
		  },
		  {
			"title": "Chapter 4 – VLIW",
			"url": "https://www.ece.ucdavis.edu/~akella/270W05/chapter4.ppt.pdf",
			"excerpts": [
			  "**Exploiting ILP through Software**",
			  "**Approaches**",
			  "**Venkatesh Akella**",
			  "**EEC 270**",
			  "**Winter 2005**",
			  "**Based on Slides from Prof. Al. Davis @**",
			  "**cs.utah.edu**",
			  "• **Pros**",
			  "– **No window size limitation, the whole program is there to see**",
			  "– **Hardware is simple, so can push the clock rate**",
			  "– **Pragmas and profile information can be used**",
			  "• **Cons**",
			  "– **Binary code compatibility**",
			  " **Lack of run-time knowledge – eg: memory dataflow problems**",
			  "• **Perhaps a mixture of the two?**",
			  "• **Basic Compiler Techniques for Exposing ILP**",
			  " **Loop Unrolling**\n",
			  " **Instruction Scheduling**\n",
			  " **Static Branch Prediction**\n",
			  " **Static Multiple Issue: VLIW Approach**\n",
			  "**Overview of the Chapter**",
			  "**Overview of the Chapter**",
			  "• **Advanced Compiler Support for Exposing and**",
			  "**Exploiting ILP**",
			  "– **Detecting and Enhancing Loop-Level Parallelism**",
			  " **Dependence Analysis**\n",
			  " **Software Pipelining**\n",
			  "– **Global Code Scheduling**",
			  " **Hardware Support for Exposing More**\n",
			  "**Parallelism at Compile Time**",
			  "– **Predicated Execution**",
			  "– **Compiler Speculation with HW Support**",
			  " **CASE STUDY : ITANIUM PROCESSOR**\n",
			  "**Overview of Software Approaches**",
			  "**Assumptions for the Examples**",
			  "**A Simple Loop**",
			  " ... ",
			  "**Reorder instructions to avoid hazards, hence expose ILP - code**",
			  "**size increases, some branches are eliminated**",
			  "**Software pipelining is different - it involves reorganizing a loop**",
			  "**with instructions from different iterations:**",
			  "**A; (repeat (n-1) [** **B** **C** **A** **]) B; C**",
			  "**Software Pipelining**",
			  "**A;**",
			  "**(repeat (n-1) [** **B** **C** **A** **])**",
			  "**B; C**",
			  "**Prolog**",
			  "**New Loop**",
			  "**B,** **C,** **A** **are instructions from DIFFERENT iterations of the loop**",
			  "**B,** **C,** **A** **are instructions from DIFFERENT iterations of the loop**",
			  "**This does in SW what Tomasulo’s algorithm did in**",
			  "**HW - interleaving instructions from different**",
			  "**iterations of a loop and executing them in parallel**",
			  "**Prolog and Epilog code is to make sure that the same**",
			  "**code is executed in both the cases I.e. semantics**",
			  "**of the program are not altered.**",
			  "**A Concrete Example**",
			  "**Loop:LD F0, 0(R1)**",
			  "**SD F4, 0(R1)**",
			  "**LD F0, 0(R1)**",
			  "**ADD F4, F0, F2**",
			  "**LD F0, -8(R1)**",
			  "**Loop:SD F4, 16(R1) -- stores M[I]**",
			  "**ADD F4, F0, F2 --- adds M[I-1]**",
			  "**LD F0, 0(R1) ---- loads M[I-2]**",
			  "**DADDUI R1, R1, -8**",
			  "**DADDUI R1, R1, -8**",
			  "**DADDUI R1, R1, -8**",
			  "**BNE R1, R2, LOOP**",
			  "**BNE R1, R2, LOOP**",
			  "**ADD F4, F4, F2**",
			  "**ADD F4, F4, F2**",
			  "**STALL**",
			  "**STALL**",
			  "**STALL**",
			  "**STALL**",
			  "**STALL**",
			  "**SD F4, 8(R1)**",
			  "**SD F4, 8(R1)**",
			  "**Epilog**",
			  "**Epilog**",
			  "**PREAMBLE**",
			  "**Instructions not executed**",
			  "**In the final 2 iterations**",
			  "**Loop Unrolling vs SW Pipelining**",
			  "• **SW is symbolic so does not increase the code**",
			  "• **Does not increase register pressure, however**",
			  "*\n**register management is tricky - lifetimes of**",
			  "\n**registers values can be long**\n",
			  "• **Loop unrolling and SW pipelining attack different**",
			  "**problems so in general they can be combined**",
			  " **LU - reduces loop overhead I.e. branch and**\n",
			  "\n**counter update**\n",
			  " **SWP - reduces the time when the loop is not**\n",
			  "*\n**running at its peak**",
			  "**Figuratively Speaking**",
			  "**Going Beyond Basic Blocks**",
			  "**What are the consequences of moving**",
			  "**instruction K before the branch?**",
			  "**Instr J is dependent on K instead of P**",
			  "**If branch is not taken C[I] will get**",
			  "**wrong value**",
			  "**-- Add compensating code**",
			  "**-- keep copy of B**",
			  "**Welcome to Speculation!!**",
			  "**Static Approaches to speculation:**",
			  "**a)** **Global Scheduling:** **Trace Scheduling,**",
			  "**Superblocks**",
			  "**Predicated Execution**",
			  "Software pipelining is different - it involves reorganizing a loop with ... • This helps both hw based scheduling and SW pipelining and global ..."
			]
		  },
		  {
			"title": "HEXAGON DSP: AN ARCHITECTURE OPTIMIZED FOR ...",
			"url": "https://pages.cs.wisc.edu/~danav/pubs/qcom/hexagon_micro2014_v6.pdf",
			"excerpts": [
			  "VLIW instruction packets are variable sized and contain one to four instructions. If a packet contains more than one instruction, the instructions execute in ...Read more"
			]
		  },
		  {
			"title": "VLIW and the MC Layer - Presented by",
			"url": "https://llvm.org/devmtg/2013-11/slides/Guerra-VLIW.pdf",
			"excerpts": [
			  "▫ Example VLIW instruction “packet”, performs 29 RISC-style operations in one ... ▫ Packet optimizations are applied to entire bundle in Hexagon-specific passes.Read more"
			]
		  },
		  {
			"title": "TMS320C64x+ DSP Megamodule - Reference Guide",
			"url": "https://www.ti.com/lit/pdf/spru871",
			"excerpts": [
			  "responsibility to hide much of the latency associated with reading from and writing to the slower system memory. The cache controller design supports a ...Read more"
			]
		  },
		  {
			"title": "Computer Architecture: HASE Predication Model",
			"url": "https://www.icsa.inf.ed.ac.uk/research/groups/hase/models/dlx-pred/pred.html",
			"excerpts": [
			  "The Intel IA-64 is an example of a commercial microprocessor that uses predication. Most instructions in the IA-64 (excluding some such as `return from ...Read more"
			]
		  },
		  {
			"title": "IA-64 / VLIW from a Compiler Optimization Perspective",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s07/www/papers/presentations/ia64_vliw.pdf",
			"excerpts": [
			  "– Chocolate: Full predication. – Vanilla: Partial predication. Page 13. Predication. Figure from B. Falsafi. Page 14. Full Predication. ○ All ...Read more"
			]
		  },
		  {
			"title": "Qualcomm Hexagon DSP",
			"url": "https://pdfs.semanticscholar.org/d174/0f7dc5955d460833dd9afac827eaf0185b78.pdf",
			"excerpts": [
			  "2-cycle packets. • VLIW packets with dependencies between long latency instructions will stall. - But many VLIW packets with simple instructions can complete ...Read more"
			]
		  },
		  {
			"title": "We are starting to implement linear assembly code for TI C64x+, but we are having some difficulty in getting it to work, can you please help? - Code Composer Studio  forum - Code Composer Studio™︎ - TI E2E support forums",
			"url": "https://e2e.ti.com/support/tools/code-composer-studio-group/ccs/f/code-composer-studio-forum/423963/we-are-starting-to-implement-linear-assembly-code-for-ti-c64x-but-we-are-having-some-difficulty-in-getting-it-to-work-can-you-please-help",
			"excerpts": [
			  "Build with the compiler switch --debug_software_pipeline. This option tells the compiler to not delete the generated assembly file. Further, the compiler emits a verbose comment block before every software pipelined loop that contains valuable information about the loop.",
			  "Please look over the C64x+ instruction set in the C6000 CPU and instruction set manual. It is difficult to optimize hand-coded assembly code, even for users who are very familiar with the instruction set. I strongly recommend using the compiler to get an idea of what instructions to use for your algorithm. Write a reference implementation in C, compile it, and look at the generated assembly code."
			]
		  },
		  {
			"title": "Qualcomm Hexagon DSP: An architecture optimized for ...",
			"url": "https://pages.cs.wisc.edu/~danav/pubs/qcom/hexagon_hotchips2013.pdf",
			"excerpts": [
			  "\nHexagon DSP ISA improves control code efficiency\nover traditional VLIW\n**",
			  "**Example C code**",
			  "void example(int *ptr, int val) {",
			  "if (ptr!=0) {",
			  "*ptr = *ptr + val + 2;",
			  "}",
			  "p0 = cmp.eq(r0,)\n{\nif (!p0) r2=memw(r0)\nif (p0) jumpr:nt r31\n}\nr2 = add(r2,)\nr1 = add(r1,r2)\n{\nmemw(r0) = r1\njumpr r31\n}",
			  "**Instr/Packet =**\n**7 instr/5 packets = 1.4**",
			  "Tradional VLIW",
			  "**Assembly Code**",
			  "p0 = cmp.eq (r0,)\nif ( !p0.new) r2=memw(r0)\nif ( p0.new ) jumpr:nt r31\n}",
			  "r2 = add(r2,)\nr1 = add(r1,r2)\n{",
			  "memw(r0) = r1\njumpr r31\n}",
			  "**Hexagon DSP:**\n**Dot-New Predication**",
			  "r1 = add(r1,add(r2,))\n{\nmemw(r0) = r1\njumpr r31\n}",
			  "**Hexagon DSP:**\n**Compound ALU**",
			  "p0 = cmp.eq(r0,)\nif ( !p0.new) r2=memw(r0)\nif ( p0.new ) jumpr:nt r31\n}",
			  "p0 = cmp.eq(r0,)\nif ( !p0.new) r2=memw(r0)\nif ( p0.new ) jumpr:nt r31\n}",
			  "{",
			  "{",
			  "{",
			  "{",
			  "r1 = add(r1,add(r2,))\nmemw(r0) = r1.new\njumpr r31\n}",
			  "**Hexagon DSP:**\n**New-Value Store**",
			  "**Instr/Packet =**",
			  "**7 instr/2packets = 3.5**",
			  " Hexagon V5 includes three hardware threads\n• ",
			  "et\nHexagon DSP V1-V4: Interleaved multi-threading\n",
			  "Simple round-robin thread scheduling",
			  "Simple round-robin thread scheduling",
			  "**Thread 0 Dispatch**",
			  "**Thread 0 Dispatch**",
			  "**Thread 1 Dispatch**",
			  "**Thread 1 Dispatch**",
			  "**T1: { St Ld Mpy Add }**",
			  "**T1: { St Ld Mpy Add }**",
			  "**T1: { St Ld Mpy Add }**",
			  "**T0: { Ld Ld Add Cmp }**",
			  "**T0: { Ld Ld Add Cmp }**",
			  "**T0: { Ld Ld Add Cmp }**",
			  "**T0: { Ld Ld Add Cmp }**",
			  "**T0: { Ld Ld Add Cmp }**",
			  "**Thread 2 Dispatch**",
			  "**Thread 2 Dispatch**",
			  "**T2: { Ld Add Jump }**",
			  "**T2: { Ld Add Jump }**"
			]
		  },
		  {
			"title": "\nBit Twiddling Hacks\n",
			"url": "https://graphics.stanford.edu/~seander/bithacks.html",
			"excerpts": [
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{"
			]
		  },
		  {
			"title": "Hexagon™ V73 HVX",
			"url": "https://docs.qualcomm.com/doc/80-N2040-54/80-N2040-54_REV_AB_Qualcomm_Hexagon_V73_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "HVX packets execute over multiple clock cycles, but typically in a pipelined manner to issue and\ncomplete a packet on every context cycle. The contexts are time interleaved to share the\nhardware such that using all contexts might be required to reach peak compute bandwidth.",
			  "An early source register produced in the previous vector packet can incur an interlock stall.\nSoftware should strive to schedule an intervening packet between the producer and an early\nsource consumer.",
			  "An instruction packet can contain up to four instructions, plus an end loop. The instructions inside\nthe packet must obey the packet grouping rules described in section 5.1.3 .",
			  "*Qualcomm Hexagon V73 HVX Programmer’s Reference Manual*",
			  "VLIW packets cannot oversubscribe resources.",
			  "5.1.1 Double vector instructions",
			  "5.7 Instruction latency",
			  "5.8 Slot/resource/latency summary",
			  "TE:** The assembler should check and flag invalid packet combinations. When an invalid packet\nexecutes, the behavior is undefined.",
			  "ly source\noperands include:\n\nInput to the multiplier. For example V3.h = vmpyh(V2.h, V4.h). V2 and V4 are multiplier\ninputs. For multiply instructions with accumulation, the accumulator is not considered an\nearly source multiplier input.\n\nInput to shift/bit count instructions. Only the shifted or counted register is considered early\nsource. Accumulators are not early sources.\n\nInput to permute instructions. Only permuted registers are considered early source (not an\naccumulator).\n\nUnaligned store data is an early so",
			  "V8 = VADD(V0,V0)\nV0 = VADD(V8,V9)\n// No stall.\nV1 = VMPY(V0,R0)\n// Stall due to V0\nV2 = VSUB(V2,V1)\n// No stall on V1\nV5:4 = VUNPACK(V2) // Stall due to V2\nV2 = VADD(V0,V4)\n// No stall on V4",
			  "HVX supports a local memory called vector TCM (VTCM) for scratch buffers and scatter/gather\noperations.",
			  "The size of the memory is implementation-defined. The size is discoverable from the\nconfiguration table.",
			  "VTCM needs normal virtual to physical translation just like other memory.",
			  "This memory has higher performance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer. It serves as the input or output\nof the scatter/gather instructions.",
			  "The following are advantages of using VTCM as the\nintermediate buffer:\n\nGuarantees no eviction (vs. L2 if the set is full)\n\nFaster than L2$ (does not have the overhead of cache management, like association)\n\nReduces L2$ pressure\n\nLower power than L2$\n\nSupports continuous read and write for every packet without contention",
			  "provides the following resources for vector instruction execution:\n\nload\n\nstore\n\nshift\n\npermute\n\ntwo multiply\nEach HVX instruction consumes some combination of these resources, as defined in section 5.1.2 .\nVLIW packets cannot oversubscribe resources.\nAn instruction packet can contain up to four instructions, plus an end loop. The instructions inside\nthe packet must obey the packet grouping rules described in section 5.1.3 ",
			  "NOTE:** The assembler should check and flag invalid packet combinations. When an invalid packet\nexecutes, the behavior is undefined.",
			  ".9.4 L2FETCH**\nUse the L2FETCH instruction to prepopulate the L2 cache with data prior to using VMEM loads.\nL2FETCH is best performed in sizes less than 8 KB and issued at least several hundred cycles prior\nto using the data. If the L2FETCH instruction is issued too early, data can be evicted before use. In\ngeneral, prefetching and processing on image rows or tiles works best",
			  ".9.5 Access data contiguously**\nWhenever possible, arrange data in memory so that it is accessed contiguously. For example,\ninstead of repeatedly striding through memory, data might be first tiled, striped, or decimated to\n ... \nin L1, followed by scalar load instructions. Execute the DCFETCH anytime after the VMEM store,\nhowever, software should budget at least 30 cycles before issuing the scalar load instructi",
			  ".9.8 Avoid scatter/gather stalls**\nScatter and gather operations compete for memory and can result in long latency, therefore take\ncare to avoid stalls. The following techniques improve performance around scatter and gather:\n\nDistribute accesses across the intra-vector address range (lower address bits). Even\ndistribution across the least significant inter-vector address bits can also be beneficial. For\nV73, address bits [10:3] are important to avoid conflicts. Ideally this applies per vector\ninstruction, but distributing these accesses out between vector instructions can help absorb\nconflicts within a vector instruction.\n\nMinimize the density of scatter and gather instructions. Spread out these instructions in a\nlarger loop rather than concentrating them in a tight loop. The hardware can process a small\nnumber of these instructions in parallel. If it is difficult to spread these instructions out, limit\nbursts to four for a given thread (for ",
			  "Table 5-7 HVX slot/resource/latency summary",
			  " VLIW packing rules**\nHVX provides the following resources for vector instruction execution:\n\nload\n\nstore\n\nshift\n\npermute\n\ntwo multiply\nEach HVX instruction consumes some combination of these resources, as defined in section 5.1.2 .\nVLIW packets cannot oversubscribe resources.\nAn instruction packet can contain up to four instructions, plus an end loop. The instructions inside\nthe packet must obey the packet grouping rules described in sectio",
			  "Peak scatter/gather performance for v73 . ... The HVX GATHER instruction subclass performs gather operations in the vector TCM."
			]
		  },
		  {
			"title": "Hexagon™ V73",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-53_REV_AB_Qualcomm_Hexagon_V73_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Instructions can be grouped into very long instruction word (VLIW) packets for parallel execution,\nwith each packet containing from one to four instructions.",
			  "considered an atomic unit: in essence, a\nsingle large instruction. From the program perspective, a packet either executes to completion or\nnot at all; it never partially executes",
			  "the Hexagon processor has only two load units, a packet with three load\ninstructions is invalid.",
			  "The processor supports up to four parallel instructions. The instructions execute in four parallel\npipelines, which are referred to as slots. The four slots are named Slot 0, Slot 1, Slot 2, and Slot 3.",
			  "Resource constraints determine how many instructions of a specific type can appear in a\npacket. The Hexagon processor has a fixed number of execution units: each instruction\nexecutes on a particular type of unit, and each unit can process at most one instruction at a\ntime.",
			  "Thus, for example, because the Hexagon processor contains only two load units, an\ninstruction packet with three load instructions is invalid.",
			  "The endloopN instructions ( Section 8.2.2 ) do not use any slots.",
			  "Each instruction belongs to specific Instruction classes . For example, jumps belong to instruction\nclass J, while loads belong to instruction class LD. The class of an instruction determines the slot in\nwhich the instruction can execute.",
			  "Packet formation is subject to the\nfollowing constraints:",
			  "A packet cannot use more hardware resources than are physically available on the processor.",
			  "The Hexagon processor has a fixed number of execution units: each instruction\nexecutes on a particular type of unit, and each unit can process at most one instruction at a\ntime.",
			  "The assembler automatically\nrejects packets that oversubscribe the hardware resources.",
			  "The processor supports up to four parallel instructions. The instructions execute in four parallel\npipelines, which are referred to as slots.",
			  "Instructions can be grouped into very long instruction word (VLIW) packets for parallel execution,\nwith each packet containing from one to four instructions. Packets of varying length can be freely\nmixed in a program.",
			  "The processor supports up to four parallel instructions. The instructions execute in four parallel\npipelines, which are referred to as slots. The four slots are named Slot 0, Slot 1, Slot 2, and Slot 3.\n**NOTE:** The endloopN instructions ( Section 8.2.2 ) do not use any slots.\nEach instruction belongs to specific Instruction classes . For example, jumps belong to instruction\nclass J, while loads belong to instruction class LD. The class of an instruction determines the slot in\nwhich the instruction can execute.",
			  "\nPackets have restrictions on the allowable instruction combinations. The instruction class of the\ninstructions in a packet determines the primary restriction. Packet formation is subject to the\nfollowing constraints:\n\nResource constraints determine how many instructions of a specific type can appear in a\npacket. The Hexagon processor has a fixed number of execution units: each instruction\nexecutes on a particular type of unit, and each unit can process at most one instruction at a\ntime. Thus, for example, because the Hexagon processor contains only two load units, an\ninstruction packet with three load instructions is invalid.\n",
			  "L2FETCH is nonblocking. After the instruction is initiated, the program continues to the next\ninstruction while the prefetching is performed in the background.",
			  " Table 8-2 , the kernel section of the pipelined loop performs three iterations of the loop in\nparallel:\n\nThe load for iteration N+2\n\nThe multiply for iteration N+1\n\nThe store for iteration N\nOne drawback to software pipelining is the extra code necessary for the prologue and epilogue\nsections of a pipelined loop.\nTo address this issue, the Hexagon processor provides the spNloop0 instruction, where the “ N ” in\nthe instruction name indicates a digit in the range 1 to 3. For example:\nP3 = sp2loop0(start,) // Set up pipelined loop\nThe spNloop0 instruction is a variant of the loop0 instruction: it sets up a normal hardware loop\nusing SA0 and LC0, but also performs the following additional operations:\n\nWhen the spNloop0 instruction executes, it assigns the truth value false to the predicate\nregister P3.\n\nAfter the associated loop executes N times, P3 is automatically set to true.\n**Table 8-2 Software pipelined loop**\nint foo(int *A, int *result)\n{\nint i;\nfor (i=0;i<100;i++) {\nresult[i]= A[i]*A[i];\n}\n}\nfoo:\n{ R3 = R1\nloop0(.kernel,) // Decrease loop count by 2\n}\nR1 = memw(R0++) // First prologue stage\n{ R1 = memw(R0++) // Second prologue stage\nR2 = mpyi(R1,R1)\n}\n.falign\n.kernel:\n{ R1 = memw(R0++) // Kernel\nR2 = mpyi(R1,R1)\nmemw(R3++) = R2\n}:endloop0\n{ R2 = mpyi(R1,R1) // First epilogue stage\nmemw(R3++) = R2\n}\nmemw(R3++) = R2 // Second epilogue stage\njumpr",
			  "When data cache prefetching is enabled, the Hexagon processor observes patterns of data cache\nmisses and attempts to predict future misses based on recurring patterns of misses where the\naddresses are separated by a constant stride. If such patterns are found, the processor attempts\nto automatically prefetch future cache lines.",
			  "Static grouping (VLIW) architecture"
			]
		  },
		  {
			"title": "Hexagon™ V69",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-50_REV_AA_Qualcomm_Hexagon_V69_ProgrammerS_Reference_Manual.pdf",
			"excerpts": [
			  "packet formation is subject to the\nfollowing constraints:",
			  "\nResource constraints determine how many instructions of a specific type can appear in a\npacket. T",
			  "e the Hexagon processor contains only two load units, an\ninstruction packet with three load instructions is invalid. ",
			  "The resource constraints are\ndescribed in Section 3.3.3",
			  "\n■\nGrouping constraints are a small set of rules that apply above and beyond the resource\nconstraints.",
			  "\n■\nDependency constraints ensure that no write-after-write hazards exist in a packet.",
			  "These\nrules are described in Section 3.3.5 .",
			  "\n■\nOrdering constraints dictate the ordering of instructions within a packet.",
			  "These rules are\ndescribed in Section 3.3.6 .",
			  "\nAlignment constraints dictate the placement of packets in memory. T",
			  "These rules are described in Section 3.3.4 .",
			  "**NOTE:** The Hexagon processor executes Individual instructions (which are not explicitly grouped in\npackets) as packets containing a single instruction.",
			  "3.3.1 Packet execution semantics",
			  "Static grouping (VLIW) architecture",
			  "The Hexagon processor has two sets of registers: general registers and control registers. The general registers include thirty-two 32-bit ...Read more",
			  "The processor supports up to four parallel instructions. The instructions execute in four parallel\npipelines, which are referred to as slots.The four slots are named slot 0, slot 1, slot 2, and slot 3.",
			  "TE:**\nendloopN instructions ( Section 8.2.2 ) do not use any slots.",
			  "Nestable zero-overhead hardware loops are supported.",
			  "spNloop0 is a variant of the loop0 instruction: it sets up a normal hardware loop using SA0 and\nLC0 , but also performs the following additional operations:",
			  "The l2fetch instruction is non-blocking: it initiates a prefetch operation that is performed in the\nbackground by the prefetch engine while the thread continues to execute Hexagon processor\ninstructions.",
			  "he two load/store units can operate on signed or unsigned bytes, halfwords (16-bit), words (32-\nbit), or double words (64-bit).",
			  "To increase the number of instruction combinations allowed in packets, the load units also\nsupport 32-bit ALU instructions."
			]
		  },
		  {
			"title": "Predication and Speculation",
			"url": "https://www.cs.utexas.edu/~lin/cs380c/handout25.pdf",
			"excerpts": [
			  "**VLIW: Very Long Instruction Word**",
			  " Each instruction controls multiple functional units\n",
			  "– Each instruction is explicitly parallel",
			  "Predication and Speculation",
			  "**VLIW and IA-64**",
			  "Explicitly Parallel Instruction Sets: IA-64",
			  "Itanium was the first IA64 implementation",
			  "**IA-64 Design Philosophy**",
			  "**IA-64 Design Philosophy**",
			  "**IA-64 Design Philosophy**",
			  "EPIC: Explicitly Parallel Instruction Computer",
			  "EPIC: Explicitly Parallel Instruction Computer",
			  "5\nPredication and Speculation",
			  "VLIW: Very Long Instruction Word"
			]
		  },
		  {
			"title": "C6000 Optimizing Compiler",
			"url": "https://www.yumpu.com/en/document/view/39224506/c6000-optimizing-compiler",
			"excerpts": [
			  "code, describes the structure of assembly code and includes examples and discusses optimizations<br />. for the assembly code, and describes ..."
			]
		  },
		  {
			"title": "ASIP Performance Enhancement by Hazard Control through Scoreboard",
			"url": "https://www.mdpi.com/2072-666X/15/11/1287",
			"excerpts": [
			  "There is interlock stall logic for Hexagon HVX when an early source register was produced in the previous vector packet, but they prefer to ...Read more"
			]
		  },
		  {
			"title": "Predicate Prediction for Efficient Out-of-order Execution",
			"url": "https://cseweb.ucsd.edu/~calder/papers/ICS-03-PP.pdf",
			"excerpts": [
			  "They were all compiled with a version of Intel's IA64. Compiler (Electron) modified for our environment. Some en- hancements we have made are binary ...Read more"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "HVX provides six resources for vector instruction execution:",
			  "load",
			  "store",
			  "shift",
			  "permute",
			  "two multiply",
			  "Each HVX instruction consumes some combination of these resources, as defined in",
			  "section 4.1.2 . VLIW packets cannot oversubscribe resources.",
			  "An instruction packet can contain up to four instructions, plus an endloop. The instructions",
			  "inside the packet must obey the packet grouping rules described in section 4.1.3 .",
			  "**NOTE**",
			  "Invalid packet combinations should be checked and flagged by the assembler.",
			  "In the case that an invalid packet is executed, the behavior is undefined.",
			  "HVX packets execute over multiple clock cycles, but typically in a pipelined manner so",
			  "that a packet can be issued and completed on every context cycle. The contexts are time",
			  "interleaved to share the hardware such that using all contexts may be required to reach",
			  "peak compute bandwidth.",
			  "With a few exceptions (i.e., histogram and extract), all results of all packets are generated",
			  "within a fixed time after execution starts. But, when the sources are required varies.",
			  "Instructions that need more pipelining require early sources. Only HVX registers are Early",
			  "Source registers. Early source operands include:",
			  "Input to the multiplier. For example “V3.h = vmpyh(V2.h, V4.h)”. V2 and V4 are",
			  "multiplier inputs. For multiply instructions with accumulation, the accumulator is",
			  "not considered an early source multiplier input.",
			  "Input to shift/bit count instructions. Only the register that is being shifted or",
			  "counted is considered early source. Accumulators are not early sources.",
			  "Input to permute instructions. Only registers that are being permuted are",
			  "considered early source (not an acccumulator).",
			  "Unaligned store data is an early source.",
			  "If an early source register is produced in the previous vector packet, an interlock stall can",
			  "be incurred. Software should strive to schedule an intervening packet between the",
			  "producer and an early source consumer.",
			  "Vector Instructions",
			  "The following example shows various interlock cases:",
			  "V8 = VADD(V0,V0)",
			  "V0 = VADD(V8,V9)",
			  "// NO STALL",
			  "V1 = VMPY(V0,R0)",
			  "// STALL due to V0",
			  "V2 = VSUB(V2,V1)",
			  "// NO STALL on V1",
			  "V5:4 = VUNPACK(V2) // STALL due to V2",
			  "V2 = VADD(V0,V4)",
			  "// NO STALL on V4",
			  "**4.7**",
			  "**Slot/resource/latency summary**",
			  "Table 4-6 summarizes the Hexagon slot, HVX resource, and latency requirements for all",
			  "HVX instruction types.",
			  "**Table 4-6**",
			  "**HVX slot/resource/latency summary**",
			  "**Category**",
			  "**Variation**",
			  "**3**",
			  "**2**",
			  "**1**",
			  "**0**",
			  "**ld**",
			  "**mpy mpy shift xlane**",
			  "**st**",
			  "**Rt**",
			  "**by 8b; 1 vec**",
			  "**by 8b; 2 vec**",
			  "**by 16b**",
			  "**Shift or count 1 vec**",
			  "**aligned; .tmp**",
			  "**aligned; .cur**",
			  "-",
			  "-",
			  "-",
			  "-",
			  "**aligned**",
			  "**aligned**",
			  "**aligned; .new**",
			  "0",
			  "**unaligned**",
			  "**unaligned**",
			  "**1 vec**",
			  "**1 vec**",
			  "**1 vec**",
			  "**1 vec**",
			  "**1 vec**",
			  "**2 vec**",
			  "**2 vec**",
			  "**2 vec**",
			  "**2 vec**",
			  "**2 vec**",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "**extract**",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "**load**",
			  "**Core slots**",
			  "**Vector resources**",
			  "either pair",
			  "**Cross-lane**",
			  "**Input**",
			  "**Latency**",
			  "**ALU**",
			  "**Abs-diff**",
			  "**Multiply**",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "either",
			  "**store**",
			  "**histogram (needs .tmp load)**",
			  "**gather (needs**",
			  "**.new store)**",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any",
			  "any"
			]
		  },
		  {
			"title": "Hexagon V60 HVX Programmer's Reference Manual",
			"url": "https://docs.qualcomm.com/doc/80-N2040-30/80-N2040-30_REV_C_Hexagon_V60_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "HVX coprocessor instructions execute over multiple clock cycles. Instructions complete\nin either 2 or 4 clock cycles.",
			  "A new instruction packet from a thread can be issued every 2\nclock cycles.",
			  "he HVX coprocessor provides six resources for vector instruction execution:\n■\nLoad\n■\nStore\n■\nShift\n■\nPermute\n■\nMul",
			  "VLIW packets cannot over-subscribe resources.",
			  "An instruction packet can contain up to four instructions, plus an endloop. The instructions\ninside the packet must obey the packet grouping rules described in Section 4.2.3 .",
			  "Certain instructions require Early Sources. Early source registers include:\n■\nInput to the multiplier. For example “ V3.h = vmpyh(V2.h, V4.h) ”. Here V2\nand V4 are multiplier inputs. For multiply instructions with accumulation, the\naccumulator is not considered an Early Source multiplier input.\n■\nInput to Shift/Bit Count instructions. For shifts, all vector sources are Early\nSource except for accumulators.\n■\nInput to Permute instructions. Only registers that are being permuted are\nconsidered Early Source (not Accumulator).\n■\nUnaligned Store Data is an Early Source.\nIf an Early Source register is produced in the previous vector packet, an interlock stall will\noccur. The software should try to schedule an intervening packet between the producer of\nan Early Source register. For example, the following shows various interlock cases:\nV8 = VADD(V0,V0)\nV0 = VADD(V8,V9) // NO STALL\nV1 = VMPY(V0,R0) // STALL due to V0\nV2 = VSUB(V2,V1) // NO STALL on V1\nV5:4 = VUNPACK(V2) // STALL due to V2\nV2 = VADD(V0,V4) // NO ST",
			  "Hexagon V60 HVX Programmer’s Reference Manual",
			  "VLIW packing rules",
			  "n instruction packet can contain up to four instructions, plus an endloop. The instructions\ninside the packet must obey the packet grouping rules described in Section 4.2.3 ."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531701.pdf",
			"excerpts": [
			  "The IA-64 instruction set is designed to allow the compiler to communicate information to the\nprocessor to manage resource characteristics such as instruction latency, issue width, and functional\nunit assignment. Although such resources can be statically scheduled, IA-64 does not require that\ncode be written for a specific microarchitecture implementation in order to be functional.",
			  "Branches limit the ability of the compiler to move instructions that alter memory state or that can\nraise exceptions, because instructions in a program are control dependent on all lexically enclosing\nbranches. In addition to the control dependencies, compound conditionals can take several cycles to\ncompute and may themselves require intermediate branches in languages like C that require\nshort-circuit evaluation.",
			  "Control speculation is the primary mechanism used to perform global code motion for IA-64\ncompilers.",
			  "Predication in IA-64",
			  "Now that the performance implications of branching have been described, this section overviews\npredication – the primary IA-64 mechanism used by optimizations described in this section.",
			  "Almost all IA-64 instructions can be tagged with a guarding predicate. If the value of the guarding\npredicate is false at execution time, then the predicated instruction’s architectural updates are",
			  "IA-64 provides extensive support for software-pipelined loops, including register rotation, special\nloop branches, and application registers. When combined with predication and support for\nspeculation, these features help to reduce code expansion, path length, and branch mispredictions\nfor loops that can be software pipelined.",
			  "The beginning of this chapter reviews basic loop terminology and instructions, and describes the\nproblems that arise when optimizing loops in the absence of architectural support. The IA-64\nspecific loop support features are then introduced. The remainder of this chapter describes the\nprogramming and optimization of various type of loops using the IA-64 features."
			]
		  },
		  {
			"title": "Itanium™ Processor Microarchitecture Reference",
			"url": "https://parallel.ru/sites/default/files/ftp/computers/intel/ia64/itanium_archreference.pdf",
			"excerpts": [
			  "The Itanium processor issues and executes instructions in software-supplied order, so compiler\nunderstanding of stall conditions is essential for generating efficient assembly code.",
			  "When a split issue condition occurs, all\ninstructions after the split point stall one or more clocks, even if there are sufficient resources for\nsome of them to execute.",
			  "Split issue refers to a stall of this sort even though the stall could be\ncaused by scoreboarding, instruction alignment, functional unit oversubscription, or other\npipeline-related stalls.",
			  "the dispersal window on the\nItanium processor is exactly two bundles wide and only moves in whole-bundle increments, any\ninstruction group that spans three or more bundles is guaranteed to split issue at the end of the\nsecond bundle even if no resources are oversubscribed."
			]
		  },
		  {
			"title": "Intel® Itanium™ Processor Reference Manual for Software ...",
			"url": "https://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-refsoftopt-24547402.pdf",
			"excerpts": [
			  "The example below shows an instruction group that spans more than two bundles. The effective\nissue times are to the right of each instruction:\nL:\n{ .mii\nadd r1=r2,r3 // 0\nadd r4=r5,r6 ;;// 0\nsub r7=r8,r9 // 1\n}\n{ .mfi\nld4 r14=[r56]// 1\nfadd f10=f12,f13// 1\nadd r16=r18,r19 // 1: Split issue occurs\n// after this instr\n}\n{ .mmi\nst4 [r16]=r67 ;; // 2\nadd r24=r56,r57 // 3\nadd r28=r58,r59 // 3\n}\nIn this sequence, the Itanium processor has sufficient functional units to execute the instruction\ngroup starting at the sub instruction and ending with the st4 instruction in one cycle; however,\nexecution will split issue at the end of the second bundle since the instruction group extends\nbeyond the dispersal window.",
			  "\nAn instruction *I* scheduled in clock *i* has a total latency of *N* cycles to instruction *J* if *J* can be\nscheduled no sooner than cycle *i+N* to avoid stalls due to latency. There are two components that\ndetermine the total latency of pairs of instructions:\n• The latency of the producing instruction.\n• The time to bypass the result to the dependent operation.\n",
			  "In the presence of predication, the actual dependences between non-unit latency instructions are\nsometimes determined at execution time rather than at compile time. *Predication does not affect*\n*bypass latency of unit latency instructions.* The discussion below provides a high-level of detail\nabout the effect of predication on bypassing, but the overall effect on performance due to these\ncases is generally very small.",
			  "On the Itanium processor, the ALAT has 32 entries and is two-way set associative based on the\nphysical target registers of advanced loads.",
			  "A ld.c that misses in the ALAT causes a 10-cycle pipeline flush.",
			  "A chk.a that misses in the ALAT\nexecutes a branch to recovery code.",
			  "Thus, the cost of a chk.a that misses\nin the ALAT is likely to be the cost of two branch mispredicts, plus the cost of the recovery code,\nplus the cost of several control register reads, plus the cost of the return — approximately 50+\ncycles plus the cost of recovery code.",
			  "The combined affects of limited associativity and partial address matching mean that if a large\nnumber of stores and/or advanced loads occur, entries in the ALAT may be invalidated without\never having an exact memory or register conflict.",
			  "In addition to the physical register number, ALAT\nentries are tagged with wrap bits that distinguish different instances of the same physical register\ndue to RSE activity.",
			  "n the example above, the third load will conflict with one of the entries allocated by the previous\ntwo advanced loads given the size and structure of the Itanium processor ’ s ALAT",
			  "Finally, the use of unaligned advanced loads may cause more ALAT entries than strictly necessary\nto be purged or for entries not be allocated in the first place. Thus, the use of the ALAT with non-\nnaturally aligned loads is not recommended on the Itanium processor.",
			  "**Data Speculation, Advanced Loads, and the ALAT**\nThe family of instructions composed of ld.a , ld.c , and chk.a provide the capability to\ndynamically disambiguate memory addresses between loads and stores. Architecturally, the ld.c\nand chk.a instructions have a 0 cycle latency to consuming instructions. However, if a ld.c or\nchk.a misses in the ALAT, additional latency is incurred.",
			  ".\nL1 Data Cache\nRegister File\nL2 Cache\nL3 Cache\nMemory Bus\n16 bytes/clk\n2.1 GB/sec\n~8 bytes/clk sustained int\nL1 Instr Cache\n~16 bytes/clk sustained FP\n32 bytes/clk\nInteger data 2 x 8 bytes/clk\nFP data 2 x 16 bytes/clk\n2 x 8 bytes/clk\n*Intel* *®* *Itanium* *™* *Processor Reference Manual for Software Optimization*\n1",
			  "On the Itanium processor, the ALAT has 32 entries and is two-way set associative based on the\n ... \nmatching means that stores that do not physically conflict could still be aliased to entries in the\nALAT and invalidate them if their low 20 bits match. In situations where a given store is close to a\nsubsequent chk.a or ld.c , fewer than 20 bits are used to determine if a memory conflict occurred:",
			  "The combined affects of limited associativity and partial address matching mean that if a large\nnumber of stores and/or advanced loads occur, entries in the ALAT may be invalidated without\never having an exact memory or register conflict. These characteristics imply that one must\ncarefully consider the number of dynamic stores past which a load can usefully be advanced.",
			  "**5.9**\n**Control Speculation**\nAlmost all Itanium instructions are speculative by default. Loads have speculative and\nnon-speculative versions. When a compiler uses control speculation in the Itanium architecture, it\nis responsible for inserting chk.s instructions to check if recovery code needs to be executed.",
			  "**Conflict Detection**\n**Algorithm**\n**Description of Conditions**\nAlways reports a\nconflict\nA store followed by a chk.a in the same cycle\nA ld.a and a ld.c in the same clock",
			  "anium* *™* *Processor Reference Manual for Software Optimization"
			]
		  },
		  {
			"title": "Writing Efficient Itanium 2 Assembly Code",
			"url": "https://yurichev.com/mirrors/RE/itanium.pdf",
			"excerpts": [
			  "d.\nThe secondary, and more practical, motivation for this that since writing\nefficient code for the Itanium 2 depends so severely on having good compilers\nand since no good compilers have ever been written, it should be expected to\nwrite assembly code.",
			  "ed.\nThe assembler will automatically package in-\nstructions into bundles, but it will not do any reordering either. If three load\ninstructions are given in sequence, for instance, the assembler will na¨ıvely break\nthem across two bundles, putting two in the first bundle with a no-op and the\nremaining in the second bundle. This is not efficient and, consequently, it is\nimperative that the programmer be aware of how instructions will be bundling\nso that he or she can manually reorder instructions.\nThe",
			  "\nA comma in a bundle template is a “stop”, an explicit serialization of execution\nof instructions.",
			  "\nEvery sequence of instructions must be packaged into bundles and executed\nas bundles. The performance of IA-64 assembly code is directly dependent on\nhow the instructions are bundled: since Itanium processors contain no out-of-\norder execution or implicit scheduling, the instructions are issue and executed\nexactly as they are packaged.\nTh"
			]
		  },
		  {
			"title": "C6000 DSP Software Pipelining Guide | PDF | Control Flow",
			"url": "https://www.scribd.com/document/651380437/GE42-DSP-C6000-SW-pipeline-23-1-1",
			"excerpts": [
			  "The document discusses software pipelining to optimize loop code by implementing parallel instructions, filling delay slots, enabling word-wide reads to ...Read more"
			]
		  },
		  {
			"title": "(PDF) Software de-pipelining technique",
			"url": "https://www.researchgate.net/publication/4120068_Software_de-pipelining_technique",
			"excerpts": [
			  "Software de-pipelining is the reverse of software pipelining; it restores the assembly code of a software-pipelined loop back to its semantically equivalent ...Read more"
			]
		  },
		  {
			"title": "comp.dsp | Texas, Analog, Motorola and others DSPs",
			"url": "https://www.dsprelated.com/showthread/comp.dsp/42890-1.php",
			"excerpts": [
			  "Most people consider TI assembly very difficult.",
			  "One of our TI experts\nsays he can teach someone TI assembly in 6 months.",
			  "Not exactly.The truth is that the TI's C6000 assembly - like most RISC assemblers - is\nvery simple and straight forward.The arhictecture is VLIW and hence\nload/store based;Hardly 60 instructions and that too highly orthognal and\nregular.",
			  "having said that, with TI's C6000 DSP's you would hardly every be needed\nto write stuff in assembly(if ever, less than 10% of your code)",
			  "It is my experience that the tools will achieve upto 80% efficiency of\nhand coded assembly; An expert tool user might be able to take this figure\nclose to 90%. (Mind you, all this without having to program anything in\nassembly)",
			  "What I indeed said was that if you wanted to achieve that extra 10% or if\nyou wanted to *know* for your own sake how the tools do software\npipelining technique, then it requires quite a bit of time and skill. It\ncan sometimes take about 6 months to achieve this level of skill. But only\nvery projects and only few engineers would need to gear up to this level,\nsimply because the tools are fantastic.",
			  "I will promise that if you can do that the results would be fantastic bot\nfrom the point of view of your own career (VLIW *IS* the future) and the\nproject.",
			  "This message was sent using the Comp.DSP web interface on\nwww.DSPRelated.com"
			]
		  },
		  {
			"title": "understanding software pipeline information - c6x",
			"url": "https://www.dsprelated.com/showthread/c6x/11435-1.php",
			"excerpts": [
			  "|11| is the source line number.",
			  "<0,9> means...",
			  "\"It [compiler] attaches a 2-tuple to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the\nsoftware pipeline. The zero-based number x\nrepresents the iteration the instruction is on during the first\nexecution of the loop kernel. The zero-based\nnumber y represents the cycle that the instruction is scheduled on\nwithin a single iteration of the loop.\"",
			  "Have you looked at the software pipeline info in the compiler manual\n[spru187] and the C6000 programmer's guide [spru198]??"
			]
		  },
		  {
			"title": "Lecture 22 Prefetching Recursive Data Structures",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s16/www/lectures/L22-Prefetching-Pointer-Structures.pdf",
			"excerpts": [
			  "History-Pointer Prefetching. •. Add new pointer(s) to each node. – history-pointers are obtained from some recent traversal. •. Trade space & time for better ...Read more",
			  "Compiler-Based Prefetching for Recursive Data\nStructures"
			]
		  },
		  {
			"title": "Informed Prefetching for Indirect Memory Accesses1",
			"url": "https://www.ele.uri.edu/faculty/sendag/papers/taco2019.pdf",
			"excerpts": [
			  "ABSTRACT. Indirect memory accesses have irregular access patterns which limit the performance of conventional software and hardware-based prefetchers.Read more"
			]
		  },
		  {
			"title": "Prodigy: Improving the Memory Latency of Data-Indirect ...",
			"url": "http://tnm.engin.umich.edu/wp-content/uploads/sites/353/2021/01/2021.02.Prodigy_HPCA2021_Camera_Ready.pdf",
			"excerpts": [
			  "propose a\ncompact representation called the **Data Indirection Graph**\n**(DIG)** to communicate workload attributes from software to\nthe hardware. The DIG representation efficiently encodes the\nprogram semantics, *i.e.,* the layout and access patterns of key\ndata structures, in a weighted directed graph structure",
			  "*Significance of ranged indirection:** For graph algorithms,\nranged indirection is responsible for prefetching 35.4–75.9%\n(55.3% on average) of all data (not shown because of space\nlimitation).",
			  " show that by using just 0.8KB of**\n**storage, Prodigy outperforms a non-prefetching baseline by 2.6",
			  "pattern-specific solution that targets *two* types of\ndata-dependent indirect memory accesses, which we call **single-**\n**valued indirection** and **ranged indirection** . Single-valued\nindirection uses data from one data structure to index into\nanother data structure; it is commonly used to find vertex\nproperties in graph algorithms. Ranged indirection uses two\nvalues from one data structure as base and bounds to index into\na series of elements in another data structure; this technique\nis commonly used to find neighbors of a vertex in gra",
			  "**core of the system is the** ***Data Indirection Graph (DIG)*** **—a pro-**\n**posed compact representation used to express program semantics**\n**such as the layout and memory access patterns of key data**\n**structures. T",
			  "Prodigy achieves an average speedup of 2.3 *×* compared to\nIMP 5 [99], because IMP can only detect streaming accesses\nto data structures that perform A[B[i]] type prefetching and\nit only supports up to two levels of indirectio",
			  "Prodigy is more flexible in that it can adapt\nwith different combinations of data structures and indirection\npatterns, whereas Ainsworth and Jones’ graph prefetcher aims\nto prefetch for BFS-like access patterns. ",
			  "We design a **low-cost hardware prefetcher** that can be\n*programmed using the DIG representation* communicated from\nsoftware. We store the DIG in prefetcher-local memory to make\ninformed prefetching choices.",
			  "Prodigy vs. hardware prefetchers:** Next we compare the\nperformance of Prodigy with the state-of-the-art hardware\nprefetchers including GHB-based G/DC data prefetcher [72],\nAinsworth and Jones’ prefetcher [6], DROPLET [15], and\nIMP [99]. N",
			  "Figure 2. Reduction in ((a)) memory stalls and ((b)) speedup of different\napproaches normalized to a non-prefetching baseline for the PageRank\nalgorithm on the livejournal data set.",
			  "Figure 17. Performance comparison of a non-prefetching baseline, Ainsworth\nand Jones’ prefetcher [6], DROPLET [15], IMP [99], and Prodigy (this\nwork). Higher is better.",
			  " Ranged indirection uses two\nvalues from one data structure as base and bounds to index into\na series of elements in another data structure; this technique\nis commonly used to find neighbors of a vertex in graph\nalgorithms. "
			]
		  },
		  {
			"title": "DX100: Programmable Data Access Accelerator for Indirection | Proceedings of the 52nd Annual International Symposium on Computer Architecture",
			"url": "https://dl.acm.org/doi/10.1145/3695053.3731015",
			"excerpts": [
			  "etcher Units** , inspired by Decoupled Access-Execute paradigm [ [115]() ], aim to improve performance by separating and offloading programs into independent access and execute cores. This decoupling allows memory accesses and computations to be processed concurrently on dedicated processors, hiding memory latency. Fetcher units serve as the access core to perform specific memory accesses and feed the data to the processor to avoid recomputation. Specialized fetchers have been proposed for various domains. HATS [ [81]() ] performs locality-aware graph traversals to reduce data movement. SPU [ [28]() ], TMU [ [114]() ] and MAPLE [ [93]() ] can support traversal of sparse tensors and processing of sparse linear algebra. Widx [ [58]() ] accelerates hash indexing operations. SQRL [ [60]() ] handles traversals of hash tables and tree data structures. General-purpose fetcher units such as SpZip [ [130]() ] and Terminus [ [65]() ] support a wider range of applications and exploit fine-grained pipelining to hide the latency of indirect memory access chains. SpZip incorporates data compression to reduce memory traffic, while Terminus supports fine-grained updates for recursive data structures like trees and linked lists. However, fetchers provide insufficient visibility into future memory accesses, which hinders effective memory access reordering and coalescing",
			  "In summary, this paper offers the following contributions:\n•\nWe propose DX100, a programmable data access accelerator that supports offloading indirect and streaming memory accesses. DX100 enhances memory bandwidth utilization by optimizing *bulk* accesses: it *reorders* to improve DRAM row-buffer hits, *coalesces* to reduce accesses, and *interleaves* to enhance DRAM channel and bank-group parallelism.",
			  "DX100, a programmable data access accelerator for indirect memory accesses. DX100 is shared across cores to offload bulk indirect memory accesses and associated address calculation operations. DX100 *reorders* , *interleaves* , and *coalesces* memory requests to improve DRAM row-buffer hit rate and memory bandwidth utilization. ",
			  "DX100 provides a general-purpose ISA to support diverse access types, loop patterns, conditional accesses, and address calculations. To support this accelerator without significant programming efforts, we discuss a set of MLIR compiler passes that automatically transform legacy code to utilize DX100.",
			  "Similar to DX100, Fetcher units [ [65]() , [130]() ] and Decoupled Access Execute (DAE) architectures [ [115]() ] hoist memory access instruction streams and offload them to tightly-coupled access coprocessors, improving the core structural limitations.",
			  "DX100’s architecture and ISA are carefully designed to support various bulk data access patterns, such as multiple degrees of indirection, diverse sets of loop patterns, loop conditions, data access types (load, store, RMW), and complex address calculation operations.",
			  "Fetcher Units** , inspired by Decoupled Access-Execute paradigm [ [115]() ], aim to improve performance by separating and offloading programs into independent access and execute cores. This decoupling allows memory accesses and computations to be processed concurrently on dedicated processors, hiding memory latenc",
			  "4. SQRL: Hardware accelerator for collecting software data structures. ",
			  "DX100 achieves performance improvements of 2.6 × over a multicore baseline and 2.0 × over the state-of-the-art indirect prefetcher.",
			  "reorders* , *interleaves* , and *coalesces* memory requests to improve DRAM row-buffer hit rate and memory bandwidth utilization"
			]
		  },
		  {
			"title": "Prodigy: Improving the Memory Latency of Data-Indirect ...",
			"url": "https://safari.ethz.ch/architecture_seminar/spring2021/lib/exe/fetch.php?media=prodigy.pdf",
			"excerpts": [
			  "Mechanism\n•\nSW: encode indirect access patterns into **Data Indirection Graph** (DIG)\n•\nHW: prefetcher **traverses DIG** at runtime",
			  "Data-indirect irregular workloads are **bottlenecked by the memory system**\n•",
			  "Key Idea\n•\nMost irregular access patterns are composed of **two specific patterns** :\n**single-valued** and **ranged indirection**",
			  "\nResults\n•\n**2.6× speedup** , **1.6× energy savings** over no prefetch at **negligible cost**\n•\nNotable speedup, savings over existing prefetchers\n",
			  "e\nResults\n•\n**2.6× speedup** , **1.6× energy savings** over no prefetch at **negligible cost**",
			  "Overview\n• Background & Motivation\n• Programming Model\n• Hardware Design\n• Results\n• Conclusion",
			  "2.6× speedup",
			  "•\nHW: prefetcher **traverses DIG** at runtime",
			  "Goal\n•\nA **general, effective, low-cost prefetcher** for data-indirect workloads",
			  "1\n[1] S. Ainsworth and T. M. Jones: “Graph Prefetching Using Data Structure Knowledge”, ICS 201",
			  "W: encode indirect access patterns into **Data Indirection Graph** (DIG",
			  "Prodigy – HPCA 2021",
			  "Prodigy – HPCA 2021",
			  "Prodigy – HPCA 2021",
			  "Common prefetchers **fail to accelerate** indirect memory accesses",
			  "Specialized prefetchers not **general, performant, or timely** enough",
			  "Weaknesses\n• **Limited Novelty** : similar indirection, workload prefetch approaches in prior work [1-4..]"
			]
		  },
		  {
			"title": "IMP: Indirect Memory Prefetcher",
			"url": "https://pages.cs.wisc.edu/~yxy/pubs/imp.pdf",
			"excerpts": [
			  "\nIn this section, we present the *Indirect Memory Pre-*\n*fetcher* (IMP) that captures indirect memory accesses.\n**3.1**\n**Key Idea**\nIndirect memory accesses in the applications we study\ninvolve two data structures: an index array *B* and a\n ... ",
			  ".\nIndirect memory accesses are the main performance\nbottleneck in most of these applications.\nThis is ex-\npected since they generate most of the misses, and these\nmisses are not captured by streaming prefetchers.\n",
			  "valuated on 7 applications, IMP shows 56% speedup\non average (up to 2 *.* 3 *×* ) compared to a baseline 64 core\nsystem with streaming prefetchers. This is within 23%\nof an idealized system",
			  "aluated on 7 applications, IMP shows 56% speedup\non average (up to 2 *.* 3 *×* ) compared to a baseline 64 core\nsystem with streaming prefetchers. This is within 23%\nof an idealized system. With partial cacheline accessing,\nwe see another 9.4% speedup on average (up to 46.6",
			  ".\nWe propose an\nefficient hardware indirect memory prefetcher (IMP)\nto capture this access pattern and hide latency.\nWe\nalso propose a partial cacheline accessing mechanism\nfor these prefetches to reduce the network and DRAM\nbandwidth pressure from the lack of spatial locality.\nEv",
			  "le 3 shows the statistics at 64 cores. Relative to\njust a streaming prefetcher, IMP significantly improves\nprefetch coverage. Stream prefetching alone has an av-\nerage coverage of 28%, while with IMP coverage is im-\nproved to 86%",
			  "This is ex-\npected since they generate most of the misses, and these\nmisses are not captured by streaming prefetchers.",
			  " majority of these irregular accesses come from in-\ndirect patterns of the form *A* [ *B* [ ",
			  "applications\nwith large trip counts ( *Pagerank* and *SpMV* ), IMP is\nvery timely, achieving memory latency close to *Perfect*\n*Prefetchi"
			]
		  },
		  {
			"title": "IMP | Proceedings of the 48th International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2830772.2830807",
			"excerpts": [
			  "A majority of these irregular accesses come from indirect patterns of the form *A* [ *B* [ *i* ]]. We propose an efficient hardware indirect memory prefetcher (IMP) to capture this access pattern and hide latency. We also propose a partial cacheline accessing mechanism for these prefetches to reduce the network and DRAM bandwidth pressure from the lack of spatial locality.",
			  "IMP shows 56% speedup on average (up to 2.3×) compared to a baseline 64 core system with streaming prefetchers. This is within 23% of an idealized system. With partial cacheline accessing, we see another 9.4% speedup on average (up to 46.6%)."
			]
		  },
		  {
			"title": "Software Prefetching for Software Pipelined Loops",
			"url": "https://core.ac.uk/download/87651180.pdf",
			"excerpts": [
			  "\nSoftware prefetching is an effective technique to toler-\nate memory latency. When it is used with a nonblocking\ncache, this technique allows the processor to hide part or all\nthe memory latency by overlapping the fetch of data and\nthe computation.",
			  "The CSMS algorithm is based on early scheduling of\nsome selectively chosen memory operations. Scheduling a\n ... \nlatency of some memory operations (following a locality\norder) until this recurrence minimizes the *II* . An upper\nbound in the number of iterations of the loop help us to\nchoose between the scheduling of both graphs.",
			  "In this paper we investigate the interactions between\nsoftware pipelining and software prefetching in a VLIW\narchitecture. Some alternatives to perform software\nprefetching are described, and a novel heuristic is pre-\nsented.",
			  "5. Conclusions**\nIn this paper we have compared the effect that some\nsoftware prefetching techniques have in software pipelined\nloops for VLIW architectures. We have seen that the pro-\nposed CSMS scheme significally outperforms previous pro-\nposals."
			]
		  },
		  {
			"title": "Software prefetching for software pipelined loops",
			"url": "http://ieeexplore.ieee.org/iel4/5217/14150/00649285.pdf",
			"excerpts": [
			  "The paper investigates the interaction between software pipelining and different software prefetching techniques for VLIW machines.",
			  "It is shown that processor stalls due to memory dependencies have a great impact into execution time.",
			  "A novel heuristic is proposed and it is show to outperform previous proposals."
			]
		  },
		  {
			"title": "Software Data Prefetching for Software Pipelined Loops - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/pii/S0743731599915538",
			"excerpts": [
			  "This paper focuses on the interaction between software prefetching (both binding and nonbinding prefetch) and software pipelining for statically scheduled machines.",
			  "First, it is shown that evaluating software pipelined schedules without considering memory effects can be rather inaccurate due to stalls caused by dependences with memory instructions (even if a lockup-free cache is considered).",
			  "Second, we show that, in general, binding schemes are more powerful than nonbinding ones for software pipelined schedules.",
			  "Finally, the main contribution of this paper is an heuristic scheme that schedules some memory operations according to the locality estimated at compile time and other attributes of the dependence graph.",
			  "The proposed scheme is shown to outperform other heuristic approaches since it achieves a better trade-off between compute and stall time than the others."
			]
		  },
		  {
			"title": "Terminus: A Programmable Accelerator for Read and ...",
			"url": "https://www.researchgate.net/publication/386406836_Terminus_A_Programmable_Accelerator_for_Read_and_Update_Operations_on_Sparse_Data_Structures",
			"excerpts": [
			  "SQRL [60] handles traversals of hash tables and tree data structures. Generalpurpose fetcher units such as SpZip [130] and Terminus [65] support a wider range ...Read more"
			]
		  },
		  {
			"title": "SpZip: Architectural Support for Effective Data Compression In ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2021.spzip.isca.pdf",
			"excerpts": [
			  "We present SpZip, an architectural approach that makes data compression practical for irregular algorithms. SpZip acceler- ates the traversal, decompression, ...Read more"
			]
		  },
		  {
			"title": "IMP: Indirect Memory Prefetcher",
			"url": "https://wangziqi2013.github.io/paper/2019/01/07/imp.html",
			"excerpts": [
			  "This paper proposes Indirect Memory Prefetcher (IMP) to address the common problem of slow indirect memory operations."
			]
		  },
		  {
			"title": "Qualcomm Hexagon V73 Programmer's Reference Manual ...",
			"url": "https://device.report/m/9617176bfefffcbc1efdcc55512335e567e0d9a1c8a2813a1538de55ed99cf3b",
			"excerpts": [
			  "... instruction scheduling. 1.3 Technical ... 5.10.6.3 Software-based l2fetch The l2fetch instruction provides powerful L2 prefetching of data or instructions.Read more"
			]
		  },
		  {
			"title": "Advanced Scheduling Guide",
			"url": "https://documentation-be.hexagon.com/bundle/Advanced-Scheduling-Notes---2021.0/raw/resource/enus/Advanced%20Scheduling%20Notes%20-%202021.0.pdf?save_local=true",
			"excerpts": [
			  "Advanced Scheduling Guide. Master>Scheduling>Advanced Scheduling Rules. Run No: Create different run numbers to enable comparison of different settings and ...Read more"
			]
		  },
		  {
			"title": "DX100: Programmable Data Access Accelerator for Indirection",
			"url": "https://deepblue.lib.umich.edu/bitstream/handle/2027.42/198624/3695053.3731015.pdf?sequence=1",
			"excerpts": [
			  "We introduce DX100, a programmable data access accelerator for\nindirect memory accesses. DX100 is shared across cores to offload\nbulk indirect memory accesses and associated address calculation\noperations. DX100 *reorders* , *interleaves* , and *coalesces* memory re-\nquests to improve DRAM row-buffer hit rate and memory band-\nwidth utilization.",
			  "DX100 provides a general-purpose ISA to support\ndiverse access types, loop patterns, conditional accesses, and ad-\ndress calculations.",
			  "To support this accelerator without significant\nprogramming efforts, we discuss a set of MLIR compiler passes that\n∗ Alireza Khadem and Kamalavasan Kamalakkannan are co-first authors.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nShareAlike 4.0 International License.",
			  "Fetcher units [ 65 , 130 ] and Decoupled Access\nExecute (DAE) architectures [ 115 ] hoist memory access instruc-\ntion streams and offload them to tightly-coupled access coproces-\nsors, improving the core structural limitations.",
			  "Similar to DX100, Fetcher units [ 65 , 130 ] and Decoupled Access\nExecute (DAE) architectures [ 115 ] hoist memory access instruc-\ntion streams and offload them to tightly-coupled access coproces-\nsors, improving the core structural limitations."
			]
		  },
		  {
			"title": "of Full and Partial Predicated Execution Support for ILP ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s06/web/handouts/mahlke-isca95.pdf",
			"excerpts": [
			  "Wen-mei\nW. Hwu",
			  "Center for Reliable\nand High-Performance\nComputing",
			  "University\nof Illinois\nUrbana-Champaign,\nIL 61801",
			  "Abstract",
			  "One can effectively\nutilize\npredicated\nexecution\nto improve\nbranch handling\nin instruction-level\nparallel\nprocessors.",
			  "though\nthe\npotential\nbenefits\nof predicated\nexecution\nare\nhigh, the tradeoffs\ninvolved\nin the design of an instruction\nset\nto support\npredicated\nexecution\ncan be difficult.\nO",
			  "partial\npredicated\nexecution\nsupport,\nsuch as conditional\nmoves, requires\nvery\nlittle\nchange to existing\narchitectures.",
			  "With\nour current\ncompiler\ntechnology,\nwe show that\nthe\ncompiler\ncan use both\npartial\nand full\npredication\nto\nachieve speedup in large control-intensive\nprograms.",
			  "In this section, the hyperblock compilation techniques for full predicate support are first summarized. Then, the trans- formation techniques to generate.Read more"
			]
		  },
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Enhanced modulo scheduling for loops with conditional branches.",
			"url": "https://puma.ub.uni-stuttgart.de/bibtex/196dcbc0b79dd30a7db4d46cb8950c83c",
			"excerpts": [
			  "Enhanced modulo scheduling for loops with conditional branches. ... N. Warter, G. Haab, K. Subramanian, and J. Bockhaus."
			]
		  },
		  {
			"title": "Improving-Branch-Prediction-and-Predicated-Execution-in- ...",
			"url": "https://www.researchgate.net/profile/Eduardo-Quinones/publication/224694508_Improving_Branch_Prediction_and_Predicated_Execution_in_Out-of-Order_Processors/links/548eb6790cf225bf66a61eb6/Improving-Branch-Prediction-and-Predicated-Execution-in-Out-of-Order-Processors.pdf",
			"excerpts": [
			  "Mahlke et al. [13], studied the benefits of partial and full predication ... Hwu. Architectural support for compiler-synthesized dynamic ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. ",
			  "An empirical study of the importance of an architectural support, referred to as predicted execution, for the effectiveness of software pipelining is presented to help future microprocessor designers to determine whether predicated execution support is worthwhile given their own estimation of the increased hardware cost.",
			  "An Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction or If-conversion with Predicated Execution. Expand",
			  "A multiple-II modulo scheduling technique is proposed for architectures with predicated execution that maintains the benefits of a local scheduling based approach while reducing the effective II. Expand",
			  "This dissertation shows that Modulo Scheduling with Predicated Execution has better performance and lower code expansion than Modulo scheduling with ICTs on processors without special hardware support. Expand"
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "Loops with conditional branches have multiple execution paths which are difficult to software pipeline. The modulo scheduling technique for software pipelining addresses this problem by converting loops with conditional branches into straight-line code before scheduling.",
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle.",
			  "However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  },
		  {
			"title": "Effective Compiler Support for Predicated Execution Using ...",
			"url": "https://cse.umich.edu/awards/pdfs/p45-mahlke.pdf",
			"excerpts": [
			  "Predicated**\n**execution is an effective technique for dealing**\n**with conditional branches in application p",
			  "rblocks are formed using a modi-**\n**fied version of if-conversio",
			  "perblock is a set of predicated basic blocks in which con-**\n**trol may only enter from the top, but may exit from one**\n**or more loca",
			  "ulative execution is provided**\n**by performing predicate promotion within a hyperblock.",
			  "locks are most effective for higher issue rate processors**\n**where there are sufficient resources to schedule instructions**\n**for multiple paths of contro",
			  "ated**\n**execution is an effective technique for dealing**\n**with conditional branches in application program",
			  "perblocks are formed by selectively including**\n**basic blocks in the hyperblock according to their execution**\n**frequency, size, and instruction characteri",
			  "hyperblock is a set of predicated basic blocks in which con-**\n**trol may only enter from the top, but may exit from one**\n**or more lo",
			  "lative execution is provided**\n**by performing predicate promotion within a hyperblock.",
			  "his paper, we propose the use of a structure, referred**\n**to as the** ***hyperblock,***\n**to overcome these two probl",
			  "yperblock is a set of predicated basic blocks in which con-**\n**trol may only enter from the top, but may exit from one**\n**or more locat",
			  "perblocks are formed using a modi-**\n**fied version of if-conversio",
			  "ative execution is provided**\n**by performing predicate promotion within a hyperblock. ",
			  "locks are formed by selectively including**\n**basic blocks in the hyperblock according to their execution**\n**frequency, size, and instruction characteristic",
			  "cally excluding basic blocks from the hyperblocks provides**\n**additional optimization and scheduling opportunities for in-**\n**structions within the hyperblock",
			  "inder of this paper consists of four sections. In**\n**Section 2, the architecture support we utilize for predicated**\n**execution is discussed.",
			  "edicated**\n**execution is an effective technique for dealing**\n**with conditional branches in application pro",
			  "ver, there are several problems associated with conventional**\n**compiler support for predicated execution.",
			  "yperblock is a set of predicated basic blocks in which con-**\n**trol may only enter from the top, but may exit from one**\n**or more loc",
			  "perblocks are formed using a modi-**\n**fied version of if-convers",
			  "locks are included in a**\n**hyperblock based on their execution frequency, size, and in-**\n**struction characteristi",
			  "peculative execution is provided**\n**by performing predicate promotion within a hyperblo",
			  "cks are most effective for higher issue rate processors**\n**where there are sufficient resources to schedule instructions**\n**for multiple paths of control.",
			  "erblocks are formed by selectively including**\n**basic blocks in the hyperblock according to their execution**\n**frequency, size, and instruction characteris"
			]
		  },
		  {
			"title": "The Program Decision Logic Approach to Predicated Execution.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/isca-99-decision.pdf",
			"excerpts": [
			  "12] P. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D.\nLichtenstein, R. P. Nix, J. S. O’Donnell, and J. C. Rutten-\nberg. The Multiflow Trace scheduling compiler. *The Journal*\n*of Supercomputing* , 7(1):51–142, January 1",
			  "[14] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, R. A.\nBringmann, and W. W. Hwu. Effective compiler support for\npredicated execution using the hyperblock. In *Proceedings*\n*of the 25th International Symposium on Microarchitecture* ,\npages 45–54, December 19",
			  "16] B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle. The\nCydra 5 departmental supercomputer.\n*IEEE Computer* ,\n22(1):12–35, January 198",
			  "[18] M. Schlansker and V. Kathail. Critical path reduction for"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization.",
			  " partitions loops\ninto long-running threads that communicate via inter-core queues.\nDSW",
			  "Parallel-Stage Decoupled Software Pipelining [16] (PS-DSWP)\nis an extension to combine pipeline parallelism with some stages\nexecuted in a DOALL, data-parallel fashion.",
			  "This paper presents the preliminary design of a new PS-DSWP method capable of handling arbitrary structured control flow, a slightly better ...Read more"
			]
		  },
		  {
			"title": "Phloem: Automatic Acceleration of Irregular Applications with ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2023.phloem.hpca.pdf",
			"excerpts": [
			  "Recent work has proposed hardware support for fine-grain pipeline parallelism, hiding long latencies by decoupling irregular applications into pipeline stages.Read more"
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation.",
			  "We evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP.",
			  "The base model used is a 6-core Itanium 2 CMP developed using\nthe Liberty Simulation Environment (LSE). The cores are validated\n(IPC and constituent error components accurate to within 6% of\nreal hardware for benchmarks measured [15]), and the details of\neach core in the model are given in Table 1. To this base model, a\nsynchronization array with 512 queues, each with 32 entries, was\nadded. The produce , consume\nand queue.set\ninstructions\nare also modeled in the pipeline of the Itanium 2 cores.",
			  "We have implemented PS-DSWP in the VELOCITY research\ncompiler [22]. The applications are first compiled into a low-level\nintermediate representation, and then transformed by a round of\nclassical optimizations. PS-DSWP was then automatically applied\nto the selected loops. Translation to Itanium 2 instructions is fol-\nlowed by another round of classical optimizations, scheduling, reg-\nister allocation, and a final scheduling phase.",
			  "t DSWP in combination\nwith speculation and parallel stages has the potential to unlock par-\nallelism among all the programs in the SPEC2000 integer bench-\nmark suite.",
			  "This paper presented a new non-speculative compiler transfor-\nmation called Parallel-Stage Decoupled Software Pipelining (PS-\nDSWP). PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformation. This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators.",
			  "For other programs, however, these tools yield poor performance because they invoke computation units for instructions sequentially, without exploiting parallelism."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages.",
			  "Decoupled software pipelining\n(DSWP) [ 35 ] uses a synchronization array to facilitate com-\nmunication between cores or the threads of a multithreaded\ncore. But DSWP focuses on pipelining a single loop across\ndi ff erent threads, which is too limiting for irregular applications.\nFor example, BFS uses a 3-level nested loop, with stages",
			  " Pipette’s ISA fea-\ntures control mechanisms that allow e ff ective decoupling under\nirregular control flow. ",
			  "es.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers.",
			  "By time-multiplexing stages on the same\ncore, Pipette avoids load imbalance and achieves high core IPC.",
			  " it uses the\nphysical register file to implement queues cheaply, avoiding the\nstorage costs of prior techniques. Sec",
			  "Pipette decouples threads within\nthe core using architecturally visible queues."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more",
			  "This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual ...Read more"
			]
		  },
		  {
			"title": "Quantifying the Semantic Gap Between Serial and Parallel ...",
			"url": "https://par.nsf.gov/servlets/purl/10337772",
			"excerpts": [
			  "This measures the TLP limit of both cyclic multithreading (CMT). (e.g., DoAcross [20], HELIX [3]) and pipeline multithreading. (PMT) (e.g., DSWP [5], PS-DSWP [8]) ...Read more"
			]
		  },
		  {
			"title": "From Sequential Programs to Concurrent Threads - Liberty ...",
			"url": "https://liberty.princeton.edu/Publications/cal05_dswp.pdf",
			"excerpts": [
			  "Ottoni, E. Raman, and D. I.. August, “Practical and accurate low-level pointer analysis,” in Proc. of the Int'l. Symp. on Code Generation and Optimization, 2005 ..."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Since technology scaling no longer provides previously seen rates of performance improvements, microprocessor designers have adopted a number of other techniques to improve processor performance at reasonable energy costs. An entire class of techniques improves performance by offloading computation intensive parts of programs from a general-purpose processor to customized accelerators [15], [17], [18], [24], [31], [34]. One approach to build these customized accelerators is to manually design dedicated circuit modules for each application using hardware description languages (HDLs) [24], [26]. While this approach often yields the best result, it requires significant non-recurring engineering costs to convert algorithms to HDL specifications. Another less labor-intensive approach is to use high-level synthesis (HLS) tools to generate customized accelerators from programs written in C or other high-level programming languages [4], [12], [13], [16], [30], [31], [32], [37].",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "WP+, splits a loop into new loops with dependence pat-\nterns amenable to parallelization using techniques that were\noriginally either inapplicable or poorly-performing.",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Decoupled software pipelining (DSWP) solves this by preferentially fetching and executing instructions from a program's critical path (CP). This allows DSWP to achieve high IPC with fast simple execution cores.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "Special microarchitectural support, called the\nsynchronization array (SA), provides low-latency inter-thread\nsynchronization and value communication and acts as a decoupling\nbuffer between the threads.",
			  "Decoupled execution ensures that stalls\nin one thread do not affect the other.",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Automatic Multithreading for DSWP: Unlike prior attempts at automatically multithreading sequential programs, DSWP does\nnot try to partition programs into totally independent threads.",
			  "We have a working compiler implementation that\nautomatically generates DSWP code.",
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies."
			]
		  },
		  {
			"title": "Parallel-stage decoupled software pipelining | Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization",
			"url": "https://dl.acm.org/doi/10.1145/1356058.1356074",
			"excerpts": [
			  "DOALL is a popular automatic technique for loop-level parallelization employed successfully in the domains of scientific and numeric computing. While DOALL generally scales well with the number of iterations of the loop, its applicability is limited by the presence of loop-carried dependences. A parallelization technique with greater applicability is decoupled software pipelining (DSWP), which parallelizes loops even in the presence of loop-carried dependences. However, the scalability of DSWP is limited by the size of the loop body and the number of recurrences it contains, which are usually smaller than the loop iteration count.",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the recurrences in their own stages in DSWP, portions of the loop suitable for DOALL parallelization may be exposed. PS-DSWP extends DSWP to benefit from these opportunities, utilizing multiple threads to execute the same stage of a DSWPed loop in parallel. This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114% (up to a maximum of 155%) with 6 threads on loops from a set of 5 applications. Our experiments also demonstrate that PS-DSWP achieves better scalability with the number of threads than DSWP."
			]
		  },
		  {
			"title": "[PDF] Parallel-stage decoupled software pipelining | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Parallel-stage-decoupled-software-pipelining-Raman-Ottoni/73538f17cbeb45fd0822cb971375ad03c515dbe7",
			"excerpts": [
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP), which is to combine the applicability of DSWP with the scalability of DOALL parallelization.",
			  "This paper proposes adding speculation to DSWP and evaluates an automatic approach for its implementation and demonstrates significant gains using speculation for 4-core chip multiprocessor models running a variety of codes.",
			  "Decoupled software pipelining (DSWP) is introduced, a technique that statically splits a single-threaded sequential loop into multiple nonspeculative threads, each of which performs useful computation essential for overall program correctness."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/144589/Nguyen-qmn-PhD-EECS-2022-thesis.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "Decoupling stages with queues hides\nlatency by allowing producer stages to run far ahead of consumer stages.",
			  "Pipette introduces\narchitectural support for pipeline parallelism within the threads of a\nmultithreaded core.",
			  "By using SMT to time-multiplex stages in the same core, Pipette avoids\nload imbalance issues that arise when decoupling stages across separate\ncores or processing elements.",
			  "Pipette also adds out-of-band control\nflow to keep producer and consumer loops running despite complex\ncontrol flow.",
			  "Pipette enables composable, fine-grain interleaving of\naccelerated accesses and general-purpose computation.",
			  "the im-\nplementation reuses core structures: it uses the physical register file to\nimplement queues cheapl",
			  "Pipette’s novel ISA (Section 3.2) allows threads to\ndefine inter-thread queues."
			]
		  },
		  {
			"title": "Compiling Irregular Software to Specialized Hardware - MICE",
			"url": "https://mice.cs.columbia.edu/getTechreport.php?techreportID=1641&format=pdf&",
			"excerpts": [
			  "The Coarse-Grained Pipelined Accelerator (CGPA) framework of Liu et al. [84] synthesizes\nnovel hardware architectures for C/C++ programs containing complex control fow or irregular\nmemory access patterns.",
			  "After translating the program to the LLVM IR, their HLS fow imple-\nments each loop’s instructions with a multi-stage pipeline of hardware “workers” separated by\nFIFO bufers: sequential workers in one stage supply data to multiple parallel workers in the\nnext, exploiting pipeline parallelism.",
			  "he parallel workers implement any independent instructions from mul-\ntiple loop iterations; this decoupling tolerates variable latency (e.g., cache misses may slow down\ntraversal, but the parallel workers can continue executing as long as they have input data in their\nFIFOs) and enables more parallelism",
			  "Our synthesized datafow\nnetworks perform dynamic scheduling on their own (no static scheduling is required), and our\ninput language is side-efect free, simplifying our translation to hardware.",
			  "Tan et al.’s ElasticFlow HLS tool [122] is similar to CGPA. Given a loop nest with a regular\nouter loop (i.e., it does not exhibit loop-carried dependencies) and at least one dynamic-bound"
			]
		  },
		  {
			"title": "(PDF) Enhancing the performance of Decoupled Software Pipeline through Backward Slicing",
			"url": "https://www.researchgate.net/publication/271532508_Enhancing_the_performance_of_Decoupled_Software_Pipeline_through_Backward_Slicing",
			"excerpts": [
			  "DSWP extracts threadlevel parallelism from the body of a loop by breaking it into stages which are then executed pipeline style: in effect ...Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more",
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized.",
			  "We chose to implement DSWP using the general-purpose POSIX threading\nlibrary (pthreads) and the Low Level Virtual Machine (LLVM) compiler\ninfrastructure.",
			  "*\n**Related Work**\nThe initial DSWP has been implemented in the context of the IMPACT\nresearch compiler using Itanium platform simulator which has customized\nhardware-level support.\nOther works of DSWP including [6, 4] also use\nhardware simulator.\n",
			  "rch but industry standard.\n1\nDue to our choice to use pthreads, Our DSWP implementation will also\nbe portable across more platforms than previous implementations since any\n ... \n*ceedings of the 38th annual IEEE/ACM International Symposium on*\n*Microarchitecture* , MICRO 38, pages 105–118, Washington, DC, USA,\n2005. IEEE Computer Society.\n[4] Easwaran Raman, Guilherme Ottoni, Arun Raman, Matthew J. Bridges,\nand David I. August. Parallel-stage decoupled software pipelining. In\n*Proceedings of the 6th annual IEEE/ACM international symposium on*\n*Code generation and optimization* , CGO ’08, pages 114–123, New York,\nNY, USA, 2008. ACM.\n[5] Ram Rangan, Neil Vachharajani, Manish Vachharajani, and David I.\nAugust. Decoupled software pipelining with the synchronization array.\nIn *Proceedings of the 13th International Conference on Parallel Archi-*\n*tectures and Compilation Techniques* , PACT ’04, pages 177–188, Wash-\nington, DC, USA, 2004. IEEE Computer Society.\n[6] Neil Vachharajani, Ram Rangan, Easwaran Raman, Matthew J. Bridges,\nGuilherme Ottoni, and David I. August. Speculative decoupled software\npipelining. In *Proceedings of the 16th International Conference on Par-*\n*allel Architecture and Compilation Techniques* , PACT ’07, pages 49–59,\nWashington, DC, USA, 2007. IEEE Computer Society.\n**Distribution of Total Credit**\nFuyao implement framework of DSWP and component for dependence anal-\nysis, SCC/DAG, thread partitioning, loop splitting and synchronization.\nMark implemented the runtime support for multithreading based on pthreads,\ncontributed to the synchronization implementation, and managed write-ups\nand poster.\n8"
			]
		  },
		  {
			"title": "Semi-automatic extraction and exploitation of hierarchical ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/1854273.1854321",
			"excerpts": [
			  "Recent work [23] on Ps-Dswp includes a speculation system which if used for parallelization can achieve good performance and scal- ability on commodity hardware ...Read more"
			]
		  },
		  {
			"title": "Efficient Data Supply for Parallel Heterogeneous ...",
			"url": "https://decades.cs.princeton.edu/TACO19.pdf",
			"excerpts": [
			  "Shelf [43] utilizes an in-order pipeline within an OoO core for higher efficiency, whereas Outrider [9] utilizes a SMT core and decoupled execution to achieve.Read more"
			]
		  },
		  {
			"title": "Parallelizing Sequential Applications on Commodity Hardware using a Low-cost Software Transactional Memory | Request PDF",
			"url": "https://www.researchgate.net/publication/220752082_Parallelizing_Sequential_Applications_on_Commodity_Hardware_using_a_Low-cost_Software_Transactional_Memory",
			"excerpts": [
			  "... Mehrara et al. [25] described STMLite, a software transactional memory model modified to support speculative parallelization. It was ..."
			]
		  },
		  {
			"title": "static and dynamic instruction mapping",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_fengliu.pdf",
			"excerpts": [
			  "arse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes\ncoarse-grained pipeline parallelism techniques to synthesize efficient specialized ac-\ncelerator modules from irregular C/C++ programs without requiring any annotati",
			  "GPA automatically partitions individual loops into separate pipelined stages and gen-\nerates buffer-connected hardware modules for these stages",
			  "ipeline Partition** : This is a coarse-grained instruction scheduling step that assigns\ninstructions to different pipeline stages. The partitioning algorithm is adapted from the\nParallel Stage Decoupled Software Pipelining (PS-DSWP) [68] algorithm",
			  "The main dif-\nference between CGPA and PS-DSWP lies in the identification of replicable sections, and\ndeciding whether it should be inserted in the parallel stage as replicas or into the sequential\nstage. Generally, inserting replicable sections into a sequential stage will increase com-\nmunication, because the results of the replicable sections must be sent explicitly to the\nfollowing stages. Conversely, duplicating the replicable section in parallel stages will in-\ncrease the amount of computations and memory accesses. The CGPA framework only\nduplicates lightweight replicable sections which do not contain load and multiply instruc",
			  "This is based on the intuition that time and resources required for the replicable\nsection without these instructions are less than those for communicating results via FIFO\nbuffers.",
			  "Figure 3.1(d) shows that replicable sections with heavyweight load instructions are\nidentified and inserted into a sequential stage (Stage 1), and the remaining instructions are\ngrouped as one parallel stage (Stage 2). The required number and stage connections of the\nbuffers are also determined at this step.",
			  "PA can also adopt another approach, called **decoupled pipeline parallelism** to im-\nprove outer loop performance and generate accelerators similar to the results of manual\naccelerator designs [68]. This approach uses a set of FIFO buffers to separate the linked\nlist traversal module from that for node updates. Since the traversal section only goes over\nthe linked list and fetches node addresses, it can progress much faster than the update sec-\ntion. Thus one *sequential* traversal module can supply node addresses to multiple *parallel*\nupdate modules in another stage, as shown in Figu",
			  "th this decoupled pipeline design, when control enters the loop, the hardware mod-\nules for both stages are invoked by the same start signal. The module in the first (sequential)\nstage begins fetching node addresses one by one, and assigns the node address values to\nthe FIFO buffers of the parallel modules in the parallel stage in a round-robin fashion. The\nsequential stage stalls when there are cache misses or the corresponding buffers are full.",
			  "GPA is the first HLS tool to automatically extract this type of parallelism from a\n*single loop* and generate efficient pipelined hardware modules for it",
			  "Figure 3.3 shows a logical view of coarse-grained pipelined accelerators with a Sequential–\nParallel–Sequential (S-P-S) pipeline",
			  "The number of stages for different applications is\nnot fixed, and is determined automatically for each application by the CGPA compiler’s\npartition algorithm.",
			  "peline Partition** : This is a coarse-grained instruction scheduling step that assigns\ninstructions to different pipeline stages. The partitioning algorithm is adapted from the\nParallel Stage Decoupled Software Pipelining (PS-DSWP) [68] algorithm"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling: An Algorithm For Software ...",
			"url": "https://web.eecs.umich.edu/~mahlke/courses/583f12/reading/rau_micro94.pdf",
			"excerpts": [
			  "Modulo scheduling is a framework within which a wide variety of algorithms and heuristics may be defined for software pipelining innermost loops. This paper ...Read more",
			  "When all operations have been considered, the usage count for the most heavily used resource constitutes the ResMII. 2.2 The recurrence-constrained MII (RecMII).Read more",
			  "The nature of the reservation tables for the opcode repertoire of a machine determine the complexity both of computing the. ResMII and of scheduling the loop. A ...Read more",
			  "This paper takes a first step in this direction by describing a practical modulo scheduling algorithm which is capable of dealing with realistic machine models.Read more",
			  "Rau, B.R. Iterative Modulo Scheduling. HPL Technical. Report. Hewlett-Packard Laboratories, 1994. 34. Rau, B.R., and Glaeser, C.D. Some scheduling techniques.Read more",
			  "At this point, the selected region is IF-converted, with the result that all branches except for the loop-closing branch disappear [4, 29, 10]. With control ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling - Springer Link",
			"url": "https://link.springer.com/content/pdf/10.1007/BF03356742.pdf",
			"excerpts": [
			  "Modulo scheduling is a framework within which algorithms for software pipelining innermost loops may be defined. The framework specifies a set of.Read more"
			]
		  },
		  {
			"title": "From VLIW to EPIC Architectures",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2002/lectures/Lecture17.pdf",
			"excerpts": [
			  "What are the shortcomings of VLIW? • Wasteful encoding with nops. • No compatibility across generations. • Even with block-coalescing techniques such as trace ...Read more"
			]
		  },
		  {
			"title": "Cluster-Level Simultaneous Multithreading for VLIW ...",
			"url": "https://www.ac.upc.edu/RR/2006/23.pdf",
			"excerpts": [
			  "Very Long Instruction Word (VLIW) is a paradigm for exploiting Instruction Level Parallelism (ILP) based\non exposing the architecture details to the compiler, so that ILP can be extracted at compile time.",
			  "Therefore,\ncontrary to superscalars, no special hardware like register renaming, instruction queues, reorder buffers, etc. is\nrequired.",
			  "Block multithreading [16, 2, 21] executes VLIW instructions from a single thread until it is blocked by an\nevent (a cache miss, for e.g.)."
			]
		  },
		  {
			"title": "Hybrid Multithreading for VLIW Processors",
			"url": "https://scispace.com/pdf/hybrid-multithreading-for-vliw-processors-6rq3gz5nhk.pdf",
			"excerpts": [
			  "Very Long Instruction Word (VLIW) processors have gained\nwide acceptance in the embedded domain due to hardware\nsimplicity, low cost and low power consumption [11, 18].",
			  "To exploit high Instruction Level Parallelism (ILP), VLIWs\nneed to be designed with a significant issue width. However,\nthe centralized Register File (RF), with all the Functional\nUnits (FUs) connected to it, becomes a bottleneck due to an\nincrease in RF delay, power and area [17].",
			  "Clustered VLIW\narchitectures have multiple RFs and cluster the Functional\nUnits (FUs) to the RFs they are connected to.",
			  "Block MultiThreading (BMT) [23] executes instructions from\na single thread until it is blocked by a long latency event\n(a cache miss, for instance).",
			  "Interleaved MultiThreading\n(IMT) [2, 19] does a zero cycle context switch every cycle,\nso that instructions from different threads are interleaved at\nexecution time.",
			  "ultaneous MultiThreading** (SMT) [20] issues in-\nstructions from multiple threads each cycle in contrast to\nBMT and IMT where, at a given time, instructions from only\none thread are issued.",
			  "In a SMT processor, issue-slots of the\nprocessor are filled by operations from different threads, con-\nverting thread level parallelism (TLP) into IL"
			]
		  },
		  {
			"title": "SMT/VLIW/EPIC, Statically Scheduled ILP",
			"url": "http://class.ece.iastate.edu/tyagi/cpre581/lectures/LecVLIWEPIC.pdf",
			"excerpts": [
			  "VLIW Compiler Responsibilities",
			  "\nThe compiler:\nSchedules to maximize parallel execution\nGuarantees intra-instruction parallelism\nSchedules to avoid data hazards (no interlocks)\n Typically separates operations with explicit NOPs",
			  "rly VLIW Machines\n**FPS AP120B (1976)**\n **scientific attached array processor**\n **first commercial wide instruction machine**\n **hand-coded vector math libraries using software**\n**pipelining and loop unrolling**\n**Multiflow Trace (1987)**\n **commercialization of ideas from Fisher** **’** **s Yale group**\n**including** **“** **trace scheduling**",
			  "Cydrome Cydra-5 (1987)**\n **7 operations encoded in 256-bit instruction word**\n **rotating register file**",
			  "63\nIntel EPIC IA-64\nEPIC is the style of architecture (CISC, RISC)\n Explicitly Parallel Instruction Computing\nIA-64 is Intel ’ s chosen ISA (x86, MIPS)\n IA-64 = Intel Architecture 64-bit\n An object-code compatible VLIW\nItanium (aka Merced) is first implementation (\n First customer shipment were expected 1997\n(actual 2001)\n McKinley, second implementation shipped in 2002\n64",
			  "\nIA-64 Predicated Execution\n**Problem: Mispredicted branches limit ILP**\n**Solution: Eliminate hard to predict branches with predicated execution**\n\n**Almost all IA-64 instructions can be executed conditionally under predicate**\n\n**Instruction becomes NOP if predicate register false**\n",
			  "IA-64 Instruction Format\n**Template bits describe grouping of these instructions**\n**with others in adjacent bundles**\n**Each group contains instructions that can execute in**\n**parallel**\nInstruction 2 Instruction 1 Instruction 0 Template\n128-bit instruction bundle\n*group i*\n*group i+1*\n*group i+2*\n*group i-1*\n***bundle j***\n***bundle j+1 bundle j+2***\n***bundle j-1***",
			  "The compiler:\nSchedules to maximize parallel execution\nGuarantees intra-instruction parallelism\nSchedules to avoid data hazards (no interlocks)",
			  "urn\nlate\n63\nIntel EPIC IA-64\nEPIC is the style of architecture (CISC, RISC)\n Explicitly Parallel Instruction Computing\nIA-64 is Intel ’ s chosen ISA (x86, MIPS)\n IA-64 = Intel Architecture 64-bit\n An object-code compatible VLIW\nItanium (aka Merced) is first implementation (\n First customer shipment were expected 1997\n(actual 2001)\n McKinley, second implementation shipped in 2002\n64\nIA-64 Inst",
			  "ps\n66\nIA-64 Predicated Execution\n**Problem: Mispredicted branches limit ILP**\n**Solution: Eliminate hard to predict branches with predicated execution**\n\n**Almost all IA-64 instructions can be executed conditionally under predicate**\n\n**Instruction becomes NOP if predicate register false**\n**Inst 1**\n**Inst 2**\n**br a==b** **,** **b2**\n**Inst 3**\n**Inst 4**\n**b"
			]
		  },
		  {
			"title": "Characterizing the Impact of Predicated Execution on ...",
			"url": "http://american.cs.ucdavis.edu/academic/readings/papers/pred-ex.mahlke.ps",
			"excerpts": [
			  "Thus, there is only one branch prediction for. all branches within a single cache block. The cache block size. simulated is 64 bytes (16 instructions). From ...Read more"
			]
		  },
		  {
			"title": "A Framework for Balancing Control Flow and Predication",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-97-framework.pdf",
			"excerpts": [
			  "Predicated execution benefits directly from\nthe advantages of compilation using the predicated representa-\ntion. In addition, the removal of branches yields performance\nbenefits in the executed code, the most notable of which is the\nremoval of branch misprediction penaltie",
			  "hyperblock* is a structure created to facilitate optimization\nand scheduling for predicated architectures [14]. A hyperblock is\na set of predicated basic blocks in which control may only enter\nfrom the top, but may exit from one or more locations. Hyper-\nblocks are formed by applying tail duplication and if-conversion\nover a set of carefully selected paths. Inclusion of a path into\na hyperblock is done by considering its profitability. The prof-\nitability is determined by four pieces of information: resource\nutilization, dependence height, hazard presence, and execution\nfrequency",
			  "A hyperblock\nwhich contains multiple paths will not complete until all of its\nconstituent paths have completed. Therefore, the overall height\nof the hyperblock is the maximum of all the original paths' de-\npendence height",
			  "he scheduler with partial reverse if-conversion operates by\nidentifying the paths composing a hyperblock. Paths which are\nprofitable to overlap remain unchanged. Conversely, a path that\ninteracts poorly with the other paths is removed from the hyper-\nblock. In particular, the partial reverse if-converter decides to\neject certain paths, or parts of paths, to enhance the schedule",
			  "The first phase of the compilation framework is to aggres-\nsively perform hyperblock formation.\nThe hyperblock former\ndoes not need to exactly compute what paths, or parts of paths,\nwill fit in the available resources and be completely compatible\nwith each other. Instead, it forms hyperblocks which are larger\nthan the target architecture can handl",
			  "tial reverse if-converter was able to adjust the\namount of if-conversion to match the available resources to effi-\nciently utilize the processor. As a result, the execution time for\nthe *update* function is reduced to 16.8 million cycles with partial\nreverse if-conversion, a 52% performance improvement over the\nsuperblock code.",
			  " compilation frame-\nwork, shown in Figure 4, consists of two phases of predicate ma-\nnipulation surrounding classical, predicate specific, and ILP op-\ntimizations. The first predicate manipulation phase, hyperblock\nformation, has been addressed thoroughly in [14]. The second\npredicate manipulation phase, adjustment of hyperblocks during\nscheduling, is proposed in this work and has been termed *partial*\n*reverse if-conve",
			  "n the context of a list scheduler, a logical heuristic is to con-\nsider each potential reverse if-conversion in a top-down fashion,\nin the order in which the predicate defines are scheduled",
			  "The framework was able to capitalize on the benefits of\npredication without being subject to the sometimes negative side\neffects of over-aggressive hyperblock formation. Furthermore,\nadditional opportunities for performance improvement were ex-\nploited by the framework, such as partial path if-conversio",
			  "resources required by over-\nlapping the execution of multiple paths are the union of the re-\nsources required for each individual path.",
			  "The\ncompiler must weigh this negative against the potential positive\neffects of if-conversion to determine whether this hyperblock is\nprofitable to form.",
			  "Partial re-\nverse if-conversion is conceptually the application of reverse if-\nconversion to a particular predicate in a hyperblock for a chosen\nset of instructio",
			  "he second phase of the compilation framework is to ad-\njust the amount of predicated code in each hyperblock as the\ncode is scheduled via partial reverse if-conversion",
			  "The framework was able to capitalize on the benefits of\npredication without being subject to the sometimes negative side\neffects of over-aggressive hyperblock formation. ",
			  "Figure 9: Performance increase over superblock exhibited by the\nhyperblock and partial reverse if-conversion frameworks with no\nmisprediction penalty.",
			  "Figure 10: Performance increase over superblock exhibited by\nthe hyperblock and partial reverse if-conversion frameworks with\na four cycle misprediction penalty.",
			  "e static code size exhibited by using the hyperblock and par-\ntial reverse if-conversion compilation frameworks with respect to\nthe superblock techniques is presented in Figure 11."
			]
		  },
		  {
			"title": "\nAn If-Conversion Algorithm Based on Predication Execution",
			"url": "https://scialert.net/fulltext/?doi=itj.2010.984.988",
			"excerpts": [
			  "A hyperblock is a structure created to facilitate optimization and scheduling for predicated architectures.",
			  "A hyperblock is a set of predicated basic blocks in which control may only enter from the top, but may exit from one or more locations.",
			  "Hyperblocks are formed by applying tail duplication and if-conversion over a set of carefully selected paths.",
			  "Inclusion of a path into a hyperblock is done by considering its profitability.",
			  "The profitability is determined by the following factors: resource utilization, invalid instructions, branch instructions inherent instruction dependence, branch execution frequency, the balance of branch execution and so on.",
			  "Not only can if-conversion eliminate the branch and provide continuous instruction stream, it can also extend the length of basic block and improve the effective parallelism of code."
			]
		  },
		  {
			"title": "Trace Scheduling: A Technique for Global Microcode ...",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs252-S12/handouts/papers/TraceScheduling.pdf",
			"excerpts": [
			  "Trace scheduling works on traces (or\npaths) through microprograms.",
			  "FISHER: TRACE SCHEDULING. During compaction we will deal with two fundamental ... algorithm for trace scheduling loop-free code. Algorithm: Trace Scheduling.Read more",
			  "Compacting is thus done with a broad\noverview of the program.",
			  "Important operations are given priority, no\nmatter what their source block was.",
			  "This is in sharp contrast with\nearlier methods, which compact one block at a time and then attempt\niterative improvement.",
			  "Loops are handled using the reducible property of most flow graphs.",
			  "The loop handling technique permits the operations to move around\nloops, as well as into loops where appropriate.",
			  "Trace scheduling is developed on a simplified and straightforward\nmodel of microinstructions. Guides to the extension to more general\nmodels are given."
			]
		  },
		  {
			"title": "IA-64 - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/IA-64",
			"excerpts": [
			  "Architecture · 128 general integer registers, which are 64-bit plus one trap bit (\"NaT\", which stands for \"not a thing\") used for speculative execution. · 128 ...Read more"
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "A VLIW Architecture for a Trace Scheduling Compiler\n*Robert P. Colwell, Robert P. Nix, John J. O'Donnell, David B. Papworth, Paul K. Rodman*\nMultiflow Computer\n175 North Main Street\nBranford, CT. 06405\n(203) 488-6090",
			  "Instruction Word (VLIW) architectures were prom-\nised to deliver far more than the factor of two or three that\ncurrent architectures achieve £rom overlapped execution. Using\na new type of compiler which compacts ordinary sequential code\ninto long instruction words, a VLIW machine was expected to\nprovide from ten to thirty times the performance of a more con-\nventional machine built of the same implementation technology.\nMultiflow Computer, Inc., has now built a VLIW called the\nTRACE'\" along with its companion Trace Scheduling\" com-\npacting compiler. This new machine has fulfilled the perfor-\nmance promises that were made. Using many fast functional\nunits in parallel, this machine extends some of the basic\nReduced-Instruction-Set\nprecepts:\nthe\narchitecture\nis\nload/store, the microarchitecture is exposed to the compiler,\nthere is no microcode, and there is almost no hardware devoted\nto synchronization, arbitration, or interlocking of any kind (the\ncompiler has sole responsibility for runtime ",
			  "The search for usable parallelism in code has been in progress\n ... \nstream and attempt to dynamically overlap execution of multiple\nfunctional units were incorporated in systems beginning with the\nIBM 360/91T°ma82 and the Control Data 6600. Th°r70 These\n\"scoreboards, perform the same scheduling task at runtime that\nMultiilow's Trace Scheduling compacting compiler performs at\ncompile time. Even with such ,'complex and costly hardware,,\nAcos86\nAcosta et al.\nreport that only a factor of 2 or 3 speedup in\nperformance is possible. This limitation, of course, is the same\nas previously discussed:\nthe hardware cannot see past basic\nblocks in order to find usable concurrenc",
			  "Over the last two decades, the cost of computer memory has\ndropped much faster than the cost of logic, making the con-\nstruction of a VLIW, which replaces scheduling logic with\ninstruction-word memory, practical and attractive. In conjunc-\ntion with the global optimization ability of our compiler, we find\nno remaining reasons to build run-time scheduling hardware.\nThe scheduling problem is much better solved in software at\ncompile-time. This \"no control hardware\" attitude permeates\nthe design of the TRACE architecture.",
			  "An obvious potential disadvantage to the VLIW approach is that\ninstruction code object size could grow unmanageably large,\nenough so that much of the performance advantage would be\nlost in extra memory costs and disk paging. We have addressed\nthis problem in the design of the TRACE, and have a very satis-\nfactory result to report in Section 9.",
			  "Multiflow's Trace Scheduling compacting compiler automatically\nfinds fine-grained parallelism throughout any appfication.\nIt\nrequires no programmer intervention, either in terms of restruc-\nturing the program so it fits the architecture or in adding direc-\ntives which explicitly identify opportunities for overlapped exe-\ncution. This is in sharp contrast to \"coarse-grained\" parallel\narchitectures, including vector machines, shared-memory mul-\ntiprocessors,\nand\nmore\nradical\nstructures\nsuch\nas\nhypercubes selt85 or massively-parallel machines, wurst The pro-\ncess by which programs are converted into highly parallel wide-\ninstruction-word code is transparent to the user.",
			  "To detect fine-grained parallelism, the compiler performs a\nthorough analysis of the source program. One subroutine or\nmodule is considered at a time. After performing a complete\nset of \"classical\" optimizations, including loop-invariant motion,\ncommon subexpression elimination, and induction variable\nsimplification, the compiler builds a flow graph of the program,\nwith each operation independently represented.",
			  "Using estimates of branch directions obtained automatically\nthrough heuristics or profiling, the compiler selects the most\nlikely path, or \"trace\", that the code will follow during execu-\ntion. This trace is then treated as if it were free of conditional\nbranches, and handed to the code generator. The code genera-\ntor schedules operations into wide instruction words, taking into\naccount data precedence, optimal scheduling of functional units,\nregister usage, memory accesses, and system buses; it emits\nthese instruction words as object code. This greedy scheduling\ncauses code motions which could cause logical inconsistencies\nwhen branches off-trace are taken. The compiler inserts special\n\"compensation code\" into the program graph on the off-trace\nbranch edges to undo these inconsistencies, thus restoring pro-\ngram correctness.",
			  "This process allows the compiler to break the \"conditional jump\nbottleneck\" and find parallelism throughout long streams of\ncode, achieving order-of-magnitude speedups due to compac-\ntion.",
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled.",
			  "A number of conventional optimizations aid the trace selection\nprocess in finding parallelism. Automatic loop unrolling and\nautomatic inline substitution of subroutines are both incor-\nporated in Multiflow's compilers;\nthe compiler heuristically\ndetermines the amount of unrolling and substitution, substan-\ntially increasing the parallelism that can be exploited."
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "Trace scheduling was originally developed for Very Long Instruction Word, or [VLIW](/wiki/VLIW \"VLIW\") machines, and is a form of global code motion.",
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace.",
			  "Trace scheduling uses a basic block scheduling method to schedule the instructions in each entire trace, beginning with the trace with the highest frequency.",
			  "This can result in large increases in code sizes and poor or erratic performance if program's behavior varies significantly with the input.",
			  "A trace is a sequence of instructions, including branches but not including loops, that is executed for some input data."
			]
		  },
		  {
			"title": "the-partial-reverse-if-conversion-framework-for-balancing- ...",
			"url": "https://scispace.com/pdf/the-partial-reverse-if-conversion-framework-for-balancing-3439qyi908.pdf",
			"excerpts": [
			  " [10] S. A. Mahlke, R. E. Hank, R. A. Bringmann, J. C. Gyllenhaal, D. M. Gallagher, and W. W. Hwu, “Characterizing\nthe impact of predicated execution on branch prediction,” in *Proceedings of the 27th International Symposium on*\n*Microarchitecture* , pp. 217–227, December 1994.",
			  "11] G. S. Tyson, “The effects of predicated execution on branch prediction,” in *Proceedings of the 27th International*\n*Symposium on Microarchitecture* , pp. 196–206, December 1994.",
			  "14] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, R. A. Bringmann, and W. W. Hwu, “Effective compiler support for\npredicated execution using the hyperblock,” in *Proceedings of the 25th International Symposium on Microarchitecture* ,\npp. 45–54, December 1992",
			  "[15] S. A. Mahlke, R. E. Hank, J. McCormick, D. I. August, and W. W. Hwu, “A comparison of full and partial predicated\nexecution support for ILP processors,” in *Proceedings of the 22th International Symposium on Computer Architecture* ,\npp. 138–150, June 1995",
			  "13] J. C. Dehnert, P. Y. Hsu, and J. P. Bratt, “Overlapped loop support in the Cydra 5,” in *Proceedings of the Third*\n*International Conference on Architectural Support for Programming Languages and Operating Systems* , pp. 26–38,\nApril 1989"
			]
		  }
		],
		"reasoning": "The user is asking for control flow optimization techniques. The provided excerpts discuss several techniques that fit this category:\n\n**Trace Scheduling:** This technique aims to improve Instruction-Level Parallelism (ILP) by optimizing across basic block boundaries. It identifies a frequent execution path (a \"trace\") and schedules it as a single unit, allowing instructions to be moved past their original basic block boundaries. The primary mechanism involves identifying these traces and then compacting them, with compensation code inserted to handle deviations from the predicted path. A key trade-off is the potential for code bloat due to this compensation code.\n\n**Superblock Scheduling:** This is presented as a refinement of trace scheduling that simplifies compiler bookkeeping. It achieves this through \"tail duplication,\" where side-entries into a trace are eliminated by duplicating code. This upfront generation of compensation code makes it more manageable than trace scheduling but can lead to significant code bloat.\n\n**Hyperblock Formation:** This technique handles irregular control flow by creating a single scheduling region that merges multiple execution paths. It uses \"if-conversion\" and predicated execution to transform control dependencies into data dependencies, allowing a larger set of instructions to be presented to the scheduler simultaneously. This requires explicit hardware support for predicated execution, which is a key trade-off.\n\n**Predicated Execution (If-Conversion):** This is both an architectural feature and a compiler transformation. It allows instructions to be executed conditionally based on a \"predicate.\" The compiler uses this to convert conditional branches into predicated instructions, effectively creating straight-line code that is easier to schedule. The primary mechanism involves compare instructions setting predicate registers, which then guard subsequent instructions. The trade-offs include the need for hardware support, potential saturation of machine resources with nullified instructions, and increased \"predicate pressure.\"",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.1",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5).",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  }
		],
		"reasoning": "The provided value for 'Superblock Scheduling' describes it as a refinement of trace scheduling that simplifies compiler bookkeeping and code generation by using tail duplication to eliminate joins. It also notes that superblocks have a single entry point but multiple exit points, making them a structured unit for scheduling. The trade-offs mentioned include potential code bloat due to tail duplication and less effectiveness with irregular control flow. Excerpts that discuss superblocks as a mechanism for trace scheduling, their definition involving tail duplication, and their relation to trace scheduling are relevant. Specifically, excerpts mentioning superblocks as a mechanism for trace scheduling, their definition involving tail duplication, and their benefit in avoiding complex bookkeeping are directly supportive. Other excerpts discuss trace scheduling in general, which is closely related, and some mention superblocks in the context of VLIW compilation. Excerpts that define a superblock as a trace with joins removed via tail duplication (0, 2) and discuss the benefit of avoiding complex bookkeeping (1) are highly relevant. Excerpts that mention superblocks in the context of trace scheduling (7, 300, 301, 304) and their relation to VLIW compilation (297, 304) are also relevant. Excerpts discussing the trade-offs, such as code size increase due to tail duplication (302) and limitations with irregular control flow (303), further support the description.",
		"confidence": "high"
	  },
	  {
		"field": "empirical_case_studies_summary",
		"citations": [
		  {
			"title": "Field-testing IMPACT EPIC Research Results in Itanium 2.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/field2004.pdf",
			"excerpts": [
			  "Using the Intel Itanium 2 microproces-*\n*sor, the SPECint2000 benchmarks and the IMPACT Com-*\n*piler for IA-64, a research compiler competitive with the*\n*best commercial compilers on the platform, we provide*\n*an* in situ *evaluation of code generated using aggres-*\n*sive, EPIC-enabled techniques in a reality-constrained mi",
			  "We have demonstrated how IMPACT’s “structural” ap-\nproach to compilation for Itanium 2 provides a 1.13 av-\nerage speedup (including single-benchmark speedups of\nup to 1.50) relative to traditional optimization of code at\nthe same inlining and interprocedural analysis levels and\na 1.55 speedup relative to ",
			  "With ILP transformations,\nIMPACT achieves an average speedup of 1.55 (maximum\n2.30) relative to GCC.",
			  "The processor is in-order\nand provides no register renaming.",
			  "rformance in control-intensive, general-purpose pro-\ngrams, therefore, depends heavily on the compiler’s ILP-\nexposing transformations, which IA-64 supports with pred-\nication, explicit control speculation, data speculation, and\nmodulo scheduling ai"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.Our work shows a 1.13 average speedup(up to 1.50) due to these compilation techniques, relativeto traditionally-optimized code at the same inlining andpointer analysis levels, and a 1.55 speedup (up to 2.30) relativeto GNU GCC, a solid traditional compiler.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs",
			  "We also demonstratethe increasing significance of runtime effects, such as datacache and TLB, in determining end performance and theinteraction of these effects with control speculation.",
			  "ALAT",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface.",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "ISCA 2004Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including\npredication and explicitcontrol speculation, intended to enhance the compiler'sability\nto expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Itanium Performance Insights from the IMPACT Compiler",
			"url": "http://impact.crhc.illinois.edu/shared/talks/hotchips-2001-final-color.pdf",
			"excerpts": [
			  "SPECcpu2000 Ratio Comparison",
			  "Dual processor HP i2000 Itanium 800 MHz with 2MB L3 cache",
			  "linux kernel 2.4.7 with PMC patch, gcc 2.9.6, ecc 5.0.1 beta",
			  "Itanium design goal: enhance scalability of parallelism by\nmoving complex decisions to the compiler",
			  "Bundling: enables static scheduling by communicating instruction\nparallelism",
			  "Predication: allows compiler to optimize across multiple paths by\nproviding an alternative to control flow",
			  "Speculation support: allows compiler to select specific instructions\nfor early execution",
			  "Static scheduling, control speculation, predication"
			]
		  },
		  {
			"title": "SMT/VLIW/EPIC, Statically Scheduled ILP",
			"url": "http://class.ece.iastate.edu/tyagi/cpre581/lectures/LecVLIWEPIC.pdf",
			"excerpts": [
			  "63\nIntel EPIC IA-64\nEPIC is the style of architecture (CISC, RISC)\n Explicitly Parallel Instruction Computing\nIA-64 is Intel ’ s chosen ISA (x86, MIPS)\n IA-64 = Intel Architecture 64-bit\n An object-code compatible VLIW\nItanium (aka Merced) is first implementation (\n First customer shipment were expected 1997\n(actual 2001)\n McKinley, second implementation shipped in 2002\n64",
			  "urn\nlate\n63\nIntel EPIC IA-64\nEPIC is the style of architecture (CISC, RISC)\n Explicitly Parallel Instruction Computing\nIA-64 is Intel ’ s chosen ISA (x86, MIPS)\n IA-64 = Intel Architecture 64-bit\n An object-code compatible VLIW\nItanium (aka Merced) is first implementation (\n First customer shipment were expected 1997\n(actual 2001)\n McKinley, second implementation shipped in 2002\n64\nIA-64 Inst"
			]
		  },
		  {
			"title": "chapter 15 - the ia-64 architecture",
			"url": "https://www.cs.helsinki.fi/u/kerola/tikra/ch15.pdf",
			"excerpts": [
			  "ntel and HP refer to this combination of concepts as explicitly parallel instruc-\ntion computing (EPIC)."
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "The IA-64 instruction set architecture is a new approach to providing\nhardware support for instruction-level parallelism and is significantly\ndifferent than the approach taken in superscalar architectures.",
			  "With the Pentium 4, the microprocessor family that began with the 8086 and that\nhas been the most successful computer product line ever appears to have come\nto an end. Intel has teamed up with Hewlett-Packard (HP) to develop a new\n64-bit architecture, called IA-64. IA-64 is not a 64-bit extension of Intel’s 32-bit\nx86 architecture, nor is it an adaptation of Hewlett-Packard’s 64-bit PA-RISC\narchitecture. Instead, IA-64 is a new architecture that builds on years of research\nat the two companies and at universities. The architecture exploits the vast cir-\ncuitry and high speeds available on the newest generations of microchips by a\nsystematic use of parallelism. IA-64 architecture represents a significant depar-\nture from the trend to superscalar schemes that have dominated recent proces-\nsor development.",
			  "Intel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC).",
			  "tanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.",
			  " architecture. Next, we look at the general organization to support the architecture.We\nthen examine in some detail the key features of the IA-64 architecture that promote\ninstruction-level parallelism. Finally, we look at the IA-64 instruction set architecture\nand the Itanium organization.\n**21.1 / MOTIVATION** **21-3**\n**21.1 MOTIVATION**\nThe basic concepts underlying IA-64 are:\n• Instruction-level parallelism that is explicit in the machine instructions rather\nthan being determined at run time by the processor\n• Long or very long instruction words (LIW/VLIW)\n• Branch predication (not the same thing as branch prediction)\n• Speculative loading\nIntel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC). Intel and HP use the term **EPIC** to refer to the tech-\nnology, or collection of techniques. **IA-64** is an actual instruction set architecture\nthat is intended for implementation using the EPIC technology.The first Intel prod-\nuct based on this architecture is referred to as **Itanium** . Other products will follow,\nbased on the same IA-64 architecture.\nTable 21.1 summarizes key differences between IA-64 and a traditional super-\nscalar approach.\nFor Intel,the move to a new architecture that is not hardware compatible with the\nx86 instruction architecture was a momentous decision.But it was driven by the dictates\nof the technology.When the x86 family began, back in the late 1970s, the processor chip\n ... \nbe restored through successive returns. This is a function of the alloc instruction,\nwhich designates a general register to save the current value of the PFS fields before\nthey are overwritten from the CFM fields.\n**Current Frame Marker and Previous Function State**\nThe CFM register describes the state of the current general register stack frame, as-\nsociated with the currently active procedure. It includes the following fields:\n• **sof:** size of stack frame\n• **sol:** size of locals portion of stack frame\n• **sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining\n• **register rename base values:** Values used in performing register rotation gen-\neral, floating-point and predicate registers\nThe PFS application register contains the following fields:\n• **pfm:** Previous frame marker; contains all of the fields of the CFM\n• **pec:** Previous epilog count\n• **ppl:** Previous privilege level\n**21.5 ITANIUM ORGANIZATION**\nIntel’s Itanium processor is the first implementation of the IA-64 instruction set ar-\nchitecture.The first version of this implementation, known as Itanium, was released\nin 2001, followed in 2002 by the Itanium 2. The Itanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.Among the\nsuperscalar features are a six-wide, ten-stage-deep hardware pipeline, dynamic\nprefetch, branch prediction, and a register scoreboard to optimize for compile time\nnondeterminism. EPIC related hardware includes support for predicated execution,\ncontrol and data speculation, and software pipelining.\nFigure 21.10 is a general block diagram of the Itanium organization. The Ita-\nnium includes nine execution units: two integer, two floating-point, four memory,\nand three branch execution units. Instructions are fetched through an L1 instruction\ncache and fed into a buffer that holds up to eight bundles of instructions. When de-\nciding on functional units for instruction dispersal, the processor views at most two\ninstruction bundles at a time.The processor can issue a maximum of six instructions\nper clock cycle.\nThe organization is in some ways simpler than a conventional contemporary\nsuperscalar organization. The Itanium does not use reservation stations, reorder\nbuffers, and memory ordering buffers, all replaced by simpler hardware for specula-\ntion.The register remapping hardware is simpler than the register aliasing typical of\nsuperscalar machines. Register dependency-detection logic is absent, replaced by\nexplicit parallelism directives precomputed by the software.\n**21.5 / ITANIUM ORGANIZATION** **21-29**\nUsing branch prediction, the fetch/prefetch engine can speculatively load\nan L1 instruction cache to minimize cache misses on instruction fetches. The\nfetched code is fed into a decoupling buffer that can hold up to eight "
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531901.pdf",
			"excerpts": [
			  "The value of the *ldhint* completer specifies the locality of the memory access."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol2.pdf",
			"excerpts": [
			  "ALAT",
			  "ALAT",
			  "ALAT",
			  "ALAT"
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "Itanium 2 Architecture",
			  "EPIC (Explicitly Parallel Instruction Computing)",
			  "VLIW-like architecture except for memory operations\n(out-of-order memory accesses)\nIPC up to 6 (Instructions Per Cycle)",
			  "VLIW-like architecture except for memory operations",
			  "\nItanium 2 Architecture"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol4.pdf",
			"excerpts": [
			  "IA-64 architecture features a revolutionary 64-bit instruction set architecture (ISA) which applies a\nnew processor architecture technology called EPIC, or Explicitly Parallel Instruction Computing."
			]
		  },
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "Itanium architecture incorporates features which enable high sustained performance\nand remove barriers to further performance increases. The Itanium architecture is\nbased on the following principles:",
			  "Memory latency is a major factor in determining the performance of integer\napplications. In order to help reduce the effects of memory latency, the Itanium\narchitecture explicitly supports software pipelining, large register files, and\ncompiler-controlled speculation.",
			  "• Features that enhance instruction level parallelism",
			  "The Itanium architecture allows issuing of independent instructions in\nbundles (three instructions per bundle) for parallel execution and can issue multiple\nbundles per clock.",
			  "The Itanium\narchitecture enables the compiler to exploit speculative information without sacrificing\nthe correct execution of an application (see “Speculation” on page 1:16 ).",
			  "A key feature of the Itanium architecture is IA-32 instruction set compatibility. The Intel® Itanium® Architecture Software Developer's Manual ...Read more"
			]
		  },
		  {
			"title": "Predication and Speculation",
			"url": "https://www.cs.utexas.edu/~lin/cs380c/handout25.pdf",
			"excerpts": [
			  "**VLIW and IA-64**",
			  "Explicitly Parallel Instruction Sets: IA-64",
			  "Itanium was the first IA64 implementation",
			  "**IA-64 Design Philosophy**",
			  "**IA-64 Design Philosophy**",
			  "**IA-64 Design Philosophy**",
			  "EPIC: Explicitly Parallel Instruction Computer",
			  "EPIC: Explicitly Parallel Instruction Computer",
			  "5\nPredication and Speculation"
			]
		  },
		  {
			"title": "A Constraint Programming Approach for Instruction Assignment",
			"url": "https://cs.uwaterloo.ca/~vanbeek/Publications/interact11.pdf",
			"excerpts": [
			  "The novelty of our approach lies in the decomposi-\ntion of the problem and our improvements to the constraint model,\nwhich reduces the effort needed in the search for the optimal so-\nlution."
			]
		  },
		  {
			"title": "The most prominent example of VLIW processors was ...",
			"url": "https://news.ycombinator.com/item?id=38676884",
			"excerpts": [
			  "The most prominent example of VLIW processors was the Itanic, er, Itanium. It, er, didn't work out well. Hence Itanic. Their premise was that the compiler ...Read more"
			]
		  },
		  {
			"title": "Microarchitectural Techniques for Instruction-driven Specialization ...",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-210.pdf",
			"excerpts": [
			  "The most ambitious attempt to apply VLIW to general-purpose workloads was Intel’s\nItanium, based on the Explicitly Parallel Instruction Computing (EPIC) model and the IA-\n64 instruction set",
			  "EPIC was an evolution of the VLIW concept—drawing on ideas from the\nearlier Cydrome and TRACE architectures—while adding ISA-level features to maximize\ngeneral-purpose ILP and software compatibility."
			]
		  },
		  {
			"title": "Restoring the Broken Covenant Between Compilers and ...",
			"url": "https://arxiv.org/pdf/2310.17912",
			"excerpts": [
			  "HVXincorporatesascalarcorethatsupports\na diverse set of scalar instructions (Add, Mul, MAC, Max, etc.)\nand uses a General Register File (GRF) for operand read/write.\nInadditiontothescalarcore,HVXincludesanadditionalSIMD\nprocessor for vector instructions, with 32 lanes each capable\nof performing a range of four 8-bit operations to a single 32-\nbit operation per lane."
			]
		  },
		  {
			"title": "Branch Predication | Request PDF",
			"url": "https://www.researchgate.net/publication/299748687_Branch_Predication",
			"excerpts": [
			  "The IA-64 architecture is rich with features that enable aggressive exploitation of instruction-level parallelism. Features such as speculation, predication, multiway branches and others provide compilers with new opportunities for the extraction of parallelism in programs.",
			  "This paper describes the implementation of the global code scheduler (GCS) in Intel's reference compiler for the IA-64 architecture. GCS schedules code over acyclic regions of control flow.",
			  "There is a tight coupling between the formation and scheduling of regions.",
			  "This paper also presents P-ready code motion, an opportunistic instruction level tail duplication which aims to strike a balance between code expansion and performance potential.",
			  "Performance results show greater than 30% improvement in speedup for wavefront scheduling over basic block scheduling on the Merced microarchitecture"
			]
		  },
		  {
			"title": "Evolutionary Algorithms for Instruction Scheduling, Operation Merging, and Register Allocation in VLIW Compilers - ADS",
			"url": "http://ui.adsabs.harvard.edu/abs/2020JSPSy..92..655G/abstract",
			"excerpts": [
			  "Code generation for VLIW processors includes several optimization problems like code optimization, instruction scheduling, and register allocation.",
			  "The high complexity of these problems usually does not allow the computation of the optimal solution.",
			  "List scheduling is a well known heuristic-based microcode compaction method, that bases its scheduling decisions on weights derived from dependency analysis of the input program.",
			  "In this paper, evolutionary algorithms are used as dynamic heuristics in code generation, which allows dynamic adaption to the given input program and target processor configuration.",
			  "Three evolutionary algorithms for operation merging, instruction scheduling, and register allocation are presented and evaluated on an exemplary image processing application, which shows different processing characteristics in the subroutines.",
			  "They outperform code generation based on static heuristics and allow compilation for restricted target architectures that cannot be handled by the static heuristics."
			]
		  },
		  {
			"title": "Chapter 24. Using Lookup Tables to Accelerate Color ...",
			"url": "https://developer.nvidia.com/gpugems/gpugems2/part-iii-high-quality-rendering/chapter-24-using-lookup-tables-accelerate-color",
			"excerpts": [
			  "In this chapter, we present an algorithm that leverages three-dimensional lookup tables to enable the real-time color processing of high-resolution imagery.Read more"
			]
		  },
		  {
			"title": "optimization techniques of the rich and famous :: pouët.net",
			"url": "https://www.pouet.net/topic.php?which=12893&page=2",
			"excerpts": [
			  "Operations on four 8-bit numbers using a single 32-bit instruction, bit rotation shuffles, etc. - it’s like solving a puzzle in the minimum number of steps :)",
			  "litting the render into “tiles”: for example, pseudo-3D spheres in Demoded are rendered in 16x16 pixel blocks, which makes cache hits more even, regardless of the angle the map is scanned from (otherwise FPS jumps from 20 to 60 depending on the sphere’s rotation).",
			  "Loop unrolling. I don’t know about Pentiums, but it still works on a 486.",
			  "Modifying the inner loop code at runtime (before entering the loop): for example, inserting numeric values directly into instructions in memory. This trick helps a lot, since registers are always limited, and accessing a variable in memory isn’t always convenient and is usually slower.",
			  "traverse stuff in a way to stay within the cache line. So if you move a bit in u or v, you want to hit same cache line.",
			  "Bitwise tricks, fixed-point arithmetic. And just little flashes of insight. That’s what makes coding in assembly worth it in the first place.",
			  "Bitwise tricks, fixed-point arithmetic. And just little flashes of insight. That’s what makes coding in assembly worth it in the first place."
			]
		  },
		  {
			"title": "Optimizing for the 8088 and 8086 CPU, Part 3: A Case Study In Speed « Oldskooler Ramblings",
			"url": "https://trixter.oldskool.org/2013/01/18/optimizing-for-the-8088-and-8086-cpu-part-3-a-case-study-in-speed/",
			"excerpts": [
			  "**Interleaved literal-match structure** : This is the innovative discovery of LZ4. Yann Collet (the author of LZ4) realized that all compressed LZ77 data consisted of both literal runs and match runs, so rather than try to devise complicated codes that branch based on run type, he organzied the format so that there are always literals, then matches, then literals, then matches, etc. repeating until the end of the compressed data. This eliminates a lot of literal/matchcode-based branching. There are really only three branches ever required for decompressing LZ4: Two length calculations (and only if they’re needed), and a check to see if we’re out of codes to expand. **Those three are the only major conditional branches!** This is REALLY HUGE and is why LZ4 is at [the top of the speed charts](http://mattmahoney.net/dc/text.html) for nearly every platform.",
			  "**64KB window size, 16-bit offsets** : A 64K window size means that decompressing LZ4 doesn’t need any special handling — if you only have 64K to decompress (likely on our platform), you don’t have to worry about switching segments after the first 64K. And a 16-bit offset is a single word read, no unpacking. It’s like LZ4 was specifically created for [Intel memory models](http://en.wikipedia.org/wiki/Intel_Memory_Model) , but it wasn’t — it was actually created in the very early 1990s for Yann’s HP48 calculator, which used a CPU based on nybbles. Which explains the code packing in the next advantage:",
			  "**Simple codes** : The literal lengths and match lengths are, if 14 or lower, stored as two nybbles in the same byte. That means both can be read with a single byte read and then cached as necessary. Even if the lengths are larger than 14, it only takes a few byte reads to get the full length. No wacky code unpacking like [LZS](http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Stac) or higher [LZO](http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer) levels.",
			  "ata analyzer.\nSection Title: Not So Obvious\nContent:\nOne of my favorite quotes from Zen of Assembly: “The best optimizer is between yours ears.” It alludes to dumb/blind optimization — what I was doing up until this point, working like a human peephole optimizer — being inferior to “smart” optimization, where you analyze the problem itself to come up with different ways to solve it.\nI ha",
			  "Not So Obvious\nContent:\nAfter I ran out of ideas looking at the analyzer output, on a hunch I looked a the LZ4 format description a second time and noticed that, by design, the last five bytes are always literals. The current structure of the code would check for this exit condition, then process matches, then jump to the next token and continue. By unrolling some code to move the “are we done?” test to the end of the loop instead of the middle, I was able to combine both the test and the loop jump into a single instruction. This saved cycles every iteration, leading to the second largest savings at 10946 usecs.",
			  "Looking at various outputs, I was struck by how many match offsets of 1 and 2 there were. Offsets that small, with lengths larger than the offsets, essentially translate into [RLE sequences](http://en.wikipedia.org/wiki/Run-length_encoding) . LZ77 code usually deals with this by just copying the same bytes over and over, but x86 architecture has a string opcode that can do this more efficiently: REP STOS. Put a value into AL or AX, a count into CX, and off you go, filling memory with that value until the count runs out. What is especially notable about STOS is that it is *faster than every other instruction for putting a value in memory* , faster than even a raw REP MOVSW copy. So if you can use it, you can exceed memcopy speeds for short sections. Adding special handling for offsets of 1 and 2 and redirecting them to special-case code that used STOS resulted in a larger-than-expected 32281 usec savings all by itself — that’s the largest single savings out of all the attempts — and it also resulted in a special surprise which I’ll cover later.",
			  "Section Title: The (de)Compression Method > The Current State Of The Art\nContent:\n**Interleaved literal-match structure** : This is the innovative discovery of LZ4. Yann Collet (the author of LZ4) realized that all compressed LZ77 data consisted of both literal runs and match runs, so rather than try to devise complicated codes that branch based on run type, he organzied the format so that there are always literals, then matches, then literals, then matches, etc. repeating until the end of the compressed data. This eliminates a lot of literal/matchcode-based branching. There are really only three branches ever required for decompressing LZ4: Two length calculations (and only if they’re needed), and a check to see if we’re out of codes to expand. **Those three are the only major conditional branches!** This is REALLY HUGE and is why LZ4 is at [the top of the speed charts](http://mattmahoney.net/dc/text.html) for nearly every platform.",
			  "Not So Obvious",
			  "Not So Obvious",
			  "After all of the above work (which incidentally took about ten hours of free time spread out over a few weeks), we managed to take an already fast routine and shave 95856 usecs from it — an 18% improvement from the already fast routine where we started.  Was 18% worth the effort? I definitely think so. It may not sound like much, but think about the savings like this: If a user would wait a full minute for decompression of a lot of data to finish, they now only have to wait 50 seconds. What was a 5-minute task now only takes 4 minutes.",
			  "Results",
			  "Results",
			  "For data that has a lot of 1-byte and 2-byte runs — runs that we specifically catch and handle with REP STOSW, as noted previously — we see that decompression *exceeds REP MOVSW copy time* . ***It is actually faster to decompress such data than it is to merely copy it*** . That is crazy."
			]
		  },
		  {
			"title": "Lookup Table Optimization - MATLAB & Simulink\n",
			"url": "https://www.mathworks.com/help/fixedpoint/ug/lookup-table-optimization.html",
			"excerpts": [
			  "A function lookup table is a method by which you can approximate a function using a table\nwith a finite number of points ( *X* , *Y* ). The *X* values of the lookup table are called the breakpoints. You approximate\nthe value of the ideal function at a point by interpolating between the two breakpoints\nclosest to the point. Because table lookups and simple estimations can be faster than\nmathematical function evaluations, using lookup table blocks often result in speed gains when\nsimulating a model.",
			  "Limit uneven lookup tables.Unevenly spaced breakpoints require a general-purpose algorithm such as a binary\nsearch to determine where the input lies in relation to the breakpoints. This additional\ncomputation increases ROM and execution time.",
			  "Prevent evenly spaced lookup tables from being treated as unevenly spaced.The position search in evenly spaced lookup tables is much faster. In addition, the\ninterpolation requires a simple division.",
			  "Use power of two spaced breakpoints in lookup tables.In power of two spaced lookup tables, a bit shift replaces the position search, and a\nbit mask replaces the interpolation making this construct the most efficient regardless of\nyour target language and hardware.",
			  "The following table summarizes the effects of lookup table breakpoint spacing.",
			  "Use the Model Advisor [Identify Questionable Fixed-Point Operations](optimizing-your-generated-code-with-the-model-advisor.html) check to\nidentify lookup table blocks where there is potential for efficiency improvements."
			]
		  },
		  {
			"title": "Lookup Table Optimization - New Feature for Embedded Efficient Designs - YouTube",
			"url": "https://www.youtube.com/watch?v=Qkxr_yfmaG8",
			"excerpts": [
			  "See a demonstration of the lookup table optimization capability in Fixed-Point Designer™. This capability enables you to approximate a function or an existing block with an optimal lookup table. You can also compress the lookup tables in your design to reduce the RAM memory usage.",
			  "Designing an optimal lookup table involves various choices: fixed-point or floating-point data, size of the table, evenly spaced data or uneven spacing, and other algorithm settings such as the interpolation method. All of these decisions are automated using the Lookup Table Optimizer.",
			  "Specialized Content > transcript",
			  "(time (seconds): 64.26) and select the lookup table block that (time (seconds): 66.48) you want to optimize (time (seconds): 71.27) you can collect the current memory usage (time (seconds): 74.1) of the block which is about 384 bytes in (time (seconds): 80.189) the next step you need to specify the (time (seconds): 82.469) absolute and relative tolerances for the (time (seconds): 84.869) output of the lookup table in practice (time (seconds): 87.36) this might be derived from the tolerance (time (seconds): 89.159) on other signals in your model and once (time (seconds): 91.59) you specify these tolerances you can run (time (seconds): 93.75) the optimization once optimisation is (time (seconds): 102.33) complete we get a compressed lookup (time (seconds): 104.369) table that needs just eight bytes of (time (seconds): 106.409) memory which constitutes a 98% reduction (time (seconds): 110.03) the data types in this case are fixed (time (seconds): 112.679) with 8-bit word length while the signals (time (seconds): 115.02) in my model are still double finally I (time (seconds): 120.959) can replace my original lookup table (time (seconds): 123.03) with this optimized version in the model (time (seconds): 126.38) you can also use the command line (time (seconds): 128.579) interface to automate this entire (time (seconds): 130.709) process please refer to the (time (seconds): 133.26) documentation link provided below to (time (seconds): 135.48)"
			]
		  },
		  {
			"title": "\nBit Twiddling Hacks\n",
			"url": "https://graphics.stanford.edu/~seander/bithacks.html",
			"excerpts": [
			  "Some CPUs don't have an integer absolute value instruction (or the\ncompiler fails to use them). On machines where branching is expensive,\nthe above expression can be faster than the obvious approach, r = (v < 0) ? -(unsigned)v : v, even though the number of operations is the same.On March 7, 2003, Angus Duggan pointed out that the 1989 ANSI C specification leaves the result of signed right-shift implementation-defined,\nso on some systems this hack might not work.",
			  "On some architectures, the lack of branching can more than make up for\nwhat appears to be twice as many operations. For instance, informal speed tests on an AMD Athlon™ XP 2100+ indicated it was 5-10% faster. An Intel Core 2 Duo ran the superscalar version about 16% faster than the first.",
			  "\nOn some architectures, the lack of branching can more than make up for\nwhat appears to be twice as many operations. For instance, informal speed tests on an AMD Athlon™ XP 2100+ indicated it was 5-10% faster. An Intel Core 2 Duo ran the superscalar version about 16% faster than the first.",
			  "If you need to negate only when a flag is false, then use the following to avoid branching:\nIf you need to negate only when a flag is true, then use this:\nAvraham Plotnitzky suggested I add the first version on June 2, 2009. Motivated to avoid the multiply, I came up with the second version on\nJune 8, 2009. Alfonso De Gregorio pointed out that some parens were\nmissing on November 26, 2009, and received a bug bounty.",
			  "The best bit counting method takes only 12 operations, which is the same as the lookup-table method, but avoids the memory and potential cache misses of a table. It is a hybrid between the purely parallel method above\nand the earlier methods using multiplies (in the section on counting bits with 64-bit instructions), though it doesn't use 64-bit instructions.",
			  "r = MultiplyDeBruijnBitPosition[((uint32_t)((v & -v) * 0x077CB531U)) >> 27];",
			  "unsigned int v;  // find the number of trailing zeros in 32-bit v\nint r;           // result goes here\nstatic const int MultiplyDeBruijnBitPosition[32] =\n{\n0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8,\n31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 1",
			  "Interleaving bits (aka computing *Morton Numbers* )",
			  "unsigned int v;     // 32-bit word input to count zero bits on right",
			  "int c;  // output: c will count v's trailing zero bits,",
			  "if (v)",
			  "for (c = 0; v; c++)",
			  "v >>= 1;",
			  "c = CHAR_BIT * sizeof(v);",
			  "v &= -signed(v);",
			  "if (v) c--;",
			  "if (v & 0x0000FFFF) c -= 16;",
			  "if (v & 0x00FF00FF) c -= 8;",
			  "if (v & 0x0F0F0F0F) c -= 4;",
			  "if (v & 0x33333333) c -= 2;",
			  "if (v & 0x55555555) c -= 1;",
			  "unsigned int c;     // c will be the number of zero bits on the right,",
			  "// so if v is 1101000 (base 2), then c will be 3",
			  "// NOTE: if 0 == v, then c = 31.",
			  "if (v & 0x1)",
			  "// special case for odd v (assumed to happen half of the time)",
			  "c = 0;",
			  "else",
			  "else",
			  "c = 1;",
			  "if ((v & 0xffff) == 0)",
			  "v >>= 16;",
			  "c += 16;",
			  "if ((v & 0xff) == 0)",
			  "v >>= 8;",
			  "c += 8;",
			  "if ((v & 0xf) == 0)",
			  "v >>= 4;",
			  "c += 4;",
			  "if ((v & 0x3) == 0)",
			  "v >>= 2;",
			  "c += 2;",
			  "c -= v & 0x1;",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "unsigned int v;            // find the number of trailing zeros in v",
			  "int r;                     // the result goes here",
			  "float f = (float)(v & -v); // cast the least significant bit in v to a float",
			  "r = (*(uint32_t *)&f >> 23) - 0x7f;",
			  "Section Title: Bit Twiddling Hacks > &                                                                                       1111 1111",
			  "Content:",
			  "unsigned int v;  // find the number of trailing zeros in v",
			  "int r;           // put the result in r",
			  "static const int Mod37BitPosition[] = // map a bit value mod 37 to its position",
			  "32, 0, 1, 26, 2, 23, 27, 0, 3, 16, 24, 30, 28, 11, 0, 13, 4,",
			  "7, 17, 0, 25, 22, 31, 15, 29, 10, 12, 6, 0, 21, 14, 9, 5,",
			  "20, 8, 19, 18",
			  "r = Mod37BitPosition[(-v & v) % 37];",
			  "int r;           // result goes here",
			  "static const int MultiplyDeBruijnBitPosition[32] =",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8,",
			  "31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9",
			  "};",
			  "};"
			]
		  },
		  {
			"title": "llvm/include/llvm/Target/TargetSchedule.td at master · llvm-mirror/llvm · GitHub",
			"url": "https://github.com/llvm-mirror/llvm/blob/master/include/llvm/Target/TargetSchedule.td",
			"excerpts": [
			  "}",
			  "}",
			  "}"
			]
		  },
		  {
			"title": "Qualcomm Hexagon DSP: An architecture optimized for ...",
			"url": "https://pages.cs.wisc.edu/~danav/pubs/qcom/hexagon_hotchips2013.pdf",
			"excerpts": [
			  "}",
			  "{",
			  "{",
			  "{",
			  "{"
			]
		  },
		  {
			"title": "The Forgotten Art of Binary Manipulation: 6 Bit-Twiddling Tricks That Will Make You Look Like a Wizard | by Sohail Saifi | Medium",
			"url": "https://medium.com/@sohail_saifi/the-forgotten-art-of-binary-manipulation-6-bit-twiddling-tricks-that-will-make-you-look-like-a-b7d46f7a4b67",
			"excerpts": [
			  "Counting the number of bits set to 1 in a value (known as the “population count” or “Hamming weight”) is a deceptively powerful operation with applications ranging from cryptography to machine learning.",
			  "This works because `num & (num - 1)` always clears the rightmost set bit. The loop runs exactly once for each set bit, rather than once for each bit position.",
			  "For fixed-width integers, there’s an even faster approach using the “SWAR” (SIMD Within A Register) technique:\nThis algorithm is mind-bending at first glance, but it works by essentially doing a parallel reduction. It counts bits in parallel within 2-bit fields, then 4-bit fields, then 8-bit fields, and so on.",
			  "The Forgotten Art of Binary Manipulation: 6 Bit-Twiddling Tricks That Will Make You Look Like a Wizard > Learning and Practicing Bit Manipulation",
			  "The naive approach iterates through each bit:",
			  "Hacker’s Delight” by Henry S. Warren Jr.** — The definitive reference on bit manipulation technique"
			]
		  },
		  {
			"title": "GitHub - ianbrayoni/bithacks: Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			"url": "https://github.com/ianbrayoni/bithacks",
			"excerpts": [
			  "Hats off to Sean Eron Anderson, [seander@cs.stanford.edu](mailto:seander@cs.stanford.edu) , for creating https://graphics.stanford.edu/~seander/bithacks.html",
			  "I **DO NOT** own any of this. The following contents comes directly from the above link.",
			  "Section Title: Acknowledgement",
			  "Section Title: Contributing > About",
			  "Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			  "Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			  "[graphics.stanford.edu/~seander/bithacks.html](https://graphics.stanford.edu/~seander/bithacks.html \"https://graphics.stanford.edu/~seander/bithacks.html\")",
			  "[graphics.stanford.edu/~seander/bithacks.html](https://graphics.stanford.edu/~seander/bithacks.html \"https://graphics.stanford.edu/~seander/bithacks.html\")"
			]
		  },
		  {
			"title": "GitHub - gibsjose/BitHacks: Markdown/PDF Versions of ...",
			"url": "https://github.com/gibsjose/BitHacks",
			"excerpts": [
			  "Sean Anderson's wonderful compilation of Bit Twiddling Hacks, converted to a more readable Markdown version and accompanying PDF.Read more"
			]
		  },
		  {
			"title": "Templatized branchless int max/min function - c++",
			"url": "https://stackoverflow.com/questions/514435/templatized-branchless-int-max-min-function",
			"excerpts": [
			  "I'm trying to write a branchless function to return the MAX or MIN of two integers without resorting to if (or ?:)."
			]
		  },
		  {
			"title": "How can I make branchless code and how does it work?",
			"url": "https://stackoverflow.com/questions/32107088/how-can-i-make-branchless-code-and-how-does-it-work",
			"excerpts": [
			  "I want to show you a more general (and much clearer) way to write branchless code. You can just use ? : operator: int t = data[c]; sum += (t >= 128 ? t : 0);"
			]
		  },
		  {
			"title": "Java Math.min/max performance - ...",
			"url": "https://stackoverflow.com/questions/22752198/java-math-min-max-performance",
			"excerpts": [
			  "Here's a branchless min operation, max can be implemented by replacing diff=a-b with diff=b-a . Copy. public static final long min(final long ..."
			]
		  },
		  {
			"title": "c++ max & min without if or else [closed]",
			"url": "https://stackoverflow.com/questions/72316958/c-max-min-without-if-or-else",
			"excerpts": [
			  "I want a way to cout << maximum & minimum of two numbers with cin >> without if() or else(). I tried before and I just found the if and else way."
			]
		  },
		  {
			"title": "Fast branchless max for unsigned integers - c++",
			"url": "https://stackoverflow.com/questions/17947718/fast-branchless-max-for-unsigned-integers",
			"excerpts": [
			  "I found a trick from the AGGREGATE Magic for fast computing max values. The only problem that this is for integers, and however I have tried some things."
			]
		  },
		  {
			"title": "Transform random integers into range [min,max] without ...",
			"url": "https://stackoverflow.com/questions/65561520/transform-random-integers-into-range-min-max-without-branching",
			"excerpts": [
			  "As far as I know there is no algorithm that generates a random integer in [0, max] when max is not of the form 2^n-1 from 01 uniform generator ..."
			]
		  },
		  {
			"title": "Bit Hacks",
			"url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cc6983c9ebd77c28e8ae85bc0e575360_MIT6_172F18_lec3.pdf",
			"excerpts": [
			  " You can access\nthem via compiler intrinsics, e.g., in GCC:\nint __builtin_popcount (unsigned int x);\nWarning: You may need to enable certain\ncompiler switches to access built-in functions,\nand your code may be less portable.\nExercise\nCompute the log base 2 of a power of 2 quickly\nusing a popcount instruction.\n© 2008–2018 by the MIT 6.172 Lecturers\n68\nFurther Readin",
			  "\nFurther Reading\nSean Eron Anderson, “Bit twiddling hacks,”\nhttp://graphics.stanford.edu/~seander/bithacks.h\ntml , 2009.\nDon",
			  "int __builtin_popcount (unsigned int x);",
			  "ttp://graphics.stanford.edu/~seander/bithacks.h\ntml",
			  "68",
			  "MIT OpenCourseWare",
			  "6.172 Performance Engineering of Software Systems",
			  "Fall 2018"
			]
		  },
		  {
			"title": ".net - Branchless min/max methods in c# - Stack Overflow",
			"url": "https://stackoverflow.com/questions/66605056/branchless-min-max-methods-in-c-sharp",
			"excerpts": [
			  "Just an FYI for anyone thinking of implementing some \"speedhack\" like this... The runtime and compiler are obviously already aware of optimizations like this and have already applied them for you. You don't need to reinvent the wheel. Quick benchmarks show only miniscule differences between the \"fast\" branchless method, naive `x > y ? x : y` , and `Math.Max()` ."
			]
		  },
		  {
			"title": ".net - Branchless min/max methods in c# - Stack Overflow",
			"url": "https://stackoverflow.com/a/66850544",
			"excerpts": [
			  "You can use bitwise and shift operators like this:\nThe `>>` operator is right shift and I use 31 for `Int` you can use 63 for `long` numbers.",
			  "If you're using the upcoming .NET 7, you can make `Min` and `Max` generic like this:",
			  "However, even with `AgressiveInlining` in IL this causes two calls to `ToInt32` so isn't really more efficient.",
			  "You can see that it doesn't produce any branching instructions: [SharpLab link](https://sharplab.io/:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAJgEYBYAKGIAYACY8gOgCUBXAOwwEt8YLAMIR8AB14AbGFADKMgG68wMXAG4a9BrIAW2KGIAy2YO258BG6pvJIGvHgwCyDgBQOMDbGnuPgASgYAXgA+BgBJADNXbAYAHgZgH29E/ysbOw9nbAR3RxSsgOCwqJj4xJ8krzSaGgBtACleDABxGC4ZZVcMAE8xGAhoj39/AF0MhgAVCMi4yZDXYAgISQZILgATZt4ILh9pjB12/aZiGElcGH8aAG8aAEhiAHYGAFUuXGxIwQBBDY3XLBIqdzpcfO9Pt8WD9cHElitKj0MDAFkC1rstnxdiMrABfWq0Wy+TwuLikPKeAp+QKhGbk2IJKopALpQmZRxOHLkrJUzxFWlRenlKpMmrWaiNZptDpQLq9fqDCkjcZsqZ0uYLeGrdaYnZ7NWHY5q4hnC5XW4PZ7ozbbXYMAD8DENXAYIBBZrxQA===)",
			  "You can see that it doesn't produce any branching instructions: [SharpLab link](https://sharplab.io/:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAJgEYBYAKGIAYACY8gOgCUBXAOwwEt8YLAMIR8AB14AbGFADKMgG68wMXAG4a9BrIAW2KGIAy2YO258BG6pvJIGvHgwCyDgBQOMDbGnuPgASgYAXgA+BgBJADNXbAYAHgZgH29E/ysbOw9nbAR3RxSsgOCwqJj4xJ8krzSaGgBtACleDABxGC4ZZVcMAE8xGAhoj39/AF0MhgAVCMi4yZDXYAgISQZILgATZt4ILh9pjB12/aZiGElcGH8aAG8aAEhiAHYGAFUuXGxIwQBBDY3XLBIqdzpcfO9Pt8WD9cHElitKj0MDAFkC1rstnxdiMrABfWq0Wy+TwuLikPKeAp+QKhGbk2IJKopALpQmZRxOHLkrJUzxFWlRenlKpMmrWaiNZptDpQLq9fqDCkjcZsqZ0uYLeGrdaYnZ7NWHY5q4hnC5XW4PZ7ozbbXYMAD8DENXAYIBBZrxQA===)"
			]
		  },
		  {
			"title": "c++ - Is there a branchless method to quickly find the min/max of two double-precision floating-point values? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/55109204/is-there-a-branchless-method-to-quickly-find-the-min-max-of-two-double-precision",
			"excerpts": [
			  "Is there a branchless method to quickly find the min/max of two double-precision floating-point values?",
			  "All 64-bit x86 processors support [SSE2](https://en.wikipedia.org/wiki/SSE2) ; it is required by the AMD64 extensions. Even most x86 processors without 64-bit support SSE2. It was released in 2000. You'd have to go back a long way to find a processor that didn't support SSE2. But what about if you did? Well, even there, [you get branchless code on most popular",
			  "The `fucomi` instruction performs a comparison, setting flags, and then the `fcmovnbe` instruction performs a conditional move, based on the value of those flags. This is all completely branchless, and relies on instructions introduced to the x86 ISA with the Pentium Pro back in 1995, supported on all x86 chips since the Pentium II.",
			  "I.\nThe only compiler that *won't* generate branchless code here is MSVC, because [it doesn't take advantage of the `FCMOVxx` instruction](https://stackoverflow.com/questions/13661285/generating-cmov-instructions-using-microsoft-compilers/41144749) . Instead, you get:\nNotice the branching `JP` instruction (jump if parity bit set). The `FCOM` instruction is used to do the comparison, which is part of the base x87 FPU instruction set. Unfortunately, this sets flags in the FPU status word, so in order to branch on those flags, they need to be extracted. That's the purpose of the `FNSTSW` instruction, which stores the x87 FPU status word to the general-purpose `AX` register (it could also store to memory, but…why?). The code then `TEST` s the appropriate bit, and branches accordingly to ensure that the correct value is returned. In addition to the branch, retrieving the FPU status word will also be relatively slow. This is why the Pentium Pro introduced the `FCOM` instructions.\nSect",
			  "However, it is *unlikely* that you would be able to improve upon the speed of any of this code by using bit-twiddling operations to determine min/max. There are two basic reasons:"
			]
		  },
		  {
			"title": "c - What is the instruction that gives branchless FP min and max on x86? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/40196817/what-is-the-instruction-that-gives-branchless-fp-min-and-max-on-x86",
			"excerpts": [
			  "Since modern floating-point instruction sets can compute min and max without branches",
			  "I was referring to `minss` / `minsd` ",
			  "gcc` and `clang` will both optimize `(a < b) ? a : b` to `minss` / `minsd` , so I don't bother using intrinsics.",
			  "If you want to use `_mm_min_ss` / `_mm_min_ps` yourself, write code that lets the compiler make good asm even without -ffast-math.",
			  "*Don't try to use `_mm_min_ss` on scalar floats; the intrinsic is only available with `__m128` operands",
			  "So with only SSE2, and we've done a conditional MINPS in two extra instructions (ORPS and MOVAPS, unless loop unrolling allows the MOVAPS to disappear).",
			  "The alternative without SSE4.1 BLENDVPS is ANDPS/ANDNPS/ORPS to blend, plus an extra MOVAPS. ORPS is more efficient than BLENDVPS anyway (it's 2 uops on most CPUs).",
			  "Note that min of an array should use vectors, not scalar; preferably with multiple accumulators to hide FP latency."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/distance/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene, shaders and more."
			]
		  },
		  {
			"title": "computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/mandelbulb/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene, shaders and more."
			]
		  },
		  {
			"title": "Population Count - Chessprogramming wiki",
			"url": "https://www.chessprogramming.org/Population_Count",
			"excerpts": [
			  "[Count bits set in parallel a.k.a.\n ... \nSection Title: References\nCo",
			  "Ramanathan, [Intel](/Intel \"Intel\") , covers SSE4 and popcnt",
			  "[↑]() [__builtin_popcountll](https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html) [GCC](/Free_Software_Foundation \"Free Software Foundation\") Intrinsic\n[↑](",
			  "[Counting bits set](http://www-graphics.stanford.edu/~seander/bithacks.html) from [Bit Twiddling Hacks](http://graphics.stanford.edu/%7Eseander/bithacks.html) by [Sean Eron Anderson](http://graphics.stanford.edu/%7Eseander/)",
			  "[Addison-Wesley](https://en.wikipedia.org/wiki/Addison%E2%80%93Wesley)"
			]
		  },
		  {
			"title": "C Math Hacks : r/cprogramming",
			"url": "https://www.reddit.com/r/cprogramming/comments/1ao9q2y/c_math_hacks/",
			"excerpts": [
			  "Bit Twiddling Hacks by Sean Eron Anderson: https://graphics.stanford.edu/~seander/bithacks.html",
			  "The Art of Computer Programming vol 4 §7.1.",
			  "Note that some of the techniques you’ll see online are useful on older computers but not newer ones. J",
			  "Just beware that some of the really “clever” tricks are sometimes beat by more simple, straightforward techniques."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/",
			"excerpts": [
			  "Section Title: [Inigo Quilez](/)   ::     :: > Indices of useful functions > Renderer/En",
			  "[Avoiding trigonometry III](noatan)",
			  "[Avoiding trigonometry II](sincos)",
			  "[Avoiding trigonometry I](noacos)",
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene"
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/noatan/",
			"excerpts": [
			  "This is another example of avoiding trigonometry that came up the other day. I recommend reading [Pat I](../noacos) and [Part II](../sincos) of this miniseries where I propose that using trigonometric functions (sin, cos, tan) or their inverses (asin, acos, atan) in any geometry code or rendering system is always a code smell.",
			  "This time, we’ll look at a challenge that got sent my way when I claimed one can always avoid the rather expensive atan()/atan2() function. The problem is that of snapping a 2D unit vector to the closest 45 degree angle (N, NE, E, SE, S, SW, W, NW). The claim was that you do need to commute the angle of such a vector, round it to the closest 45 degree angle, then reconstruct the vector.",
			  ". Something like this perhaps:\nvec2 snap45( in vec2 v )\n{ const float r = 6.283185 / 8.0 ; // 45 degrees float a = atan (v.y,v.x); // vector to angle float s = r* round (a/r); // snap angle return vec2 ( cos (s), sin (s)); // angle to vector }",
			  "This seems reasonable, and it does work - on the right side you can see how as the vector v in white moves, the yellow vector snaps to the closest 45 degrees direction. The triangular sectors are the basins of attraction of each direction. All good. But my claim was that surely you wouldn’t need trigonometry to achieve the same behavior, and that if this code happened to be in a perf critical path, you’d want to do better."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/",
			"excerpts": [
			  "Section Title: [Inigo Quilez](/) :: :: > Learning\nContent:\n[articles Articles, tutorials, code, and presentations related to computer graphics, maths, fractals, etc.](articles) [video tutorials Video tutorials and live coding sessions (some more than a decade old!), on using maths to create images.](live) [shaders Code related to my articles, many undocumented algorithms and techniques, and most of my procedural artwork and mathematical paintings.](https://www.shadertoy.com/user/iq) [recreational math Often times I find myself playing with and exploring random math ideas or facts. Here are some of them.](maths) [code Code for demos, libraries and utilities that I use, that are not hosted in Shadertoy.](code)",
			  "Section Title: [Inigo Quilez](/) :: :: > Learning > Tools, Products and Projects\nContent:\n[shadertoy A website that I co-founded with Pol Jeremias where you can create, share and learn realtime computer graphics.](https://www.shadertoy.com) [graphtoy A tool for graphing and designing mathematical functions, useful during procedural programming.](https://graphtoy.com) [human shader A collaborative project where 2000 people completed the first ever human brain powered mathematical painting!](https://humanshader.com) [demoscene Archive of all the demoscene pieces I created from 1998 to 2021, mostly featuring realtime rendering and procedural content.](demoscene) [quill I'm the current developer of \"Quill\", the VR painting and hand animation tool responsible for a few award winning VR short films.](https://quill.art/) [project Neo A SDF based 2D and 3D design tool we built at Adobe.](https://projectneo.adobe.com/)"
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/noacos/",
			"excerpts": [
			  "I think we should use less trigonometry in computer graphics. As my understanding of projections, reflections, and vector operations improved over time, I experienced a growing unease every time I saw trigonometry at the core of 3D algorithms inside rendering engines. These days I'm at a point where if I see some asin, acos, atan, sin, cos or tan in the middle of a 3D algorithm, I'll assume the code was written by an inexperienced programmer and that it needs review.",
			  "Now, don't get me wrong. Trigonometry **is** convenient and necessary for data input and for feeding the larger algorithm. What's wrong is when angles and trigonometry suddenly emerge deep in the internals of a 3D engine or algorithm out of nowhere. In other words, where the inputs and the outputs to an algorithm of function are vectors and geometry, and suddenly in the middle of the implementation trigonometry appears. Because, most of the time (if not all of the time) such a thing is unnecessarily complicated, error prone and overall unnecessary. It also gets in the way of elegance and truth, if that matters to you. Let me explain.",
			  "mat3x3 rotationAlign( const vec3 & d, const vec3 & z )\n{ const vec3 ax = cross ( z, d ); const float co = do t( z, d ); const float si = length ( ax ); const vec3 v = ax/si; const float ic = 1.0f - co; return mat3x3 ( v.x*v.x*ic + co, v.y*v.x*ic - si*v.z, v.z*v.x*ic + si*v.y,\nv.x*v.y*ic + si*v.z, v.y*v.y*ic + co, v.z*v.y*ic - si*v.x,\nv.x*v.z*ic - si*v.y, v.y*v.z*ic + si*v.x, v.z*v.z*ic + co );\n}\nGood, ",
			  "Good, no **normalize** (), no **acos** (), no **clamp** () anymore. We are definitely getting somewhere now. Let's keep going and also get rid of the length/square root operation by noticing that **si** 2 =1- **co** 2 (since sine and cosine lay on the unit circle) and by propagating the 1/si term into the matrix and getting some cancellations. What we get is:\nmat3x3 rotationAlign( const vec3 & d, const vec3 & z )\n{ const vec3 v = cross ( z, d ); const float c = dot ( z, d ); const float k = ( 1.0f -c)/( 1.0f -c*c); return mat3x3 ( v.x*v.x*k + c, v.y*v.x*k - v.z, v.z*v.x*k + v.y,\nv.x*v.y*k + v.z, v.y*v.y*k + c, v.z*v.y*k - v.x,\nv.x*v.z*K - v.y, v.y*v.z*k + v.x, v.z*v.z*k + c );\n}"
			]
		  },
		  {
			"title": "8088 Domination Post-Mortem, Conclusion",
			"url": "https://trixter.oldskool.org/2014/06/20/8088-domination-post-mortem-conclusion/",
			"excerpts": [
			  "The major factor in optimizing 8088 code for speed is minimizing memory accesses, because the CPU takes 4 cycles to read (or write) a byte — any ...Read more"
			]
		  },
		  {
			"title": "BitScan",
			"url": "https://www.chessprogramming.org/BitScan",
			"excerpts": [
			  "A 64-bit De Bruijn Sequence contains 64-overlapped unique 6-bit sequences, thus a circle of 64 bits, where five leading zeros overlap five hidden \"trailing\" ..."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/blog/page_201409.php",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, demoscene, shaders and more."
			]
		  },
		  {
			"title": "SSE for CPU tracers",
			"url": "https://iquilezles.org/articles/sse/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, demoscene, shaders and more."
			]
		  },
		  {
			"title": "GitHub - jdbruijn/bit-twiddling-hacks: Implementation of Sean Anderson's Bit Twiddling Hacks",
			"url": "https://github.com/jdbruijn/bit-twiddling-hacks",
			"excerpts": [
			  "Documentation can be found in the [documentation page](http://vidavidorra.github.io/BitOperations/html/index.html) , and for the\nfunction macros and functions on the [BitOperations.h documentation page](http://vidavidorra.github.io/BitOperations/html/BitOperations_8h.html) .",
			  "For the functions that are based on Sean Eron Anderson's [Bit Twiddling Hacks](https://graphics.stanford.edu/~seander/bithacks.html) the arithmetics are explained on Joe Gibson's [markdown version](https://github.com/gibsjose/BitHacks/blob/master/BitHacks.md) of\nSean Eron Anderson's Bit Twiddling Hacks."
			]
		  },
		  {
			"title": "Bit Twiddling Hacks - HackMD",
			"url": "https://hackmd.io/@derailment/bit-twiddling-hacks",
			"excerpts": [
			  "Bit Twiddling Hacks"
			]
		  },
		  {
			"title": "LZ4 on the 8088: One small drop « Oldskooler Ramblings",
			"url": "https://trixter.oldskool.org/2013/02/09/lz4-on-the-8088-one-small-drop/",
			"excerpts": [
			  "I thought I had squeezed every drop of blood from the stone that is LZ4 decompression on the 8088, but with some help from Peter Ferrie and Terje Mathisen, we’ve managed to improve the decompression speed by another 1%.  1% may seem laughable, but believe me, it’s quite an accomplishment if you followed [my previous 3-part series on optimizing for the 8088](https://trixter.oldskool.org/2013/01/10/optimizing-for-the-8088-and-8086-cpu-part-1/) ."
			]
		  },
		  {
			"title": "Optimizing for the 8088 and 8086 CPU: Part 1 « Oldskooler Ramblings",
			"url": "https://trixter.oldskool.org/2013/01/10/optimizing-for-the-8088-and-8086-cpu-part-1/",
			"excerpts": [
			  "There is a small but slowly growing hobby around retroprogramming for old PCs and compatibles.",
			  "This hobby has existed for decades for other platforms, as evidenced by the active demoscenes on [each](http://www.pouet.net/prodlist.php?platform%5B%5D=Commodore+64&order=&x=23&y=4&page=1&order=) [retro](http://www.pouet.net/prodlist.php?platform%5B%5D=ZX+Spectrum&order=&x=31&y=7&page=1&order=) [platform](http://www.pouet.net/prodlist.php?platform%5B%5D=Atari+XL%2FXE&order=&x=28&y=7&page=1&order=) , but the IBM PC (and other 4.77MHz 8088 compatibles) has only recently started to gain that same sort of attention.",
			  "Section Title: [Oldskooler Ramblings](https://trixter.oldskool.org/) > Optimizing for the 8088 and 8086 CPU: Part 1",
			  "Before we begin, let me clarify that while I’m using “8088” throughout most of this text, what I am writing applies equally to the 8086 as well. The 8086 and 8088 are functionally identical, with the 8086 being slightly faster due to a having a 16-bit bus and a larger prefetch queue, both of which are covered later.",
			  "Contrary to what you might think about a CPU old enough to be Justin Bieber’s father, it is possible to wring acceptable speed out of an 8088 if you understand the situation most 8088s are forced into (the slow RAM of the IBM PC) and how to deal with it, as well as the CISC-like advantages the chip has.",
			  "By understanding both, it is possible to write assembly code that can run faster than the best 6502 or Z80 code at similar clock speeds."
			]
		  },
		  {
			"title": "Lifetime-Sensitive Modulo Scheduling in a Production Environment | IEEE Transactions on Computers",
			"url": "https://dl.acm.org/doi/10.1109/12.910814",
			"excerpts": [
			  "Swing Modulo Scheduling (SMS). It generates schedules that are near optimal in terms of initiation interval, register requirements, and stage count.",
			  "This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS).",
			  "SMS is compared with other heuristic methods, showing that it outperforms them in terms of the quality of the obtained schedules and compilation time.",
			  "Swing Modulo Scheduling is a heuristic approach that has a low computational cost.",
			  "This paper first describes the technique and evaluates it for the Perfect Club benchmark suite on a generic VLIW architecture.",
			  "To further explore the effectiveness of SMS, the experience of incorporating it into a production quality compiler for the Equator MAP1000 processor is described; implementation issues are discussed, as well as modifications and improvements to the original algorithm.",
			  "Finally, experimental results from using a set of industrial multimedia applications are presented."
			]
		  },
		  {
			"title": "17-18-vliw.pdf",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15740-f18/www/lectures/17-18-vliw.pdf",
			"excerpts": [
			  "Hyperblock: A single-entry, multiple-exit block with internal\ncontrol flow eliminated using predication (if-conversion)",
			  "\nHyperblock formation\n1. Block selection\n2. Tail duplication\n3. If-conversion\n"
			]
		  },
		  {
			"title": "TMS320C6000 Assembly Language Tools v 7.4 User's ...",
			"url": "http://www.ihsys.com/ohsspds/Documents/TI_TMS320C6000_Assembly_Language_Tools.pdf",
			"excerpts": [
			  "... Optimizing Compiler v7.4 User's Guide. Describes the TMS320C6000 C compiler and the assembly optimizer. This C compiler accepts ANSI standard C source code ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "http://bitsavers.informatik.uni-stuttgart.de/components/ti/TMS320xx/320C6000/SPRU198G_TMS320C6000_Programmers_Guide_200208.pdf",
			"excerpts": [
			  "he C64x provides double-word load and store instructions (LDDW and\nSTDW) which can access 64 bits of data at a time. Up to two double-word load\nor store instructions can be issued every cycle. This provides a peak band-\nwidth of 128 bits per cycle to on-chip memory"
			]
		  },
		  {
			"title": "Efficient formulation for optimal modulo schedulers",
			"url": "https://dl.acm.org/doi/pdf/10.1145/258915.258933",
			"excerpts": [
			  "We present the performance characteristics of the four modulo scheduling algorithms with the struc- tured formulation in Table 1. In addition to the number.Read more"
			]
		  },
		  {
			"title": "Fast enumeration-based modulo scheduling heuristic for VLIW architectures | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Fast-enumeration-based-modulo-scheduling-heuristic-Bahtat-Belkouch/34f94fb3997e68c55d4f71b8697b4747d015c71f",
			"excerpts": [
			  "IEEE Trans. Computers. 2001. TLDR. This paper first describes the Swing Modulo Scheduling technique and evaluates it for the Perfect Club benchmark suite on a ..."
			]
		  },
		  {
			"title": "TI 'C6x dispatch pipeline stage with the addition of ...",
			"url": "https://www.researchgate.net/figure/TI-C6x-dispatch-pipeline-stage-with-the-addition-of-the-Modulo-Schedule-Buffers-and_fig4_3940504",
			"excerpts": [
			  "This paper describes a new architectural mechanism, called the Modulo Schedule Buffers, that provides an elegant interface for the execution of modulo scheduled ...Read more"
			]
		  },
		  {
			"title": "Modulo schedule buffers | Proceedings of the 34th annual ACM/IEEE international symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.5555/563998.564018",
			"excerpts": [
			  "This paper describes a new architectural mechanism, called the Modulo Schedule Buffers, that provides an elegant interface for the execution of modulo scheduled ...Read more"
			]
		  },
		  {
			"title": "Software pipelining via rotating registers",
			"url": "https://cs.ijs.si/courses/processor/Chapter4/tsld159.htm",
			"excerpts": [
			  "Software pipelining via rotating registers. Software pipelining - improves performance by overlapping execution of different software loops - execute more ..."
			]
		  },
		  {
			"title": "A comparative study of modulo scheduling techniques",
			"url": "https://dl.acm.org/doi/pdf/10.1145/514191.514208",
			"excerpts": [
			  "**A comparative study of modulo scheduling techniques**",
			  "2.2.4*\n*Swing Modulo Scheduling*\nThe\nSwing\nMo\ndulo\nSc\nheduling\n(SMS)\nsc\nheme\nw\nas\npro-\np\nosed\nb\ny\nLlosa,\nGonz�\nalez,\nAyguad\n�\ne\nand\nV\nalero\n[\u00020].\nI",
			  "A comparative study of modulo scheduling techniques"
			]
		  },
		  {
			"title": "Register allocation for software pipelined loops",
			"url": "https://dl.acm.org/doi/pdf/10.1145/143103.143141",
			"excerpts": [
			  "Software pipelining is an important instruction scheduling\ntechnique for efficiently overlapping successive iterations of\nlooPs\nand executing\nthem\nin parallel.",
			  "Modulo\nscheduling\nis applicable\nto RISC,\nCISC,\nsuperscalar,\nsuperpipelined,\nand VLIW processors,\nand is useful whenever\na\nprocessor\nimplementation\nhas\nparallelism\neither\nby\nhaving",
			  "Permission",
			  "to",
			  "copy",
			  "without",
			  "fee"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed.",
			  "L.-F. Chao, A. S. LaPaugh, and E. H.-M. Sha. Rotation scheduling: a loop pipelining algorithm."
			]
		  },
		  {
			"title": "Using Intel® Developer on Itanium® Architecture for ...",
			"url": "https://wwwuser.gwdguser.de/~parallel/parallelrechner/altix_documentation/Vtune_TP.pdf",
			"excerpts": [
			  "– Itanium Reference Manual for Software Developers. – Itanium-2 Reference Manual for Software Optimization. – Introduction to Micro-architectural Software ...Read more"
			]
		  },
		  {
			"title": "KeyStone Training KeyStone C66x CorePac Instruction Set ...",
			"url": "https://www.ti.com.cn/content/dam/videos/external-videos/en-us/1/3816841626001/3877481353001.mp4/subassets/instruction-set-architecture-presentation.pdf",
			"excerpts": [
			  "Datapaths of the .L and .S units have been increased from 32-bit to 64-bit. • Datapaths of the .M units have been increased from. 64-bit to 128-bit.Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C/C++ Compiler v8.3.x",
			"url": "https://www.ti.com/lit/ug/sprui04d/sprui04d.pdf",
			"excerpts": [
			  "**4.6.2 Software Pipelining Information**",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to optimize\nC/C++ code or linear assembly code.",
			  "Example 4-1 illustrates the information that is\ngenerated for each loop.",
			  "**4.6.2.1 Software Pipelining Information Terms**",
			  "op unroll factor.** The number of times the loop was unrolled specifically to increase performance.",
			  "nown minimum trip count.** The minimum number of times the loop will be executed",
			  "own maximum trip count.** The maximum number of times the loop will be executed.",
			  "Known max trip count factor.** Factor that would always evenly divide the loops trip count. This information\ncan be used to possibly unroll the loo",
			  " dependency bound.** The distance of the largest loop carry path. A loop carry path occurs\nwhen one iteration of a loop writes a value that must be read in a future iteration. Instructions that are part of\nthe loop carry bound are marked with the ^ symbol.",
			  "nitiation interval** (ii). The number of cycles between the initiation of successive iterations of the loop. The\nsmaller the initiation interval, the fewer cycles it takes to execute a loop",
			  "nitiation interval** (ii). The number of cycles between the initiation of successive iterations of the loop. The\nsmaller the initiation interval, the fewer cycles it takes to execute a loop",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each cycle of\nthe loop kernel and displays the instruction ordering of a single iteration of the software pipelined loop.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute in\nparallel.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute in\nparallel."
			]
		  },
		  {
			"title": "Software Pipelining Irregular Loops On the TMS320C6000 VLIW ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/384198.384216",
			"excerpts": [
			  "*\nSoftware pipelining is a powerful compile-time transformation\nfor optimizing loops to aggressively exploit the many functional\nunits on today’s VLIW and superscalar architectures [2,5]. This\noptimization is essential for maximizing the performance of the\nTexas Instruments TMS320C6000 [7,8], a family of VLIW\nprocessors targeted at Digital Signal Processing (DSP)\napplications.",
			  "In contrast to what one might\nexpect, we found that the best heuristic for choosing between\nthe two techniques was to select register copying over\npredication whenever possible.",
			  "predication and a new technique called register copying to\ntransform irregular loops into loops that can be easily pipelined\nusing our existing infrastructure.",
			  "Such\nhardware is not currently available in the TMS320C6000\narchitecture family.",
			  "Although there are only a limited number of irregular loops in\nthe DSP domain, there are several important application areas\nwhere performance-critical loops are irregular.",
			  "However, we are not aware of previous research that exploits\nconditional moves to enable irregular loop pipelining.",
			  "However, we are not aware of previous research that exploits\nconditional moves to enable irregular loop pipelining.",
			  "\nRegister moves have been previously used in other contexts to\nfacilitate software pipelining in many other ways — perhaps the\nmost well known being an alternative to modulo variable\nexpansion [2] for preventing excessively long register lifetimes.",
			  "\nRegister moves have been previously used in other contexts to\nfacilitate software pipelining in many other ways — perhaps the\nmost well known being an alternative to modulo variable\nexpansion [2] for preventing excessively long register lifetimes.",
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "In this paper, we discuss our software-based preprocessing\ntechnique that allows compilers to software pipeline irregular\nloops on architectures with no special support beyond the ability\nto predicate (conditionally nullify) instructions using a small\nnumber of static registers.",
			  "More recently, researchers have extended software pipelining to\nthe more general class of *irregular* (WHILE) loops which\nterminate when an arbitrary condition occurs, possibly mid-\niteration [3,6,11].",
			  "Traditional software pipelining is only applicable\nto *regular* (FOR) loops. Loops of this type are each controlled\nby a regularly updated counter and terminate when the counter\nreaches a value that was established before loop execution\nbegan."
			]
		  },
		  {
			"title": "Accelerating control-flow intensive code in spatial hardware",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-870.pdf",
			"excerpts": [
			  "Pointer Arithmetic and Memory Disambiguation",
			  "Overcoming the performance limitations due to explicit control-flow is the key issue\nthat needs to be addressed for custom hardware to become performance-competitive with\nconventional processors on sequential code [BAG05].",
			  "thesis, we found that when the resource utilization is already high – because of either high\ninstruction level parallelism in the design or as a result of loop unrolling – the control and\ninterconnect (multiplexing) costs of further loop unrolling or loop pipelining outweigh the\ngains achieved in performance. This is because frequently the longest combinational path\nin the circuit (the critical path length) increases so much that the input to output circuit\ndelay becomes worse”.",
			  "Together with the use of data-dependent control-flow, the implication of\nsuch pointer arithmetic is that the precise sequence of locations that a pointer points-to\nat runtime cannot be determined at compile-time, except in the most trivial cases (i.e.\nstatic points-to analysis is undecidable [Lan92]). Pointer arithmetic can therefore often\nobfuscate the data dependence between producers and consumers, leaving the potential\nfor a true or name dependence that can only be resolved at runtime, when the values of\npointer variables are known. This is the ‘unknown address problem’ [PMHS85].",
			  "Pointer arithmetic is an established feature of many imperative languages (especially C\nand C++)."
			]
		  },
		  {
			"title": "program demultiplexing: data-flow based speculative",
			"url": "https://ftp.cs.wisc.edu/sohi/theses/saisantosh.pdf",
			"excerpts": [
			  "This dissertation proposes Program Demultiplexing or PD, a speculative parallelization model that has two novel contributions. In PD, a speculative thread is ...Read more"
			]
		  },
		  {
			"title": "Speculative Trace Scheduling in VLIW Processors",
			"url": "https://www.researchgate.net/publication/2843206_Speculative_Trace_Scheduling_in_VLIW_Processors",
			"excerpts": [
			  "Traces are speculatively scheduled on the processor based on their probability of execution. We have developed a tool \"SpliTree\" to generate traces ..."
			]
		  },
		  {
			"title": "Predicated software pipelining technique for loops with conditions",
			"url": "https://www.researchgate.net/publication/3744185_Predicated_software_pipelining_technique_for_loops_with_conditions",
			"excerpts": [
			  "PDF | An effort to formalize the process of software pipelining loops with conditions is presented. A formal framework for scheduling such loops, based."
			]
		  },
		  {
			"title": "The Bene t of Predicated Execution for Software Pipelining",
			"url": "http://impact.crhc.illinois.edu/shared/papers/hicss-93-ppredicate.pdf",
			"excerpts": [
			  "Lam\nprop\noses\nmo\ndulo\nv\nariable\nexpansion\nand\nhierarc\nhical\nreduction\nfor\nhandling\nregister\nallo\ncation\nand\ncondi-\ntional\nconstructs\nresp\nectiv\nely\n[\n].",
			  "o\ndulo\nSc\nheduling\nIn\na\nmo\ndulo\nsc\nheduled\nsoft\nw\nare\npip\neline,\na\nlo\nop\nit-\neration\nis\ninitiated\nev\nery\nII\ncycles,\nwhere\nII\nis\nthe\nIni-\ntiation\nInterva"
			]
		  },
		  {
			"title": "Loop transformations for clustered VLIW architectures",
			"url": "https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=1180&context=etds",
			"excerpts": [
			  "The transformed loops exploit considerable parallelism and data locality, and therefore are amenable to software pipelining to generate high performance code.Read more"
			]
		  },
		  {
			"title": "A Clustered VLIW Architecture Based",
			"url": "https://era.ed.ac.uk/bitstream/1842/366/1/fernandes.PDF",
			"excerpts": [
			  "Software pipelining generally increases register pressure in VLIW machines. The register file required in such cases may compromise scalability, which has ...Read more"
			]
		  },
		  {
			"title": "phd-thesis-matai.pdf - Kastner Research Group",
			"url": "https://kastner.ucsd.edu/wp-content/uploads/2013/08/admin/phd-thesis-matai.pdf",
			"excerpts": [
			  "irregular kernels have properties\nsuch as indirect memory access, pointer chasing that are hard for modern high-level\nsynthesis tools to parallelize.",
			  "irregular kernels are hard to\nsynthesis with modern high-level synthesis tools than applications that purely contain\n89\nregular kerne",
			  "One solution to this problem is finding the irregular kernel\nbottlenecks and making them high-level synthesis friendly by restructuring the portion of\nthe kernel that causes problem.",
			  "It is often involves completely re-writing (for restructuring purposes) the whole kernel\nor application from scratch."
			]
		  },
		  {
			"title": "Alternative formats If you require this document in an alternative ...",
			"url": "https://purehost.bath.ac.uk/ws/portalfiles/portal/187945987/main_thises.pdf",
			"excerpts": [
			  "Rangan et al. (2004) introduced a new technique to utilize a decoupled software pipeline\nfor optimizing the performance of recursive data structures (RDS)(e.g., linked lists, trees and\ngraphs).",
			  "For this kind of structure difficulties have been encountered when trying to execute it\nin parallel, because the instructions of a given iteration of a loop depend on the pointer value\nthat is loaded from a previous iteration.",
			  "Therefore to address this problem, a decoupled software\npipeline has been used so as to avoid stalls that are happening with the long variable-latency\ninstruction in RDS loops.",
			  "It is clear that RDS loops consist of two parts. The first contains\nthe traversal code (critical path of execution) and the second represents the computation that\nshould be carried out on each node traversed by the first part."
			]
		  },
		  {
			"title": "(PDF) The Superblock: An Effective Technique for VLIW and Superscalar Compilation",
			"url": "https://www.researchgate.net/publication/220358960_The_Superblock_An_Effective_Technique_for_VLIW_and_Superscalar_Compilation",
			"excerpts": [
			  "A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware. However, ILP within basic blocks is extremely limited for control-intensive programs. We have developed a set of techniques for exploiting ILP across basic block boundaries. These techniques are based on a novel structure called thesuperblock. The superblock enables the optimizer and scheduler to extract more ILP along the important execution paths by systematically removing constraints due to the unimportant paths.",
			  "Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler. Superblock optimizations and scheduling are shown to be useful while taking into account a variety of architectural features."
			]
		  },
		  {
			"title": "Trace Scheduling",
			"url": "https://www.researchgate.net/publication/303672755_Trace_Scheduling",
			"excerpts": [
			  "A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware.",
			  "Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler."
			]
		  },
		  {
			"title": "Class 12 Software Pipelining - Modulo Scheduling",
			"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
			"excerpts": [
			  "Iterative Modulo Scheduling: An Algorithm for Software Pipelining Loops”, B.\nRau, MICRO-27, 1994, pp. 63-74.\n",
			  " Rotating registers\n",
			  "Itanium-style hardware\nsupport, then remove it later\n»"
			]
		  },
		  {
			"title": "Monica S. Lam",
			"url": "https://scholar.google.com.hk/citations?user=4hS0jZ8AAAAJ&hl=th",
			"excerpts": [
			  "Software pipelining: An effective scheduling technique for VLIW machines. M Lam. Proceedings of the ACM SIGPLAN 1988 conference on Programming Language …, 1988."
			]
		  },
		  {
			"title": "A hierarchical parallelizing compiler for VLIW/MIMD machines",
			"url": "https://link.springer.com/chapter/10.1007/3-540-57502-2_39",
			"excerpts": [
			  "Monica S. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proceedings ACM SIGPLAN '91 Conference on ...Read more"
			]
		  },
		  {
			"title": "an effective scheduling technique for VLIW machines",
			"url": "https://semopenalex.org/work/W4232919122",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling - B. Ramakrishna Rau",
			"url": "https://courses.engr.illinois.edu/ece511/fa2005/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "The objective of modulo scheduling is to engineer a schedule for one iteration' of the loop such that when this same schedule is repeated at regular intervals, ...Read more"
			]
		  },
		  {
			"title": "A Programmable Memory Hierarchy for Prefetching Linked ...",
			"url": "https://users.cs.duke.edu/~alvy/papers/ishpc02.pdf",
			"excerpts": [
			  "Pointer dereferences are required to generate addresses for successive elements in a linked data structure (LDS). This is commonly called the pointer- chasing ...Read more"
			]
		  },
		  {
			"title": "Behavior Specialized Processors by Tony Nowatzki",
			"url": "https://research.cs.wisc.edu/vertical/papers/thesis/tony.pdf",
			"excerpts": [
			  "The region selection heuristic performs a bottom-up tree traversal, selecting code which has the highest dynamic to static instruction ratio. TDG Transform ...Read more"
			]
		  },
		  {
			"title": "Compiler-Hardware Co-Design in High-Level Synthesis",
			"url": "https://theses.gla.ac.uk/85229/2/2025szafarczykphd.pdf",
			"excerpts": [
			  "predictable memory addresses are not the only source of irregularity. Unpredictable con-\ntrol flow can cause similar problems. Consider the loop in Listing 1.2 for example.\n**Listing 1.2:** Another example of an irregular code (control-dependent reduction).\nfor ( int i = 0; i < N; ++i)\nif (condition)\nreduction += f(reduction);\nHere, we have a clear inter-iteration dependency on the reduction variable—assuming that\nthe f() function takes more than one clock cycle, the compiler cannot pipeline the loop with\nan II of 1. The static schedule needs to allocate enough cycles in the loop to calculate the\nf(reduction) value before the next iterations starts, because it might need the result of this\ncalculation. Crucially, this schedule is rigid and has to be repeated for every loop iteration.",
			  "The above problems are the same challenges that were burdening compiler writers for Very\nLarge Instruction Word (VLIW) machines a few decades ago [140, 86]. In VLIW machines, as\nin FSMD HLS, the goal of discovering ILP lies entirely on the shoulders of the compiler, with\nthe difference that in VLIW the compiler decides which instructions can execute together by\npacking them in a single instruction word, instead of merging FSM states. At a high level, the\nproblem of scheduling irregular code in VLIW and FSMD HLS is essentially the same. In both\ncases, the compiler has limited information about the dependencies and control flow in the\ncode and has to assume a worst-case scenario, sequentializing most of the execution.",
			  "Today, VLIW machines are mainly used in domains that contain regular codes [147], such as\nsignal processing or machine learning. Most general-purpose processors are not VLIW ma-\nchines; they are RISC machines that contain many hardware structures to enable dynamic,\nsuperscalar, out-of-order execution. Similarly, an efficient HLS of irregular codes calls for\nadditional hardware structures to guide circuit execution at runtime, at which time the data\ndependencies and control-flow become know",
			  "ynamic Dataflow (DDF) HLS forgoes static scheduling completely, leaving the job of schedul-\ning to *latency-insensitive handshaking logic* that determines the execution of operators at\nruntime [126, 74]. Latency-insensitive channels allow for communication between a sender\nand receiver without having to rely on their cycle behavior [87]. Each operator in a dataflow\ncircuit has a wrapper with handshaking logic that stalls the operator until all inputs have\n ... \nis different in that we decouple only address generating instructions and speculate only on\nmemory operatio",
			  "peculation has its roots in compilers for Very Large Instruction Word (VLIW) ma-\nchines. Instruction scheduling in HLS is very similar to VLIW scheduling (no hardware sup-\nport for speculation, static mapping to functional units, etc.), with many algorithms like\nmodulo-scheduling and *if* -conversion originally developed for VLIW directly applicable to\nHLS [196, 3, 178]. Most recently, predicated execution in the form of gated SSA was proposed\nfor HLS with speculation support [97]. The speculation support in this and other works re-\nquires costly recovery on mis-speculation [127, 220, 14, 154, 93, 231]. Efficiently squashing\nspeculative computation on the wrong paths in a spatial dataflow architecture is hard, be-\ncause the architectural state is distributed [32]. Our speculative DAE sidesteps this issue, not\nrequiring any recovery: we speculate early (run ahead) in the AGU, and later handle mis-\nspeculations in the CU by taking an appropriate path in its CFG.",
			  "Control Flow Handling in GPUs",
			  "trol-flow handling in GPUs is most commonly implemented via predication. The al-\ngorithms used to calculate predicate masks and re-convergence points bear a striking re-\nsemblance to our work [148]. The Single Instruction Multiple Threads (SIMT) stack approac",
			  "In VLIW machines, as\nin FSMD HLS, the goal of discovering ILP lies entirely on the shoulders of the compiler, with\nthe difference that in VLIW the compiler decides which instructions can execute together by\npacking them in a single instruction word, instead of merging FSM states.",
			  "ltimately, the problem of scheduling VLIW machines for general purpose code, which con-\ntains many of the irregularities that we highlight here, was recognized to be intractable",
			  "ltimately, the problem of scheduling VLIW machines for general purpose code, which con-\ntains many of the irregularities that we highlight here, was recognized to be intractable",
			  "Most recently, predicated execution in the form of gated SSA was proposed\nfor HLS with speculation support [97].",
			  "ynamic Dataflow (DDF) HLS forgoes static scheduling completely, leaving the job of schedul-\ning to *latency-insensitive handshaking logic* that determines the execution of operators at\nruntim",
			  "ntrol speculation has its roots in compilers for Very Large Instruction Word (VLIW) ma-\nchines.",
			  "nstruction scheduling in HLS is very similar to VLIW scheduling (no hardware sup-\nport for speculation, static mapping to functional units, etc.), with many algorithms like\nmodulo-scheduling and *if* -conversion originally developed for VLIW directly applicable to\nHL",
			  "The static schedule needs to allocate enough cycles in the loop to calculate the\nf(reduction) value before the next iterations starts, because it might need the result of this\ncalculation."
			]
		  },
		  {
			"title": "Separation Logic for High-Level Synthesis - Spiral - Imperial College ...",
			"url": "https://spiral.imperial.ac.uk/bitstreams/420906a2-445a-411c-881c-dba6edcba25e/download",
			"excerpts": [
			  "Inter-Iteration Dependencies and Pipelining",
			  "Apart from replication, acceleration of the manual RTL design is obtained from pipelining\nthe tree traversal. This corresponds to pipelining the loop nest in Listing 3 which must take\ntwo (potential) inter-iteration dependencies into account.",
			  "The first occurs between fetching\npointers to data from the stack and pushing new pointers onto the stack, which hinders\npipelining.",
			  "However, because there are two push statements and one fetch statement, the\nitems stored on the stack (pointers *u* and *Z* , *d* ) accumulate if the condition in Line 9 holds\nin several iterations.",
			  "Once there are multiple pointers on the stack, these do not cause\nany read-write dependencies between iterations and hence can be overlapped in pipelined\nexecution.",
			  "Listing 4 shows a transformation of the loop in Listing 3 to implement this\nschedule.",
			  "The transformation distributes the execution of the original loop body over two\n(pipelineable) inner loops which exchange data via a newly inserted queue.",
			  "etail and elaborate on their implications for e ffi cient HLS. To this end, we present a case\nstudy comparing the implementations of two algorithms for a compute-intensive machine\nlearning application ( *K* -means clustering). A",
			  "the latency is initially degraded\nby 26 . 6 ⇥ compared to the hand-crafted RTL implementation. We narrow this significant\nperformance gap and improve the former latency by 8 ⇥ with source code transformations\nthat partition and privatise data structures accessed through pointers to enable paral-\nlelisation and pipelining of the loop traversing the pointer-linked data structure."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads",
			  "In this paper, we propose an effective, *fully automatic* ap-\nproach to non-speculative thread extraction, called *Decou-*\n*pled Software Pipelinin",
			  "igure 1. A simple linked list traversal loop**\n**executed as DOACROSS and DS"
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining | Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2005.13",
			"excerpts": [
			  "Automatic Thread Extraction with Decoupled Software Pipelining > Abstract",
			  "DSWP exploits the finegrained pipeline parallelism lurking in most applications to extract long-running, concurrently executing threads.",
			  "Use of the non-speculative and truly decoupled threads produced by DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "Automatic Thread Extraction with Decoupled Software Pipelining > References",
			  "20} R. Rangan, N. Vachharajani, M. Vachharajani, and D. I. August. Decoupled software pipelining with the synchronization array. In *Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques* , pages 177-188, September 2004.\n",
			  "21} K. Rich and M. Farrens. Code partitioning in decoupled compilers. In *Proceedings of the 6th European Conference on Parallel Processing* , pages 1008-1017, Munich, Germany, September 2000.\n"
			]
		  },
		  {
			"title": "Improving Hash Join Performance through Prefetching",
			"url": "http://www.pdl.cmu.edu/ftp/Database/icde04.pdf",
			"excerpts": [
			  "**Challenges in Applying Prefetching to Hash Join.**",
			  "A\nnaive approach to prefetching for hash join might simply try\nto hide the latency within the processing of a single tuple.\nFor example, to improve hash table probing performance,\none might try to prefetch hash bucket headers, hash buck-\nets, build tuples, etc. Unfortunately, such an approach would\nhave little benefit because later memory references often de-\npend upon previous ones (via pointer dereferences). Existing\ntechniques for overcomingthis *pointer-chasing problem* [18]\nwill not work because the randomness of hashing makes it\nimpossible to predict the memory locations to be prefetched."
			]
		  }
		],
		"reasoning": "The platform is identified as the Intel Itanium 2 processor, which is a 6-issue EPIC (Explicitly Parallel Instruction Computing) processor. EPIC is described as a VLIW-like philosophy where the compiler explicitly encodes instruction-level parallelism into the machine code. The workload used for evaluation is the SPECint2000 benchmark suite, which consists of general-purpose integer programs with irregular control flow and complex memory access patterns. The key finding is that an empirical study using the IMPACT compiler demonstrated an average of 2.63 useful operations per cycle (IPC), representing a 43.8% slot utilization on the 6-issue machine. This performance was achieved through aggressive ILP transformations, including hyperblock formation (using predication to merge multiple control paths into a single schedulable block) and hardware-assisted data and control speculation to hide memory latency. Data speculation alone yielded performance gains of 1% to 7%. The implication is that despite advanced architecture and a sophisticated compiler, the Itanium 2 struggled to utilize even half of its available execution slots on irregular integer code, suggesting that a much wider 23-slot VLIW will face a severe scarcity of instruction-level parallelism. Therefore, multi-stream interleaving is a fundamental necessity to keep the hardware busy.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.0",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "Trace scheduling was originally developed for Very Long Instruction Word, or [VLIW](/wiki/VLIW \"VLIW\") machines, and is a form of global code motion.",
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace.",
			  "Trace scheduling uses a basic block scheduling method to schedule the instructions in each entire trace, beginning with the trace with the highest frequency.",
			  "This can result in large increases in code sizes and poor or erratic performance if program's behavior varies significantly with the input.",
			  "A trace is a sequence of instructions, including branches but not including loops, that is executed for some input data."
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "A VLIW Architecture for a Trace Scheduling Compiler\n*Robert P. Colwell, Robert P. Nix, John J. O'Donnell, David B. Papworth, Paul K. Rodman*\nMultiflow Computer\n175 North Main Street\nBranford, CT. 06405\n(203) 488-6090",
			  "Instruction Word (VLIW) architectures were prom-\nised to deliver far more than the factor of two or three that\ncurrent architectures achieve £rom overlapped execution. Using\na new type of compiler which compacts ordinary sequential code\ninto long instruction words, a VLIW machine was expected to\nprovide from ten to thirty times the performance of a more con-\nventional machine built of the same implementation technology.\nMultiflow Computer, Inc., has now built a VLIW called the\nTRACE'\" along with its companion Trace Scheduling\" com-\npacting compiler. This new machine has fulfilled the perfor-\nmance promises that were made. Using many fast functional\nunits in parallel, this machine extends some of the basic\nReduced-Instruction-Set\nprecepts:\nthe\narchitecture\nis\nload/store, the microarchitecture is exposed to the compiler,\nthere is no microcode, and there is almost no hardware devoted\nto synchronization, arbitration, or interlocking of any kind (the\ncompiler has sole responsibility for runtime ",
			  "Multiflow's Trace Scheduling compacting compiler automatically\nfinds fine-grained parallelism throughout any appfication.\nIt\nrequires no programmer intervention, either in terms of restruc-\nturing the program so it fits the architecture or in adding direc-\ntives which explicitly identify opportunities for overlapped exe-\ncution. This is in sharp contrast to \"coarse-grained\" parallel\narchitectures, including vector machines, shared-memory mul-\ntiprocessors,\nand\nmore\nradical\nstructures\nsuch\nas\nhypercubes selt85 or massively-parallel machines, wurst The pro-\ncess by which programs are converted into highly parallel wide-\ninstruction-word code is transparent to the user.",
			  "Using estimates of branch directions obtained automatically\nthrough heuristics or profiling, the compiler selects the most\nlikely path, or \"trace\", that the code will follow during execu-\ntion. This trace is then treated as if it were free of conditional\nbranches, and handed to the code generator. The code genera-\ntor schedules operations into wide instruction words, taking into\naccount data precedence, optimal scheduling of functional units,\nregister usage, memory accesses, and system buses; it emits\nthese instruction words as object code. This greedy scheduling\ncauses code motions which could cause logical inconsistencies\nwhen branches off-trace are taken. The compiler inserts special\n\"compensation code\" into the program graph on the off-trace\nbranch edges to undo these inconsistencies, thus restoring pro-\ngram correctness.",
			  "This process allows the compiler to break the \"conditional jump\nbottleneck\" and find parallelism throughout long streams of\ncode, achieving order-of-magnitude speedups due to compac-\ntion.",
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled.",
			  "A number of conventional optimizations aid the trace selection\nprocess in finding parallelism. Automatic loop unrolling and\nautomatic inline substitution of subroutines are both incor-\nporated in Multiflow's compilers;\nthe compiler heuristically\ndetermines the amount of unrolling and substitution, substan-\ntially increasing the parallelism that can be exploited."
			]
		  },
		  {
			"title": "Very Long Instruction Word Architectures and the ELI-512",
			"url": "https://courses.cs.washington.edu/courses/cse548/16wi/Fisher-VLIW.pdf",
			"excerpts": [
			  "TRACE SCIIEDULING. The VLIW compiler we have built uses a recent global compaction technique called trace scheduling Fisher 811. This technique ww originally ...Read more"
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations.",
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "We also give a very simple proof that the trace scheduling algorithm always terminates."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant).",
			  "The performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%)."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "**In addition,** we provide a certified checker for path du-\nplications, which we use to prove tail duplication and loop\nunrolling optimizations. These optimizations increase sched-\nuling opportunities (at the price of code size increase). Our\nchecker of path duplications illustrates the interest of trans-\nlation validation designs: a simple formally verified checker\nenables to certify the correctness of an important class of\ntransformations, modulo small hints provided by oracles.",
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.classes.cs.uchicago.edu/archive/2017/fall/32001-1/papers/meta-optimization.pdf",
			"excerpts": [
			  "perblock for-\nmation. Here we introduce the optimization, and then we\ndiscuss factors that might be important when creating a pri-\nority function for it. We conclude the section by presenting\nexperimental results for hyperblock formatio",
			  "chitects have proposed two noteworthy methods for de-\ncreasing the costs associated with control transfers 3 : im-\nproved branch prediction, and predication. Improved branch\nprediction algorithms would obviously increase processor uti-\nlization. Unfortunately, some branches are inherently unpre-\ndictable, and hence, even the most sophisticated algorithm\nwould fail. For such branches, predication may be a fruitful\nalternative.",
			  "*Path predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [15]",
			  "ath frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively a ff ect performance",
			  "th ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels of ILP.",
			  "*Number of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc"
			]
		  },
		  {
			"title": "An Overview of IA-64 Architectural Features and Compiler ...",
			"url": "https://cseweb.ucsd.edu/classes/fa99/cse240/lee_ucsd_ia64.PDF",
			"excerpts": [
			  "Setting and propagation of deferred exceptions. – Speculative loads (ld.s) set the NaT bit or NaTVal if a deferrable exception occurs.Read more",
			  "❒ Parallel compares allow compound conditionals to be executed in a single instruction group. – Permit WAW in an instruction group. ❒ Example: if ( A && B && C ) ...Read more",
			  "ALAT - Advanced load address table",
			  "– HW structure containing information about outstanding advanced\nload addresses",
			  "– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "– ld.a - advanced load",
			  "– ld.c - check load",
			  "– ld.sa - combined control and data speculation.",
			  "Architectural Features",
			  "Compiler Optimization",
			  "Control and data speculation,\nPredication,\nMulti-way branch",
			  "Global scheduling with\ncontrol and data speculation,\nPredicate promotio",
			  "Modulo scheduling",
			  "cality hints,\nPrefetch,\nPost-increment",
			  "Memory hierarchy control",
			  "The compiler should take full advantage of IA-64 architectural\nfeatures to generate optimized code",
			  "O riginal\nR e schedule\nuse\nload\ncheck addressing conflict\nTransform\nstore\nuse\nstore\nload ignoring conflict\ncheck addressing conflict\nuse\nstore\nload ignoring conflict\naliased",
			  "chitectural Support for**\n**Data Speculati",
			  "ALAT - Advanced load address table\n– HW structure containing information about outstanding advanced\nload addresses\n– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "Compiler Directed Data Speculation",
			  "Separate load behavior from overlap detection\n– ld8.a can be scheduled passing aliased stores\n– chk.a detects conflict\n",
			  "ld8.a r1 = r2 ;;",
			  "add r3 = r1,r4",
			  "ld8.a r1 = r2",
			  "Data Speculation",
			  "chk.a r1, recovery",
			  "chk.a r1, recovery",
			  "Instructions",
			  "– chk.a - advance load checks"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.0 User's Guide ( ...",
			"url": "https://www.ti.com/litv/pdf/spru187q",
			"excerpts": [
			  "Memory Alias Disambiguation ... speculative execution of loads with bounded address ranges. --speculate_unknown_loads. Allows speculative execution of loads ...Read more",
			  "currently specified by the --speculate_loads option. You must increase the --speculate_loads threshold as recommended in the software-pipeline feedback ...",
			  "--speculate_unknown_loads. Allows speculative execution of loads with ... When the --speculate_loads=n option is used, the speculative threshold is ...Read more",
			  "**The MUST_ITERATE Pragma Syntax**",
			  "The syntax of the pragma for C and C++ is:\n**\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);\n",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple.",
			  "\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  "The arguments *min* and *max* are programmer-guaranteed minimum and maximum trip counts. The trip\ncount is the number of times a loop iterates.",
			  "The trip count of the loop must be evenly divisible by *multiple* .",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);",
			  "However, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple. Also, results of the program are undefined if the trip count is\nless than the minimum or greater than the maximum specified.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. You guarantee that\nthese properties are always true.",
			  "Through the use of the MUST_ITERATE pragma, you can guarantee that a loop executes a certain\nnumber of times.",
			  "Anytime the UNROLL pragma is applied to a loop,\nMUST_ITERATE should be applied to the same loop.",
			  "For loops the MUST_ITERATE pragma's third\nargument, multiple, is the most important and should always be specified.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/sprui04b/sprui04b.pdf",
			"excerpts": [
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "**Speculative Execution**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v7.4 - User's Guide",
			"url": "https://www.ti.com/lit/pdf/spru187",
			"excerpts": [
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "**3.2.3.1**",
			  "**3.2.3.1**",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops."
			]
		  },
		  {
			"title": "Architecture Software Developer's Manual",
			"url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/itanium-architecture-software-developer-rev-2-3-vol-1-manual.pdf",
			"excerpts": [
			  "hen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly.",
			  "The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags.",
			  "The Not A Thing (NaT) bit is an extra bit on each of the general registers.",
			  "e check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Data Speculation and Instructions",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and\nfloating-point pair ( ldfp.a ) forms.",
			  "Data speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated result",
			  "When the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Support for this is enabled by special NaT bits that are attached to integer\nregisters and by special NatVal values for floating-point registers.",
			  " When a speculative\nload causes an exception, it is not immediately raised. Instead, the NaT bit is set on the\ndestination register (or NatVal is written into the floating-point register). Subsequent\nspeculative instructions that use a register with a set NaT bit propagate the setting until\na non-speculative instruction checks for or raises the deferred exception.",
			  "The advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "The advanced load check works like the\n**Figure 4-",
			  "Recovery code may use either a normal or advanced load to obtain the correct value for\nthe failed advanced load.",
			  "An advanced load is used only when it is advantageous to\nhave an ALAT entry reallocated after a failed speculation.",
			  " an advanced load is executed, it allocates an\nentry in a structure called the Advanced Load Address Table (ALAT). Later, when a\ncorresponding check instruction is executed, the presence of an entry indicates that the\ndata speculation succeeded; otherwise, the speculation failed and one of two kinds of\ncompiler-generated recovery is perfo",
			  "1.\nThe check load instruction ( ld.c , ldf.c , or ldfp.c ) is used for recovery when\nthe only instruction scheduled before a store that is ambiguous relative to the\nadvanced load is the advanced load itself.",
			  "2.\nThe advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "g (NaT) bit is an extra bit on each of the general registers. A register\nNaT bit indicates whether the content of a register is valid. If the NaT bit is set to one,\nthe register contains a deferred exception token due to an earlier speculation fault.",
			  "**Architecture**\nData speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated results.\nW",
			  "\nWhen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT). The ALAT is indexed by physical\nregister number and records the load address, the type of the load, and the size of the\nload.\n",
			  "\nA check instruction must be executed before the result of an advanced load can be used\nby any non-speculative instruction. The check instruction must specify the same\nregister number as the corresponding advanced load.\n",
			  "\nWhen a check instruction is executed, the ALAT is searched for an entry with the same\ntarget physical register number and type. If an entry is found, execution continues\nnormally with the next instruction.\n",
			  "\nIf no matching entry is found, the speculative results need to be recomputed:\n• Use a chk.a if a load and some of its uses are speculated. The chk.a jumps to\ncompiler-generated recovery code to re-execute the load and dependent\ninstructions.\n• Use a ld.c if no uses of the load are speculated. The ld.c reissues the load.\nEntries are removed from the ALAT due to:\n• Stores that write to addresses overlapping with ALAT entries.\n• Other advanced loads that target the same physical registers as ALAT entries.\n• Implementation-defined hardware or operating system conditions needed to\nmaintain correctness.\n• Limitations of the capacity, associativity, and matching algorithm used for a given\nimplementation of the ALAT.\n",
			  "\nThe ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly. The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags. An ALAT register tag is a unique number derived from the\nphysical target register number and type in conjunction with other\nimplementation-specific state. Implementation-specific state might include register\nstack wraparound information to distinguish one instance of a physical register that\nmay have been spilled by the RSE from the current instance of that register, thus\navoiding the need to purge the ALAT on all register stack wraparounds.\n",
			  "The Itanium architecture provides extensive support for software-pipelined loops,\nincluding register rotation, special loop branches, and application registers. When\ncombined with predication and support for speculation, these features help to reduce\ncode expansion, path length, and branch mispredictions for loops that can be software\npipelined."
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "Control Speculation"
			]
		  },
		  {
			"title": "IA64 Architecture and Compilers",
			"url": "https://old.hotchips.org/wp-content/uploads/hc_archives/hc11/1_Sun/hc99.t2.s1.IA64tut.pdf",
			"excerpts": [
			  "Control Speculation",
			  "Original:",
			  "(p1)",
			  "br.cond",
			  "ld8.s r1 = [ r2 ]",
			  "chk.s r1, recovery",
			  "Data Speculation",
			  "original",
			  "ld8 r1 = [ r2 ]",
			  "ld8 r1 = [ r2 ]",
			  "Transformed:",
			  "Transformed:",
			  "ld8.a r1 = [ r2 ]",
			  "...",
			  "...",
			  "st4 [ r3 ] = r7",
			  "st4 [ r3 ] = r7",
			  "chk.a r1, recovery",
			  "Separates loads into 2 parts: speculative\nloading of data and detection of conflicts/faults .",
			  "Hoisting Uses",
			  "Uses can be hoisted, but then chk.a needed for recovery",
			  "ld.c r1=",
			  "ld8.a r1=",
			  "ld8.a r1=",
			  "instr 1",
			  "uses =r1",
			  "uses =r1",
			  "instr 2",
			  "instr 2",
			  "st8",
			  "st8",
			  "chk.a r1,rec",
			  "br home",
			  "Recovery code",
			  "No hoisted uses",
			  "With hoisted uses",
			  "Hot Chips 1999",
			  "Architectural Support for Data Speculation",
			  "ALAT - HW structure containing information about\noutstanding advanced loads",
			  "Instructions",
			  "– ld.a - advanced loads",
			  "– ld.c - check loads",
			  "– chk.a - advance load checks",
			  "Speculative Advanced loads - ld.sa - is an control\nspeculative advanced load with fault deferral\n(combines ld.a and ld.s)",
			  "Almost all instructions can have a qualifying predicate (qp). – Ex: (p1) add r1 = r2, r3. – if qp is true, instruction executed normally. – if qp is false ...Read more",
			  "Two kinds of instructions in IA-64 programs. – Non-Speculative Instructions -- known to be useful/needed. • would have been executed in the original program.Read more"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "..."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol2.pdf",
			"excerpts": [
			  "..."
			]
		  },
		  {
			"title": "C6000 Compiler: Tuning Software Pipelined Loops - TI E2E",
			"url": "https://e2e.ti.com/cfs-file/__key/communityserver-discussions-components-files/791/C6000-Compiler_5F00_-Tuning-Software-Pipelined-Loops-_2D00_-Texas-Instruments-Wiki.pdf",
			"excerpts": [
			  "...",
			  "..."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 6.1 User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/spru187o/spru187o.pdf",
			"excerpts": [
			  "**--speculate_loads** = *n*\nSpecifies speculative load byte count threshold. Allows speculative\nexecution of loads with bounded addresses. See Section 3.2.3.1 .",
			  "Specifies speculative load byte count threshold. Allows speculative",
			  ".com\n*Optimizing Software Pipelining*\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "**--speculate_unknown_loads**",
			  "**--speculate_unknown_loads**\nAllows speculative execution of loads with bounded addresses.",
			  "Alias disambiguation is a technique that determines when two pointer expressions cannot point to the\nsame location, allowing the compiler to freely optimize such expressions.",
			  ";*\nMinimum required memory pad\n: 8 bytes",
			  "**Speculative Execution**",
			  "**3.2.3.1**",
			  "**3.2.3.2**",
			  "When you are writing your linear assembly, your code does *not* need to indicate the following:\n–\nPipeline latency\n–\nRegister usage\n–\nWhich unit is being used",
			  "e size reduction optimizations.\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.\nThe software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running. Example 3-1 illustrates the\ninformation that is generated for each loop.\nT",
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "\nThe macro works regardless of which C6x device you build for, or if you port the code to another target.\nA more rare case is to pass the address of an offset from an array, for example:\nf(&buffer[3]);\nThis code passes an unaligned address to ptr, thus violating the presumption coded in the _nassert().\nThere is no direct remedy for this case. Avoid this practice whenever possible.",
			  "**Selecting the Best Threshold Value**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Collapsing can usually reduce the minimum safe trip count.",
			  "If the minimum known trip count is less than\nthe minimum safe trip count, a redundant loop is required.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "Minimum required memory pad",
			  ": 8 bytes",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "Known Minimum Trip Count",
			  "Known Maximum Trip Count",
			  "Loop Carried Dependency Bound(^)",
			  "Unpartitioned Resource Bound",
			  "Partitioned Resource Bound(*)",
			  "5",
			  "Resource Partition:",
			  "A-side",
			  ".L units",
			  "2",
			  "2",
			  "2",
			  ".S units",
			  "4",
			  "4",
			  "4",
			  ".D units",
			  ".M units",
			  ".X cross paths",
			  ".T address paths",
			  "1",
			  "1",
			  "1",
			  "Long read paths",
			  "Long write paths",
			  "0",
			  "0",
			  "0",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "Optimizing Software Pipelining",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops.",
			  "The\n--disable_software_pipelining option affects both compiled C/C++ code and assembly optimized code.",
			  "**Software Pipelining May Increase Code Size**",
			  "**Turn Off Software Pipelining (--disable_software_pipelining Option)**",
			  "If you enter comments on instructions in your linear assembly input file, the compiler moves the comments\nto the output file along with additional information. It attaches a 2-tuple <x, y> to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the software pipeline.",
			  "Figure 3-2. Software-Pipelined Loop*",
			  "This chapter provides an overview of these tools and introduces the features of the optimizing C/C++\ncompiler."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.3 - User's Guide",
			"url": "https://coecsl.ece.illinois.edu/se423/datasheets/OMAP138Ref_Guides/C6000CCompiler7.3.pdf",
			"excerpts": [
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531901.pdf",
			"excerpts": [
			  "2",
			  "4",
			  "1"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "0"
			]
		  },
		  {
			"title": "Hand-Tuning Loops and Control Code on the TMS320C6000",
			"url": "https://www.ti.com/lit/pdf/spra666",
			"excerpts": [
			  "4"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler User's Guide",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/DSPs/Texas_Instrument_TMS320Cxx/DSP%20C6000/spru187.pdf",
			"excerpts": [
			  "Optimizing Software Pipelining"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/sprui04b",
			"excerpts": [
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous.",
			  "Memory alias disambiguation is the process of determining when such\nambiguity is not possible. When you cannot determine whether two memory references are ambiguous,\nyou presume they are ambiguous. This is the same as saying the two instructions have a memory\ndependence between them.",
			  "The assembly optimizer assumes memory references are aliased, unless it can prove otherwise.",
			  "ne side** . The C6000 has predicate, or conditional, registers available\nfor use with conditional instructions. There are six predicate registers. There are three on the A side\nand three on the B side. ",
			  "peculative threshold exceeded** . It would be necessary to speculatively load beyond the threshold\ncurrently specified by the --speculate_loads option. You must increase the --speculate_loads threshold\nas recommended in the software-pipeline feedback located in the assembly file",
			  "The resource breakdown\ndisplayed in the software pipelining information in the asm file, is computed after the compiler has\npartitioned instructions to either the A-side or the B-side. If the resources are imbalanced (i.e.; some\nresources on one side are used more than resources on the other) software pipelining is resource-bound,\nand the loop cannot be efficiently scheduled. If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining. You can give loop trip-\ncount information to the compiler using the \"MUST_ITERATE\" pragma",
			  "advice #30011: Consider adding assertions to indicate n-byte alignment\nof variables input1, input2, output if they are actually n-byte\naligned: _nassert((int)(input1) % 8 == 0).\nMost loops have memory access instructions. The compiler attempts to use wider load instructions, and\naligned memory accesses instead of non-aligned memory accesses to reduce/balance out resources used",
			  "**LDDW**",
			  "**STDW**",
			  "Allows aligned loads and stores of 8 bytes to memory. The pointer\nmust be aligned to an eight-byte boundary. This is defined as a\nmacro. You must include c6x.h. (2)(1)",
			  "Allows aligned loads of 8 bytes from memory. The pointer must be\naligned to an eight-byte boundary.",
			  "*Example 2",
			  "_nassert (((int)(x) & 0x3) == 0);",
			  "\\ MUST_ITERATE(20, , 4);",
			  "The C66x DSP generation comprises floating-point devices in the C6000 DSP platform. SPRUFE8— TMS320C674x CPU and Instruction Set Reference Guide. Describes ...Read more",
			  ".2.2***\n***Software Pipelining Information***\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "**4.2.3.1**\n**Speculative Execution**\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "collapsing which can be performed.\nWhen the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop. If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited. To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.\nCo",
			  "**4.3**\n**Redundant Loops**\nEvery loop iterates some number of times before the loop terminates. The number of iterations is called\nthe *trip count* . The variable used to count each iteration is the *trip counter* . When the trip counter reaches\na limit equal to the trip count, the loop terminates. The C6000 tools use the trip count to determine\nwhether or not a loop can be pipelined. The structure of a software pipelined loop requires the execution\nof a minimum number of loop iterations (a minimum trip count) in order to fill or prime the pipeline.",
			  "*4.4**\n**Utilizing the Loop Buffer Using SPLOOP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory",
			  "ized code.\n**Software Pipelining May Increase Code Size**\n**NOTE:**\nSoftware pipelining without the use of SPLOOP can lead to significant increases in code\nsize. To control code size for loops that get software pipelined, the --opt_for_space option is\nrecommended over the --disable_software_pipelining option. The --opt_for_space option is\ncapable of disabling non-SPLOOP software pipelining if necessary to achieve code size\nsavings, but it does not affect the SPLOOP capability (). SPLOOP does not significantly\nincrease code size, but can greatly speed up a loop. Using the --disable_software_pipelining\noption disables all software pipelining including SPLOOP.\n"
			]
		  },
		  {
			"title": "Register Allocation on the Intel® Itanium® Architecture",
			"url": "https://ssw.jku.at/Teaching/PhDTheses/Hoflehner/Diss_GerolfHoflehner_2010.pdf",
			"excerpts": [
			  "7.2 Data Speculation. IA-64 provides an advanced load (ld.a) and two advanced load check (chk.a, ld.c) instructions for data speculation. This enables the ...Read more",
			  "On IA-64 there are 64 predicate registers. Therefore the encoding of the qualifying predicate consumes 6 bits, which is one of the reasons why operations. ( ...Read more"
			]
		  },
		  {
			"title": "\n            BibTeX bibliography intel-ia-64.bib\n        ",
			"url": "http://ftp.math.utah.edu/pub/tex/bib/intel-ia-64.html",
			"excerpts": [
			  "\", URL = \"http://ieeexplore.ieee.org/iel5/6577/17552 ... data dependence speculation, and predication have individually been shown to be very effective.Read more"
			]
		  }
		],
		"reasoning": "The user is asking about 'Trace Scheduling' as a control flow optimization technique. The provided JSON object contains a detailed description of Trace Scheduling, including its definition, primary mechanism, and key trade-offs. The excerpts that are most relevant to this field value are those that explicitly mention 'Trace Scheduling' and describe its function in optimizing control flow and increasing Instruction-Level Parallelism (ILP). Excerpts that discuss traces, code motion across basic block boundaries, compensation code, and the goal of increasing ILP are highly relevant. Excerpts that mention related concepts like superblocks, hyperblocks, and predicated execution are also relevant as they often work in conjunction with or as alternatives to trace scheduling. Excerpts that discuss VLIW architectures and their compilation techniques are also relevant due to the historical context of trace scheduling's development for such architectures.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.1.description",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5).",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  }
		],
		"reasoning": "The target description defines a refinement of trace scheduling called a superblock, characterized by a trace with a single entry point and multiple possible exits, intended to simplify compiler bookkeeping and code generation. Several excerpts state that a superblock is a trace with all joins removed via tail duplication, which directly aligns with the idea of a single-entry, multi-exit unit that eases scheduling and reduces bookkeeping overhead. Additionally, the excerpts discuss the primary benefit of superblocks as avoiding the complex bookkeeping involved with moving code past joins, reinforcing why this refinement would simplify code generation. Supporting context from the excerpts on trace scheduling and its historical discussion provides background for why such a refinement would be valuable in VLIW compilation. Taken together, these excerpts coherently support the fine-grained field value, with emphasis on the definition of a superblock and its bookkeeping advantages.",
		"confidence": "high"
	  },
	  {
		"field": "repurposed_fault_tolerance_techniques",
		"citations": [
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S0141933122001168",
			"excerpts": [
			  "The proposed mechanism dynamically replicates instructions and reschedules original and replicated instructions both in the current bundle (space scheduling - S) and in the next upcoming bundle (time scheduling - T), improving performance, with reduced complexity and area overhead. To achieve this, a scheduling exploration window of two instruction bundles is used.",
			  "To address these limitations, a hardware mechanism is proposed that enhances VLIW processors with heterogeneous FUs with fault tolerant capabilities. The proposed mechanism dynamically replicates instructions and reschedules original and replicated instructions both in the current bundle (space scheduling - S) and in the next upcoming bundle (time scheduling - T), improving performance, with reduced complexity and area overhead. To achieve this, a scheduling exploration window of two instruction bundles is used.",
			  "The replication scheduler dynamically reschedules original and replicated instructions, avoiding the insertion of additional time slots, by postponing the execution of independent instructions to the next bundle. The voting scheduler synchronizes original and replicated instruction in order to compare their outputs, masking any occurring error. Due to dynamic rescheduling, switches have to be inserted in the VLIW pipeline. To reduce the area and critical delay overhead, we propose a switch hardware design that simplifies the complexity of the scheduler, compared to commonly used switches. Last, but not least, to enable scalability, we propose a novel cluster-based approach; a large VLIW is divided in several clusters, each consisting of a smaller VLIW.",
			  "In this way, new opportunities are created for optimizing the schedule during execution, while the scheduling complexity remains low.",
			  "The obtained results show an average speed-up of 24.99% in performance with an almost 10% area and power overhead, when time scheduling is also considered on top of space scheduling.",
			  "The proposed dynamic hardware mechanism is applied within a cluster (intra-cluster), providing significant gains in area, with small performance loss.",
			  "To support our contributions, we implemented on the VEX VLIW processor and compare the proposed approach that applies instruction triplication, considering space scheduling (TMR-S) and space and time scheduling based on clusters (TMR-STCB), with typical hardware FUs triplication methods and the unprotected baseline VLIW.",
			  "to enable scalability, we propose a novel cluster-based approach; a large VLIW is divided in several clusters, each consisting of a smaller VLIW.",
			  "Compared to the unprotected version, the instruction reliability has increased by ."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Microprocessors & Microsystems",
			"url": "https://dl.acm.org/doi/10.1016/j.micpro.2022.104564",
			"excerpts": [
			  "us FUs. This work presents a hardware mechanism for processors with heterogeneous FUs that dynamically replicates instructions and schedules both original and replicated instructions considering space and time scheduling. ",
			  "The proposed approach uses a small scheduling window of two cycles, leading to a hardware mechanism with small hardware area.",
			  "To reduce the area overhead, a cluster-based approach is proposed, enabling scalability for larger hardware designs.",
			  "The proposed mechanism is implemented on VEX VLIW processor.",
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Request PDF",
			"url": "https://www.researchgate.net/publication/360874051_Dynamic_fault-tolerant_VLIW_processor_with_heterogeneous_Function_Units",
			"excerpts": [
			  "This work presents a hardware mechanism for processors with heterogeneous FUs that dynamically replicates instructions and schedules both original and replicated instructions considering space and time scheduling.",
			  "The proposed approach uses a small scheduling window of two cycles, leading to a hardware mechanism with small hardware area.",
			  "To reduce the area overhead, a cluster-based approach is proposed, enabling scalability for larger hardware designs.",
			  "The proposed mechanism is implemented on VEX VLIW processor.",
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability.",
			  "In order to perform such a flexible dynamic instruction scheduling, switches are required, which, however, increase the hardware area.",
			  "Very Long Instruction Word, or VLIW, architectures have received much attention in specific-purpose applications such as scientific computation, digital signal processing, and even safety-critical systems.",
			  "In this study, a fault-tolerant design framework of VLIW processor is proposed. Specifically, this paper concentrates on the issue of dependable data path design.",
			  "The instruction duplication mechanism is further supported by a hardware enhancement for efficient result verification, which avoids the need of additional comparison instructions.",
			  "cle, we propose a compiler-assisted dynamic code duplication method to minimize the code size overhead, and present vulnerability-aware duplication algorithms to maximize the effectiveness of instruction duplication with least overheads for VLIW architecture.",
			  "In this work, we experiment with complier-directed instruction duplication to detect soft errors in VLIW datapaths. In the proposed approach, the compiler determines the instruction schedule by balancing the permissible performance degradation with the required degree of duplication. Our experimental results show that our algorithms allow the designer to perform tradeoff analysis between performance and reliability.",
			  "The proposed methodology aims to achieve processor data paths for VLIW architectures able to autonomously detect transient and permanent hardware faults while executing their applications.",
			  "The advantage of a software approach to hardware fault detection is interesting because it allows one to apply it only to the critical applications executed on the VLIW architecture, thus not causing a delay in the execution of noncritical tasks."
			]
		  },
		  {
			"title": "Removing communications in clustered microarchitectures through instruction replication | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1011528.1011529",
			"excerpts": [
			  "The need to communicate values between clusters can result in a significant performance loss for clustered microarchitectures.",
			  "The proposed scheme is built on top of a previously proposed state-of-the-art modulo-scheduling algorithm.",
			  "his work presents a new compilation technique that usesinstruction replication in\norder to reduce the number ofcommunications executed on a clusteredmicroarchitecture.",
			  "For such architectures, the need tocommunicate values between clusters can result",
			  "the number of communications can be further decreased by around one-third through replication, which results in a significant speedup.",
			  "IPC is increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for selected programs."
			]
		  },
		  {
			"title": "Removing communications in clustered microarchitectures ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/1011528.1011529",
			"excerpts": [
			  "In this work, we describe an optimization technique that removes\n\ncommunications by selectively replicating an appropriate set of instructions. Instruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources. The proposed scheme is built on top of a previously proposed state-of-\n\nthe-art modulo-scheduling algorithm.",
			  "IPC\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs. We also show that replicating appropriate sets of instructions is more effective\n\nthan doubling the intercluster connection network bandwidth.",
			  "The main conclusion is that instruction replication\n\nACM Transactions on Architecture and Code Optimization, Vol. 1, No. 2, June 2004. 140\n\n•\n\nA. Alet`a et al. Fig. 8. IPC for mgrid. Fig. 9. Reduction of the II for applu. increases performance for all the benchmarks and for all the tested architec-\n\ntures. It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications. Benefits would be even higher for more basic\n\nschedulers. For example, for the 4c2b4l64r configuration, the average speedup\n\nprovided by replication is 25%. For some programs such as su2cor, the benefits\n\ncan be up to 70%, 65% for tomcatv, and 50% for swim",
			  "It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications.",
			  "e have implemented our replication technique as a part of a research compiler\n\n[Ayguad´e et al. 1996]. To drive our evaluation, we have used the SPECfp95\n\nbenchmarks.",
			  "e assumed a VLIW architecture with an issue width of 12. In this\n\narchitecture, we assume four floating-point (FP) FUs, four integer (INT) FUs\n\nand four memory ports (MEM). The different cluster configurations are pre-\n\nsented in Table 1.",
			  "repli-\n\ncation scheme is implemented on top of a state-of-the-art scheduling algorithm\n\nfor clustered processors. The proposed technique is evaluated for a clustered VLIW machine, though it\n\ncan be used for any statically scheduled architecture.",
			  "For instance, for the\n\n4c1b2l64r, 36% of the communications are removed and every communication\n\nrequires the replication of 2.1 instructions on average"
			]
		  },
		  {
			"title": "Removing Communications in Clustered Microarchitectures ...",
			"url": "https://ece.northeastern.edu/groups/nucar/publications/TACO04.pdf",
			"excerpts": [
			  "nstruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources",
			  "results show that the number of communications can be further\n\ndecreased by around one-third through replication, which results in a significant speedup",
			  "C\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs.",
			  " remainder of this paper is organized as follows. Section 2 provides some\n\nbackground on modulo-scheduling and graph partitioning as well an overview\n\nof the baseline algorithm. Section 3 presents some statistics that motivate this\n\nwork. Section 4 describes our replication heuristics. Section 5 analyzes its per-\n\nforman",
			  "nce, Aggarwal and Franklin work\n\nrelies on simple heuristics to dynamically replicate instructions that can be\n\nimplemented at run-time, as opposed to our technique that is implemented in\n\nthe compiler",
			  "k, we have presented a compiler technique to replicate selected in-\n\nstructions in order to reduce intercluster communications. The proposed tech-\n\nnique is shown to reduce the number of communications by approximately one\n\nthird, depending on the processor configuration. Replication has been shown to\n\nproduce significant speedups for all configurations and for all programs. For in-\n\nstance, for a four-cluster processor, the average speedup is 25%, and for selected\n\nprograms like _su2cor_ , speedup can be as high as 70%",
			  "replicating appropriate sets of instructions is more effective\n\nthan doubling the intercluster connection network bandwidth",
			  "e evaluate this ap-\n\nproach using 678 loops taken from the SPECfp95 benchmark suit"
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous ...",
			"url": "https://people.rennes.inria.fr/Olivier.Sentieys/publications/2022/Psiakis22MICPRO.pdf",
			"excerpts": [
			  "The proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhead. To achieve\nthis, a scheduling exploration window of two instruc-\ntion bundles is used.",
			  "he proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhea",
			  "order to perform such a flexible dynamic instruction\nscheduling, switches are required, which, however, in-\ncrease the hardware area. To reduce the area over-\nhead, a cluster-based approach is proposed, enabling\nscalability for larger hardware designs",
			  "To support our contributions, we implemented on the\nVEX VLIW processor and compare the proposed ap-\nproach that applies instruction triplication, consid-\nering space scheduling (TMR-S) and space and time\nscheduling based on clusters (TMR-S+T+CB), with\ntypical hardware FUs triplication methods and the\nunprotected baseline VLIW.",
			  "mechanism is implemented on VEX VLIW proces-\nsor. The obtained results show an average speed-up\nof 24.99% in performance with an almost 10% area\nand power overhead, when time scheduling is also\nconsidered on top of space scheduling.\nC",
			  "we implemented on the\nVEX VLIW processor and compare the proposed ap-\nproach that applies instruction triplication, consid-\nering space scheduling (TMR-S) and space and time\nscheduling based on clusters (TMR-S+T+CB), with\ntypical hardware FUs triplication methods and the\nunprotected baseline VLIW. We performed extensive\nexperimental results, providing performance, area,\npower and reliability metrics for ten benchmarks.",
			  "Static approaches are usually implemented by soft-\nware. They replicate and schedule the instructions\nat design-time and additional instructions are in-\nserted for comparison of the results.",
			  "The remaining hardware dynamic approaches ap-\nply dynamic scheduling, assuming that VLIW issues\ninclude all FUs, thus can execute any instruction.\nHowever, when the VLIW consists of issues with dif-\nferent type of FUs, the existing schedulers are not\napplicable, as they ignore the type of ",
			  "The proposed dynamic hardware mechanism is ap-\nplied within a cluster (intra-cluster), providing sig-\nnificant gains in area, with small performance loss.",
			  "The proposed dynamic hardware mechanism is ap-\nplied within a cluster (intra-cluster), providing sig-\nnificant gains in area, with small performance loss."
			]
		  },
		  {
			"title": "Fault Tolerance in Super-Scalar and VLIW Processsors",
			"url": "https://escholarship.org/content/qt045296h7/qt045296h7_noSplash_bc64414330f80a28858c967c3e50825f.pdf",
			"excerpts": [
			  "These operations are begun during the same clock cycle\nand their results are compared when the operations are completed. This duplication\nand comparison will detect any fault within the primary or redundant functional\nunit that affects the operation being performed.",
			  "In VLIW, a more involved reconfiguration\nprocedure is necessary.",
			  "we outline several alternative reconfiguration strategies for VLIW\nprocessors.",
			  "In general, the set of functional unit failures that can be tolerated by our ap-\nproach depends on the way in which units are grouped for comparison purposes,\non the diagnosis mechanism utilized, and on the reconfiguration procedur",
			  "The results show\nthat, while super-scalar and VLIW with recompilation provide the high-\nest reliability, all proposed strategies significantly increase reliability over\nthat of an unprotected processo",
			  "In super-scalar processors, this is not\ndifficult since the scheduling is dynamic.",
			  "These strategies sacrifice storage space and execution time,\nrespectively, in order to reconfigure without recompiling."
			]
		  },
		  {
			"title": "The Application of Compiler-Assisted Multiple Instruction Retry ...",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA266930.pdf",
			"excerpts": [
			  "Very Long Instruction Word (VLIW) architectures enhance performance by exploiting fin-e-\ngrained instruction level parallelism. In this paper, we describe the development of two\ncompiler assisted multiple instruction word retry schemes for VLIW architectures",
			  "The\nfirst scheme utilizes the compiler techniques previously developed for processors with single\nfunctional units [1]. Compiler generated hazard-free code with different degrees of rollback\ncapability for uni-processors is compacted by a modified VLIW trace scheduling algorithm.",
			  "Nops are then inserted in the scheduled code words to resolve data hazards for VLIW ar-",
			  "The machine model consists of several functional units, each of which has two read ports,",
			  "and one write port connecting to a general register file.",
			  "The functional units operate simultaneously accessing the"
			]
		  },
		  {
			"title": "Compiler-Assisted Soft Error Correction by Duplicating ...",
			"url": "https://sasimi.jp/new/sasimi2012/files/program/archive/pdf/p54_R1-11.pdf",
			"excerpts": [
			  "Using replicated instructions and a software\nmethodology to detect hardware faults in VLIW datapaths\nhave been presented in the previous work [6], [10].",
			  "Using replicated instructions and a software\nmethodology to detect hardware faults in VLIW datapaths\nhave been presented in the previous work [6], [10]."
			]
		  },
		  {
			"title": "Run Time Instruction Replication for Permanent and Soft ...",
			"url": "https://inria.hal.science/hal-01633778/file/Run%20Time%20Instruction%20Replication%20for%20Permanent%20and%20Soft%20Error%20Mitigation%20in%20VLIW%20Processors.pdf",
			"excerpts": [
			  "To provide correct system functionality, the system has to be enhanced with fault tolerant techniques for both temporary errors, caused by heat, ...Read more",
			  "ardware mechanism is proposed**\n**which at run-time replicates the instructions and schedules them**\n**at the idle slots considering the resource constra",
			  "f a resource**\n**becomes faulty, the proposed approach efficiently rebinds both**\n**the original and replicated instructions during exe",
			  "ults**\n**show up to 49% performance gain over existing techniques",
			  "it is applicable for any VLIW structure, i.e. any\nissue width and number and type of FUs."
			]
		  },
		  {
			"title": "Adaptive and Polymorphic VLIW Processor to Optimize ...",
			"url": "http://www.ecs.umass.edu/ece/koren/research/slkkb-cf-2018.pdf",
			"excerpts": [
			  "The authors evaluate the duplication (for fault detection only) and the triplica- tion of instructions in order to provide fault tolerance.Read more"
			]
		  },
		  {
			"title": "Run-time Coarse-Grained Hardware Mitigation for Multiple ...",
			"url": "https://inria.hal.science/hal-02344282/document",
			"excerpts": [
			  "A hardware mechanism is proposed for multiple soft faults that characterizes the FUs of the VLIW processor and resched- ules at run-time the ...Read more"
			]
		  },
		  {
			"title": "Dynamic Trade-off among Fault Tolerance, Energy ...",
			"url": "https://ieeexplore.ieee.org/document/8060605/",
			"excerpts": [
			  "Based on that, we propose a new VLIW-based processor design capable of adapting the execution of the application at run-time in a totally ...Read more"
			]
		  },
		  {
			"title": "High-Performance and Low-Cost Dual-Thread VLIW ...",
			"url": "https://prod.tinker.cc.gatech.edu/journal/TPDS_weld.pdf",
			"excerpts": [
			  "The dual-thread VLIW processor model is a low-cost subset of the Weld architecture paradigm. It supports one main thread and one speculative thread running ...Read more"
			]
		  },
		  {
			"title": "INCORPORATING FAULT-TOLERANT FEATURES IN ...",
			"url": "https://worldscientific.com/doi/pdf/10.1142/S0218539305001914?download=true&srsltid=AfmBOopsah5xYqISXFGFlP9g55cIiQ_8K38CUxnvuzuA75-za2ToMJvi",
			"excerpts": [
			  "They are deficient in the analysis of error recovery time demanded to overcome the transient faults. The error recovery time mainly depends on the error ...Read more"
			]
		  },
		  {
			"title": "(PDF) SWIFT: software implemented fault tolerance",
			"url": "https://www.researchgate.net/publication/4129281_SWIFT_software_implemented_fault_tolerance",
			"excerpts": [
			  "SWIFT demonstrates a 51% average speedup.",
			  "As a software-based approach, SWIFT requires no\nhardware beyond ECC in the memory subsystem.",
			  "SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs.",
			  "This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT."
			]
		  },
		  {
			"title": "(PDF) SWIFT: Software Implemented Fault Tolerance",
			"url": "https://www.academia.edu/9018951/SWIFT_Software_Implemented_Fault_Tolerance",
			"excerpts": [
			  "SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs.",
			  "We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost.",
			  "Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup.",
			  "SWIFT achieves a 14% average speedup over existing single-threaded fault tolerance techniques.",
			  "The technique efficiently utilizes unused instruction-level resources for fault detection without hardware modifications.",
			  "It provides high fault coverage comparable to hardware redundancy methods without additional hardware costs.",
			  "SWIFT demonstrates exceptional fault tolerance with a tested implementation on the Itanium 2 architecture.",
			  "To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates.",
			  "This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT.",
			  "SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism."
			]
		  },
		  {
			"title": "SWIFT: Software Implemented Fault Tolerance",
			"url": "https://liberty.princeton.edu/Publications/cgo05_swift.pdf",
			"excerpts": [
			  "T efficiently manages re-*\n*dundancy by reclaiming unused instruction-level resources*\n*present during the execution of most program",
			  "IFT also*\n*provides a high level of protection and performance with an*\n*enhanced control-flow checking mechanis",
			  "IFT demon-\nstrates a 14% average speedup.",
			  "Compared to the best known single-threaded ap-\nproach utilizing an ECC memory system, SWIFT demon-\nstrates a 14% average spee",
			  "his paper introduces SWIFT, the\nbest performing single-threaded software-based approach\nfor full out fault detection. ",
			  "SWIFT demonstrates\nexceptional fault-coverage with a reasonable performance\ncost.",
			  "SWIFT demon-\nstrates a 14% average speedu"
			]
		  },
		  {
			"title": "Exploiting Idle Hardware to Provide Low Overhead Fault ...",
			"url": "https://www.researchgate.net/publication/312259995_Exploiting_Idle_Hardware_to_Provide_Low_Overhead_Fault_Tolerance_for_VLIW_Processors",
			"excerpts": [
			  "In this paper, an effective fault-tolerant framework offering very high error coverage with zero detection latency is proposed to protect the data paths of ...Read more"
			]
		  },
		  {
			"title": "Selective Code Duplication for Soft Error Protection on VLIW Architectures",
			"url": "https://www.mdpi.com/2079-9292/10/15/1835",
			"excerpts": [
			  " present the packing-oriented duplication in order to maximize the number of duplicated instructions within the same performance overheads. We define the pack as the number of duplicable instructions in the same code line, and we select larger packs for instruction duplication",
			  "All the instructions cannot be replicated without additional code lines. Additional code lines are required to increase the number of duplicated instructions in VLIW architectures.",
			  "Additional code lines are required to increase the number of duplicated instructions in VLIW architectures.",
			  "Our experimental results show that 52% performance overhead as compared to unprotected source code when we duplicate all the instructions.",
			  "However, the previous static scheme duplicates instructions just in sequential order. In this work, we propose packing-oriented duplication to maximize the number of duplicated instructions within the same peroformance overhead bounds.",
			  "In this work, we propose packing-oriented duplication to maximize the number of duplicated instructions within the same peroformance overhead bounds.",
			  "This considerable performance overhead can be inapplicable for resource-constrained embedded systems so that we can limit the number of additional NOP instructions for selective protection.",
			  "exploiting unused instruction slots with minimal hardware modules such as register queues and comparator.",
			  "In order to bridge the gap between them, compiler-assisted instruction duplication techniques have been proposed for VLIW architecture."
			]
		  },
		  {
			"title": "Analysis and Characterization of Intel Itanium Instruction ...",
			"url": "http://ieeexplore.ieee.org/document/4673579/",
			"excerpts": [
			  "In order to achieve high instruction level parallelism (ILP), designers are turning to very long instruction word (VLIW) based designs, in which different types of instructions are grouped together as bundles of 128 bits or longer.",
			  "In VLIW, the added nops increase the code size, limit processor performance by the under-utilization of functional units.",
			  " present a comprehensive analysis of the problem of under-utilization due to nops and stops across a wide range of application domains through the use of three different benchmark suites: SPEC CPU 2000, MediaBench, and Pack",
			  "Our results show that, on average, nops create an under-utilization factor of 28.46% in the case of SPEC CPU, 32.27% in MediaBench, and 29.76% in PacketBench."
			]
		  },
		  {
			"title": "Integer Performance Evaluation of the Dynamically Trace ...",
			"url": "https://discovery.ucl.ac.uk/10107651/1/Integer_performance_evaluation.pdf",
			"excerpts": [
			  "he wakeup logic is\nresponsible for matching the results produced by functional units with the source\n72\noperands of instructions waiting in the instruction window or reservation stations and\nfor setting the instructions as ready. The selection logic is responsible for selecting\ninstructions for execution from the pool of ready instructions. To reduce the impact\nof wire delays and wakeup and selection logic overheads, Palacharla, Jouppi, and\nSmith have suggested dividing the Superscalar core into several smaller clusters of\nfunctional units (A similar proposal specifically tailored to Trace Cache architectures\nis reported in [Vajapeyam97].) The resulting architecture has been named the\n*Dependence-Based* architecture. This architecture groups dependent instructions and\nsends them to the same cluster. This grouping of dependent instructions in clusters\nsimplifies the wakeup and selection logic and helps mitigate the wire delays to some\nextent by using short local connections more frequently than long inter-cluster\nconnections. The Dependence-Based architecture has inferior performance than non­\nclustered Superscalar, however. Therefore, the DTSVLIW can compete in\nperformance with this variant of Superscalar as well. Moreover, we believe that\nclustering can also be employed in the DTSVLIW, although we do not examine\nclustered DTSVLIWs in this thes",
			  "The results of Figure 6.4 represent the highest achievable SPECint95 performance of\nthe DTSVLIW for the block sizes shown. However, when the VLIW Cache is\nsmaller, the performance is expected to be lower due to premature flushing of useful\nscheduled blocks by replacement blocks, leading to the need to rebuild the blocks\nflushed. This requires the Primary Processor to run, reducing parallelism.\nFigure 6.5 shows the impact of different VLIW Cache sizes (in Kbytes, the\ndirectory information is not included) on the performance of a DTSVLIW machine\nwith 8x8-block geometry. The associativity is the same for all sizes and equal to 4.\nAs the graph shows, some benchmark programs do not demand a large VLIW Cache\nin order to exploit the performance of the DTSVLIW. The benchmarks compress,\nijpeg, and xlisp have small instruction working set [Chamey97] and are insensitive to\nthe VLIW Cache size, achieving the same performance for the range of sizes used.",
			  "Our results show that the DTSVLIW can achieve\naverage ILP higher than 4 IPC with 16 untyped functional units and perfect caches,\nand 2 IPC with a feasible machine configuration with 12 typed functional units and\ncaches of reasonable sizes.",
			  "The design of the DTSVLIW architecture has been driven by the requirement to\ndevelop an architecture that can be effectively implemented to realise the fast"
			]
		  },
		  {
			"title": "Itanium Performance Insights from the IMPACT Compiler",
			"url": "http://impact.crhc.illinois.edu/shared/talks/hotchips-2001-final-color.pdf",
			"excerpts": [
			  "Itanium design goal: enhance scalability of parallelism by\nmoving complex decisions to the compiler",
			  "Bundling: enables static scheduling by communicating instruction\nparallelism",
			  "Predication: allows compiler to optimize across multiple paths by\nproviding an alternative to control flow",
			  "Speculation support: allows compiler to select specific instructions\nfor early execution",
			  "IMPACT compiler supports ILP compilation and research",
			  "Extensible framework for easy implementation of new optimizations",
			  "Versatile and pervasive intermediate representation (IR)",
			  "Comprehensive predicate-aware dataflow and predicate analyses",
			  "Memory disambiguation, profiling",
			  "Static scheduling, control speculation, predication",
			  "SPECcpu2000 Ratio Comparison",
			  "Dual processor HP i2000 Itanium 800 MHz with 2MB L3 cache",
			  "linux kernel 2.4.7 with PMC patch, gcc 2.9.6, ecc 5.0.1 beta"
			]
		  },
		  {
			"title": "Branch Predication | Request PDF",
			"url": "https://www.researchgate.net/publication/299748687_Branch_Predication",
			"excerpts": [
			  "The IA-64 architecture is rich with features that enable aggressive exploitation of instruction-level parallelism. Features such as speculation, predication, multiway branches and others provide compilers with new opportunities for the extraction of parallelism in programs.",
			  "This paper describes the implementation of the global code scheduler (GCS) in Intel's reference compiler for the IA-64 architecture. GCS schedules code over acyclic regions of control flow.",
			  "There is a tight coupling between the formation and scheduling of regions.",
			  "This paper also presents P-ready code motion, an opportunistic instruction level tail duplication which aims to strike a balance between code expansion and performance potential.",
			  "Performance results show greater than 30% improvement in speedup for wavefront scheduling over basic block scheduling on the Merced microarchitecture"
			]
		  },
		  {
			"title": "From VLIW to EPIC Architectures",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2002/lectures/Lecture17.pdf",
			"excerpts": [
			  "What are the shortcomings of VLIW? • Wasteful encoding with nops. • No compatibility across generations. • Even with block-coalescing techniques such as trace ...Read more"
			]
		  },
		  {
			"title": "Hand-Optimizing VLIW Assembly Language as a Game | Hacker News",
			"url": "https://news.ycombinator.com/item?id=24525372",
			"excerpts": [
			  "VLIW directly exposes hardware parallelism to the programmer, making more throughput available without requiring complex scheduling machinery (save die area + power). This works well for code targeting a single architecture (like embedded) where timing is predictable (like DSP code).",
			  "Essentially VLIW machine code embeds a bunch of very specific assumptions about processor's internal design.",
			  "There's no reason for the VLIW machine not to get wider with time. The fact that Itanium didn't speaks more of its economic unhealthiness than of anything else.",
			  "Summarizing what I learned when I looked into this:\nVLIW directly exposes hardware parallelism to the programmer, making more throughput available without requiring complex scheduling machinery (save die area + power). This works well for code targeting a single architecture (like embedded) where timing is predictable (like DSP code). Essentially VLIW machine code embeds a bunch of very specific assumptions about processor's internal design."
			]
		  },
		  {
			"title": "The most prominent example of VLIW processors was ...",
			"url": "https://news.ycombinator.com/item?id=38676884",
			"excerpts": [
			  "The most prominent example of VLIW processors was the Itanic, er, Itanium. It, er, didn't work out well. Hence Itanic. Their premise was that the compiler ...Read more"
			]
		  },
		  {
			"title": "Microarchitectural Techniques for Instruction-driven Specialization ...",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-210.pdf",
			"excerpts": [
			  "The most ambitious attempt to apply VLIW to general-purpose workloads was Intel’s\nItanium, based on the Explicitly Parallel Instruction Computing (EPIC) model and the IA-\n64 instruction set",
			  "EPIC was an evolution of the VLIW concept—drawing on ideas from the\nearlier Cydrome and TRACE architectures—while adding ISA-level features to maximize\ngeneral-purpose ILP and software compatibility."
			]
		  }
		],
		"reasoning": "The core idea of repurposing fault tolerance mechanisms for performance is supported by several excerpts. Some describe hardware mechanisms that dynamically replicate instructions and schedule them in idle slots (space scheduling) and upcoming bundles (time scheduling) to improve performance and reliability. This dynamic replication and scheduling is explicitly mentioned as a way to exploit idle functional units. The original technique of dynamic instruction replication and space-time scheduling for fault tolerance is detailed in multiple sources. These sources explain how instructions are replicated and rescheduled, sometimes across two instruction bundles, to detect and mitigate faults. The performance application of hiding inter-cluster communication latency is directly addressed by research showing that instruction replication can reduce the need for communication between clusters, leading to significant speedups. For instance, one study on a 12-issue VLIW architecture demonstrated an average IPC increase of 25% and up to 70% for specific programs by replicating instructions to avoid inter-cluster data transfers. The justification for this approach, based on the under-utilization of functional units in VLIW machines, is also well-supported. Several excerpts highlight that VLIW processors often have idle functional units because applications lack sufficient instruction-level parallelism to keep all units busy. Data from the Intel Itanium, a VLIW-like processor, indicates an average functional unit under-utilization of 28-32% on various benchmarks. This substantial amount of unused capacity presents a clear opportunity to repurpose these idle slots for performance-enhancing redundant or speculative computations, as proposed in the field value.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.1.technique_name",
		"citations": [
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited"
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5).",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  }
		],
		"reasoning": "The most directly relevant excerpts explicitly present the concept of Superblock Scheduling and its practical workflow. Excerpt 4 introduces a Superblock Scheduler for code-size sensitive applications and outlines steps that include constructing traces, forming superblocks, and scheduling within them, aligning exactly with the technique name. Excerpt 5 follows with concrete steps to construct and schedule within superblocks, illustrating the end-to-end process. Excerpt 9 provides a formal definition of what superblocks are in this context, describing them as blocks with a single entry and (often) multiple exits, which grounds the concept. Excerpts 0–2 discuss superblocks as a mechanism and definition (a trace with joins removed via tail duplication), reinforcing the core idea behind the technique. Excerpt 8 cites the foundational paper that introduces the concept in the VLIW/superscalar domain, further validating the technique’s relevance. Excerpts 11–12 discuss formal scheduling aspects of superblocks, deepening the theoretical backing. Excerpt 10 reiterates the same definition from a conference paper, reinforcing the terminology. Excerpt 13 provides additional cross-reference to the foundational work, reinforcing the established lineage. Excerpts 3, 6, and 7 offer broader context about trace scheduling and practical limitations, which are useful for understanding the ecosystem around Superblock Scheduling but are less direct than the pieces that name or implement the technique. Excerpt  (combined) thus supports the notion that Superblock Scheduling is a central, well-studied approach for VLIW scheduling, particularly when dealing with limited code size, irregular control flow, and the need to form large, efficiently scheduled blocks of instructions under complex hardware constraints.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.1.primary_mechanism",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5).",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  }
		],
		"reasoning": "The description identifies tail duplication as the core mechanism to achieve a single-entry trace by eliminating all side entries (joins) within the trace and duplicating the code from the join point to the end of the trace for each side entry, thereby generating the compensation code upfront. The most directly supportive content states that a superblock is a trace with all joins removed via tail duplication, and that this is tied to the notion of introducing upfront compensation to maintain correct execution. Additional passages articulate that the primary benefit of superblocks is the avoidance of complex bookkeeping associated with moving code past joins, which aligns with the goal of creating a single-entry trace by duplicating code around joins. Further references discuss trace scheduling and the broader context of superblocks in VLIW/superscalar compilation, reinforcing that tail duplication is the concrete technique used to achieve the single-entry property of a trace. Some sources emphasize the trade-offs and design considerations (such as code size and the mechanics of constructing traces and superblocks), which provide contextual support for the technique but are secondary to the core mechanism described. Taken together, the most relevant excerpts directly confirm that tail duplication is used to remove joins by duplicating code, thereby establishing the single-entry trace and upfront compensation, with supporting context from related scheduling literature and practical considerations.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.0.primary_mechanism",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "Using estimates of branch directions obtained automatically\nthrough heuristics or profiling, the compiler selects the most\nlikely path, or \"trace\", that the code will follow during execu-\ntion. This trace is then treated as if it were free of conditional\nbranches, and handed to the code generator. The code genera-\ntor schedules operations into wide instruction words, taking into\naccount data precedence, optimal scheduling of functional units,\nregister usage, memory accesses, and system buses; it emits\nthese instruction words as object code. This greedy scheduling\ncauses code motions which could cause logical inconsistencies\nwhen branches off-trace are taken. The compiler inserts special\n\"compensation code\" into the program graph on the off-trace\nbranch edges to undo these inconsistencies, thus restoring pro-\ngram correctness.",
			  "Instruction Word (VLIW) architectures were prom-\nised to deliver far more than the factor of two or three that\ncurrent architectures achieve £rom overlapped execution. Using\na new type of compiler which compacts ordinary sequential code\ninto long instruction words, a VLIW machine was expected to\nprovide from ten to thirty times the performance of a more con-\nventional machine built of the same implementation technology.\nMultiflow Computer, Inc., has now built a VLIW called the\nTRACE'\" along with its companion Trace Scheduling\" com-\npacting compiler. This new machine has fulfilled the perfor-\nmance promises that were made. Using many fast functional\nunits in parallel, this machine extends some of the basic\nReduced-Instruction-Set\nprecepts:\nthe\narchitecture\nis\nload/store, the microarchitecture is exposed to the compiler,\nthere is no microcode, and there is almost no hardware devoted\nto synchronization, arbitration, or interlocking of any kind (the\ncompiler has sole responsibility for runtime ",
			  "Multiflow's Trace Scheduling compacting compiler automatically\nfinds fine-grained parallelism throughout any appfication.\nIt\nrequires no programmer intervention, either in terms of restruc-\nturing the program so it fits the architecture or in adding direc-\ntives which explicitly identify opportunities for overlapped exe-\ncution. This is in sharp contrast to \"coarse-grained\" parallel\narchitectures, including vector machines, shared-memory mul-\ntiprocessors,\nand\nmore\nradical\nstructures\nsuch\nas\nhypercubes selt85 or massively-parallel machines, wurst The pro-\ncess by which programs are converted into highly parallel wide-\ninstruction-word code is transparent to the user.",
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled.",
			  "A number of conventional optimizations aid the trace selection\nprocess in finding parallelism. Automatic loop unrolling and\nautomatic inline substitution of subroutines are both incor-\nporated in Multiflow's compilers;\nthe compiler heuristically\ndetermines the amount of unrolling and substitution, substan-\ntially increasing the parallelism that can be exploited.",
			  "This process allows the compiler to break the \"conditional jump\nbottleneck\" and find parallelism throughout long streams of\ncode, achieving order-of-magnitude speedups due to compac-\ntion."
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace.",
			  "Trace scheduling uses a basic block scheduling method to schedule the instructions in each entire trace, beginning with the trace with the highest frequency.",
			  "This can result in large increases in code sizes and poor or erratic performance if program's behavior varies significantly with the input.",
			  "A trace is a sequence of instructions, including branches but not including loops, that is executed for some input data."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant).",
			  "The performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%)."
			]
		  },
		  {
			"title": "Very Long Instruction Word Architectures and the ELI-512",
			"url": "https://courses.cs.washington.edu/courses/cse548/16wi/Fisher-VLIW.pdf",
			"excerpts": [
			  "TRACE SCIIEDULING. The VLIW compiler we have built uses a recent global compaction technique called trace scheduling Fisher 811. This technique ww originally ...Read more"
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations.",
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "We also give a very simple proof that the trace scheduling algorithm always terminates."
			]
		  }
		],
		"reasoning": "The field value asserts that a compiler identifies a frequent execution path (the trace) and schedules it as a single unit, while ensuring correctness by inserting compensation code on off-trace paths when control flow splits or joins deviate from the predicted path. The most directly supportive information states that trace scheduling compacts a trace by code motion and that compensation code is inserted to undo inconsistencies after moving instructions past branches or splits. Additional passages explain how the trace is chosen (the most likely path), how the code generator schedules operations into wide instruction words with data precedence and memory considerations, and how compensation code is used to preserve program correctness when the trace deviates. Supporting excerpts also explain that multiple traces can be selected iteratively, and that compensation code can be avoided or suppressed, with quantified performance implications. Foundational and historical context about trace scheduling, its goals, and its limitations further-ground the description of the mechanism as a core approach to optimizing VLIW throughput by exploiting trace-level parallelism while managing control-flow deviations. Taken together, the most relevant material directly confirms that trace scheduling identifies a frequent execution path and plans it as a unit, with compensation code handling off-trace correctness, which matches the finegrained field value. Additional excerpts flesh out how compensation code is generated, moved, or avoided, and how the trace-based approach interacts with performance and code motion constraints, aligning with the described mechanism in the field value.",
		"confidence": "high"
	  },
	  {
		"field": "empirical_case_studies_summary.platform",
		"citations": [
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "With the Pentium 4, the microprocessor family that began with the 8086 and that\nhas been the most successful computer product line ever appears to have come\nto an end. Intel has teamed up with Hewlett-Packard (HP) to develop a new\n64-bit architecture, called IA-64. IA-64 is not a 64-bit extension of Intel’s 32-bit\nx86 architecture, nor is it an adaptation of Hewlett-Packard’s 64-bit PA-RISC\narchitecture. Instead, IA-64 is a new architecture that builds on years of research\nat the two companies and at universities. The architecture exploits the vast cir-\ncuitry and high speeds available on the newest generations of microchips by a\nsystematic use of parallelism. IA-64 architecture represents a significant depar-\nture from the trend to superscalar schemes that have dominated recent proces-\nsor development."
			]
		  },
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "The Itanium architecture allows issuing of independent instructions in\nbundles (three instructions per bundle) for parallel execution and can issue multiple\nbundles per clock.",
			  "Itanium architecture incorporates features which enable high sustained performance\nand remove barriers to further performance increases. The Itanium architecture is\nbased on the following principles:",
			  "• Features that enhance instruction level parallelism",
			  "A key feature of the Itanium architecture is IA-32 instruction set compatibility. The Intel® Itanium® Architecture Software Developer's Manual ...Read more"
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "VLIW-like architecture except for memory operations\n(out-of-order memory accesses)\nIPC up to 6 (Instructions Per Cycle)"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Itanium Performance Insights from the IMPACT Compiler",
			"url": "http://impact.crhc.illinois.edu/shared/talks/hotchips-2001-final-color.pdf",
			"excerpts": [
			  "Itanium design goal: enhance scalability of parallelism by\nmoving complex decisions to the compiler"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol4.pdf",
			"excerpts": [
			  "IA-64 architecture features a revolutionary 64-bit instruction set architecture (ISA) which applies a\nnew processor architecture technology called EPIC, or Explicitly Parallel Instruction Computing."
			]
		  }
		],
		"reasoning": "The statement asserts that Intel Itanium 2 is a 6-issue EPIC processor. The evidence directly describing this configuration includes: Itanium’s organization blending superscalar features with EPIC-specific features and explicitly noting a limit of six instructions per clock cycle, which matches the 6-issue capacity. Additional strong support comes from explicit mentions of EPIC as the design approach for IA-64 and the notion that the Itanium supports issuing multiple instruction bundles, with a maximum IPC of six per cycle. Supporting material also discusses predication, control speculation, and modulo scheduling within the EPIC/IA-64 framework, reinforcing that the Itanium/Itanium 2 family operates under the EPIC paradigm with a bounded, six-instruction-per-cycle throughput. These points collectively corroborate the finegrained field value that the Itanium 2 is a 6-issue EPIC VLIW-like architecture where the compiler encodes instruction-level parallelism.\"",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.0.technique_name",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "Instruction Word (VLIW) architectures were prom-\nised to deliver far more than the factor of two or three that\ncurrent architectures achieve £rom overlapped execution. Using\na new type of compiler which compacts ordinary sequential code\ninto long instruction words, a VLIW machine was expected to\nprovide from ten to thirty times the performance of a more con-\nventional machine built of the same implementation technology.\nMultiflow Computer, Inc., has now built a VLIW called the\nTRACE'\" along with its companion Trace Scheduling\" com-\npacting compiler. This new machine has fulfilled the perfor-\nmance promises that were made. Using many fast functional\nunits in parallel, this machine extends some of the basic\nReduced-Instruction-Set\nprecepts:\nthe\narchitecture\nis\nload/store, the microarchitecture is exposed to the compiler,\nthere is no microcode, and there is almost no hardware devoted\nto synchronization, arbitration, or interlocking of any kind (the\ncompiler has sole responsibility for runtime ",
			  "Multiflow's Trace Scheduling compacting compiler automatically\nfinds fine-grained parallelism throughout any appfication.\nIt\nrequires no programmer intervention, either in terms of restruc-\nturing the program so it fits the architecture or in adding direc-\ntives which explicitly identify opportunities for overlapped exe-\ncution. This is in sharp contrast to \"coarse-grained\" parallel\narchitectures, including vector machines, shared-memory mul-\ntiprocessors,\nand\nmore\nradical\nstructures\nsuch\nas\nhypercubes selt85 or massively-parallel machines, wurst The pro-\ncess by which programs are converted into highly parallel wide-\ninstruction-word code is transparent to the user.",
			  "Using estimates of branch directions obtained automatically\nthrough heuristics or profiling, the compiler selects the most\nlikely path, or \"trace\", that the code will follow during execu-\ntion. This trace is then treated as if it were free of conditional\nbranches, and handed to the code generator. The code genera-\ntor schedules operations into wide instruction words, taking into\naccount data precedence, optimal scheduling of functional units,\nregister usage, memory accesses, and system buses; it emits\nthese instruction words as object code. This greedy scheduling\ncauses code motions which could cause logical inconsistencies\nwhen branches off-trace are taken. The compiler inserts special\n\"compensation code\" into the program graph on the off-trace\nbranch edges to undo these inconsistencies, thus restoring pro-\ngram correctness.",
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled.",
			  "A number of conventional optimizations aid the trace selection\nprocess in finding parallelism. Automatic loop unrolling and\nautomatic inline substitution of subroutines are both incor-\nporated in Multiflow's compilers;\nthe compiler heuristically\ndetermines the amount of unrolling and substitution, substan-\ntially increasing the parallelism that can be exploited."
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace.",
			  "Trace scheduling uses a basic block scheduling method to schedule the instructions in each entire trace, beginning with the trace with the highest frequency.",
			  "This can result in large increases in code sizes and poor or erratic performance if program's behavior varies significantly with the input.",
			  "A trace is a sequence of instructions, including branches but not including loops, that is executed for some input data."
			]
		  },
		  {
			"title": "Very Long Instruction Word Architectures and the ELI-512",
			"url": "https://courses.cs.washington.edu/courses/cse548/16wi/Fisher-VLIW.pdf",
			"excerpts": [
			  "TRACE SCIIEDULING. The VLIW compiler we have built uses a recent global compaction technique called trace scheduling Fisher 811. This technique ww originally ...Read more"
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations.",
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant).",
			  "The performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%)."
			]
		  }
		],
		"reasoning": "The target field value corresponds to Trace Scheduling as a specific technique used to extract parallelism in VLIW-style compilation. Excerpts that explicitly describe trace scheduling as a compiler technique for finding fine-grained parallelism across long instruction streams, often by selecting likely execution traces and replacing loops with long instruction words, are directly relevant. Several excerpts present trace scheduling as a core method for global code motion and for breaking conditional bottlenecks, including discussions of selecting traces, moving code, and inserting compensation code to preserve correctness. Foundational and mathematical analyses of trace scheduling illustrate how compensation code is managed and how the technique fits into ILP-based optimization for VLIW systems. Additional sources compare trace scheduling to block scheduling, showing substantial performance gains and explaining practical considerations like code size and compensation overhead. Finally, related documents describe how trace scheduling interacts with loop transformations, unrolling, and prediction/predication strategies to maximize utilization of heterogeneous execution units, which are central to the research question’s interest in advanced VLIW scheduling tricks for irregular or data-dependent control flow. Direct quotations or paraphrased content from these excerpts establish that trace scheduling is used to uncover and exploit parallelism across traces, to manage off-trace compensation, and to mitigate bottlenecks in long dependency chains by interleaving independent computations across the available resources.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.0.key_trade_offs",
		"citations": [
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant)."
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace."
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled."
			]
		  }
		],
		"reasoning": "The most directly relevant material discusses compensation code in trace scheduling, including the existence of compensation copies when moving code across traces and the techniques to manage them. One excerpt explicitly notes that compensation code is inserted after a trace and that there are two techniques to deal with compensation code: avoidance (restricting code motion so that no compensation code is required) and suppression (analyzing the global flow to detect redundant copies). This aligns with the notion that compensation code can be a source of complexity and growth, and that strategies exist to mitigate it. Another excerpt provides a mathematical treatment of the compensation code size, including deriving an upper bound on the size of compensation code and showing that this bound can be attained, which directly supports concerns about potentially exponential growth and the limits of compensation-based approaches. Additional excerpts describe trace scheduling and compensation in broader terms, reinforcing the context in which compensation code appears (as part of moving instructions along traces and handling off-trace correctness). Taken together, these sources support the idea that compensation code is a central trade-off in VLIW trace scheduling: it can blow up code size, but there are recognized methods to limit or avoid it, and there are theoretical bounds governing how large compensation can get. A supplementary discussion highlights that trace scheduling can yield large performance gains, but the compensation aspect remains a critical factor in code growth and practical viability, reinforcing the trade-off theme. Overall, the strongest support comes from the explicit discussion of compensation code generation, the strategies to manage it, and the formal bounding of its size, which directly addresses the field value’s claim about complexity, growth potential, and pathway-dependent effectiveness. The additional excerpts help by framing compensation within the broader trace scheduling narrative, underscoring its central role in the optimization trade-offs.\n",
		"confidence": "high"
	  },
	  {
		"field": "empirical_case_studies_summary.workload",
		"citations": [
		  {
			"title": "Field-testing IMPACT EPIC Research Results in Itanium 2.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/field2004.pdf",
			"excerpts": [
			  "Using the Intel Itanium 2 microproces-*\n*sor, the SPECint2000 benchmarks and the IMPACT Com-*\n*piler for IA-64, a research compiler competitive with the*\n*best commercial compilers on the platform, we provide*\n*an* in situ *evaluation of code generated using aggres-*\n*sive, EPIC-enabled techniques in a reality-constrained mi",
			  "We have demonstrated how IMPACT’s “structural” ap-\nproach to compilation for Itanium 2 provides a 1.13 av-\nerage speedup (including single-benchmark speedups of\nup to 1.50) relative to traditional optimization of code at\nthe same inlining and interprocedural analysis levels and\na 1.55 speedup relative to ",
			  "With ILP transformations,\nIMPACT achieves an average speedup of 1.55 (maximum\n2.30) relative to GCC."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.Our work shows a 1.13 average speedup(up to 1.50) due to these compilation techniques, relativeto traditionally-optimized code at the same inlining andpointer analysis levels, and a 1.55 speedup (up to 2.30) relativeto GNU GCC, a solid traditional compiler."
			]
		  }
		],
		"reasoning": "The key workload described is the SPECint2000 benchmark suite, a collection of general-purpose integer programs used to assess performance, especially in contexts with irregular control flow and memory access patterns. The most directly supportive excerpt states that SPECint2000 benchmarks were used in field testing of the IMPACT EPIC compiler for IA-64 on Itanium 2, providing in situ evaluation of code generated with aggressive, EPIC-enabled techniques. This establishes that the SPECint2000 suite was a central workload in the described research on VLIW/EPIC compilation techniques. Additional excerpts reinforce the same context by describing performance results on Itanium/IA-64 with IMPACT/EPIC, including average speedups relative to traditional optimization and GCC, which aligns with exploring how SPECint2000-like general-purpose integer workloads behave under these advanced scheduling and code-generation strategies. Other excerpts elaborate on architectural features (predication, control/d data speculation, modulo scheduling, and ITANIUM organization) that would influence how such irregular memory and control-flow-heavy benchmarks behave on a VLIW/EPIC device, thereby providing indirect support for understanding how SPECint2000-like workloads might be scheduled and optimized in this context. Collectively, these excerpts support the idea that SPECint2000-style workloads were a focal point for evaluating EPIC/VLIW techniques, particularly in environments with irregular access patterns and data-dependent control flow.",
		"confidence": "medium"
	  },
	  {
		"field": "control_flow_optimization_techniques.0.description",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or"
			]
		  },
		  {
			"title": "Very Long Instruction Word Architectures and the ELI-512",
			"url": "https://courses.cs.washington.edu/courses/cse548/16wi/Fisher-VLIW.pdf",
			"excerpts": [
			  "TRACE SCIIEDULING. The VLIW compiler we have built uses a recent global compaction technique called trace scheduling Fisher 811. This technique ww originally ...Read more"
			]
		  },
		  {
			"title": "Trace scheduling - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Trace_scheduling",
			"excerpts": [
			  "It works by converting a loop to long straight-line code sequence using [loop unrolling](/wiki/Loop_unrolling \"Loop unrolling\") and static [branch prediction](/wiki/Branch_prediction \"Branch prediction\") . This process separates out \"unlikely\" code and adds handlers for exits from trace.",
			  "A trace is a sequence of instructions, including branches but not including loops, that is executed for some input data.",
			  "Trace scheduling was originally developed for Very Long Instruction Word, or [VLIW](/wiki/VLIW \"VLIW\") machines, and is a form of global code motion.",
			  "This can result in large increases in code sizes and poor or erratic performance if program's behavior varies significantly with the input."
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations."
			]
		  },
		  {
			"title": "A VLIW Architecture for a Trace Scheduling Compiler",
			"url": "https://www.eecg.utoronto.ca/~moshovos/ACA07/readings/trace.pdf",
			"excerpts": [
			  "Multiflow's Trace Scheduling compacting compiler automatically\nfinds fine-grained parallelism throughout any appfication.\nIt\nrequires no programmer intervention, either in terms of restruc-\nturing the program so it fits the architecture or in adding direc-\ntives which explicitly identify opportunities for overlapped exe-\ncution. This is in sharp contrast to \"coarse-grained\" parallel\narchitectures, including vector machines, shared-memory mul-\ntiprocessors,\nand\nmore\nradical\nstructures\nsuch\nas\nhypercubes selt85 or massively-parallel machines, wurst The pro-\ncess by which programs are converted into highly parallel wide-\ninstruction-word code is transparent to the user.",
			  "Using estimates of branch directions obtained automatically\nthrough heuristics or profiling, the compiler selects the most\nlikely path, or \"trace\", that the code will follow during execu-\ntion. This trace is then treated as if it were free of conditional\nbranches, and handed to the code generator. The code genera-\ntor schedules operations into wide instruction words, taking into\naccount data precedence, optimal scheduling of functional units,\nregister usage, memory accesses, and system buses; it emits\nthese instruction words as object code. This greedy scheduling\ncauses code motions which could cause logical inconsistencies\nwhen branches off-trace are taken. The compiler inserts special\n\"compensation code\" into the program graph on the off-trace\nbranch edges to undo these inconsistencies, thus restoring pro-\ngram correctness.",
			  "The process then repeats; the next-most-likely execution path is\nchosen as a trace and handed to the code generator. This trace\nmay include original operations and compensation code. It is\ncompacted; new compensation code may be generated; and the\nprocess repeats until the entire program has been compiled.",
			  "A number of conventional optimizations aid the trace selection\nprocess in finding parallelism. Automatic loop unrolling and\nautomatic inline substitution of subroutines are both incor-\nporated in Multiflow's compilers;\nthe compiler heuristically\ndetermines the amount of unrolling and substitution, substan-\ntially increasing the parallelism that can be exploited.",
			  "Instruction Word (VLIW) architectures were prom-\nised to deliver far more than the factor of two or three that\ncurrent architectures achieve £rom overlapped execution. Using\na new type of compiler which compacts ordinary sequential code\ninto long instruction words, a VLIW machine was expected to\nprovide from ten to thirty times the performance of a more con-\nventional machine built of the same implementation technology.\nMultiflow Computer, Inc., has now built a VLIW called the\nTRACE'\" along with its companion Trace Scheduling\" com-\npacting compiler. This new machine has fulfilled the perfor-\nmance promises that were made. Using many fast functional\nunits in parallel, this machine extends some of the basic\nReduced-Instruction-Set\nprecepts:\nthe\narchitecture\nis\nload/store, the microarchitecture is exposed to the compiler,\nthere is no microcode, and there is almost no hardware devoted\nto synchronization, arbitration, or interlocking of any kind (the\ncompiler has sole responsibility for runtime ",
			  "This process allows the compiler to break the \"conditional jump\nbottleneck\" and find parallelism throughout long streams of\ncode, achieving order-of-magnitude speedups due to compac-\ntion."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,"
			]
		  }
		],
		"reasoning": "The target field describes a global compaction technique that boosts instruction-level parallelism by looking across basic blocks to identify a commonly executed path (a trace) and scheduling it as a long, straight-line sequence. This directly maps to excerpts that explicitly define and explain trace scheduling, its global scope beyond single basic blocks, and the use of compensation code to preserve correctness after moving instructions across block boundaries. Excerpts that discuss trace scheduling from various sources (including classic papers and introductory summaries) provide direct support for the concept of treating a trace as a single sequence for scheduling, which is the core idea of global compaction in VLIW scheduling. Text excerpts that frame trace scheduling as a compiler technique, show how a trace is chosen and compacted, and discuss compensation or off-trace adjustments are particularly pertinent. Foundational mathematical treatments of trace scheduling further reinforce the theoretical basis of this technique. In addition, discussions about the trade-offs, such as code size and compensation code management, help explain practical aspects of this approach. Less directly related excerpts that touch on related topics (e.g., predication, IA-64 features, or other unrelated optimizations) are acknowledged but are deprioritized since they do not directly illuminate the cross-block trace compaction concept described in the field value.",
		"confidence": "high"
	  },
	  {
		"field": "repurposed_fault_tolerance_techniques.original_technique",
		"citations": [
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous ...",
			"url": "https://people.rennes.inria.fr/Olivier.Sentieys/publications/2022/Psiakis22MICPRO.pdf",
			"excerpts": [
			  "The proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhead. To achieve\nthis, a scheduling exploration window of two instruc-\ntion bundles is used.",
			  "he proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhea",
			  "order to perform such a flexible dynamic instruction\nscheduling, switches are required, which, however, in-\ncrease the hardware area. To reduce the area over-\nhead, a cluster-based approach is proposed, enabling\nscalability for larger hardware designs",
			  "To support our contributions, we implemented on the\nVEX VLIW processor and compare the proposed ap-\nproach that applies instruction triplication, consid-\nering space scheduling (TMR-S) and space and time\nscheduling based on clusters (TMR-S+T+CB), with\ntypical hardware FUs triplication methods and the\nunprotected baseline VLIW.",
			  "mechanism is implemented on VEX VLIW proces-\nsor. The obtained results show an average speed-up\nof 24.99% in performance with an almost 10% area\nand power overhead, when time scheduling is also\nconsidered on top of space scheduling.\nC",
			  "we implemented on the\nVEX VLIW processor and compare the proposed ap-\nproach that applies instruction triplication, consid-\nering space scheduling (TMR-S) and space and time\nscheduling based on clusters (TMR-S+T+CB), with\ntypical hardware FUs triplication methods and the\nunprotected baseline VLIW. We performed extensive\nexperimental results, providing performance, area,\npower and reliability metrics for ten benchmarks.",
			  "Static approaches are usually implemented by soft-\nware. They replicate and schedule the instructions\nat design-time and additional instructions are in-\nserted for comparison of the results.",
			  "The remaining hardware dynamic approaches ap-\nply dynamic scheduling, assuming that VLIW issues\ninclude all FUs, thus can execute any instruction.\nHowever, when the VLIW consists of issues with dif-\nferent type of FUs, the existing schedulers are not\napplicable, as they ignore the type of ",
			  "The proposed dynamic hardware mechanism is ap-\nplied within a cluster (intra-cluster), providing sig-\nnificant gains in area, with small performance loss.",
			  "The proposed dynamic hardware mechanism is ap-\nplied within a cluster (intra-cluster), providing sig-\nnificant gains in area, with small performance loss."
			]
		  },
		  {
			"title": "Run Time Instruction Replication for Permanent and Soft ...",
			"url": "https://inria.hal.science/hal-01633778/file/Run%20Time%20Instruction%20Replication%20for%20Permanent%20and%20Soft%20Error%20Mitigation%20in%20VLIW%20Processors.pdf",
			"excerpts": [
			  "ardware mechanism is proposed**\n**which at run-time replicates the instructions and schedules them**\n**at the idle slots considering the resource constra",
			  "f a resource**\n**becomes faulty, the proposed approach efficiently rebinds both**\n**the original and replicated instructions during exe",
			  "ults**\n**show up to 49% performance gain over existing techniques",
			  "it is applicable for any VLIW structure, i.e. any\nissue width and number and type of FUs."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S0141933122001168",
			"excerpts": [
			  "The proposed mechanism dynamically replicates instructions and reschedules original and replicated instructions both in the current bundle (space scheduling - S) and in the next upcoming bundle (time scheduling - T), improving performance, with reduced complexity and area overhead. To achieve this, a scheduling exploration window of two instruction bundles is used.",
			  "To address these limitations, a hardware mechanism is proposed that enhances VLIW processors with heterogeneous FUs with fault tolerant capabilities. The proposed mechanism dynamically replicates instructions and reschedules original and replicated instructions both in the current bundle (space scheduling - S) and in the next upcoming bundle (time scheduling - T), improving performance, with reduced complexity and area overhead. To achieve this, a scheduling exploration window of two instruction bundles is used.",
			  "The replication scheduler dynamically reschedules original and replicated instructions, avoiding the insertion of additional time slots, by postponing the execution of independent instructions to the next bundle. The voting scheduler synchronizes original and replicated instruction in order to compare their outputs, masking any occurring error. Due to dynamic rescheduling, switches have to be inserted in the VLIW pipeline. To reduce the area and critical delay overhead, we propose a switch hardware design that simplifies the complexity of the scheduler, compared to commonly used switches. Last, but not least, to enable scalability, we propose a novel cluster-based approach; a large VLIW is divided in several clusters, each consisting of a smaller VLIW.",
			  "In this way, new opportunities are created for optimizing the schedule during execution, while the scheduling complexity remains low.",
			  "The obtained results show an average speed-up of 24.99% in performance with an almost 10% area and power overhead, when time scheduling is also considered on top of space scheduling.",
			  "The proposed dynamic hardware mechanism is applied within a cluster (intra-cluster), providing significant gains in area, with small performance loss.",
			  "To support our contributions, we implemented on the VEX VLIW processor and compare the proposed approach that applies instruction triplication, considering space scheduling (TMR-S) and space and time scheduling based on clusters (TMR-STCB), with typical hardware FUs triplication methods and the unprotected baseline VLIW.",
			  "to enable scalability, we propose a novel cluster-based approach; a large VLIW is divided in several clusters, each consisting of a smaller VLIW."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Microprocessors & Microsystems",
			"url": "https://dl.acm.org/doi/10.1016/j.micpro.2022.104564",
			"excerpts": [
			  "us FUs. This work presents a hardware mechanism for processors with heterogeneous FUs that dynamically replicates instructions and schedules both original and replicated instructions considering space and time scheduling. ",
			  "The proposed approach uses a small scheduling window of two cycles, leading to a hardware mechanism with small hardware area.",
			  "To reduce the area overhead, a cluster-based approach is proposed, enabling scalability for larger hardware designs.",
			  "The proposed mechanism is implemented on VEX VLIW processor.",
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Request PDF",
			"url": "https://www.researchgate.net/publication/360874051_Dynamic_fault-tolerant_VLIW_processor_with_heterogeneous_Function_Units",
			"excerpts": [
			  "This work presents a hardware mechanism for processors with heterogeneous FUs that dynamically replicates instructions and schedules both original and replicated instructions considering space and time scheduling.",
			  "The proposed approach uses a small scheduling window of two cycles, leading to a hardware mechanism with small hardware area.",
			  "To reduce the area overhead, a cluster-based approach is proposed, enabling scalability for larger hardware designs.",
			  "The proposed mechanism is implemented on VEX VLIW processor.",
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability.",
			  "In order to perform such a flexible dynamic instruction scheduling, switches are required, which, however, increase the hardware area.",
			  "Very Long Instruction Word, or VLIW, architectures have received much attention in specific-purpose applications such as scientific computation, digital signal processing, and even safety-critical systems.",
			  "In this study, a fault-tolerant design framework of VLIW processor is proposed. Specifically, this paper concentrates on the issue of dependable data path design.",
			  "The instruction duplication mechanism is further supported by a hardware enhancement for efficient result verification, which avoids the need of additional comparison instructions.",
			  "cle, we propose a compiler-assisted dynamic code duplication method to minimize the code size overhead, and present vulnerability-aware duplication algorithms to maximize the effectiveness of instruction duplication with least overheads for VLIW architecture.",
			  "In this work, we experiment with complier-directed instruction duplication to detect soft errors in VLIW datapaths. In the proposed approach, the compiler determines the instruction schedule by balancing the permissible performance degradation with the required degree of duplication. Our experimental results show that our algorithms allow the designer to perform tradeoff analysis between performance and reliability.",
			  "The proposed methodology aims to achieve processor data paths for VLIW architectures able to autonomously detect transient and permanent hardware faults while executing their applications.",
			  "The advantage of a software approach to hardware fault detection is interesting because it allows one to apply it only to the critical applications executed on the VLIW architecture, thus not causing a delay in the execution of noncritical tasks."
			]
		  },
		  {
			"title": "Removing communications in clustered microarchitectures through instruction replication | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1011528.1011529",
			"excerpts": [
			  "The need to communicate values between clusters can result in a significant performance loss for clustered microarchitectures.",
			  "The proposed scheme is built on top of a previously proposed state-of-the-art modulo-scheduling algorithm.",
			  "his work presents a new compilation technique that usesinstruction replication in\norder to reduce the number ofcommunications executed on a clusteredmicroarchitecture.",
			  "For such architectures, the need tocommunicate values between clusters can result",
			  "the number of communications can be further decreased by around one-third through replication, which results in a significant speedup.",
			  "IPC is increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for selected programs."
			]
		  },
		  {
			"title": "Removing communications in clustered microarchitectures ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/1011528.1011529",
			"excerpts": [
			  "In this work, we describe an optimization technique that removes\n\ncommunications by selectively replicating an appropriate set of instructions. Instruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources. The proposed scheme is built on top of a previously proposed state-of-\n\nthe-art modulo-scheduling algorithm.",
			  "IPC\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs. We also show that replicating appropriate sets of instructions is more effective\n\nthan doubling the intercluster connection network bandwidth.",
			  "The main conclusion is that instruction replication\n\nACM Transactions on Architecture and Code Optimization, Vol. 1, No. 2, June 2004. 140\n\n•\n\nA. Alet`a et al. Fig. 8. IPC for mgrid. Fig. 9. Reduction of the II for applu. increases performance for all the benchmarks and for all the tested architec-\n\ntures. It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications. Benefits would be even higher for more basic\n\nschedulers. For example, for the 4c2b4l64r configuration, the average speedup\n\nprovided by replication is 25%. For some programs such as su2cor, the benefits\n\ncan be up to 70%, 65% for tomcatv, and 50% for swim",
			  "It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications.",
			  "e have implemented our replication technique as a part of a research compiler\n\n[Ayguad´e et al. 1996]. To drive our evaluation, we have used the SPECfp95\n\nbenchmarks.",
			  "e assumed a VLIW architecture with an issue width of 12. In this\n\narchitecture, we assume four floating-point (FP) FUs, four integer (INT) FUs\n\nand four memory ports (MEM). The different cluster configurations are pre-\n\nsented in Table 1.",
			  "repli-\n\ncation scheme is implemented on top of a state-of-the-art scheduling algorithm\n\nfor clustered processors. The proposed technique is evaluated for a clustered VLIW machine, though it\n\ncan be used for any statically scheduled architecture.",
			  "For instance, for the\n\n4c1b2l64r, 36% of the communications are removed and every communication\n\nrequires the replication of 2.1 instructions on average"
			]
		  },
		  {
			"title": "Removing Communications in Clustered Microarchitectures ...",
			"url": "https://ece.northeastern.edu/groups/nucar/publications/TACO04.pdf",
			"excerpts": [
			  "nstruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources",
			  "results show that the number of communications can be further\n\ndecreased by around one-third through replication, which results in a significant speedup",
			  "C\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs.",
			  " remainder of this paper is organized as follows. Section 2 provides some\n\nbackground on modulo-scheduling and graph partitioning as well an overview\n\nof the baseline algorithm. Section 3 presents some statistics that motivate this\n\nwork. Section 4 describes our replication heuristics. Section 5 analyzes its per-\n\nforman",
			  "nce, Aggarwal and Franklin work\n\nrelies on simple heuristics to dynamically replicate instructions that can be\n\nimplemented at run-time, as opposed to our technique that is implemented in\n\nthe compiler",
			  "k, we have presented a compiler technique to replicate selected in-\n\nstructions in order to reduce intercluster communications. The proposed tech-\n\nnique is shown to reduce the number of communications by approximately one\n\nthird, depending on the processor configuration. Replication has been shown to\n\nproduce significant speedups for all configurations and for all programs. For in-\n\nstance, for a four-cluster processor, the average speedup is 25%, and for selected\n\nprograms like _su2cor_ , speedup can be as high as 70%",
			  "replicating appropriate sets of instructions is more effective\n\nthan doubling the intercluster connection network bandwidth",
			  "e evaluate this ap-\n\nproach using 678 loops taken from the SPECfp95 benchmark suit"
			]
		  }
		],
		"reasoning": "The passages describe a hardware mechanism that dynamically replicates instructions and schedules both original and replicated instructions across the current bundle (space scheduling) and the next bundle (time scheduling). This directly aligns with the targeted fine-grained field value, which names dynamic instruction replication and space-time scheduling as the core techniques being adapted for fault tolerance in a VLIW with heterogeneous function units. The cited material also discusses cluster-based scalability, small scheduling windows, and performance-area tradeoffs, which reinforce how replicas are managed within a VLIW’s parallel resources to mask faults without excessive overhead. Several passages provide explicit statements about proposing replication of instructions, scheduling decisions that occur across cycles, and the resulting performance/power-area implications, all of which corroborate the described approach. Other excerpts extend this by showing related fault-tolerance strategies (compiler-assisted replication, trace/modulo scheduling variants, and experimental results) that illuminate the broader context of replication and scheduling in VLIW architectures and support the idea of interleaving independent computations to keep all slots filled in the presence of dependencies.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.1.key_trade_offs",
		"citations": [
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "TraceList = Construct Traces(MF);",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or",
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  "A superblock is simply a trace with all joins removed via tail duplication (figure 5)."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13]."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling."
			]
		  }
		],
		"reasoning": "The most relevant content directly addresses the core claim: tail duplication can inflate code size and represents a significant bookkeeping burden in certain scheduling strategies. A highlighted statement notes that code size is paramount in embedded applications and explicitly says you cannot afford a code-size increase due to tail duplication, which aligns with the field value’s emphasis on “worst-case bookkeeping” and code bloat. Related passages describe superblocks as a mechanism to manage complex bookkeeping and joins, and contrast them with trace scheduling, which involves off-trace compensation when moving instructions past splits or joins. This provides context for why tail duplication might be deemed less favorable for irregular control flow with many join points. Additional excerpts discuss the general idea of trace scheduling, the concept of forming and using superblocks, and the limitations and remedies related to code movement and edge handling, further supporting the notion that tail duplication contributes to bloated code size and complex bookkeeping in practice. Specific lines mention constructing traces, forming superblocks, and the trade-off between avoiding complex bookkeeping versus the risk of code-size growth due to duplication, all of which connect directly to the stated fine-grained field value and its critique of tail duplication in irregular control-flow scenarios.",
		"confidence": "medium"
	  },
	  {
		"field": "repurposed_fault_tolerance_techniques.performance_application",
		"citations": [
		  {
			"title": "Removing communications in clustered microarchitectures ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/1011528.1011529",
			"excerpts": [
			  "In this work, we describe an optimization technique that removes\n\ncommunications by selectively replicating an appropriate set of instructions. Instruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources. The proposed scheme is built on top of a previously proposed state-of-\n\nthe-art modulo-scheduling algorithm.",
			  "IPC\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs. We also show that replicating appropriate sets of instructions is more effective\n\nthan doubling the intercluster connection network bandwidth.",
			  "The main conclusion is that instruction replication\n\nACM Transactions on Architecture and Code Optimization, Vol. 1, No. 2, June 2004. 140\n\n•\n\nA. Alet`a et al. Fig. 8. IPC for mgrid. Fig. 9. Reduction of the II for applu. increases performance for all the benchmarks and for all the tested architec-\n\ntures. It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications. Benefits would be even higher for more basic\n\nschedulers. For example, for the 4c2b4l64r configuration, the average speedup\n\nprovided by replication is 25%. For some programs such as su2cor, the benefits\n\ncan be up to 70%, 65% for tomcatv, and 50% for swim",
			  "It is important to highlight that the baseline scheduler, that does not\n\nperform replication, is a technique that has been shown to be very effective\n\nat minimizing communications.",
			  "e have implemented our replication technique as a part of a research compiler\n\n[Ayguad´e et al. 1996]. To drive our evaluation, we have used the SPECfp95\n\nbenchmarks.",
			  "repli-\n\ncation scheme is implemented on top of a state-of-the-art scheduling algorithm\n\nfor clustered processors. The proposed technique is evaluated for a clustered VLIW machine, though it\n\ncan be used for any statically scheduled architecture.",
			  "For instance, for the\n\n4c1b2l64r, 36% of the communications are removed and every communication\n\nrequires the replication of 2.1 instructions on average"
			]
		  },
		  {
			"title": "Removing Communications in Clustered Microarchitectures ...",
			"url": "https://ece.northeastern.edu/groups/nucar/publications/TACO04.pdf",
			"excerpts": [
			  "k, we have presented a compiler technique to replicate selected in-\n\nstructions in order to reduce intercluster communications. The proposed tech-\n\nnique is shown to reduce the number of communications by approximately one\n\nthird, depending on the processor configuration. Replication has been shown to\n\nproduce significant speedups for all configurations and for all programs. For in-\n\nstance, for a four-cluster processor, the average speedup is 25%, and for selected\n\nprograms like _su2cor_ , speedup can be as high as 70%",
			  "nstruction replication\n\nis done carefully because it might degrade performance due to the increased contention it can place\n\non processor resources",
			  "results show that the number of communications can be further\n\ndecreased by around one-third through replication, which results in a significant speedup",
			  "C\n\nis increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for\n\nselected programs.",
			  " remainder of this paper is organized as follows. Section 2 provides some\n\nbackground on modulo-scheduling and graph partitioning as well an overview\n\nof the baseline algorithm. Section 3 presents some statistics that motivate this\n\nwork. Section 4 describes our replication heuristics. Section 5 analyzes its per-\n\nforman",
			  "nce, Aggarwal and Franklin work\n\nrelies on simple heuristics to dynamically replicate instructions that can be\n\nimplemented at run-time, as opposed to our technique that is implemented in\n\nthe compiler"
			]
		  },
		  {
			"title": "Removing communications in clustered microarchitectures through instruction replication | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1011528.1011529",
			"excerpts": [
			  "his work presents a new compilation technique that usesinstruction replication in\norder to reduce the number ofcommunications executed on a clusteredmicroarchitecture.",
			  "For such architectures, the need tocommunicate values between clusters can result",
			  "the number of communications can be further decreased by around one-third through replication, which results in a significant speedup.",
			  "IPC is increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for selected programs."
			]
		  },
		  {
			"title": "Run Time Instruction Replication for Permanent and Soft ...",
			"url": "https://inria.hal.science/hal-01633778/file/Run%20Time%20Instruction%20Replication%20for%20Permanent%20and%20Soft%20Error%20Mitigation%20in%20VLIW%20Processors.pdf",
			"excerpts": [
			  "ardware mechanism is proposed**\n**which at run-time replicates the instructions and schedules them**\n**at the idle slots considering the resource constra",
			  "f a resource**\n**becomes faulty, the proposed approach efficiently rebinds both**\n**the original and replicated instructions during exe",
			  "ults**\n**show up to 49% performance gain over existing techniques",
			  "it is applicable for any VLIW structure, i.e. any\nissue width and number and type of FUs."
			]
		  },
		  {
			"title": "Selective Code Duplication for Soft Error Protection on VLIW Architectures",
			"url": "https://www.mdpi.com/2079-9292/10/15/1835",
			"excerpts": [
			  " present the packing-oriented duplication in order to maximize the number of duplicated instructions within the same performance overheads. We define the pack as the number of duplicable instructions in the same code line, and we select larger packs for instruction duplication",
			  "All the instructions cannot be replicated without additional code lines. Additional code lines are required to increase the number of duplicated instructions in VLIW architectures.",
			  "Additional code lines are required to increase the number of duplicated instructions in VLIW architectures.",
			  "Our experimental results show that 52% performance overhead as compared to unprotected source code when we duplicate all the instructions.",
			  "However, the previous static scheme duplicates instructions just in sequential order. In this work, we propose packing-oriented duplication to maximize the number of duplicated instructions within the same peroformance overhead bounds.",
			  "In this work, we propose packing-oriented duplication to maximize the number of duplicated instructions within the same peroformance overhead bounds.",
			  "This considerable performance overhead can be inapplicable for resource-constrained embedded systems so that we can limit the number of additional NOP instructions for selective protection.",
			  "exploiting unused instruction slots with minimal hardware modules such as register queues and comparator.",
			  "In order to bridge the gap between them, compiler-assisted instruction duplication techniques have been proposed for VLIW architecture."
			]
		  }
		],
		"reasoning": "The target field value describes a compiler-driven replication technique that avoids cross-cluster data transfers in a clustered VLIW, yielding IPC improvements and even high-speedups in best cases. Excerpts explicitly discuss replication to reduce inter-cluster communications and improve performance on clustered VLIW/multi-cluster designs. For example, replication schemes built on top of advanced modulo-scheduling concepts are shown to decrease inter-cluster communications and achieve notable speedups (IPC gains around 25% on average, with higher gains on certain programs). Several excerpts describe replication being used to remove the need for cross-cluster data transfers by duplicating instructions so that a value produced in one cluster can be consumed without requiring a cross-cluster transfer, effectively hiding communication latency. Additional excerpts describe cluster-based approaches where a large VLIW is partitioned into smaller clusters, and replication across clusters is employed to maintain ILP and throughput. There are also mentions of compiler-assisted or hardware-assisted replication mechanisms that schedule replicated instructions to coincide with the idle or available slots, further supporting the idea that replication can be driven by the compiler to optimize inter-cluster data flow. The body of evidence collectively supports the claim that a compiler-driven replication strategy in a clustered VLIW can hide inter-cluster communication latency, improve IPC (average around 25%), and yield substantial speedups (up to 70%) in favorable programs, while discussing associated tradeoffs such as area/power overhead and scheduling complexity.",
		"confidence": "high"
	  },
	  {
		"field": "machine_learning_in_scheduling",
		"citations": [
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two ...",
			"url": "https://dl.acm.org/doi/10.1145/3643135",
			"excerpts": [
			  " we propose a **two-dimensional constrained dynamic programming** ( **TDCDP** ) approach for VLIW instruction scheduling. This approach combines both phases and solves them simultaneously, minimizing the likelihood of suboptimal solutions. B",
			  "This article presents TDCDP, a novel instruction scheduling approach for VLIW compilers. TDCDP addresses the simultaneous assignment of time slots and functional units for instructions while incorporating strict constraints to refine the search space.",
			  " TDCDP approach utilizes DP to generate near-optimal solutions for instruction scheduling, while keeping the time overhead in check by incorporating constraints",
			  "In contrast to the aforementioned methods, our proposed TDCDP approach combines the assignment of time slots and functional units via DP. By incorporating strict constraints, TDCDP is capable of generating near-optimal solutions for instruction scheduling while effectively reducing the time overhead to an acceptable level.",
			  " proposed mathematical model is designed specifically for instruction scheduling in VLIW architecture. This model formulates the problem as an integer **linear programming problem** (**ILP**), with a time complexity of O",
			  "the integration of Post-RA Scheduling greatly enhances the quality of the generated code. The quantitative evaluation highlights the adaptability and robustness of TDCDP.",
			  "the input programs are divided into multiple basic blocks. The analyzer extracts information from instructions and establishes dependency relationships among them. These dependencies can be effectively represented using a DAG.",
			  "Shobaki et al. [ [30]() ] presented a B&B algorithm for solving instruction scheduling problems, utilizing it to search for an optimal solution to a subproblem. However, due to the limitations of exact algorithms, their instruction scheduling was ultimately implemented in a heuristic manner.",
			  "The experimental results showcase the high efficiency of TDCDP in solving instruction scheduling problems, surpassing the performance of heuristic algorithms.",
			  "ment our TDCDP approach in a VLIW compiler. This integration allows for practical applicability and compatibility with existing compiler frameworks.\n—\nQuant",
			  "However, there is still potential for further improvement in scheduling efficiency. In future research, exploring the adjustment of constraints to optimize the generated code would be an interesting avenue.",
			  "During the Pre-RA Scheduling, the scheduler aims to optimize the utilization of functional units while ensuring that the width of the long instruction word remains within the designated limit.",
			  "Post-RA Scheduling can effectively reduce the impact of load/store instructions. Additionally, it enhances target code by increasing ILP, optimizing code layout, and eliminating instruction conflicts.",
			  "This article proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling.",
			  "The optimization workflow, as shown in Figure [5]() , involves several steps. Firstly, the input programs are divided into multiple basic blocks. The analyzer extracts information from instructions and establishes dependency relationships among them. These dependencies can be effectively represented using a DAG.",
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | Request PDF",
			"url": "https://www.researchgate.net/publication/377718811_Optimizing_VLIW_Instruction_Scheduling_via_a_Two-Dimensional_Constrained_Dynamic_Programming",
			"excerpts": [
			  "The performance of VLIW processors heavily relies on Instruction-Level Parallelism (ILP).",
			  ". This paper proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling. ",
			  "g (TDCDP) approach and a quantitative model for instruction scheduling.",
			  "Furthermore, we integrate our TDCDP approach into mainstream compiler architecture, encompassing Pre- and Post-RA (register allocation) scheduling.",
			  "Our approach achieves an efficiency improvement of up to 58.34% in final solutions compared to the heuristic algorithms.",
			  "We conduct a quantitative evaluation of TDCDP compared to four heuristic algorithms on a typical VLIW processor.",
			  "The TDCDP approach achieves near-optimal solutions within an acceptable time overhead."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3643135",
			"excerpts": [
			  "Integration into a VLIW Compiler: Moreover, we have integrated our TDCDP approach into a specific VLIW compiler. This compiler schedules instructions in two phases, namely Pre-RA Scheduling and Post-RA Scheduling. We have also conducted quantitative evaluations of solutions obtained from each phase.",
			  "Additionally, we have successfully minimized the time overhead by imposing stringent constraints.",
			  "In this work, we have enhanced the instruction scheduling by introducing the TDCDP approach. TDCDP merges the phases and generates near-optimal solutions, effectively reducing the occurrence of local optima.",
			  "Improved Instruction Scheduling: In our prior article, the proposed approach scheduled instructions in two separate phases, resulting in solutions that were susceptible to local optima.",
			  "This article presents an extension of our previous work, which was initially presented at ASP-DAC 2022 [ [4]() ]. We have made significant advancements in this article, which include:",
			  "Integration with Modern Scheduling Architecture: Aligning the scheduling architecture of modern compilers, we implement our TDCDP approach in a VLIW compiler. This integration allows for practical applicability and compatibility with existing compiler frameworks.",
			  "TDCDP Approach: Recognizing that instruction scheduling is a two-dimensional assignment problem involving time slots and functional units, we propose the TDCDP approach for VLIW instruction scheduling.",
			  "During this procedure, TDCDP assigns time slots and functional units to instructions in a single step.",
			  "Subsequently, the scheduler component applies the TDCDP algorithm to reorder instructions, culminating in a schedule solution.",
			  "In the initial phase, an analyzer component extracts instruction-level details and constructs dependency graphs.",
			  "Within the compiler, the TDCDP approach is implemented in two distinct phases.",
			  "The compiler infrastructure depicted in Figure [1]() accepts **machine instructions** ( **MIs** ) as input and produces target code as output.",
			  "In our work, we incorporate the scheduling architecture of modern compilers into a specific VLIW compiler.",
			  "The Pre-RA Scheduling focuses on reordering instructions to maximize ILP, while the Post-RA Scheduling primarily addresses issues related to physical register contention and data dependencies, with the goal of further enhancing the quality of the generated code.",
			  "By imposing strict constraints, TDCDP narrows down the solution space and provides stable near-optimal solutions within an acceptable time overhead.",
			  "In this article, we propose a **two-dimensional constrained dynamic programming** ( **TDCDP** ) approach for VLIW instruction scheduling. This approach combines both phases and solves them simultaneously, minimizing the likelihood of suboptimal solutions.",
			  "However, heuristics often divide instruction scheduling into two separate subproblems and solve them sequentially, which can lead to local optima.",
			  "Heuristic algorithms typically address this problem in two phases. Firstly, time slots are assigned to instructions based on their dependency relationships. Then, functional units are allocated to the instructions, considering resource constraints.",
			  "Instruction scheduling for VLIW compilers can be conceptualized as a two-dimensional assignment problem involving time slots and functional units [ [22]() , [34]() ].",
			  "By incorporating strict constraints, TDCDP is capable of generating near-optimal solutions for instruction scheduling while effectively reducing the time overhead to an acceptable level.",
			  "In contrast to the aforementioned methods, our proposed TDCDP approach combines the assignment of time slots and functional units via DP.",
			  "Shobaki et al. [ [30]() ] presented a B&B algorithm for solving instruction scheduling problems, utilizing it to search for an optimal solution to a subproblem.",
			  "This integrated approach allows for better coordination and optimization of instruction scheduling, ultimately improving the overall efficiency and performance of the compiler.",
			  "In contrast, our TDCDP approach combines these two steps into a single integrated process. By doing so, we streamline the instruction scheduling procedure, eliminating the need for separate priority calculation and functional unit allocation stages.",
			  "The aforementioned heuristic algorithms follow a two-step process to generate instruction scheduling solutions. This process involves calculating priorities for instructions and subsequently allocating functional units to schedule those instructions.",
			  "These dependencies can be effectively represented using a DAG.",
			  "The optimization workflow, as shown in Figure [5]() , involves several steps. Firstly, the input programs are divided into multiple basic blocks. The analyzer extracts information from instructions and establishes dependency relationships among them.",
			  "To mitigate this issue, we adopt the architecture of modern compilers, which employs two phases for instruction scheduling: Pre-RA Scheduling and Post-RA Scheduling.",
			  "When the required number of registers exceeds the available allocation limit, the compiler must insert load/store instructions to spill code.",
			  "However, it is important to note that increasing ILP also raises the risk of code spills.",
			  "This approach is particularly useful in exploring ILP for VLIW processors.",
			  "Our proposed TDCDP approach utilizes DP to generate near-optimal solutions for instruction scheduling, while keeping the time overhead in check by incorporating constraints.",
			  "In this section, we present a comprehensive model for addressing the instruction scheduling problems encountered in VLIW processors, and then introduce our TDCDP approach.",
			  "Post-RA Scheduling can effectively reduce the impact of load/store instructions. Additionally, it enhances target code by increasing ILP, optimizing code layout, and eliminating instruction conflicts.",
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/full/10.1145/3643135",
			"excerpts": [
			  "Subsequently, the scheduler component applies the TDCDP algorithm to reorder instructions, culminating in a schedule solution.",
			  "The TDCDP approach achieves near-optimal solutions within an acceptable time overhead.",
			  "This article proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling.",
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling."
			]
		  },
		  {
			"title": "Evolutionary Algorithms for Instruction Scheduling, Operation Merging, and Register Allocation in VLIW Compilers | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Evolutionary-Algorithms-for-Instruction-Scheduling%2C-Giesemann-Gerlach/744f2235c8003526e79a44f3f4482b69be9c9e6c",
			"excerpts": [
			  "R\nA two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling that achieves near-optimal solutions within an acceptable time overhead are proposed and integrated into mainstream compiler architecture. Expand",
			  "# Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming"
			]
		  },
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "In this thesis, a novel reinforcement learning (RL) based model for the\ninstruction scheduling problem is developed including modelling features\nof processors such as forwarding, resource utilisation and treatment of the\naction space.",
			  "an RL agent that interacts with\nthe model of the problem is presented using three different types of graph\nneural networks for the state processing: graph conventional networks, graph\nattention networks, and graph attention based on the work of Lee *et al* .",
			  "The architecture consists of a state encoder based on a GNN, thereafter, two\nseparate additional neural networks are used for the policy network and value\nfunction network. The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "The results showed a minimal improvement over the heuristic agent.",
			  "Moreover, it was shown that the embedding of stages results in a significant\nimprovement with respect to the explained variance and an improvement to\nthe overall performance. Unfortunately, the third and final goal of the thesis\nwas not properly met, as the results that were obtained from integrating our\nmodel into the DSP compiler were meaningless because of additional compiler\npasses.",
			  "An efficient optimal scheduler is presented to be used for an\noptimal schedule length based reward function, however, this is not used\nin the final results as a heuristic based reward function was deemed to be\nsufficient and faster to compute.",
			  "It is likely the case that further compiler passes downstream impose restrictions\nwhich means that the variation in the instruction scheduling pass is rather\nlimited.",
			  "In particular, it is possible that the restriction is due to register\nallocation, which was explicitly decided to be out of the scope of this work.",
			  "The proposed solution is validated against\nthe modelled environment and favourable but not significant improvements\nwere found compared to the most common heuristic method.",
			  "Abstract",
			  "Furthermore,\nit was found that having embeddings relating to resource utilisation was very\nimportant for the explained variance of the RL models.",
			  "Additionally, a trained\nmodel was tested in an actual compiler, however, no informative results were\nfound likely due to register allocation or other compiler stages that occur after\ninstruction scheduling.",
			  "Keywords",
			  "2.3.7\nUse of DAGs to represent Dependencies . . . . .",
			  "2.5.4\nBank Conflicts . . .",
			  "2.6\nReinforcement Learning",
			  "2.6.1\nAlgorithms",
			  "2.6.1.1\nPolicy Gradient methods",
			  "2.6.1.1.2\nProximal Policy Optimization (PPO) 24",
			  "2.6.1.2\nQ-learning methods . . .",
			  "2.7\nGraph Neural Networks (GNNs)",
			  "2.7.3\nGraph Embeddings",
			  "2.7.4\nGraph Convolutional Networks . . . . . . .",
			  "2.7.5\nGraph Attention Networks . . . . . . . .",
			  "results showed a minimal improvement over the heuristic agent.",
			  "2.5.2\nVery Long Instruction Word (VLIW)",
			  "2.5.2\nVery Long Instruction Word (VLIW)",
			  "For the scope of this thesis, the focus will be on DSPs with a VLIW\narchitecture.",
			  "Therefore, a DSP is well designed\nfor performance for the type of operation it needs to perform.",
			  "Background | 19",
			  "that the speed of instructions are increased.",
			  "*\n**Relevant Modern Machine Learning Work",
			  "*\n**Relevant Modern Machine Learning Work",
			  "| Methodology",
			  "| Methodology",
			  "The MDP can take place as an isolated instruction scheduling problem or it\ncan be integrated into the compiler directly.",
			  "The MDP can take place as an isolated instruction scheduling problem or it\ncan be integrated into the compiler directly.",
			  "The compiler in the loop MDP\nversion, however, is a far bigger engineering challenge.",
			  "The compiler in the loop MDP\nversion, however, is a far bigger engineering challenge.",
			  "An\napproach could be to train an RL agent in a custom environment rapidly with a\nhigh throughput of experiences and then fine-tune said agent in an environment\nintegrated into the actual compiler.",
			  "The architecture consists of a state encoder based on a GNN, thereafter, two\nseparate additional neural networks are used for the policy network and value\nfunction network.",
			  "The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "Moreover, it was shown that the embedding of stages results in a significant\nimprovement with respect to the explained variance and an improvement to\nthe overall performance.",
			  "Unfortunately, the third and final goal of the thesis\nwas not properly met, as the results that were obtained from integrating our\nmodel into the DSP compiler were meaningless because of additional compiler\npasses.",
			  "68 | Conclusions and Future Work"
			]
		  },
		  {
			"title": "of Full and Partial Predicated Execution Support for ILP ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s06/web/handouts/mahlke-isca95.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems",
			"url": "https://arxiv.org/html/2504.20198v1",
			"excerpts": [
			  "Graph compilers analyze and optimize computational graphs representing neural networks as nodes (operations) and edges (data dependencies).",
			  "They provide abstraction to lower-level implementation details by converting models from high-level frameworks (PyTorch, TensorFlow) into hardware-agnostic intermediate representations.",
			  "Moreover, they may apply transformations that improve execution speed and memory efficiency across AI accelerators, and hardware-specific code generation for low-level kernels tailored to target architectures (e.g., CUDA for NVIDIA GPUs, OpenCL for FPGAs).",
			  "raph Compilers can improve throughput by exploiting the repeated patterns present in such organizations and by providing specialized hardware modules for particular compositions (e.g., Conv-BatchNorm-ReLU )."
			]
		  },
		  {
			"title": "GALA: A High Performance Graph Neural Network Acceleration LAnguage and Compiler | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3763113",
			"excerpts": [
			  "GALA: A High Performance Graph Neural Network Acceleration LAnguage and Compiler > Abstract > Abstract\nContent:\nMultiple frameworks and optimizations have been proposed for accelerating Graph Neural Network (GNN) workloads over the years, achieving sizable runtime performance improvements. However, we notice that existing systems usually explore optimizing either at the intra-operator level or at the inter-operator level, missing synergies that exist due to their compositions. Further, most existing works focus primarily on optimizing the forward computation of GNNs, often overlooking opportunities for training-specific optimizations. To exploit these missed optimization opportunities, we introduce GALA, a domain-specific language (DSL) and a compiler that allows composing optimizations at different levels. The GALA DSL exposes intra-operator transformations as scheduling commands, while we introduce novel inter-operator transformations as part of the compiler. The composition of these transformations is made possible through the introduction of two novel intermediate representations (IR) in the GALA compiler that tracks and composes transformations at both the intra- and inter-operator levels. Further, the IRs maintain a global view of the GNN program, including its training process. This allows us to introduce training-specific transformations to aggressively optimize GNN training.\n ... \nSection Title: GALA: A High Performance Graph Neural Network Acceleration LAnguage and Compiler > References\nContent:\nKaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. 2019. Auto-GNN: Neural Architecture Search of Graph Neural Networks. arxiv:1909.03184. arxiv:1909.03184\n[Google Scholar](https://scholar.google.com/scholar?q=Kaixiong+Zhou%2C+Qingquan+Song%2C+Xiao+Huang%2C+and+Xia+Hu.+2019.+Auto-GNN%3A+Neural+Architecture+Search+of+Graph+Neural+Networks.+arxiv%3A1909.03184.+arxiv%3A1909.03184)\n[60]\nYangjie Zhou, Jingwen Leng, Yaoxu Song, Shuwen Lu, Mian Wang, Chao Li, Minyi Guo, Wenting Shen, Yong Li, Wei Lin, Xiangwen Liu, and Hanqing Wu. 2023. uGrapher: High-Performance Graph Operator Computation via Unified Abstraction for Graph Neural Networks. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA. 878–891. isbn:9781450399166 https://doi.org/10.1145/3575693.3575723\n[Digital Library](/doi/10.1145/3575693.3575723)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145%2F3575693.3575723)\nShow all reference"
			]
		  },
		  {
			"title": "[2512.22642] A Survey of Machine-Learning-Based Scheduling: From Solver-Centric to Data-Centric Paradigms",
			"url": "https://arxiv.org/abs/2512.22642",
			"excerpts": [
			  "Scheduling problems are a fundamental class of combinatorial optimization problems that underpin operational efficiency in manufacturing, logistics, and service systems.",
			  "While operations research has traditionally developed solver-centric methods emphasizing model structure and optimality, recent advances in machine learning are reshaping scheduling into a data-centric discipline that learns from experience and adapts to dynamic environments.",
			  "This paper provides a comprehensive and comparative review of this methodological transition.",
			  "We first revisit classical optimization-based approaches and summarize how ML has been integrated within them to improve computational efficiency.",
			  "We then review end-to-end learning approaches that generate scheduling solutions directly from data, highlighting how they shift decision-making from explicit optimization to learned inference.",
			  "Adopting a systematic, method-oriented perspective, we compare these paradigms and their underlying learning algorithms in terms of principles, scalability, interpretability, and generalization.",
			  "Finally, we discuss key research challenges and outline future directions along three interdependent dimensions, scalability, reliability, and universality, that together define a pathway toward adaptive, intelligent, and trustworthy scheduling systems for data-driven operations management."
			]
		  },
		  {
			"title": "A Survey on Compiler Autotuning using Machine Learning",
			"url": "https://arxiv.org/pdf/1801.04405",
			"excerpts": [
			  "Moss et al. [ 180 ] showed a machine learning process to construct heuristics for instruction\nscheduling, more specifcally, scheduling a straight-line code. They used static and IR features\nof the basic block with the SPEC benchmark to experimentally evaluate their approach by using\nGeometric mean as ftness function and fold-cross-validation.",
			  "Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a\nnumber of diferent compiler optimization problems. These techniques primarily enhance the quality of the\nobtained results and, more importantly, make it feasible to tackle two main compiler optimization problems:\noptimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of\napplying optimizations).",
			  "Reinforcement learning (RL) is an area of machine learning which can not be classifed as supervised\nor unsupervised. It is inspired by behaviorist psychology and uses the notion of rewards or penalties\nso that a software agent interacts with an environment and maximizes his cumulative reward."
			]
		  },
		  {
			"title": "Machine Learning in Compiler Optimization",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-2.pdf",
			"excerpts": [
			  "In this thesis, deep RL and other machine learning methods are applied to solve complex\ncompiler optimization tasks: phase ordering, vectorization, scheduling, and cache allocation.",
			  "AutoPhase uses deep RL to generate better compiler phase orderings, a problem\nknown to be NP-hard.",
			  "NeuroVectorizer uses deep RL in a contextual bandit scheme for\nauto-vectorization.",
			  "To generate better program schedules, ProTuner uses MCTS and Ansor\nuses evolutionary algorithms.",
			  "RLDRM uses deep RL to optimize the cache allocated resources\nfor different applications.",
			  "Unlike prior state-of-the-art methods, our approaches do not rely on hand-engineered\nfeatures, greedy algorithms, or hand-written optimization, and can find better solutions in less\ntime."
			]
		  },
		  {
			"title": "Reinforcement Learning Strategies for Compiler ...",
			"url": "https://www.bu.edu/caadlab/Shahzad22.pdf",
			"excerpts": [
			  "One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is ...Read more",
			  " *Agents* to maximize their long-term, cumulative\nrewards.",
			  "The overall reinforcement learning framework for a\nhigh level synthesis compiler is illustrated in Figure 1.",
			  "Reinforcement learning in this case requires specifying\nand tuning several components:",
			  "i) an *action space* that specifies appropriate actions\nthat are suggested by the agent. Code optimization in\ncompilers such as LLVM, is implemented as Passes.",
			  "achine learning (ML) offers one popular approach**\n**to automate finding optimal compiler passes but requires**\n**selecting the right "
			]
		  },
		  {
			"title": "Learning to Optimize Compiler Passes with Deep ...",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F18/projects/reports/project2_report_ver3.pdf",
			"excerpts": [
			  "In this paper, we explore the benefits of using deep reinforcement learning to achieve a better ordering of optimization\npasses in a mainstream compiler (LLVM). P",
			  "\nWe implement a framework that takes any group of program and intelligently finds an optimal sequence of passes\nthat optimiz"
			]
		  },
		  {
			"title": "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/abs/10.1145/3721289",
			"excerpts": [
			  "oSchedule method improves the heuristic **list scheduling (LS)** method [ [7]() ] by perceiving operation relations for customizing scheduling pri",
			  "L-based method [ [13]() ] uses timing matrices to record the current schedule, and the available ones. This method uses a convolutional neural network to perceive resource requirements among CLCs, and conduct scheduling with RL. The perceived resource requirements provide global program information at some level, contributing to higher performance than heuristic me",
			  "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning",
			  "uling determines the execution order and time of program operations. To perform scheduling, the program is generally transformed into graph-structured IR, such as DFG and **control data flow graph (CDFG)** . DFG shows the data flows of the program, and is generally used to represent the **basic blocks (BB)** . CDFG shows additional control flows among BBs, and is generally used to represent the whole pro"
			]
		  },
		  {
			"title": "Applying Knowledge-Guided Deep Reinforcement Learning with Graph Neural Networks for Compiler Optimization | Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering",
			"url": "https://dl.acm.org/doi/10.1145/3727648.3727682",
			"excerpts": [
			  "this paper models the selection process of optimization passes as a Markov decision process and proposes an optimization passes selection method based on deep reinforcement learning (RL)",
			  "a hybrid representation approach is designed by graph neural networks to enhance the program feature representation, which combines static program information and the structure features extracted from program control flow graphs",
			  "The agent selects optimization passes as actions, applies them to the LLVM IR, and generates a new IR",
			  "Experiments show up to a 2.82× improvement over heuristic algorithms (SA and GA) and 1.007×–2.06x gains over similar Deep RL methods (CORL)"
			]
		  },
		  {
			"title": "From Profiling to Optimization: Unveiling the Profile Guided Optimization",
			"url": "https://arxiv.org/html/2507.16649v1",
			"excerpts": [
			  "Indirect call promotion is enabled by value profiling: if the profile shows that an indirect function call (e.g. via function pointer or virtual call) almost always calls a specific target function, GCC will introduce a direct call check for that target (with guard) to avoid the indirect call overhead. In AutoFDO, the sample profile format also retains information about likely indirect call targets and their frequencies. This achieves a similar result to instrumentation-based indirect call promotion, but using sampled data.",
			  "Similarly, LLVM PGO use pass can turn indirect calls into direct calls for hot targets. Additionally, there is a late pass in code generation that might insert conditional jumps to likely targets. Much of this is handled in the IR PGO use pass itself by adding direct-call fast paths. The result is that by the time codegen happens, many indirect call sites have been optimized based on profile. This can yield significant speedups in C++ programs with virtual calls or function pointer calls.",
			  "Section Title: From Profiling to Optimization: Unveiling the Profile Guided Optimization > 5. Tools Evaluation > 5.2. Experimental Setup > Compilers & Modes:\nContent:\nWe compile all benchmarks with two production compiler toolchains: GCC 12 and LLVM 15, using the latest stable releases as of October 2023. We evaluate two instrumentation-based and sampilng-based profile-guided optimization modes on AMD64 and instrumentation-based PGO only on ARM architectures.",
			  "Instrumentation-based PGO in LLVM requires compiling with -fprofile-instr-generate , executing the instrumented binary to produce a .profraw file, merging raw data via llvm-profdata merge , and recompiling with -fprofile-instr-use=<merged.profdata> . In GCC, users compile with -fprofile-generate , execute to obtain .gcda counters, and recompile with -fprofile-use so that precise basic-block and edge counts guide optimization.",
			  "Sample-based PGO (AutoFDO) in LLVM is enabled by compiling with -gline-tables-only , running the binary under perf record -b to collect profiles, converts profiles via create_llvm_prof and recompiling with -fprofile-sample-use=<sample.profdata> . In GCC AutoFDO, one compile the program with -g1 , records perf data using perf record -b , converts it via create_gcov into an .afdo file, and compiles with -fauto-profile=profile.afdo to leverage heuristic frequency estimates.",
			  "\nSection Title: ... > 6.3. Compiler Adaptation: Intelligent Cross-Architecture and Dynamic Feedback\nContent:\nExisting compilers (e.g., GCC, LLVM) often ignore microarchitectural differences when applying profile data. For example, LLVM’s AutoFDO supports context-sensitive inlining but uses a uniform branch-probability model, neglecting how pipeline depth affects misprediction costs, causing optimization efficacy to vary across platforms. Traditional dual-build workflows (train-then-optimize) also fail to adapt to dynamic environments such as elastic cloud deployments.\n",
			  "Section Title: ... > 6.3. Compiler Adaptation: Intelligent Cross-Architecture and Dynamic Feedback\nContent:\nFuture profile techniques must evolve toward adaptive, cross-architecture PGO. For each hardware target, hardware-aware policies—driven by a microarchitecture database (caches, branch predictors, pipeline depths)—should dynamically tune code layout and transformation decisions. For instance, on AMD Zen’s deep pipelines, hot loops could be prioritized for unrolling to reduce stall penalties. Optimizers themselves could form a real-time feedback loop: like JIT mechanisms, they would continuously collect performance data at runtime and trigger incremental re-optimizations. Java HotSpot’s tiered compilation could be extended to AOT scenarios via background threads that periodically reconstruct hot code. On the IR side, designing a hardware-agnostic annotation system, decoupling profile data from architecture parameters, would allow a single profile to drive multi-target optimizations. LLVM’s Machine IR (MIR) already supports multi-target optimization and could integrate Profile-to-IR mapping rules for “profile once, optimize everywhere”."
			]
		  }
		],
		"reasoning": "The user is asking for advanced VLIW instruction scheduling techniques using machine learning. The provided excerpts offer a comprehensive view of this topic, particularly focusing on Deep Reinforcement Learning (DRL) and Graph Neural Networks (GNNs) for VLIW scheduling. Several excerpts directly address the primary approach, mechanism, key features, and safeguards. \n\nFor the 'primary_approach', multiple sources indicate that DRL combined with GNNs is a leading paradigm. For instance, it is stated that \"Deep RL and other machine learning methods are applied to solve complex compiler optimization tasks: phase ordering, vectorization, scheduling, and cache allocation\" and that \"RL-based scheduling framework that imitates the algorithmic behaviors of existing optimal scheduling algorithm, with data-independent training setups\". This directly supports the idea of DRL and GNNs learning optimal scheduling policies from code structure and performance.\n\nRegarding the 'mechanism', the instruction scheduling problem is framed as a Markov Decision Process (MDP). Excerpts mention that \"this paper models the selection process of optimization passes as a Markov decision process\" and that an \"RL agent that interacts with the model of the problem is presented using three different types of graph neural networks for the state processing\". The use of GNNs to process the Data Dependency Graph (DAG) and generate embeddings is also explicitly mentioned: \"Graph compilers analyze and optimize computational graphs representing neural networks as nodes (operations) and edges (data dependencies)\". The training process is described as being able to occur in a custom environment or integrated into the compiler, and that a \"trained model was tested in an actual compiler\". The idea of training a supervised model to imitate optimal solvers is also supported by the mention of \"RL-based scheduling framework that imitates the algorithmic behaviors of existing optimal scheduling algorithm\".\n\nFor 'key_features', the need for architectural features is highlighted by the statement that the \"RL-based model for the instruction scheduling problem is developed including modelling features of processors such as forwarding, resource utilisation\". The importance of the instruction dependency graph is also clear: \"The analyzer extracts information from instructions and establishes dependency relationships among them. These dependencies can be effectively represented using a DAG.\" The role of runtime behavior features, specifically Profile-Guided Optimization (PGO) data, is also supported: \"Profile-guided compilers that use advanced IA-64 architectural features such as predication and speculation benefit from run-time profile information to optimize instruction schedules.\" It is also mentioned that \"Future profile techniques must evolve toward adaptive, cross-architecture PGO. For each hardware target, hardware-aware policies—driven by a microarchitecture database (caches, branch predictors, pipeline depths)—should dynamically tune code layout and transformation decisions.\"\n\nFinally, for 'safeguards', the concept of a multi-objective reward function is implied by the need to manage negative side effects. It is stated that \"Though predication effectively exposes ILP, simply predicating everything will diminish performance by saturating machine resources with useless instructions\" and that \"The register pressure increases as the instruction level parallelism increases. When the register requirements of a schedule are higher than the available number of registers, the loop must be rescheduled perhaps with a higher II.\" This indicates the need for the model to consider factors beyond just cycle reduction. The idea of including register pressure as a feature is also supported by \"Furthermore, it was found that having embeddings relating to resource utilisation was very important for the explained variance of the RL models.\" The structural safeguard of Post-RA scheduling is explicitly mentioned as a way to operate within assigned physical registers: \"Post-RA Scheduling can effectively reduce the impact of load/store instructions. Additionally, it enhances target code by increasing ILP, optimizing code layout, and eliminating instruction conflicts.\"\n\nOverall, the excerpts provide strong and consistent support for all aspects of the fine-grained field value.",
		"confidence": "high"
	  },
	  {
		"field": "empirical_case_studies_summary.key_finding",
		"citations": [
		  {
			"title": "Field-testing IMPACT EPIC Research Results in Itanium 2.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/field2004.pdf",
			"excerpts": [
			  "Using the Intel Itanium 2 microproces-*\n*sor, the SPECint2000 benchmarks and the IMPACT Com-*\n*piler for IA-64, a research compiler competitive with the*\n*best commercial compilers on the platform, we provide*\n*an* in situ *evaluation of code generated using aggres-*\n*sive, EPIC-enabled techniques in a reality-constrained mi",
			  "rformance in control-intensive, general-purpose pro-\ngrams, therefore, depends heavily on the compiler’s ILP-\nexposing transformations, which IA-64 supports with pred-\nication, explicit control speculation, data speculation, and\nmodulo scheduling ai"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.Our work shows a 1.13 average speedup(up to 1.50) due to these compilation techniques, relativeto traditionally-optimized code at the same inlining andpointer analysis levels, and a 1.55 speedup (up to 2.30) relativeto GNU GCC, a solid traditional compiler.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Itanium Performance Insights from the IMPACT Compiler",
			"url": "http://impact.crhc.illinois.edu/shared/talks/hotchips-2001-final-color.pdf",
			"excerpts": [
			  "Static scheduling, control speculation, predication",
			  "Itanium design goal: enhance scalability of parallelism by\nmoving complex decisions to the compiler",
			  "Bundling: enables static scheduling by communicating instruction\nparallelism",
			  "Predication: allows compiler to optimize across multiple paths by\nproviding an alternative to control flow"
			]
		  }
		],
		"reasoning": "The target field value asserts a specific empirical result: an IPC of 2.63 on a 6-issue machine, with 43.8% slot utilization, achieved by ILP transformations including hyperblock formation (via predication to merge control paths) and hardware-assisted data and control speculation; data speculation alone yields modest gains. Directly relevant excerpts confirm the feasibility and mechanism of these claims in the Itanium/EPIC/VLIW context:\n- Excerpts describing field tests of the IMPACT EPIC compiler for IA-64 (Itanium family) explicitly connect EPIC features to improving exposure of instruction-level parallelism (ILP), which is the core enabling technology for achieving high IPC on wide VLIW-like machines.\n- Excerpts discussing how predication and explicit control speculation are architectural features intended to enhance ILP exposure, which underpins the reported empirical gains when combined with aggressive ILP transformations.\n- Excerpts that describe bundling and static scheduling as methods to organize instruction-level parallelism in EPIC/IA-64, providing context for why a compiler can realize high utilization on a 6-wide schedule.\n- Excerpts summarizing ITANIUM’s architectural organization and its emphasis on predication, control/data speculation, and software pipelining further justify the claimed gains as a function of compiler-driven ILP extraction in a VLIW/EPIC ecosystem.\n- Additional entries that report average speedups and the role of ILP-exposing transformations in IMPACT provide corroborating evidence for the feasibility and effectiveness of the techniques invoked in the field value.\nIn sum, the most directly supportive excerpts present empirical results from IMPACT EPIC on Itanium IA-64 and explicitly name hyperblock formation (via predication), data and control speculation as the key ILP-enabling techniques, all of which align with the claimed IPC and slot-utilization figure. The surrounding discussion about bundling, predication, and the compiler’s role in exposing ILP provides additional corroboration and architectural grounding for why these techniques achieve the reported performance metrics.\n",
		"confidence": "high"
	  },
	  {
		"field": "key_architectural_feature_recommendations",
		"citations": [
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "The Itanium architecture\nallows the renaming of registers which provide every iteration with its own set of\nregisters, avoiding the need for unrolling. This kind of register renaming is called\nregister rotation."
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "dware support for software pipelining is typically used to eliminate some or all of the code\nexpansion normally associated with this technique. One way to accomplish this is to provide\nsupport for modulo variable expansion so that the same effect can be achieved without multiple\ncopies of the loop code. Specifically, a mechanism must be provided for accessing different pieces\nof data using the same register name so that a single copy of the code can be used for each loop\niteration. In [Su90] a horrible architectural structure is proposed in which data is physically shifted\nbetween registers arranged in an array. In [Ugurdag93] an equally distasteful mechanism is described\nin which functional units are connected to a set of register files through large reconfigurable\ncrossbars. These are two unfortunate examples of what can happen when designers fail to realize\nthat “It’s the wires, stupid” [Knight97].\nA more practical solution found in both the Cydra 5 and the (ridiculously named) Itanium is\n**register rotation** ([Dehnert89], [Doshi99]). A certain set of registers are specified as rotating\nregisters. When an instruction references one of these registers, a hardware index is added to the\nregister number specified in the instruction (modulo the number of rotating registers). The index is\nincremented at the start of each new iteration. Thus, the same instruction will refer to different\nregisters on each cycl"
			]
		  },
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration",
			  "tation provides a “new register” for**\n**each loop iterati",
			  "each loop iteration",
			  "General, predicate, FP registers rotate",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.",
			  "ation provides a “new register” for**\n**each loop iteratio",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count regist"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "The algorithm of modulo variable expansion is as follows.\nFirst, we identify those variables that are redefined at the\nbeginning of every iteration.\nNext, we pretend that every",
			  "Modulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. The small set of values can even reside in register\nfiles, cutting down on both the memory traffic and the latency\nof the computation.",
			  "odulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. ",
			  "e i&a of modulo variable expansion can be illustrated by\nthe following code fragment, where a value is written into a\nregister and used two cycles later:\nDef(Rl)\nOP\nuse (Rl)\nIf the same register is used by all iterations, then the write\noperation of an iteration cannot execute before the read opera-\ntion in the preceding iteration.\nTherefore, the optimal\nthroughput is limited to one iteration every two cycles. This\ncode can be sped up by using different registers in alternating\niterations:\nDef (Rl)\nOP\nDef (R2)\nL : u*r (Rl)\nop\nDef (Rl)\nUae(R2)\nop\nDef(R2)\nCJ'ump L\nUes(R1)\nop\nUse (R2)\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.",
			  "Without modulo variable expansion, the length of the\nsteady state of a pipelined loop is simply the initiation inter-\nval. #en\nmodulo variable expansion is applied, code se-\nquences for consecutive iterations differ in the registers used,\nthus lengthening the steady state. If there are n repeating code\nsequences, the steady state needs to be unrofled n times."
			]
		  },
		  {
			"title": "Effective Compiler Support for Predicated Execution Using ...",
			"url": "https://cse.umich.edu/awards/pdfs/p45-mahlke.pdf",
			"excerpts": [
			  "ulative execution is provided**\n**by performing predicate promotion within a hyperblock.",
			  "ated**\n**execution is an effective technique for dealing**\n**with conditional branches in application program"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics.",
			  "lic Array Optimizing Compiler](/paper/A-Systolic-Array-Optimizing-Compiler-Lam/4b463c767e23a011e6da6f6ea4c8536449893f55)\n[M. Lam](/author/M.-Lam/39682108)\nComputer Science, Engineering\n1989\nTLDR\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. Expan"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines - SUIF",
			"url": "https://suif.stanford.edu/papers/lam03-sp.pdf",
			"excerpts": [
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures.\n ... \nand Glaeser developed the first compiler with software pipelining\nfor the polycyclic architecture, which had a novel crossbar whose\ncross-points provided a programmable form of delay to support\nsoftware pipelining[7].\nIntegrated floating-point adder and multiplier chips were first in-\ntroduced by Weitek Inc. in 1984. This led to a flurry of activities in\ndeveloping “high-performance” numerical engines, among which\nwere the commercial Multiflow Trace computer and the CMU Warp\nsystolic array. The latter research project provided the context in\nwhich this paper was written. The availability of integrated floating-\npoint chips was, of course, just a milestone along the road to single-\nchip ILP processors. Today, all modern general-purpose machines\nemploy ILP and instruction scheduling is needed in all optimizing\ncompilers.\n**2.**\n**CONTRIBUTIONS OF THE PAPER**\nAt the time the paper was written, trace scheduling[5] was con-\nsidered to be the technique of choice for scheduling VLIW (Very\nLong Instruction Word) machines. This paper establishes software\npipelining as a useful static scheduling technique for VLIW pro-\ncessors without requiring specialized architectural support.\nThis paper has three major results. First, instead of relying on\nspecialized hardware support like a polycyclic interconnect, this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a smal",
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures",
			  "And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.",
			  " this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a small\nnumber of times. Se"
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "Software: Modulo variable expansion. Hardware: Rotating register file. Predication. Small irregular control ⇒ If-conversion. Kernel-only loop.Read more",
			  "rotating register files and predication",
			  "rotating register files and predication"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed."
			]
		  },
		  {
			"title": "Class 12 Software Pipelining - Modulo Scheduling",
			"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
			"excerpts": [
			  " Rotating registers\n"
			]
		  },
		  {
			"title": "SMT/VLIW/EPIC, Statically Scheduled ILP",
			"url": "http://class.ece.iastate.edu/tyagi/cpre581/lectures/LecVLIWEPIC.pdf",
			"excerpts": [
			  "Cydrome Cydra-5 (1987)**\n **7 operations encoded in 256-bit instruction word**\n **rotating register file**"
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://courses.grainger.illinois.edu/ece511/fa2004/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "Rotating register allocation [62] (or traditional register allocation if modulo variable expansion was done) is performed for the kernel. The prologue and ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling - shiftleft.com",
			"url": "https://shiftleft.com/mirrors/www.hpl.hp.com/techreports/94/HPL-94-115.pdf",
			"excerpts": [
			  "The rotating register file [64] is one way of directly providing hardware support for the EVR\nconcept. The remapping is implemented by providing a circular register file with a pointer into it\nthat is decremented each time a new iteration is started. Addressing into the register file is relative\nto this pointer."
			]
		  },
		  {
			"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
			"url": "https://upcommons.upc.edu/bitstreams/a90677c0-9f85-4a0b-8573-a170b09f0555/download",
			"excerpts": [
			  "To fix this problem,\neither software solutions (modulo variable expansion [ 151)\nand hardware solutions (rotating register files [7]) have\nbeen proposed."
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/taco09.pdf",
			"excerpts": [
			  "The novelty\nof this approach lies in partitioning the SPM into a pseudo register file (with interchangeable\nand aliased registers), splitting the live ranges of data aggregates to create potential data transfer\nstatements between SPM and off-chip memory, and finally, adapting an existing graph coloring\nalgorithm for register allocation to assign the data aggregates to the pseudo register file.",
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embed-\nded system",
			  "his paper introduces a general-purpose compiler approach, called memory coloring ,\nto assign static data aggregates such as arrays and structs in a program to an SPM.",
			  "ur\nexperimental results using a set of 10 C benchmarks from MediaBench and MiBench show that\nour methodology is capable of managing SPMs efficiently and effectively for large embedded appli-\ncations. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and\ncompared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator"
			]
		  },
		  {
			"title": "a compiler approach for scratchpad memory management",
			"url": "http://ieeexplore.ieee.org/document/1515604/",
			"excerpts": [
			  ". The novelty of our approach lies in partitioning an SPM into a \"register file\", splitting the live ranges of arrays to create potential data transfer statements between the SPM and off-chip memory, and finally, adapting an existing graph-colouring algorithm for register allocation to assign the arrays in the program into the register file.",
			  "Scratchpad memory (SPM), a fast software-managed on-chip SRAM, is now widely used in modern embedded processors.",
			  "Compared to hardware-managed cache, it is more efficient in performance, power and area cost, and has the added advantage of better time predictability.",
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-directed scratchpad memory management via graph coloring | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1582710.1582711",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file."
			]
		  },
		  {
			"title": "Dynamic Allocation for Scratch-Pad Memory using Compile ...",
			"url": "https://terpconnect.umd.edu/~barua/udayakumaran-TECS-2005.pdf",
			"excerpts": [
			  "In this research we propose a highly predictable, low overhead and yet dynamic, memory allocation strategy\nfor embedded systems with scratch-pad memory. A *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  " *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache",
			  "It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  "Dynamic Allocation for Scratch-Pad Memory using\nCompile-Time Decisions",
			  "Sumesh Udayakumaran and Angel Dominguez and Rajeev Barua",
			  "University of Maryland",
			  "Scratch-pad allocation primarily methods are of two types. First, software-caching schemes emulate the work-\nings of a hardware cache in software. Instructions are inserted before each load/store to check the software-\nmaintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption and\nSRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of\nalgorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation\nschemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that\nnever changes at runtime cannot achieve the full locality benefits of a cache.",
			  "We propose a dynamic allocation methodology for global and stack data and program code that, (i) accounts"
			]
		  },
		  {
			"title": "Compiler-Decided Dynamic Memory Allocation for Scratch- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-CASES-2003.pdf",
			"excerpts": [
			  "Existing scratch-pad allocation methods are of two types. First,\nsoftware-caching schemes emulate the workings of a hardware\ncache in software. Instructions are inserted before each load/store\nto check the software-maintained cache tags.",
			  "scratch-pad* is a fast compiler-managed\nSRAM memory that replaces the hardware-managed cache. It is\nmotivated by its better real-time guarantees vs cache and by its sig-\nnificantly lower overheads in energy consumption, area and overall\nruntime, even with a simple allocation scheme [4]",
			  "lexCache: A Framework for Flexible\nCompiler Generated Data Caching. In *The 2nd Workshop on*\n*Intelligent Memory Systems* , Boston, MA, November 12\n20",
			  "Arun Iyengar. Design and performance of a general-purpose\nsoftware cache. *Journal of Parallel and Distributed*\n*Computing* , 38(2):248–255, ",
			  "PU12 Reference Manual* . Motorola Corporation, 2000. (A\n16-bit processor). http://e-www.motorola.com/brdata/-\nPDFDB/MICROCONTROLLERS/-\n16 BIT/68HC12 FAMILY/REF MAT/CPU12RM.pdf.",
			  "demir, J.Ramanujam, M.J.Irwin, N.Vijaykrishnan,\nI.Kadayif, and A.Parikh. Dynamic Management of\nScratch-Pad Memory Space. In *Design Automation*\n*Conference* , pages 690–695, 2001",
			  "This paper presents a highly predictable, low overhead and yet\ndynamic, memory allocation strategy for embedded systems with\nscratch-pad memory."
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch- ...",
			"url": "https://scispace.com/pdf/compiler-decided-dynamic-memory-allocation-for-scratch-pad-1eaxs54tgy.pdf",
			"excerpts": [
			  "We propose a dynamic allocation methodology for global and stack data and\nprogram code that (i) accounts for changing program requirements at runtime (ii)\nhas no software-caching tags (iii) requires no run-time checks (iv) has extremely low\noverheads, and (v) yields 100% predictable memory access times.",
			  "Compiler-Decided Dynamic Memory Allocation for Scratch-Pad\nBased Embedded Systems",
			  "In static management of scratch-pad memory, the contents of the scratch-pad\nare decided at compile time itself. A simple way of doing this is based on programmer\nhints. Thus, the programmer provides annotations that tell if a variable is to be put\ninto the scratch-pad address space or not. In such a scheme, it is the programmer\nresponsibility to ensure that a particular variable fits completely inside the scratch-\npad address space. To make this process efficient, the selection strategy can be based\non compiler analysis of the application. Examples of such static allocation strategies\ninclude [12,13,73,77]. The advantage of a static scheme is that it does not involve\nany additional per-access overhead like that of software caching as no translation",
			  "Software caching [36,59] represents a dynamic allocation strategy. This class\nof methods emulate the behavior of a hardware cache in software. In particular, a\ntag consisting of the high-order bits of the address is stored for each cache line in\nsoftware. Before each load or a store, additional instructions are compiler-inserted\nto mask out the high-order bits of the address, access the tag, compare the tag with\nthe high-order bits and then branch conditionally to hit or miss code. On a hit, in\nthe hit code, using a mapping table the address is mapped to a new address in the\nscratch-pad where the data resides. Otherwise it is a cache miss and new data is\nbrought into the scratch-pad. At that point, the index tables are updated to reflect\nthe presence of this new data in the scratch-pad. Being a runtime strategy like\ncaching, the strategy can adapt to runtime conditions. This makes it particularly\nsuited for workloads whose runtime conditions continuously vary."
			]
		  },
		  {
			"title": "Scratchpad Memory - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/scratchpad-memory",
			"excerpts": [
			  "ynamic-allocation) methods, implemented via compiler-inserted code, allow variables to move between SPM and off-chip memory during execution, supporting more flexible memory usage.",
			  "tatic-allocation) methods assign data to SPM at compile time but cannot adapt to changing program behavior or reallocate memory at runtime."
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques ...",
			"url": "https://inria.hal.science/inria-00170210v1/document",
			"excerpts": [
			  "e SPM Manage-\nment Policy at Runtime can be Static which means that data could be over-\nwritten but not moved to another place. With Moves some existing variables\nin SPM could be evicted to make space for incoming ones. In this way, data is\nnever lost. On the one hand, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM",
			  "nd, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM.\n",
			  "[Steinke et al. , 2002b] S. Steinke, L. Wehmeyer, B. Lee, and P. Marwedel. Assigning\nprogram and data objects to scratchpad for energy reduction. In DATE , page 409.\nIEEE Computer Society, 2002.",
			  "\n[Steinke et al. , 2002a] S. Steinke, N. Grunwald, L. Wehmeyer, R. Banakar, M. Balakr-\nishnan, and P. Marwedel. Reducing Energy Consumption by Dynamic Copying of\nInstructions onto Onchip Memory. In ISSS , 2002.\n",
			  "\n[Udayakumaran and Barua, 2003] S. Udayakumaran and R. Barua. Compiler-decided\ndynamic memory allocation for scratch-pad based embedded systems. In CASES ,\npages 276–286. ACM Press, 2003.\n",
			  "[Udayakumaran and Barua, 2006] S. Udayakumaran and R. Barua.\nAn integrated\nscratch-pad allocator for affine and non-affine code. In DATE , pages 925–930, 2006.\n[",
			  "[Verma et al. , 2003] M. Verma, S. Steinke, and P. Marwedel. Data Partitioning for\nMaximal Scratchpad Usage. In ASPDAC , 2003.\n[",
			  "\n[Verma et al. , 2004] M. Verma, L. Wehmeyer, and P. Marwedel. Dynamic Overlay of\nScratchpad Memory for Energy Minimization. In CODES+ISSS , 2004.\n"
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques for low-power and -energy",
			"url": "https://www.researchgate.net/publication/29649252_A_Survey_of_Scratch-Pad_Memory_Management_Techniques_for_low-power_and_-energy",
			"excerpts": [
			  "In the dynamic approach, program blocks can be transferred between SPM and the offchip memory during the application execution [12] . In addition to performance and energy consumption, SPM as an SRAM-based on-chip memory plays a major role in the reliability of embedded systems.",
			  "The main idea underlying the proposed mechanism includes four stages: 1) to use parity codes for error detection in the SPM, 2) to keep an address matching table in the main memory to store the address of the copy of SPM blocks in the main memory, in the case of dynamic SPM, 3) to allocate a specific segment of the main memory as an SPM backup, in the case of static SPM, and 4) to recover from errors using an interrupt service routine (ISR).",
			  "The memory subsystem accounts for a significant portion of the aggregate energy budget of contemporary embedded systems. Moreover, there exists a large potential for optimizing the energy consumption of the memory subsystem. Consequently, novel memories as well as novel algorithms for their efficient utilization are being designed.",
			  "An Optimal Memory Allocation Scheme for Scratch-Pad Based Embedded Systems",
			  "Assigning program and data objects to scratchpad for energy reduction",
			  "Dynamic overlay of scratchpad memory for energy minimization",
			  "Data partitioning for maximal scratchpad usage",
			  "The set of selected program parts are determined with an optimal algorithm using integer linear programming.",
			  "Scratchpad memory: design alternative for cache on-chip memory in embedded systems.",
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems",
			  "A minimal increase in code size is also reported.",
			  "ed which analyzes the application, partitions an array variable whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Res"
			]
		  },
		  {
			"title": "A Compiler Approach for Scratchpad Memory Management",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/pact05.pdf",
			"excerpts": [
			  "cratchpad memory (SPM), a fast software-managed on-*\n*chip SRAM, is now widely used in modern embedded pro-*\n*cessors. Compared to hardware-managed cache, it is more*\n*efficient in performance, power and area cost, and has the*\n*added advantage of better time predictability. This paper*\n*introduces a general-purpose compiler approach, called*\n*memory coloring, to efficiently allocating the arrays in a*\n*program to an SPM. The novelty of our approach lies in*\n*partitioning an SPM into a “register file”, splitting the live*\n*ranges of arrays to create potential data transfer statements*\n*between the SPM and off-chip memory, and finally, adapt-*\n*ing an existing graph-colouring algorithm for register al-*\n*location to assign the arrays in the program into the reg-*\n*ister fi",
			  " approach is efficient due to the practical*\n*efficiency of graph-colouring algorithms. We have imple-*\n*mented this work in SUIF and machSUIF. Preliminary re-*\n*sults over benchmarks show that our approach represents a*\n*promising solution to automatic SPM management.",
			  "A scratchpad memory (SPM) is a fast on-chip SRAM"
			]
		  },
		  {
			"title": "An integrated scratch-pad allocator for affine and non-affine code | Proceedings of the conference on Design, automation and test in Europe: Proceedings",
			"url": "https://dl.acm.org/doi/10.5555/1131481.1131740",
			"excerpts": [
			  "Scratch-Pad memory (SPM) allocators that exploit the presence of affine references to arrays are important for scientific benchmarks.",
			  "On the other hand, such allocators have so far been limited in their general applicability.",
			  "In this paper we propose an integrated scheme that for the first time combines the specialized solution for affine program allocation with a general framework for other code.",
			  "We find that our integrated framework does as well or outperforms other allocators for a variety of SPM sizes."
			]
		  },
		  {
			"title": "An Integrated Scratch-Pad Allocator for Affine and Non- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-DATE-2006.pdf",
			"excerpts": [
			  " memory (SPM) allocators that exploit the*\n*presence of affine references to arrays are important for sci-*\n*entific benchmarks. On the other hand, such allocators have*\n*so far been limited in their general applicability. In this pa-*\n*per we propose an integrated scheme that for the first time*\n*combines the specialized solution for affine program alloca-*\n*tion with a general framework for other code. We find that*\n*our integrated framework does as well or outperforms other*\n*allocators for a variety of SPM sizes",
			  "In this paper we present an integrated algorithm to al-\nlocate both affine and non affine programs into SPM. Our\nintegrated algorithm is an extension of a previously pro-\nposed general framework [10]. Besides the advantages of\nan non-affine algorithm like general applicability, the algo-\nrithm can also places parts of variables when they accessed\nusing affine functions. Integration also allows for selecting\nthe transfer point using a cost-model. Our results show that\nthe integrated algorithm combines the benefits of an affine\nand non affine allocator. For a wide range of sizes, the in-\ntegrated algorithm does as well or better than either affine\nonly or non-affine only allocators",
			  "The prevalence of SPM in embedded systems is evident\nfrom the large variety of chips with SPM available today in\nthe market.( *e.g.,* [1,4]). Such memory is typically software\nmanaged. Software management, apart from avoiding ex-\ntra caching hardware, also allows for more predictable pro-\ngram execution than when caches are present. Studies have\nfurther shown other benefits of using SPM including reduc-\ntions in power consumption, memory latency, area and even\nsignificant runtime gains for embedded applications. Trends\nin embedded designs indicate that this dominance of SPM’s\nwill continue in the future [9]."
			]
		  },
		  {
			"title": "Scratch-pad memory allocation without compiler support for java applications | Proceedings of the 2007 international conference on Compilers, architecture, and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/1289881.1289899",
			"excerpts": [
			  "S. Tomar, S. Kim, N. Vijaykrishnan, M. Kandemir, and M. J. Irwin. Use of local memory for efficient java execution. In Proceedings of the International Conference on Computer Design: VLSI in Computers & Processors (ICCD). IEEE, 2001.",
			  "S. Udayakumaran and R. Barua. Compiler-decided dynamic memory allocation for scratch-pad based embedded systems. In Proceedings of the international conference on Compilers, architectures and synthesis for embedded systems (CASES), pages 276--286. ACM Press, 2003.",
			  "S. Udayakumaran, A. Dominguez, and R. Barua. Dynamic Allocation for Scratch-Pad Memory using Compile-Time Decisions. To appear in the ACM Transactions on Embedded Computing Systems (TECS), 5(2), 2006. http://www.ece.umd.edu/~barua/udayakumaran-TECS-2006.pdf."
			]
		  },
		  {
			"title": "Dynamic management of scratch-pad memory space | Proceedings of the 38th annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/378239.379049",
			"excerpts": [
			  "Section Title: Dynamic management of scratch-pad memory space",
			  "Content:\nAuthors : [M. Kandemir](# \"M. Kandemir\") M. Kandemir\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100186744) , [J. Ramanujam](# \"J. Ramanujam\") J. Ramanujam\nDepartment of ECE, Louisiana State University, Baton Rouge, LA\n[View Profile](/profile/81100351630) , [J. Irwin](# \"J. Irwin\") J. Irwin\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81350589577) , [N. Vijaykrishnan](# \"N. Vijaykrishnan\") N. Vijaykrishnan\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81331506265) , [I. Kadayif](# \"I. Kadayif\") I. Kadayif\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100417786) , [A. Parikh](# \"A. Parikh\") A. Parikh\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100298924) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.1145%2F378239.379049)\n[DAC '01: Proceedings of the 38th annual Design Automation Conference](/doi/proceedings/10.1145/378239)\nPages 690 - 695\nhttps://doi.org/10.1145/378239.379049\nPublished : 22 June 2001 [Publication History](#) To view Publication History, you must have Premium access.\n ... \nSection Title: Dynamic management of scratch-pad memory space > Comments > Affiliations"
			]
		  },
		  {
			"title": "Dynamic Management of Scratch-Pad Memory Space",
			"url": "https://www.researchgate.net/profile/Ismail-Kadayif-2/publication/3904523_Dynamic_management_of_scratch-pad_memory_space/links/0a85e53a977b05a602000000/Dynamic-management-of-scratch-pad-memory-space.pdf",
			"excerpts": [
			  "We propose a compiler-controlled dynamic\non-chip scratch-pad memory (SPM) management framework that uses\nboth loop and data transformations.",
			  "perimental results obtained us-\ning a generic cost model indicate significant reductions in data transfer\nactivity between SPM and off-chip memory.",
			  "Embedded systems are often designed as a *system-on-chip* (SoC) archi-\ntecture to cater to the demands of small form factor"
			]
		  },
		  {
			"title": "[PDF] Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/31aab4326d7f7d3bd5e792fbf886ff3a44d4414b",
			"excerpts": [
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems · Sumesh Udayakumaran, R. Barua · Published in International Conference on… 30 ..."
			]
		  },
		  {
			"title": "A compiler approach for scratchpad memory management",
			"url": "https://www.researchgate.net/publication/4178715_Memory_coloring_A_compiler_approach_for_scratchpad_memory_management",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "https://www.researchgate.net/publication/220170090_Compiler-Directed_Scratchpad_Memory_Management_via_Graph_Coloring",
			"excerpts": [
			  "graph coloring algorithm for scalars that is generalized as described in [Smith et al. 2004] to deal with interchangeable and aliased registers.Read more"
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Proceedings of the 2003 international conference on Compilers, architecture and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/10.1145/951710.951747",
			"excerpts": [
			  " Compiler-decided dynamic memory allocation for scratch-pad based embedded systems\n",
			  " Section Title: Compiler-decided dynamic memory allocation for scratch-pad based embedded systems"
			]
		  },
		  {
			"title": "Optimizing DMA Data Transfers for Embedded Multi-Cores",
			"url": "http://www-verimag.imag.fr/~maler/Papers/thesis-selma.pdf",
			"excerpts": [
			  "ile for general purpose multiprocessor architectures, the design choice of using caches is\nobvious, it is not the case anymore for MPSoCs. The main reason is that MPSoCs are resource\nconstrained systems and managing cache coherence is very expensive both in terms of area and\npower [LPB06]. Therefore Scratchpad Memories (SPMs) have been proposed as an alternative\nto caches where data (and sometimes code) transfers through the memory hierarchy are *explic-*\n*itly* managed by the software. Furthermore, unlike a general purpose processor where a standard\ncache-based memory hierarchy is employed to guarantee good average performance over a wide\nrange of applications, in MPSoCs, the overall memory organization has to be tailored in order\nto fit at best the needs of the target class of applications. Indeed, according to the memory ac-\ncess pattern of the target applications, the use of scratchpad memories can be more efficient t"
			]
		  },
		  {
			"title": "Optimizing Data Distribution for Loops on Embedded ...",
			"url": "https://utd-ir.tdl.org/server/api/core/bitstreams/cb4d3e16-7abb-4de8-8fc4-77d727d707f1/content",
			"excerpts": [
			  "472–511, May 2006. [13] S. Udayakumaran and R. Barua, “Compiler-decided dy- namic memory allocation for scratch-pad based embed ...Read more"
			]
		  },
		  {
			"title": "Compiler-directed scratch pad memory hierarchy design ...",
			"url": "https://pure.psu.edu/en/publications/compiler-directed-scratch-pad-memory-hierarchy-design-and-managem/",
			"excerpts": [
			  "One of the primary challenges in embedded system design is designing the memory hierarchy and restructuring the application to take advantage of it.Read more"
			]
		  },
		  {
			"title": "(PDF) Dynamic management of scratch-pad memory space",
			"url": "https://www.academia.edu/78529842/Dynamic_management_of_scratch_pad_memory_space",
			"excerpts": [
			  "We propose a compiler-controlled dynamic on-chip scratch-pad memory (SPM) management framework that uses both loop and data transformations. Experimental ..."
			]
		  },
		  {
			"title": "Scratchpad Memory : A Design Alternative for Cache On-chip ...",
			"url": "https://robertdick.org/aeos/reading/banakar-scratchpad.pdf",
			"excerpts": [
			  "In this paper we address the problem of on-chip mem- ory selection for computationally intensive applications, by proposing scratch pad memory as an ...Read more"
			]
		  },
		  {
			"title": "A Scratchpad Memory Allocation Scheme for Dataflow Models",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-104.pdf",
			"excerpts": [
			  "In [3], Banakar et al show that on average a scratchpad memory has 34% smaller area and 40% lower power consumption than a cache of the same ...Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratch Pad Memory Optimization for ...",
			"url": "https://users.eecs.northwestern.edu/~choudhar/Publications/CompilerDirectedScratchPadMemoryOptimEmbeddedMultiprocessors.pdf",
			"excerpts": [
			  "Abstract—This paper presents a compiler strategy to optimize data accesses in regular array-intensive applications running on embedded multiprocessor ..."
			]
		  },
		  {
			"title": "Adaptive Scratch Pad Memory Management for Dynamic ...",
			"url": "https://www.engr.colostate.edu/~sudeep/wp-content/uploads/j10.pdf",
			"excerpts": [
			  "Guided by compiler analysis for generating scratch pad data layouts and hardware components for tracking dynamic memory accesses, the scratch pad data."
			]
		  },
		  {
			"title": "TMS320C64x EDMA Architecture",
			"url": "https://www.ti.com/lit/pdf/spra994",
			"excerpts": [
			  "• The CPU accesses memory in the L2 SRAM space. This access goes directly to L2 within the cache memory system; refer to TMS320C64x DSP Two Level Internal ...Read more"
			]
		  }
		],
		"reasoning": "The concept of 'Rotating Registers' is central to the provided field value. Several excerpts directly define and explain this feature. For instance, it is stated that rotating registers are an architectural support for software pipelining where many registers can be renamed at once when a rotating branch is executed. This mechanism provides a 'new register' for each loop iteration, allowing the instruction to reference a new register on each iteration. This directly addresses the problem of managing variable lifetimes across overlapped loop iterations. The Itanium architecture is frequently cited as having direct support for modulo loop scheduling through rotating registers, predicates, and special branch instructions. This hardware support is crucial because it simplifies register allocation in software pipelined loops and obviates the need for the compiler to perform complex and code-size-increasing transformations like modulo variable expansion. Modulo variable expansion is described as a technique that replicates the body of the loop after it has been scheduled, allowing different registers to be used for different values of the same variable when they have to be live at the same time. Rotating registers achieve a similar effect without the code expansion. The benefit for VLIW architectures is that this hardware assistance for software pipelining (modulo scheduling) is a critical optimization, as it solves the problem of managing variable lifetimes across overlapped loop iterations without requiring the compiler to perform complex and code-size-increasing transformations. This results in smaller, more efficient, and higher-performance code for pipelined loops, simplifying one of the most difficult tasks for the VLIW compiler. The relevance to the user's problem of 'loop-carried dependencies across tree traversal iterations' is directly addressed by rotating registers. By automatically providing a fresh set of registers for each new iteration, they break the false dependencies (Write-After-Read hazards across iterations) that would otherwise prevent aggressive overlapping of loop iterations. This allows the compiler to achieve a much lower Initiation Interval (II) and higher throughput in loops with dependencies, without the significant code bloat associated with software-only solutions.",
		"confidence": "high"
	  },
	  {
		"field": "repurposed_fault_tolerance_techniques.justification",
		"citations": [
		  {
			"title": "Analysis and Characterization of Intel Itanium Instruction ...",
			"url": "http://ieeexplore.ieee.org/document/4673579/",
			"excerpts": [
			  "In order to achieve high instruction level parallelism (ILP), designers are turning to very long instruction word (VLIW) based designs, in which different types of instructions are grouped together as bundles of 128 bits or longer.",
			  "In VLIW, the added nops increase the code size, limit processor performance by the under-utilization of functional units.",
			  " present a comprehensive analysis of the problem of under-utilization due to nops and stops across a wide range of application domains through the use of three different benchmark suites: SPEC CPU 2000, MediaBench, and Pack",
			  "Our results show that, on average, nops create an under-utilization factor of 28.46% in the case of SPEC CPU, 32.27% in MediaBench, and 29.76% in PacketBench."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous ...",
			"url": "https://people.rennes.inria.fr/Olivier.Sentieys/publications/2022/Psiakis22MICPRO.pdf",
			"excerpts": [
			  "The proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhead. To achieve\nthis, a scheduling exploration window of two instruc-\ntion bundles is used.",
			  "he proposed mechanism dynamically repli-\ncates instructions and reschedules original and repli-\ncated instructions both in the current bundle (space\nscheduling - S) and in the next upcoming bundle\n(time scheduling - T), improving performance, with\nreduced complexity and area overhea",
			  "order to perform such a flexible dynamic instruction\nscheduling, switches are required, which, however, in-\ncrease the hardware area. To reduce the area over-\nhead, a cluster-based approach is proposed, enabling\nscalability for larger hardware designs",
			  "To support our contributions, we implemented on the\nVEX VLIW processor and compare the proposed ap-\nproach that applies instruction triplication, consid-\nering space scheduling (TMR-S) and space and time\nscheduling based on clusters (TMR-S+T+CB), with\ntypical hardware FUs triplication methods and the\nunprotected baseline VLIW.",
			  "mechanism is implemented on VEX VLIW proces-\nsor. The obtained results show an average speed-up\nof 24.99% in performance with an almost 10% area\nand power overhead, when time scheduling is also\nconsidered on top of space scheduling.\nC"
			]
		  },
		  {
			"title": "Fault Tolerance in Super-Scalar and VLIW Processsors",
			"url": "https://escholarship.org/content/qt045296h7/qt045296h7_noSplash_bc64414330f80a28858c967c3e50825f.pdf",
			"excerpts": [
			  "These operations are begun during the same clock cycle\nand their results are compared when the operations are completed. This duplication\nand comparison will detect any fault within the primary or redundant functional\nunit that affects the operation being performed.",
			  "In VLIW, a more involved reconfiguration\nprocedure is necessary.",
			  "we outline several alternative reconfiguration strategies for VLIW\nprocessors."
			]
		  },
		  {
			"title": "Run Time Instruction Replication for Permanent and Soft ...",
			"url": "https://inria.hal.science/hal-01633778/file/Run%20Time%20Instruction%20Replication%20for%20Permanent%20and%20Soft%20Error%20Mitigation%20in%20VLIW%20Processors.pdf",
			"excerpts": [
			  "ardware mechanism is proposed**\n**which at run-time replicates the instructions and schedules them**\n**at the idle slots considering the resource constra",
			  "f a resource**\n**becomes faulty, the proposed approach efficiently rebinds both**\n**the original and replicated instructions during exe",
			  "ults**\n**show up to 49% performance gain over existing techniques",
			  "it is applicable for any VLIW structure, i.e. any\nissue width and number and type of FUs."
			]
		  },
		  {
			"title": "Adaptive and Polymorphic VLIW Processor to Optimize ...",
			"url": "http://www.ecs.umass.edu/ece/koren/research/slkkb-cf-2018.pdf",
			"excerpts": [
			  "The authors evaluate the duplication (for fault detection only) and the triplica- tion of instructions in order to provide fault tolerance.Read more"
			]
		  }
		],
		"reasoning": "The finegrained field value asserts that there is substantial idle capacity in very wide VLIW machines (e.g., 23 slots) and that exploiting those idle units via redundant or speculative computation can yield performance benefits, supported by empirical data from Itanium showing average functional unit under-utilization in the 28-32% range on common benchmarks. The cited Itanium-focused analyses provide precise figures for idle capacity in a real VLIW-like design, which directly underpins the claim that there is room to hide latency or increase throughput by scheduling additional work into idle slots. Further, literature on dynamic replication and fault-tolerant scheduling demonstrates concrete techniques for utilizing idle FUs: replication of instructions and interleaved scheduling across bundles to keep resources busy, with documented gains in performance or reliability when idle resources are repurposed. Additional discussion of clustered VLIW and modulo-scheduling-based replication shows the viability of using idle capacity across multiple clusters, reinforcing the general approach of interleaving independent computations to fill empty slots. Taken together, these excerpts substantiate the premise that wide-issue VLIW architectures often have idle or under-utilized functional units and that carefully designed replication or interleaving strategies can effectively exploit that slack for performance improvements.",
		"confidence": "high"
	  },
	  {
		"field": "repurposed_fault_tolerance_techniques.core_idea",
		"citations": [
		  {
			"title": "Run Time Instruction Replication for Permanent and Soft ...",
			"url": "https://inria.hal.science/hal-01633778/file/Run%20Time%20Instruction%20Replication%20for%20Permanent%20and%20Soft%20Error%20Mitigation%20in%20VLIW%20Processors.pdf",
			"excerpts": [
			  "f a resource**\n**becomes faulty, the proposed approach efficiently rebinds both**\n**the original and replicated instructions during exe",
			  "ardware mechanism is proposed**\n**which at run-time replicates the instructions and schedules them**\n**at the idle slots considering the resource constra"
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Microprocessors & Microsystems",
			"url": "https://dl.acm.org/doi/10.1016/j.micpro.2022.104564",
			"excerpts": [
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability."
			]
		  },
		  {
			"title": "Dynamic fault-tolerant VLIW processor with heterogeneous Function Units | Request PDF",
			"url": "https://www.researchgate.net/publication/360874051_Dynamic_fault-tolerant_VLIW_processor_with_heterogeneous_Function_Units",
			"excerpts": [
			  "Instruction Level Parallelism (ILP) of applications is typically limited and variant in time, thus during application execution some processor Function Units (FUs) may not be used all the time. Therefore, these idle FUs can be used to execute replicated instructions, improving reliability."
			]
		  },
		  {
			"title": "Exploiting Idle Hardware to Provide Low Overhead Fault ...",
			"url": "https://www.researchgate.net/publication/312259995_Exploiting_Idle_Hardware_to_Provide_Low_Overhead_Fault_Tolerance_for_VLIW_Processors",
			"excerpts": [
			  "In this paper, an effective fault-tolerant framework offering very high error coverage with zero detection latency is proposed to protect the data paths of ...Read more"
			]
		  },
		  {
			"title": "Selective Code Duplication for Soft Error Protection on VLIW Architectures",
			"url": "https://www.mdpi.com/2079-9292/10/15/1835",
			"excerpts": [
			  " present the packing-oriented duplication in order to maximize the number of duplicated instructions within the same performance overheads. We define the pack as the number of duplicable instructions in the same code line, and we select larger packs for instruction duplication"
			]
		  }
		],
		"reasoning": "The central field value asserts that the core idea is to repurpose hardware mechanisms designed for reliability to achieve performance gains by using idle functional units in wide-issue VLIW machines to run opportunistic computations and hide latency. The most directly supportive material states that instruction replication can be used to utilize idle Function Units to improve reliability, and that such idle resources can be used to execute replicated instructions, effectively increasing ILP and throughput rather to only check errors. This is reinforced by runtime replication works that explicitly schedule replicated instructions in idle slots, achieving measurable speedups (for example, reporting speed-ups on the order of tens of percent when time scheduling is added on top of space scheduling). Additional evidence discusses exploiting idle hardware to provide fault tolerance, illustrating a concrete mechanism where idle resources are repurposed for reliability-related tasks, with performance implications. Taken together, these excerpts connect the concept of reusing idle hardware (replication, scheduling, and occupancy of otherwise unused FUs) to both latency hiding and throughput improvements, consistent with the described repurposing strategy. The strongest alignment comes from explicit statements that idle FUs execute replicated instructions to improve reliability and that runtime replication schemes yield tangible performance benefits, including cluster-based scalability considerations and measurable speedups.",
		"confidence": "high"
	  },
	  {
		"field": "empirical_case_studies_summary.implication",
		"citations": [
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "The Itanium architecture allows issuing of independent instructions in\nbundles (three instructions per bundle) for parallel execution and can issue multiple\nbundles per clock."
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "With the Pentium 4, the microprocessor family that began with the 8086 and that\nhas been the most successful computer product line ever appears to have come\nto an end. Intel has teamed up with Hewlett-Packard (HP) to develop a new\n64-bit architecture, called IA-64. IA-64 is not a 64-bit extension of Intel’s 32-bit\nx86 architecture, nor is it an adaptation of Hewlett-Packard’s 64-bit PA-RISC\narchitecture. Instead, IA-64 is a new architecture that builds on years of research\nat the two companies and at universities. The architecture exploits the vast cir-\ncuitry and high speeds available on the newest generations of microchips by a\nsystematic use of parallelism. IA-64 architecture represents a significant depar-\nture from the trend to superscalar schemes that have dominated recent proces-\nsor development.",
			  "The IA-64 instruction set architecture is a new approach to providing\nhardware support for instruction-level parallelism and is significantly\ndifferent than the approach taken in superscalar architectures.",
			  "Intel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC)."
			]
		  },
		  {
			"title": "Microarchitectural Techniques for Instruction-driven Specialization ...",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-210.pdf",
			"excerpts": [
			  "The most ambitious attempt to apply VLIW to general-purpose workloads was Intel’s\nItanium, based on the Explicitly Parallel Instruction Computing (EPIC) model and the IA-\n64 instruction set"
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "VLIW-like architecture except for memory operations\n(out-of-order memory accesses)\nIPC up to 6 (Instructions Per Cycle)"
			]
		  },
		  {
			"title": "Itanium Performance Insights from the IMPACT Compiler",
			"url": "http://impact.crhc.illinois.edu/shared/talks/hotchips-2001-final-color.pdf",
			"excerpts": [
			  "Bundling: enables static scheduling by communicating instruction\nparallelism",
			  "Predication: allows compiler to optimize across multiple paths by\nproviding an alternative to control flow",
			  "Speculation support: allows compiler to select specific instructions\nfor early execution",
			  "Static scheduling, control speculation, predication",
			  "Itanium design goal: enhance scalability of parallelism by\nmoving complex decisions to the compiler"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs"
			]
		  }
		],
		"reasoning": "The Itanium/IA-64 lineage is central to understanding ILP challenges on modern VLIW-like designs. Excerpts describing how Itanium issues multiple instruction bundles per cycle and can reach a certain level of parallelism—yet still faces limits in fully exploiting execution slots on irregular, control- and data-dependent workloads—directly support the field value’s core claim about scarce ILP under irregular code on advanced VLIW architectures. The material also discusses architectural features intended to unlock ILP (predication, control speculation, and explicit parallelism via EPIC), which contextualizes why even sophisticated compilers may struggle with irregular patterns and why multi-stream interleaving becomes a practical necessity for keeping a wide VLIW busy. Additional sources describe compiler techniques (superblocks, trace scheduling, modulo scheduling) that try to widen ILP exposure by跨-block optimization and across iterations, aligning with the idea that interleaving independent streams is a fundamental strategy to fill all slots when the critical path contains long dependent chains. Finally, several excerpts highlight that real hardware (Itanium/IA-64) can issue multiple bundles and still be limited by memory latency, data dependencies, or irregular control flow, reinforcing the need for interleaving techniques to keep the wide engine utilized. Directly relevant points include: the Itanium design goal to expose ILP via compiler-pushed parallelism, and the fact that Itanium can issue six instructions per clock cycle across bundles while predication and speculation are intended to expose more ILP; discussions of EPIC/predication/speculation; and examples of scheduling techniques (superblock/trace scheduling/modulo scheduling) that attempt to maximize ILP across blocks or iterations. Collectively, these excerpts support the conclusion that without multi-stream interleaving, a 23-slot VLIW is unlikely to achieve high utilization on irregular workloads, echoing the Itanium-era lessons while pointing toward practical scheduling strategies to mitigate the issue.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods",
		"citations": [
		  {
			"title": "Software Pipelining Irregular Loops On the TMS320C6000 VLIW ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/384198.384216",
			"excerpts": [
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "In this paper, we discuss our software-based preprocessing\ntechnique that allows compilers to software pipeline irregular\nloops on architectures with no special support beyond the ability\nto predicate (conditionally nullify) instructions using a small\nnumber of static registers."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads",
			  "In this paper, we propose an effective, *fully automatic* ap-\nproach to non-speculative thread extraction, called *Decou-*\n*pled Software Pipelinin"
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining | Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2005.13",
			"excerpts": [
			  "DSWP exploits the finegrained pipeline parallelism lurking in most applications to extract long-running, concurrently executing threads.",
			  "Use of the non-speculative and truly decoupled threads produced by DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "20} R. Rangan, N. Vachharajani, M. Vachharajani, and D. I. August. Decoupled software pipelining with the synchronization array. In *Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques* , pages 177-188, September 2004.\n"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "Decoupled execution ensures that stalls\nin one thread do not affect the other.",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Automatic Multithreading for DSWP: Unlike prior attempts at automatically multithreading sequential programs, DSWP does\nnot try to partition programs into totally independent threads.",
			  "We have a working compiler implementation that\nautomatically generates DSWP code.",
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies."
			]
		  },
		  {
			"title": "[PDF] Parallel-stage decoupled software pipelining | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Parallel-stage-decoupled-software-pipelining-Raman-Ottoni/73538f17cbeb45fd0822cb971375ad03c515dbe7",
			"excerpts": [
			  "Decoupled software pipelining (DSWP) is introduced, a technique that statically splits a single-threaded sequential loop into multiple nonspeculative threads, each of which performs useful computation essential for overall program correctness."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/144589/Nguyen-qmn-PhD-EECS-2022-thesis.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "Decoupling stages with queues hides\nlatency by allowing producer stages to run far ahead of consumer stages.",
			  "Pipette introduces\narchitectural support for pipeline parallelism within the threads of a\nmultithreaded core.",
			  "By using SMT to time-multiplex stages in the same core, Pipette avoids\nload imbalance issues that arise when decoupling stages across separate\ncores or processing elements.",
			  "Pipette also adds out-of-band control\nflow to keep producer and consumer loops running despite complex\ncontrol flow.",
			  "Pipette enables composable, fine-grain interleaving of\naccelerated accesses and general-purpose computation.",
			  "the im-\nplementation reuses core structures: it uses the physical register file to\nimplement queues cheapl",
			  "Pipette’s novel ISA (Section 3.2) allows threads to\ndefine inter-thread queues."
			]
		  },
		  {
			"title": "(PDF) Enhancing the performance of Decoupled Software Pipeline through Backward Slicing",
			"url": "https://www.researchgate.net/publication/271532508_Enhancing_the_performance_of_Decoupled_Software_Pipeline_through_Backward_Slicing",
			"excerpts": [
			  "DSWP extracts threadlevel parallelism from the body of a loop by breaking it into stages which are then executed pipeline style: in effect ...Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more",
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized.",
			  "We chose to implement DSWP using the general-purpose POSIX threading\nlibrary (pthreads) and the Low Level Virtual Machine (LLVM) compiler\ninfrastructure.",
			  "*\n**Related Work**\nThe initial DSWP has been implemented in the context of the IMPACT\nresearch compiler using Itanium platform simulator which has customized\nhardware-level support.\nOther works of DSWP including [6, 4] also use\nhardware simulator.\n",
			  "rch but industry standard.\n1\nDue to our choice to use pthreads, Our DSWP implementation will also\nbe portable across more platforms than previous implementations since any\n ... \n*ceedings of the 38th annual IEEE/ACM International Symposium on*\n*Microarchitecture* , MICRO 38, pages 105–118, Washington, DC, USA,\n2005. IEEE Computer Society.\n[4] Easwaran Raman, Guilherme Ottoni, Arun Raman, Matthew J. Bridges,\nand David I. August. Parallel-stage decoupled software pipelining. In\n*Proceedings of the 6th annual IEEE/ACM international symposium on*\n*Code generation and optimization* , CGO ’08, pages 114–123, New York,\nNY, USA, 2008. ACM.\n[5] Ram Rangan, Neil Vachharajani, Manish Vachharajani, and David I.\nAugust. Decoupled software pipelining with the synchronization array.\nIn *Proceedings of the 13th International Conference on Parallel Archi-*\n*tectures and Compilation Techniques* , PACT ’04, pages 177–188, Wash-\nington, DC, USA, 2004. IEEE Computer Society.\n[6] Neil Vachharajani, Ram Rangan, Easwaran Raman, Matthew J. Bridges,\nGuilherme Ottoni, and David I. August. Speculative decoupled software\npipelining. In *Proceedings of the 16th International Conference on Par-*\n*allel Architecture and Compilation Techniques* , PACT ’07, pages 49–59,\nWashington, DC, USA, 2007. IEEE Computer Society.\n**Distribution of Total Credit**\nFuyao implement framework of DSWP and component for dependence anal-\nysis, SCC/DAG, thread partitioning, loop splitting and synchronization.\nMark implemented the runtime support for multithreading based on pthreads,\ncontributed to the synchronization implementation, and managed write-ups\nand poster.\n8"
			]
		  },
		  {
			"title": "Semi-automatic extraction and exploitation of hierarchical ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/1854273.1854321",
			"excerpts": [
			  "Recent work [23] on Ps-Dswp includes a speculation system which if used for parallelization can achieve good performance and scal- ability on commodity hardware ...Read more"
			]
		  },
		  {
			"title": "Efficient Data Supply for Parallel Heterogeneous ...",
			"url": "https://decades.cs.princeton.edu/TACO19.pdf",
			"excerpts": [
			  "Shelf [43] utilizes an in-order pipeline within an OoO core for higher efficiency, whereas Outrider [9] utilizes a SMT core and decoupled execution to achieve.Read more"
			]
		  },
		  {
			"title": "Parallelizing Sequential Applications on Commodity Hardware using a Low-cost Software Transactional Memory | Request PDF",
			"url": "https://www.researchgate.net/publication/220752082_Parallelizing_Sequential_Applications_on_Commodity_Hardware_using_a_Low-cost_Software_Transactional_Memory",
			"excerpts": [
			  "... Mehrara et al. [25] described STMLite, a software transactional memory model modified to support speculative parallelization. It was ..."
			]
		  },
		  {
			"title": "static and dynamic instruction mapping",
			"url": "https://liberty.princeton.edu/Publications/phdthesis_fengliu.pdf",
			"excerpts": [
			  "arse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes\ncoarse-grained pipeline parallelism techniques to synthesize efficient specialized ac-\ncelerator modules from irregular C/C++ programs without requiring any annotati",
			  "GPA automatically partitions individual loops into separate pipelined stages and gen-\nerates buffer-connected hardware modules for these stages",
			  "ipeline Partition** : This is a coarse-grained instruction scheduling step that assigns\ninstructions to different pipeline stages. The partitioning algorithm is adapted from the\nParallel Stage Decoupled Software Pipelining (PS-DSWP) [68] algorithm",
			  "The main dif-\nference between CGPA and PS-DSWP lies in the identification of replicable sections, and\ndeciding whether it should be inserted in the parallel stage as replicas or into the sequential\nstage. Generally, inserting replicable sections into a sequential stage will increase com-\nmunication, because the results of the replicable sections must be sent explicitly to the\nfollowing stages. Conversely, duplicating the replicable section in parallel stages will in-\ncrease the amount of computations and memory accesses. The CGPA framework only\nduplicates lightweight replicable sections which do not contain load and multiply instruc",
			  "This is based on the intuition that time and resources required for the replicable\nsection without these instructions are less than those for communicating results via FIFO\nbuffers.",
			  "Figure 3.1(d) shows that replicable sections with heavyweight load instructions are\nidentified and inserted into a sequential stage (Stage 1), and the remaining instructions are\ngrouped as one parallel stage (Stage 2). The required number and stage connections of the\nbuffers are also determined at this step.",
			  "PA can also adopt another approach, called **decoupled pipeline parallelism** to im-\nprove outer loop performance and generate accelerators similar to the results of manual\naccelerator designs [68]. This approach uses a set of FIFO buffers to separate the linked\nlist traversal module from that for node updates. Since the traversal section only goes over\nthe linked list and fetches node addresses, it can progress much faster than the update sec-\ntion. Thus one *sequential* traversal module can supply node addresses to multiple *parallel*\nupdate modules in another stage, as shown in Figu",
			  "th this decoupled pipeline design, when control enters the loop, the hardware mod-\nules for both stages are invoked by the same start signal. The module in the first (sequential)\nstage begins fetching node addresses one by one, and assigns the node address values to\nthe FIFO buffers of the parallel modules in the parallel stage in a round-robin fashion. The\nsequential stage stalls when there are cache misses or the corresponding buffers are full.",
			  "GPA is the first HLS tool to automatically extract this type of parallelism from a\n*single loop* and generate efficient pipelined hardware modules for it",
			  "Figure 3.3 shows a logical view of coarse-grained pipelined accelerators with a Sequential–\nParallel–Sequential (S-P-S) pipeline",
			  "The number of stages for different applications is\nnot fixed, and is determined automatically for each application by the CGPA compiler’s\npartition algorithm."
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization.",
			  " partitions loops\ninto long-running threads that communicate via inter-core queues.\nDSW",
			  "Parallel-Stage Decoupled Software Pipelining [16] (PS-DSWP)\nis an extension to combine pipeline parallelism with some stages\nexecuted in a DOALL, data-parallel fashion.",
			  "This paper presents the preliminary design of a new PS-DSWP method capable of handling arbitrary structured control flow, a slightly better ...Read more"
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation.",
			  "We evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP.",
			  "The base model used is a 6-core Itanium 2 CMP developed using\nthe Liberty Simulation Environment (LSE). The cores are validated\n(IPC and constituent error components accurate to within 6% of\nreal hardware for benchmarks measured [15]), and the details of\neach core in the model are given in Table 1. To this base model, a\nsynchronization array with 512 queues, each with 32 entries, was\nadded. The produce , consume\nand queue.set\ninstructions\nare also modeled in the pipeline of the Itanium 2 cores.",
			  "We have implemented PS-DSWP in the VELOCITY research\ncompiler [22]. The applications are first compiled into a low-level\nintermediate representation, and then transformed by a round of\nclassical optimizations. PS-DSWP was then automatically applied\nto the selected loops. Translation to Itanium 2 instructions is fol-\nlowed by another round of classical optimizations, scheduling, reg-\nister allocation, and a final scheduling phase.",
			  "t DSWP in combination\nwith speculation and parallel stages has the potential to unlock par-\nallelism among all the programs in the SPEC2000 integer bench-\nmark suite.",
			  "This paper presented a new non-speculative compiler transfor-\nmation called Parallel-Stage Decoupled Software Pipelining (PS-\nDSWP). PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformation. This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators.",
			  "For other programs, however, these tools yield poor performance because they invoke computation units for instructions sequentially, without exploiting parallelism."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages.",
			  "Decoupled software pipelining\n(DSWP) [ 35 ] uses a synchronization array to facilitate com-\nmunication between cores or the threads of a multithreaded\ncore. But DSWP focuses on pipelining a single loop across\ndi ff erent threads, which is too limiting for irregular applications.\nFor example, BFS uses a 3-level nested loop, with stages",
			  " Pipette’s ISA fea-\ntures control mechanisms that allow e ff ective decoupling under\nirregular control flow. ",
			  "es.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers.",
			  "By time-multiplexing stages on the same\ncore, Pipette avoids load imbalance and achieves high core IPC.",
			  " it uses the\nphysical register file to implement queues cheaply, avoiding the\nstorage costs of prior techniques. Sec",
			  "Pipette decouples threads within\nthe core using architecturally visible queues."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more",
			  "This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual ...Read more"
			]
		  },
		  {
			"title": "Quantifying the Semantic Gap Between Serial and Parallel ...",
			"url": "https://par.nsf.gov/servlets/purl/10337772",
			"excerpts": [
			  "This measures the TLP limit of both cyclic multithreading (CMT). (e.g., DoAcross [20], HELIX [3]) and pipeline multithreading. (PMT) (e.g., DSWP [5], PS-DSWP [8]) ...Read more"
			]
		  },
		  {
			"title": "From Sequential Programs to Concurrent Threads - Liberty ...",
			"url": "https://liberty.princeton.edu/Publications/cal05_dswp.pdf",
			"excerpts": [
			  "Ottoni, E. Raman, and D. I.. August, “Practical and accurate low-level pointer analysis,” in Proc. of the Int'l. Symp. on Code Generation and Optimization, 2005 ..."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Since technology scaling no longer provides previously seen rates of performance improvements, microprocessor designers have adopted a number of other techniques to improve processor performance at reasonable energy costs. An entire class of techniques improves performance by offloading computation intensive parts of programs from a general-purpose processor to customized accelerators [15], [17], [18], [24], [31], [34]. One approach to build these customized accelerators is to manually design dedicated circuit modules for each application using hardware description languages (HDLs) [24], [26]. While this approach often yields the best result, it requires significant non-recurring engineering costs to convert algorithms to HDL specifications. Another less labor-intensive approach is to use high-level synthesis (HLS) tools to generate customized accelerators from programs written in C or other high-level programming languages [4], [12], [13], [16], [30], [31], [32], [37].",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "WP+, splits a loop into new loops with dependence pat-\nterns amenable to parallelization using techniques that were\noriginally either inapplicable or poorly-performing.",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  "n the approxi-\nmately equivalent performance of these two techniques, fu-\nture designers can determine the more cost-effective hard-\nware implementatio",
			  "The EMS algorithm consists of five steps:",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  "*like Hierarchical Reduction, the code must be regenerated**\n**by inserting conditional branches after modulo scheduling.*",
			  "MS), combines\nthe benefits of the previous Modulo Scheduling techniques,\nIf-conversion with Predicated Execution and Hierarchical\nReduction, to remove the limitations of each technique",
			  "f-\nconversion eliminates the need for prescheduling the paths\nof the conditional construct, which is essential to Hierar-\nchical Reductio",
			  "ll three Modulo Scheduling techniques and GURPR*\nhave been implemented in a prototype compiler.",
			  "r exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reductio"
			]
		  },
		  {
			"title": "Enhanced modulo scheduling for loops with conditional branches.",
			"url": "https://puma.ub.uni-stuttgart.de/bibtex/196dcbc0b79dd30a7db4d46cb8950c83c",
			"excerpts": [
			  "Enhanced modulo scheduling for loops with conditional branches. ... N. Warter, G. Haab, K. Subramanian, and J. Bockhaus."
			]
		  },
		  {
			"title": "An Implementation of Swing Modulo Scheduling with ...",
			"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
			"excerpts": [
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.\nE",
			  "\nEMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.\n",
			  "\nIf-conversion is performed to transform the loop body into straight line predicated code using\nthe RK algorithm [32].\n",
			  "While If-conversion does allow loops with conditional statements to be software pipelined, the\ndownside is that both execution path’s resources must be summed when determining the resource\nconstraints for the loop. This can lead to a pipelined loop that does not have an optimal II.\n3",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,. Modulo Scheduling the loop, applying modulo variable expansion, ...Read more",
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.",
			  "The best type of hardware support is one that provides Predicated Execution , such as IA64 [20].\nPredicated Execution allows instructions to be nullified during their execution in the pipeline. So if\nan instruction is speculatively moved above a branch and the branch is taken, even if the instruction\nis already in the processor’s pipeline, the instruction can be nullified",
			  "If-conversion replaces conditional branches with a compare instruction that sets a flag. Instruc-\ntions that were dependent upon the conditional branch are now instructions that only execute if\nthe flag is set. If-conversion typically requires hardware support, but some algorithms [41] have\nmade slight modifications to avoid this. Hardware support such as predicated execution, common\non VLIW and EPIC architectures (such as IA64) set a conditional flag per instruction and allow\ninstructions to execute only when the conditional flag is tru",
			  "The second main contribution of this thesis is to extend the Swing Modulo Scheduling algorithm\nto handle more than single basic block loops. We have modified the algorithm to handle superblock\nloops (single-entry, multiple-exit, multiple basic block loops). These extensions allow the algorithm\nto be applied at various stages of compilation:\n• Statically: Just as the original Swing Modulo Scheduling algorithm was done statically, the\nextensions simply expand the number of loops acceptable for this transformation.\n• Offline: Using profile information, hot paths (subsets of the loop in the form of single entry,\nmultiple exit loops), can be recognized and be transformed by our extensions.\n• Runtime: Similar to the profile driven optimization, hot paths are found during program\n .."
			]
		  },
		  {
			"title": "Effective Cluster Assignment for Modulo Scheduling",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-98-modulo.pdf",
			"excerpts": [
			  "Llosa, et al., 'Swing Modulo Scheduling: A lifetime-. Sensitive Approach,' IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques,.Read more"
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. ",
			  "An empirical study of the importance of an architectural support, referred to as predicted execution, for the effectiveness of software pipelining is presented to help future microprocessor designers to determine whether predicated execution support is worthwhile given their own estimation of the increased hardware cost.",
			  "An Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction or If-conversion with Predicated Execution. Expand",
			  "A multiple-II modulo scheduling technique is proposed for architectures with predicated execution that maintains the benefits of a local scheduling based approach while reducing the effective II. Expand",
			  "This dissertation shows that Modulo Scheduling with Predicated Execution has better performance and lower code expansion than Modulo scheduling with ICTs on processors without special hardware support. Expand",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "Loops with conditional branches have multiple execution paths which are difficult to software pipeline. The modulo scheduling technique for software pipelining addresses this problem by converting loops with conditional branches into straight-line code before scheduling.",
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle.",
			  "However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "m](/author/M.-Lam/39682108)\nPublished in SIGP 1 June 1988\nComputer Science\nTLDR\nThis paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. Expan",
			  "lic Array Optimizing Compiler](/paper/A-Systolic-Array-Optimizing-Compiler-Lam/4b463c767e23a011e6da6f6ea4c8536449893f55)\n[M. Lam](/author/M.-Lam/39682108)\nComputer Science, Engineering\n1989\nTLDR\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. Expan",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines",
			  "This thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics.",
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme ...",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  " software pipelining, itera-\ntions of a loop in a source program are continuously initiated\nat constant intervals without having to wait for preceding\niterations to complete.",
			  "this paper proposes a hierarchical\nreduction\nscheme whereby entire control constructs are reduced to an\nobject similar to an operation in a basic block. Scheduling\ntechniques previously defined for basic blocks can be applied\nacross basic blocks.",
			  "is paper extends previous results of software pipelining\nin two ways: First, this paper shows that by using an im-\nproved algorithm, near-optimal performance can be obtained\nwithout\nspecialized hardware.",
			  "cond, we propose a\nhierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block.",
			  "The Warp array is a linear array of VLIW\nprocessors, each capable of a peak computation rate of 10\nmillion floating-point operations per second (10 MFLGPS).",
			  " techniques proposed have been validated by an im-\nplementation of a compiler for Warp, a systolic array consist-\ning of 10 VLIW processors",
			  "Software pipelining, as addressed here, is the problem of\nscheduling the operations within an iteration, such that the\niterations can be pipelined to yield optimal throughput,",
			  "This paper shows that software pipelining is a practical,\nefficient, and general technique for scheduling the parallelism\nin a VLIW machine.",
			  "he hierarchical reduction technique is\nto make software pipelining applicable to all innermost loops,\nincluding those containing conditional statements.\n",
			  "With this scheme, all innermost loops, including\nthose containing conditional statements, can be software\npipelined.",
			  "archical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtained.",
			  "Software Pipelining:\nAn Effective Scheduling Technique for VLIW Machines",
			  "In\nsoftware pipelining, iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete.",
			  "This paper extends previous results of software pipelining\nin two ways: First, this paper shows that by using an im-\nproved algorithm, near-optimal performance can be obtained\nwithout\nspecialized hardwar",
			  "Second, we propose a\nhierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block. With this scheme, all innermost loops, including\nthose containing conditional statements, can be software\npipelined.",
			  "ierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtain",
			  "The thesis of this paper is that software pipelining [24,25,30] is a viable alternative technique for scheduling VLIW processors. In software pipelining, itera ...",
			  "Software pipelining is more attractive than source code\nunrolling for two reasons. First, software pipelining offers the\npossibility of achieving optimal throughput.",
			  "The second reason, a practical concern, is perhaps\nmore important. In trace scheduling, the performance almost\nalways improves as more iterations are unrolled.",
			  " As the degree of unrolling increases, so do the\nproblem size and the fmal code size.",
			  "Therefore, the compilation time is unaffected.",
			  "urthermore, unlike source unrolling, there is an optimal de-\ngree of unrolling for each schedule, and can easily be deter-\nmined when the schedule is comple",
			  "relies primarily on source code unrolling. At the end of each\niteration in the original source is an exit out of the loop; the\nmajor trace is constructed by assuming that the exits off the\nloop are not taken.",
			  "Software pipelining, as addressed here, is the problem of scheduling the operations within an iteration, such that the iterations can be pipelined to yield ...Read more",
			  "\nThis paper shows that software pipelining is an effective\nand viable scheduling technique for VLIW processors. ",
			  " In\nsoftware pipelining, iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete. ",
			  " The advantage of software pipelining is that\noptimal performance\ncan be achieved with compact object\ncode.\n",
			  "\nThe techniques proposed have been validated by an im-\nplementation of a compiler for Warp, a systolic array consist-\ning of 10 VLIW processors.",
			  "t software pipelining is a practical,\nefficient, and general technique for scheduling the parallelism\nin a VLIW machine.",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion.",
			  "ion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine",
			  "In software pipelining, itera-\ntions of a loop in a source program are continuously initiated\nat constant intervals without having to wait for preceding\niterations to complete.",
			  "This compiler has been used for\ndeveloping a large number of applications in the areas of",
			  "hierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block.",
			  "The advantage of software\npipelining is that optimal performance can be achieved with\ncompact object code.",
			  "iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete.",
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine",
			  "e have extended previous results of\nsoftware pipelining in two ways. First, we show that near-\noptimal results can often be obtained for loops containing\nboth intra- and inter-iteration data dependency, using software\nheuristics. We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsio"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines - SUIF",
			"url": "https://suif.stanford.edu/papers/lam03-sp.pdf",
			"excerpts": [
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures.\n ... \nand Glaeser developed the first compiler with software pipelining\nfor the polycyclic architecture, which had a novel crossbar whose\ncross-points provided a programmable form of delay to support\nsoftware pipelining[7].\nIntegrated floating-point adder and multiplier chips were first in-\ntroduced by Weitek Inc. in 1984. This led to a flurry of activities in\ndeveloping “high-performance” numerical engines, among which\nwere the commercial Multiflow Trace computer and the CMU Warp\nsystolic array. The latter research project provided the context in\nwhich this paper was written. The availability of integrated floating-\npoint chips was, of course, just a milestone along the road to single-\nchip ILP processors. Today, all modern general-purpose machines\nemploy ILP and instruction scheduling is needed in all optimizing\ncompilers.\n**2.**\n**CONTRIBUTIONS OF THE PAPER**\nAt the time the paper was written, trace scheduling[5] was con-\nsidered to be the technique of choice for scheduling VLIW (Very\nLong Instruction Word) machines. This paper establishes software\npipelining as a useful static scheduling technique for VLIW pro-\ncessors without requiring specialized architectural support.\nThis paper has three major results. First, instead of relying on\nspecialized hardware support like a polycyclic interconnect, this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a smal",
			  "The basic idea behind software pipelining was first developed by. Patel and Davidson for scheduling hardware pipe-lines. As instruction- level parallelism made ...Read more",
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures",
			  "The basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling.",
			  "In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines",
			  "The most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support.",
			  "Our understanding of software pipelining\nsubsequently deepened with the work of many others.",
			  "And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.",
			  "software pipelining is effective on VLIW machines without compli-\ncated hardware support. O",
			  "At the time the paper was written, trace scheduling[5] was con-\nsidered to be the technique of choice for scheduling VLIW (Very\nLong Instruction Word) machines. This paper establishes software\npipelining as a useful static scheduling technique for VLIW pro-\ncessors without requiring specialized architectural support.",
			  " this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a small\nnumber of times. Se",
			  "The main difference is that tasks in their pipelines are\nindependent of each other, whereas iterations in a loop may have\ndependences between them.\nThe",
			  ". Software pipelining is applicable\nonly to codes with predictable behavior like numerical applications;\nas such, it only expands the number of instructions in innermost\nloops slightly. O",
			  ". Software pipelining has been used in proces-\nsors from HP, IBM, Intel, SGI, Sun and Texas Instrument, none of\nwhich has any specialized software pipelining support.",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without compli-.",
			  "**Software Pipelining:**",
			  "**An Effective Scheduling Technique for VLIW Machines**",
			  "Monica S. Lam",
			  "Computer Systems Laboratory",
			  "Stanford University",
			  "**ABSTRACT**",
			  "The basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations.",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines > Abstract",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines > References",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "‪Monica S. Lam‬ - ‪Google Scholar‬",
			"url": "https://scholar.google.com/citations?user=4hS0jZ8AAAAJ&hl=en",
			"excerpts": [
			  "Software pipelining: An effective scheduling technique for VLIW machines",
			  "Lam",
			  "Software pipelining: An effective scheduling technique for VLIW machines. M Lam. Proceedings of the ACM SIGPLAN 1988 conference on Programming Language …, 1988.Read more"
			]
		  },
		  {
			"title": "(PDF) Modulo scheduling without overlapped lifetimes",
			"url": "https://www.academia.edu/125636329/Modulo_scheduling_without_overlapped_lifetimes",
			"excerpts": [
			  "M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proc. of the ACM SIGPLAN '88 Conference on Programming Language Design and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for ...",
			"url": "https://dl.acm.org/doi/10.1145/989393.989420",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in ...Read more",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without complicated hardware support. Our ...Read more"
			]
		  },
		  {
			"title": "Software pipelining, VLIW, EPIC, instruction-set support",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch06-StaticSchedulingForILP.pdf",
			"excerpts": [
			  "Monica S. Lam, Robert P. Wilson. 19th ISCA, May 1992, pages 19-21. Limits of ... To help trace scheduling and software pipelining, the Itanium.Read more",
			  "IA-64",
			  "EPIC = “2nd generation VLIW”?",
			  "VLIW: Very Large Instruction Word"
			]
		  },
		  {
			"title": "Modulo Scheduling with Reduced Register Pressure",
			"url": "https://upcommons.upc.edu/bitstreams/9de3a477-f5f8-4307-88c5-16ab35f9e150/download",
			"excerpts": [
			  "Lam, “Software Pipelining: An Effective Scheduling Tech-\nnique for VLIW Machines,” *Proc. SIGPLAN ’88 Conf. Programming*\n*Language Design, and Implementation* , pp. 318-328, June 1988."
			]
		  },
		  {
			"title": "Compile-time ILP extraction Modulo Scheduling",
			"url": "https://courses.e-ce.uth.gr/CE658/lectures/Lecture05_ModuloScheduling.pdf",
			"excerpts": [
			  "Static scheduling – Explicit loop unrolling",
			  "Static scheduling – Explicit loop unrolling",
			  "Static scheduling – Explicit loop unrolling",
			  "(software pipelining)",
			  "Modulo scheduling* is an algorithm for software\npipelinin",
			  "All loop iterations have a common schedule\nModulo scheduling requires hardware support\nThe number of cycles between scheduling\nsuccessive loop iterations is called iteration\ninterval (II)\nWe want to minimize II",
			  "Initiation Interval\nDetermine lower bound on II (MII)\nAttempt to generate a schedule at that II\nThere are two constraints on the MII\nResource constraints\nRecurrence constraints",
			  "uter Architecture\n1\n**CE 658**\n**Advanced Computer**\n**Architecture**\n**Fall 2018**\n**Compile-time ILP extraction**\n**Modulo Scheduling**\n**Nikos Bellas**\n**Computer and Communications Engineering Department**\n**University of Thessaly**\nParallel Computer Architecture\n2\n**Readings for this lecture**\n**Hardware and Software for VLIW and EPIC**\nH&P, Appendix G in v.4\nH&P, Chapter 4 in v.3\n**S** **cheduling techniques**\nB. Rau, et. al *“Code Generation Schema for Modulo Scheduled Loops”,*\nMicro 24, December 1992\nCT Hwuang, et. al *“PLS: A Scheduler for Pipeline Synthesis”, IEEE*\n*Transactions on CAD,* September 1993\nJosep Llosa, et. al. *“Swing Modulo Scheduling: A Life* *-Time Sensitive*\n*Approach,* ” , PACT 1996\nParallel Computer Architecture\n3\nStatic scheduling – Explicit loop unrolling\nSchedule code for a superscalar processor considering the\nfollowing latencies:\nMemory instr: 2 cycles\nFP instr: 3 cycles\nBranch : 1 cycle\nParallel Computer Architecture\n4\nStatic scheduling – Explicit loop unrolling\nFirst attempt: straightforward scheduling\n5 instructions in 9 cycles (IPC = 0.56)\n11% scheduling efficiency (5 instructions in 9*5=45 slots)\nOnly 2 FP registers used ($f0, $f4)\nParallel Computer Architecture\n5\nStatic scheduling – Explicit loop unrolling\nMuch better with implicit loop unrolling (7 times) and scheduling\n(software pipelining)\n23 instructions in 9 cycles (IPC = 2.56)\n51% scheduling efficiency\n14 FP registers used (register pressure)\nSchedule that eliminates all stalls\nParallel Computer Architecture\n6\nDynamic vs Static Scheduling\nSuperscalar processors decide on-the-fly how\nmany and which instructions to issue\nComplex hardware is used to check for\ndependencies and schedule operations\nThe window of instructions that are checked\neach time is limited by the hardware resources\n•\nAll checking has to be done at real time\n•\nMinimal compiler assistance required\nParallel Computer Architecture\n7\nDynamic vs Static Scheduling\nStatically scheduled processors rely on the\ncompiler to achieve good performance\nMain objective of the compiler is to reduce\npotential data hazards\nThe processor still has to stall in case of\nhazards\nThe extreme case of static scheduling is VLIW\nParallel Computer Architecture\n8\nVLIW technology\nDue to the growing complexity to scale the dynamic\nscheduling techniques, the industry started to re-\nexamine explicit encoding of multiple operations (mid\n90's)\nRely on compiler technology to form the instructions in\nan issue packet and schedule all of them\nsimultaneously\n•\nVLIW – Very Long Instruction Word\n•\nMulti-operation instructions (64-128 bits or more)\nThe hardware does not check for dependencies\nno dependencies among operations in the same issue\nword\nParallel Computer Architecture\n9\nVLIW approach\nOnly sensible for wide-issue processors (> 4 slots)\nMultiple operations packed in one instruction\nEach operation slot corresponds to a specific FU\nConstant operation latencies specified\nParallelism within an instruction.\nBasic compiler techniques are the same for VLIW or statically\nscheduled processors\nParallel Computer Architecture\n10\nVLIW approach\nThe compiler:\nschedules for maximum throughput\nAs many useful operations as possible within an\ninstruction\nWith the help of profiling feedback\nModulo scheduling, trace scheduling, speculation,\npredication, superblocks, hyperblocks ........\nGuarantees intra-instruction parallelism\nSchedules to avoid data hazards\npure VLIW engines have no interlocks !\nParallel Computer Architecture\n11\nVLIW approach\nVLIW (at least the original forms) has several short-comings\nthat precluded it from becoming mainstream:\n•\nVLIW instruction sets are not backward compatible between implementations.\nWhen wider implementations (more execution units) are built, the instruction set\nfor the wider machines is not backward compatible with older, narrower\nimplementations.\n•\nLoad responses from a memory hierarchy which includes CPU caches and\nDRAM do not have a deterministic delay. This makes static scheduling of load\ninstructions by the compiler very difficult and inefficient.\n•\nCode explosion with all these compiler techniques\nJosh Fisher. Very Long Instruction Word architectures and the ELI-512 .\n*Proceedings of the 10th annual international symposium on Computer architecture* .\n1983. New York, NY\nThis work resulted in a company called Multiflow.\nParallel Computer Architecture\n12\nModulo Scheduling\nA Class of Methods for Software Pipelining\nLoops\nParallel Computer Architecture\n13\nSoftware Pipelining\nA technique for scheduling instructions to exploit\nILP in inner loops\nMany programs spend most of their time there\nParallelism between loop iterations\nThe technique is based on overlapping multiple\nloop iterations and executing them in parallel\nSimilar to multiple instruction execution in hardware\npipelining\nIt attempts to achieve the benefits of loop unrolling\nwithout performing explicit loop unrolling\nParallel Computer Architecture\n14\nModulo scheduling\n*Modulo scheduling* is an algorithm for software\npipelining\nAll loop iterations have a common schedule\nModulo scheduling requires hardware support\nThe number of cycles between scheduling\nsuccessive loop iterations is called iteration\ninterval (II)\nWe want to minimiz"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete.",
			  "The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined.",
			  "It also diminishes the start-up cost of loops with small number of iterations.",
			  "The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors.",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining ...",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Chapter 4 – VLIW",
			"url": "https://www.ece.ucdavis.edu/~akella/270W05/chapter4.ppt.pdf",
			"excerpts": [
			  "• **SW is symbolic so does not increase the code**",
			  "• **Does not increase register pressure, however**",
			  "*\n**register management is tricky - lifetimes of**",
			  "\n**registers values can be long**\n",
			  "• **Loop unrolling and SW pipelining attack different**",
			  "**problems so in general they can be combined**",
			  " **LU - reduces loop overhead I.e. branch and**\n",
			  "\n**counter update**\n",
			  " **SWP - reduces the time when the loop is not**\n",
			  "*\n**running at its peak**",
			  "Software pipelining is different - it involves reorganizing a loop with ... • This helps both hw based scheduling and SW pipelining and global ..."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531701.pdf",
			"excerpts": [
			  "IA-64 provides extensive support for software-pipelined loops, including register rotation, special\nloop branches, and application registers. When combined with predication and support for\nspeculation, these features help to reduce code expansion, path length, and branch mispredictions\nfor loops that can be software pipelined.",
			  "The beginning of this chapter reviews basic loop terminology and instructions, and describes the\nproblems that arise when optimizing loops in the absence of architectural support. The IA-64\nspecific loop support features are then introduced. The remainder of this chapter describes the\nprogramming and optimization of various type of loops using the IA-64 features."
			]
		  },
		  {
			"title": "Developing High-Performance Applications for HP-UX on IA-64",
			"url": "http://www.3kranger.com/Interact/Features/0101devIA64.shtm",
			"excerpts": [
			  "The IA-64 architecture provides special branches, along with rotating registers (including predicates), which allow the compiler to generate software pipelined ...Read more"
			]
		  },
		  {
			"title": "Software pipelining - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Software_pipelining",
			"excerpts": [
			  "Lam showed that special hardware is unnecessary for effective modulo scheduling. Her technique, *modulo variable expansion* is widely used in practice.",
			  "The trick is to replicate the body of the loop after it has been scheduled, allowing different registers to be used for different values of the same variable when they have to be live at the same time.",
			  "ca_Lam \"Monica Lam\") presents an elegant solution to this problem in her thesis, *A Systolic Array Optimizing Compiler* (1989) ( [ISBN](/wiki/ISBN_(identifier) \"ISBN (identifier)\") [0-89838-300-5](/wiki/Special:BookSources/0-89838-300-5 \"Special:BookSources/0-89838-300-5\") ). She calls it *modulo variable expansion* . "
			]
		  },
		  {
			"title": "The most prominent example of VLIW processors was ...",
			"url": "https://news.ycombinator.com/item?id=38676884",
			"excerpts": [
			  "The most prominent example of VLIW processors was the Itanic, er, Itanium. It, er, didn't work out well. Hence Itanic. Their premise was that the compiler ...Read more"
			]
		  },
		  {
			"title": "Microarchitectural Techniques for Instruction-driven Specialization ...",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-210.pdf",
			"excerpts": [
			  "The most ambitious attempt to apply VLIW to general-purpose workloads was Intel’s\nItanium, based on the Explicitly Parallel Instruction Computing (EPIC) model and the IA-\n64 instruction set",
			  "EPIC was an evolution of the VLIW concept—drawing on ideas from the\nearlier Cydrome and TRACE architectures—while adding ISA-level features to maximize\ngeneral-purpose ILP and software compatibility."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC Research Results in Itanium 2.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/field2004.pdf",
			"excerpts": [
			  "red-\nication, explicit control speculation, data speculation, and\nmodulo scheduling aids",
			  "rformance in control-intensive, general-purpose pro-\ngrams, therefore, depends heavily on the compiler’s ILP-\nexposing transformations, which IA-64 supports with pred-\nication, explicit control speculation, data speculation, and\nmodulo scheduling ai"
			]
		  }
		],
		"reasoning": "The `finegrained_field_value` describes four advanced software pipelining methods: Decoupled Software Pipelining (DSWP), Coarse-Grained Pipelined Accelerators (CGPA), Enhanced Modulo Scheduling (EMS), and Hierarchical Reduction. I will evaluate the provided excerpts against each of these methods.\n\n**Decoupled Software Pipelining (DSWP)**\n*   **Core Principle**: Several excerpts describe DSWP as a technique that automatically extracts threads from loops by partitioning them into long-running threads that communicate via inter-core queues. This partitioning is based on identifying Strongly Connected Components (SCCs) in the dependence graph. It aims to combine pipeline parallelism with iteration-level parallelism. For example, one excerpt states that DSWP partitions loops into long-running threads that communicate in a pipelined manner via inter-core queues. Another mentions that DSWP finds loop recurrences and groups dependence cycles into SCCs. It also highlights that DSWP executes parts of the loop body in parallel, with dependences flowing unidirectionally in a pipelined fashion.\n*   **Primary Use Case**: The excerpts indicate that DSWP is designed for handling loops with complex loop-carried dependencies, particularly in irregular applications like linked-list or tree traversals. One excerpt explicitly states that DSWP is an approach to automatically extract threads from loops and is applicable to codes with unpredictable behavior like numerical applications. Another mentions that DSWP is used for optimizing the performance of recursive data structures (RDS) such as linked lists, trees, and graphs, where instructions of a given iteration depend on pointer values from previous iterations.\n*   **Key Benefit**: DSWP's main advantage is its ability to hide memory latency by decoupling memory access from computation. Excerpts explain that decoupling stages with queues hides latency by allowing producer stages to run far ahead of consumer stages. This ensures that the compute units are not stalled waiting for memory. It is noted that DSWP can achieve high IPC with fast simple execution cores by preferentially fetching and executing instructions from a program's critical path.\n\n**Coarse-Grained Pipelined Accelerators (CGPA)**\n*   **Core Principle**: CGPA is presented as an HLS framework that uses coarse-grained pipeline parallelism to generate efficient hardware accelerators for loops from unannotated C/C++ programs. It automatically partitions individual loops into separate pipelined stages and generates buffer-connected hardware modules for these stages. The partitioning algorithm is adapted from Parallel Stage Decoupled Software Pipelining (PS-DSWP). It leverages two insights: separating and modularizing complex loop bodies with irregular memory access patterns, and exploiting parallelizable sections within these bodies.\n*   **Primary Use Case**: CGPA is specifically designed for synthesizing efficient specialized accelerator modules from irregular C/C++ programs, particularly for complex loop bodies with irregular memory accesses and imperfect loops. It aims to improve efficiency and applicability for HLS by addressing these types of loops.\n*   **Key Benefit**: The key benefit of CGPA is resource optimization in hardware design. It explicitly avoids duplicating 'heavyweight' sections (those with load and multiply instructions) in parallel stages to minimize hardware area and power. It also allows the synthesized hardware to tolerate variable memory latency and utilizes hidden data-level parallelism within pipelined stages to achieve high performance. Experiments show significant performance improvements over non-parallelized hardware specialization.\n\n**Enhanced Modulo Scheduling (EMS)**\n*   **Core Principle**: EMS is described as a technique that combines if-conversion and hierarchical reduction to schedule loops with conditional branches. It uses if-conversion to transform the loop body into straight-line predicated code, then performs modulo scheduling, and finally regenerates the explicit control structure by inserting conditional branches. This approach aims to avoid the limitations of previous modulo scheduling techniques.\n*   **Primary Use Case**: EMS is used for software pipelining loops containing conditional branches on VLIW architectures, especially those without specialized hardware support for predicated execution. It addresses the challenge of scheduling loops with multiple execution paths.\n*   **Key Benefit**: EMS achieves better performance than simpler techniques like Hierarchical Reduction. One excerpt states that EMS performs approximately 18% better than Modulo Scheduling with Hierarchical Reduction. It avoids the need for prescheduling conditional constructs and uses a more optimal resource model, providing the benefits of if-conversion without requiring specific hardware support.\n\n**Hierarchical Reduction**\n*   **Core Principle**: Hierarchical reduction is a software-only technique that enables software pipelining of loops with conditionals. It works by reducing entire control constructs (like if-then-else blocks) to a single pseudo-operation. This pseudo-operation's resource usage is the union of all internal paths, and its length is the maximum length of any path. This allows scheduling techniques for basic blocks to be applied across control constructs.\n*   **Primary Use Case**: This method is foundational for applying software pipelining to innermost loops, including those with conditional statements, on any VLIW machine, without requiring specialized hardware support for predication. It makes software pipelining applicable to a broader range of loops.\n*   **Key Benefit**: It allows software pipelining to be applied to all innermost loops, not just straight-line code, using a purely software-based approach. It is highly portable and diminishes the start-up cost of loops with a small number of iterations. However, it is noted that it often produces suboptimal schedules compared to more advanced techniques like EMS, as it uses a conservative resource model (union of all paths).\n\nOverall, the excerpts provide strong support for the core principles, primary use cases, and key benefits of each method, with specific details and comparisons where appropriate.",
		"confidence": "high"
	  },
	  {
		"field": "software_managed_memory_strategies",
		"citations": [
		  {
			"title": "Dynamic Allocation for Scratch-Pad Memory using Compile ...",
			"url": "https://terpconnect.umd.edu/~barua/udayakumaran-TECS-2005.pdf",
			"excerpts": [
			  "In this research we propose a highly predictable, low overhead and yet dynamic, memory allocation strategy\nfor embedded systems with scratch-pad memory. A *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  " *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache",
			  "It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  "University of Maryland",
			  "Scratch-pad allocation primarily methods are of two types. First, software-caching schemes emulate the work-\nings of a hardware cache in software. Instructions are inserted before each load/store to check the software-\nmaintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption and\nSRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of\nalgorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation\nschemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that\nnever changes at runtime cannot achieve the full locality benefits of a cache.",
			  "Dynamic Allocation for Scratch-Pad Memory using\nCompile-Time Decisions",
			  "We propose a dynamic allocation methodology for global and stack data and program code that, (i) accounts"
			]
		  },
		  {
			"title": "Compiler-Decided Dynamic Memory Allocation for Scratch- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-CASES-2003.pdf",
			"excerpts": [
			  "scratch-pad* is a fast compiler-managed\nSRAM memory that replaces the hardware-managed cache. It is\nmotivated by its better real-time guarantees vs cache and by its sig-\nnificantly lower overheads in energy consumption, area and overall\nruntime, even with a simple allocation scheme [4]",
			  "Existing scratch-pad allocation methods are of two types. First,\nsoftware-caching schemes emulate the workings of a hardware\ncache in software. Instructions are inserted before each load/store\nto check the software-maintained cache tags.",
			  "This paper presents a highly predictable, low overhead and yet\ndynamic, memory allocation strategy for embedded systems with\nscratch-pad memory."
			]
		  },
		  {
			"title": "Dynamic Management of Scratch-Pad Memory Space",
			"url": "https://www.researchgate.net/profile/Ismail-Kadayif-2/publication/3904523_Dynamic_management_of_scratch-pad_memory_space/links/0a85e53a977b05a602000000/Dynamic-management-of-scratch-pad-memory-space.pdf",
			"excerpts": [
			  "We propose a compiler-controlled dynamic\non-chip scratch-pad memory (SPM) management framework that uses\nboth loop and data transformations.",
			  "perimental results obtained us-\ning a generic cost model indicate significant reductions in data transfer\nactivity between SPM and off-chip memory.",
			  "Embedded systems are often designed as a *system-on-chip* (SoC) archi-\ntecture to cater to the demands of small form factor"
			]
		  },
		  {
			"title": "[PDF] Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/31aab4326d7f7d3bd5e792fbf886ff3a44d4414b",
			"excerpts": [
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems · Sumesh Udayakumaran, R. Barua · Published in International Conference on… 30 ..."
			]
		  },
		  {
			"title": "Scratch-pad memory allocation without compiler support for java applications | Proceedings of the 2007 international conference on Compilers, architecture, and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/1289881.1289899",
			"excerpts": [
			  "S. Udayakumaran, A. Dominguez, and R. Barua. Dynamic Allocation for Scratch-Pad Memory using Compile-Time Decisions. To appear in the ACM Transactions on Embedded Computing Systems (TECS), 5(2), 2006. http://www.ece.umd.edu/~barua/udayakumaran-TECS-2006.pdf.",
			  "S. Tomar, S. Kim, N. Vijaykrishnan, M. Kandemir, and M. J. Irwin. Use of local memory for efficient java execution. In Proceedings of the International Conference on Computer Design: VLSI in Computers & Processors (ICCD). IEEE, 2001.",
			  "S. Udayakumaran and R. Barua. Compiler-decided dynamic memory allocation for scratch-pad based embedded systems. In Proceedings of the international conference on Compilers, architectures and synthesis for embedded systems (CASES), pages 276--286. ACM Press, 2003."
			]
		  },
		  {
			"title": "Dynamic management of scratch-pad memory space | Proceedings of the 38th annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/378239.379049",
			"excerpts": [
			  "Section Title: Dynamic management of scratch-pad memory space",
			  "Content:\nAuthors : [M. Kandemir](# \"M. Kandemir\") M. Kandemir\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100186744) , [J. Ramanujam](# \"J. Ramanujam\") J. Ramanujam\nDepartment of ECE, Louisiana State University, Baton Rouge, LA\n[View Profile](/profile/81100351630) , [J. Irwin](# \"J. Irwin\") J. Irwin\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81350589577) , [N. Vijaykrishnan](# \"N. Vijaykrishnan\") N. Vijaykrishnan\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81331506265) , [I. Kadayif](# \"I. Kadayif\") I. Kadayif\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100417786) , [A. Parikh](# \"A. Parikh\") A. Parikh\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100298924) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.1145%2F378239.379049)\n[DAC '01: Proceedings of the 38th annual Design Automation Conference](/doi/proceedings/10.1145/378239)\nPages 690 - 695\nhttps://doi.org/10.1145/378239.379049\nPublished : 22 June 2001 [Publication History](#) To view Publication History, you must have Premium access.\n ... \nSection Title: Dynamic management of scratch-pad memory space > Comments > Affiliations"
			]
		  },
		  {
			"title": "An Integrated Scratch-Pad Allocator for Affine and Non- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-DATE-2006.pdf",
			"excerpts": [
			  "The prevalence of SPM in embedded systems is evident\nfrom the large variety of chips with SPM available today in\nthe market.( *e.g.,* [1,4]). Such memory is typically software\nmanaged. Software management, apart from avoiding ex-\ntra caching hardware, also allows for more predictable pro-\ngram execution than when caches are present. Studies have\nfurther shown other benefits of using SPM including reduc-\ntions in power consumption, memory latency, area and even\nsignificant runtime gains for embedded applications. Trends\nin embedded designs indicate that this dominance of SPM’s\nwill continue in the future [9].",
			  "The rest of the paper is organized as follows. Section 2\noverviews related work. Section 3 describes our integrated\nmethod for affine and non-affine code. Section 4 presents an\nevaluation of our methodology. Section 5 concludes."
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Proceedings of the 2003 international conference on Compilers, architecture and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/10.1145/951710.951747",
			"excerpts": [
			  " Compiler-decided dynamic memory allocation for scratch-pad based embedded systems\n",
			  " Section Title: Compiler-decided dynamic memory allocation for scratch-pad based embedded systems"
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques for low-power and -energy",
			"url": "https://www.researchgate.net/publication/29649252_A_Survey_of_Scratch-Pad_Memory_Management_Techniques_for_low-power_and_-energy",
			"excerpts": [
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems",
			  "In the dynamic approach, program blocks can be transferred between SPM and the offchip memory during the application execution [12] . In addition to performance and energy consumption, SPM as an SRAM-based on-chip memory plays a major role in the reliability of embedded systems.",
			  "The main idea underlying the proposed mechanism includes four stages: 1) to use parity codes for error detection in the SPM, 2) to keep an address matching table in the main memory to store the address of the copy of SPM blocks in the main memory, in the case of dynamic SPM, 3) to allocate a specific segment of the main memory as an SPM backup, in the case of static SPM, and 4) to recover from errors using an interrupt service routine (ISR).",
			  "The memory subsystem accounts for a significant portion of the aggregate energy budget of contemporary embedded systems. Moreover, there exists a large potential for optimizing the energy consumption of the memory subsystem. Consequently, novel memories as well as novel algorithms for their efficient utilization are being designed.",
			  "An Optimal Memory Allocation Scheme for Scratch-Pad Based Embedded Systems",
			  "Assigning program and data objects to scratchpad for energy reduction",
			  "Dynamic overlay of scratchpad memory for energy minimization",
			  "Data partitioning for maximal scratchpad usage",
			  "The set of selected program parts are determined with an optimal algorithm using integer linear programming.",
			  "Scratchpad memory: design alternative for cache on-chip memory in embedded systems.",
			  "A minimal increase in code size is also reported.",
			  "ed which analyzes the application, partitions an array variable whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Res"
			]
		  },
		  {
			"title": "Scratchpad Memory - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/scratchpad-memory",
			"excerpts": [
			  "tatic-allocation) methods assign data to SPM at compile time but cannot adapt to changing program behavior or reallocate memory at runtime.",
			  "ynamic-allocation) methods, implemented via compiler-inserted code, allow variables to move between SPM and off-chip memory during execution, supporting more flexible memory usage."
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques ...",
			"url": "https://inria.hal.science/inria-00170210v1/document",
			"excerpts": [
			  "e SPM Manage-\nment Policy at Runtime can be Static which means that data could be over-\nwritten but not moved to another place. With Moves some existing variables\nin SPM could be evicted to make space for incoming ones. In this way, data is\nnever lost. On the one hand, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM",
			  "type. These approaches try to answer the question of which program variables\nshould be allocated to which memory or memory bank. In these techniques,\nbecause of the reduced size of SRAM, the lesser-used variables are first allocated\nto slower memory banks (DRAM), while the most frequently used variables are\nkept in fast memory (SRAM) as much as possible. These methods use profile\ndata to gather access frequency information in order to place frequently used\ndata in fast memory, and other data in slower memory. To do so, most authors\nmodel the problem as a 0/1 integer linear programming (ILP) problem and then\nuse an available IP solver to solve it.",
			  "These methods use profile\ndata to gather access frequency information in order to place frequently used\ndata in fast memory, and other data in slower memory.",
			  "nd, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM.\n",
			  "\n[Udayakumaran and Barua, 2003] S. Udayakumaran and R. Barua. Compiler-decided\ndynamic memory allocation for scratch-pad based embedded systems. In CASES ,\npages 276–286. ACM Press, 2003.\n",
			  "[Udayakumaran and Barua, 2006] S. Udayakumaran and R. Barua.\nAn integrated\nscratch-pad allocator for affine and non-affine code. In DATE , pages 925–930, 2006.\n[",
			  "[Verma et al. , 2003] M. Verma, S. Steinke, and P. Marwedel. Data Partitioning for\nMaximal Scratchpad Usage. In ASPDAC , 2003.\n[",
			  "\n[Verma et al. , 2004] M. Verma, L. Wehmeyer, and P. Marwedel. Dynamic Overlay of\nScratchpad Memory for Energy Minimization. In CODES+ISSS , 2004.\n"
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/taco09.pdf",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embed-\nded system",
			  "his paper introduces a general-purpose compiler approach, called memory coloring ,\nto assign static data aggregates such as arrays and structs in a program to an SPM.",
			  "The novelty\nof this approach lies in partitioning the SPM into a pseudo register file (with interchangeable\nand aliased registers), splitting the live ranges of data aggregates to create potential data transfer\nstatements between SPM and off-chip memory, and finally, adapting an existing graph coloring\nalgorithm for register allocation to assign the data aggregates to the pseudo register file.",
			  "ur\nexperimental results using a set of 10 C benchmarks from MediaBench and MiBench show that\nour methodology is capable of managing SPMs efficiently and effectively for large embedded appli-\ncations. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and\ncompared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator"
			]
		  },
		  {
			"title": "A Compiler Approach for Scratchpad Memory Management",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/pact05.pdf",
			"excerpts": [
			  "cratchpad memory (SPM), a fast software-managed on-*\n*chip SRAM, is now widely used in modern embedded pro-*\n*cessors. Compared to hardware-managed cache, it is more*\n*efficient in performance, power and area cost, and has the*\n*added advantage of better time predictability. This paper*\n*introduces a general-purpose compiler approach, called*\n*memory coloring, to efficiently allocating the arrays in a*\n*program to an SPM. The novelty of our approach lies in*\n*partitioning an SPM into a “register file”, splitting the live*\n*ranges of arrays to create potential data transfer statements*\n*between the SPM and off-chip memory, and finally, adapt-*\n*ing an existing graph-colouring algorithm for register al-*\n*location to assign the arrays in the program into the reg-*\n*ister fi",
			  " approach is efficient due to the practical*\n*efficiency of graph-colouring algorithms. We have imple-*\n*mented this work in SUIF and machSUIF. Preliminary re-*\n*sults over benchmarks show that our approach represents a*\n*promising solution to automatic SPM management.",
			  "A scratchpad memory (SPM) is a fast on-chip SRAM"
			]
		  },
		  {
			"title": "a compiler approach for scratchpad memory management",
			"url": "http://ieeexplore.ieee.org/document/1515604/",
			"excerpts": [
			  ". The novelty of our approach lies in partitioning an SPM into a \"register file\", splitting the live ranges of arrays to create potential data transfer statements between the SPM and off-chip memory, and finally, adapting an existing graph-colouring algorithm for register allocation to assign the arrays in the program into the register file.",
			  "We have implemented this work in SUIF and machSUIF.",
			  "Preliminary results over benchmarks show that our approach represents a promising solution to automatic SPM management.",
			  "Scratchpad memory (SPM), a fast software-managed on-chip SRAM, is now widely used in modern embedded processors.",
			  "Compared to hardware-managed cache, it is more efficient in performance, power and area cost, and has the added advantage of better time predictability.",
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "A compiler approach for scratchpad memory management",
			"url": "https://www.researchgate.net/publication/4178715_Memory_coloring_A_compiler_approach_for_scratchpad_memory_management",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "https://www.researchgate.net/publication/220170090_Compiler-Directed_Scratchpad_Memory_Management_via_Graph_Coloring",
			"excerpts": [
			  "graph coloring algorithm for scalars that is generalized as described in [Smith et al. 2004] to deal with interchangeable and aliased registers.Read more"
			]
		  },
		  {
			"title": "Compiler-directed scratchpad memory management via graph coloring | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1582710.1582711",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
			  "Section Title: Compiler-directed scratchpad memory management via graph coloring > Abstract",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM.",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file.",
			  "Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications.",
			  "In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator."
			]
		  },
		  {
			"title": "[PDF] Memory coloring: a compiler approach for scratchpad memory management | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Memory-coloring%3A-a-compiler-approach-for-scratchpad-Li-Gao/6e123b31043f629fd77700d0d7684c1af9231e7d",
			"excerpts": [
			  "Date Range\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by Relevance Sort by Most Influenced Papers Sort by Citation Count Sort by Recency\n[### Compiler-directed scratchpad memory management via graph coloring](/paper/Compiler-directed-scratchpad-memory-management-via-Li-Feng/ac3f185928e87fc915cb0dab338fc713db8d9008)\n[Lian Li](/author/Lian-Li/50704040) [Hui Feng](/author/Hui-Feng/2113275806) [Jingling Xue](/author/Jingling-Xue/144198209)\nComputer Science, Engineering\nTACO\n2009\nTLDR\nThis article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM, and shows that this methodology is capable of managing SPMs efficiently and effectively for large embedded applications. Expand\n[42](/paper/ac3f185928e87fc915cb0dab338fc713db8d9008)\n[PDF](/paper/ac3f185928e87fc915cb0dab338fc713db8d9008)\n3 Excerpts\nSave\n[### LMStr: exploring shared hardware controlled scratchpad memory for multicores](/paper/LMStr%3A-exploring-shared-hardware-controlled-memory-Siddique-Badawy/b36781cb547f4c5b67d99fc3208f628662787b4f)\n[N. Siddique](/author/N.-Siddique/8755162) [Abdel-Hameed A. Badawy](/author/Abdel-Hameed-A.-Badawy/3059326) [Jeanine E. Cook](/author/Jeanine-E.-Cook/8648310) [D. Resnick](/author/D.-Resnick/3133479)\nComputer Science, Engineering\n ... \nSection Title: Memory coloring: a compiler approach for scratchpad memory management > 22 References\nContent:\n[### Storage allocation for embedded processors](/paper/Storage-allocation-for-embedded-processors-Sj%C3%B6din-Platen/263b7d5a3c9020cf0d57ebe3727416ff0a2379a1)\n[J. Sjödin](/author/J.-Sj%C3%B6din/38070142) [C. V. Platen](/author/C.-V.-Platen/2143289)\nComputer Science, Engineering\n[International Conference on Compilers…](/venue?name=International%20Conference%20on%20Compilers%2C%20Architecture%2C%20and%20Synthesis%20for%20Embedded%20Systems)\n2001\nTLDR\nA model for storage allocation that is capable of describing architectures with irregular memory organization and with several native pointer types is presented and an ILP solver is applied to get an optimal solution under the model. Expand\n[96](/paper/263b7d5a3c9020cf0d57ebe3727416ff0a2379a1)\n[PDF](/paper/263b7d5a3c9020cf0d57ebe3727416ff0a2379a1)\n1 Excerpt\nSave\n[### Register allocation & spilling via graph coloring](/paper/Register-allocation-%26-spilling-via-graph-coloring-Chaitin/5e6196f5744da120b69e9a50da6ed4048020ec55)\n[G. Chaitin](/author/G.-Chaitin/1756533)\nComputer Science\n[SIGPLAN Conferences and Workshops](/venue?name=SIGPLAN%20Conferences%20and%20Workshops)\n1982\nTLDR\nThis work has discovered how to extend the graph coloring approach so that it naturally solves the spilling problem, and produces better object code and takes much less compile time. Expand\nSection Title: Memory coloring: a compiler approach for scratchpad memory management > 22 References\nContent:\n[648](/paper/5e6196f5744da120b69e9a50da6ed4048020ec55)\n[PDF](/paper/5e6196f5744da120b69e9a50da6ed4048020ec55)\n1 Excerpt\nSave\n[### The priority-based coloring approach to register allocation](/paper/The-priority-based-coloring-approach-to-register-Chow-Hennessy/c5b28a6007a23cd085b2136f55c4fe7f03043932)\n[Fred C. Chow](/author/Fred-C.-Chow/145268291) [J. Hennessy](/author/J.-Hennessy/1772133)\nComputer Science\n[ACM Transactions on Programming Languages and…](/venue?name=ACM%20Transactions%20on%20Programming%20Languages%20and%20Systems)\n1990\nTLDR\nThe detailed algorithms for a priority-based coloring approach are presented and are contrasted with the basic graph-coloring algorithm and various extensions to the basic algorithms are also presented. Expand\n[369](/paper/c5b28a6007a23cd085b2136f55c4fe7f03043932)\n[PDF](/paper/c5b28a6007a23cd085b2136f55c4fe7f03043932)\n1 Excerpt\nSave\n[### Improvements to graph coloring register allocation](/paper/Improvements-to-graph-coloring-register-allocation-Briggs-Cooper/26ae5ec6b744877edba267d2f4d0d0c8d8c7578d)\n[Preston Briggs](/author/Preston-Briggs/37779433) [K. Cooper](/author/K.-Cooper/145971118) [Linda Torczon](/author/Linda-Torczon/1750325)\nComputer Science\n[ACM Transactions on Programming Languages and…](/venue?name=ACM%20Transactions%20on%20Programming%20Languages%20and%20Systems)\n1994\nTLDR",
			  "a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM, and shows that this methodology is capable of managing SPMs efficiently and effectively for large embedded applications.",
			  "Memory coloring: a compiler approach for scratchpad memory management > 116 Citations"
			]
		  },
		  {
			"title": "Optimizing DMA Data Transfers for Embedded Multi-Cores",
			"url": "http://www-verimag.imag.fr/~maler/Papers/thesis-selma.pdf",
			"excerpts": [
			  "Double buffering or more generally multi-buffering is a well known programming technique\nreferred to as *software pipelining* used to hide memory latency. The main idea is to overlap com-\nputations and data transfers and it is expressed at the software level.",
			  "This takes advantage from the fact that DMA can work in parallel with a processor. Pro-\ngram 2.4.2 uses *double buffering* for input blocks *B* *x* [0] , *B* *x* [1] and output blocks *B* *y* [0] , *B* *y* [1] . At\neach iteration before the processor starts working on the current buffer, it issues an *asynchronous*\nDMA call to fetch the next buffer. Therefore, the processor can work on a super block *j* residing\nin one buffer while the DMA brings *in parallel* the super block *j* + 1 to the other buffer. Note that\n*comp* ( *j* ) is used as a shorthand for the inner double loop in Program 2.4.1. In practice, events are\nassociated with each *dma_get* and *dma_put* command to ensure the synchronization between each\ndata transfer and its corresponding computation.",
			  "Hence, program 2.4.2 defines a *software pipeline* with 3 stages: input of super block ( *j* + 1) ,\ncomputation on super block *j* and output of super block ( *j* *−* 1) , see Figure 2.10. Reading the first\nblock and writing back the last block are, respectively, the *prologue* and *epilogue* of the pipeline.\nNote that Figure 2.10 only describes the obvious precedences between the computations and data\ntransfers but tells us nothing about their relative durations.",
			  "ile for general purpose multiprocessor architectures, the design choice of using caches is\nobvious, it is not the case anymore for MPSoCs. The main reason is that MPSoCs are resource\nconstrained systems and managing cache coherence is very expensive both in terms of area and\npower [LPB06]. Therefore Scratchpad Memories (SPMs) have been proposed as an alternative\nto caches where data (and sometimes code) transfers through the memory hierarchy are *explic-*\n*itly* managed by the software. Furthermore, unlike a general purpose processor where a standard\ncache-based memory hierarchy is employed to guarantee good average performance over a wide\nrange of applications, in MPSoCs, the overall memory organization has to be tailored in order\nto fit at best the needs of the target class of applications. Indeed, according to the memory ac-\ncess pattern of the target applications, the use of scratchpad memories can be more efficient t"
			]
		  },
		  {
			"title": "Automated Scratchpad Mapping and Allocation for Embedded ...",
			"url": "https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=4035&context=etd",
			"excerpts": [
			  "If the per-core on-chip memory or the share memory (MSMC) are programmat-\nically configured to behave partially or fully as a scratchpad memory (SPM) space,\nintegrated DMA controller allows data to be exchanged between on- and off-chip\nmemories in parallel to operations being performed on the DSP.",
			  "With EDMA, data accessed in a predictable, regular access pattern can be con-\ncurrently prefetched using EDMA to exchange data between scratchpad and DRAM.",
			  "Data accessed irregularly in a data-dependent pattern can be cached in a separate\nregion of scratchpad to take advantage of locality.",
			  "Direct memory access (DMA) is a feature of modern computers that allows certain\nhardware subsystems within the computer to access system memory independently\nof the central processing unit (CPU).",
			  "Without DMA, when the CPU is using pro-\ngrammed input/output, it is typically fully occupied for the entire duration of the\nread or write operation, and is thus unavailable to perform other work.",
			  "With DMA,\nthe CPU initiates the transfer, does other operations while the transfer is in progress,\nand receives an interrupt from the DMA controller when the operation is done."
			]
		  },
		  {
			"title": "Applications Using the TMS320C6000 Enhanced DMA",
			"url": "https://www.ti.com/lit/pdf/spra636",
			"excerpts": [
			  "Ping-Pong Buffering for McBSP Data",
			  "While the EDMA is transferring\ndata in to and out of the ping buffers, the CPU is manipulating the data in the pong buffers.",
			  "The ping–pong scheme for this example is shown in\nFigure 27.",
			  "The EDMA channels may be configured to access any location in the device’s memory map."
			]
		  },
		  {
			"title": "TMS320C6000 DSP Enhanced Direct Memory Access ( ...",
			"url": "https://www.ti.com/lit/pdf/spru234",
			"excerpts": [
			  "Each channel has two parameter sets, ping and pong.",
			  "Ping-pong buffering is a simple technique that allows the CPU activity to be\ndistanced from the EDMA activity. This means that there are multiple (usually\ntwo) sets of data buffers for all incoming and outgoing data streams.",
			  "The link address for the\nping entry is set to the PaRAM offset of the pong parameter set (Figure 1−27)."
			]
		  },
		  {
			"title": "ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing. | Request PDF",
			"url": "https://www.researchgate.net/publication/221569161_ROS-DMA_A_DMA_Double_Buffering_Method_for_Embedded_Image_Processing_with_Resource_Optimized_Slicing",
			"excerpts": [
			  "This paper presents our work on PfeLibÃ¢Â€Â”a high performance software library for image processing and computer vision algorithms for an embedded system. The main target platform for PfeLib is the TMS320C6000 series of digital signal processors (DSPs) from Texas instrume",
			  "ROS-DMA can yield substantially better performance compared to using L2 cache.",
			  "Section Title: ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing."
			]
		  },
		  {
			"title": "Optimizing Data Distribution for Loops on Embedded ...",
			"url": "https://utd-ir.tdl.org/server/api/core/bitstreams/cb4d3e16-7abb-4de8-8fc4-77d727d707f1/content",
			"excerpts": [
			  "472–511, May 2006. [13] S. Udayakumaran and R. Barua, “Compiler-decided dy- namic memory allocation for scratch-pad based embed ...Read more"
			]
		  },
		  {
			"title": "(PDF) Dynamic management of scratch-pad memory space",
			"url": "https://www.academia.edu/78529842/Dynamic_management_of_scratch_pad_memory_space",
			"excerpts": [
			  "We propose a compiler-controlled dynamic on-chip scratch-pad memory (SPM) management framework that uses both loop and data transformations. Experimental ..."
			]
		  },
		  {
			"title": "Scratchpad Memory : A Design Alternative for Cache On-chip ...",
			"url": "https://robertdick.org/aeos/reading/banakar-scratchpad.pdf",
			"excerpts": [
			  "In this paper we address the problem of on-chip mem- ory selection for computationally intensive applications, by proposing scratch pad memory as an ...Read more"
			]
		  },
		  {
			"title": "A Scratchpad Memory Allocation Scheme for Dataflow Models",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-104.pdf",
			"excerpts": [
			  "In [3], Banakar et al show that on average a scratchpad memory has 34% smaller area and 40% lower power consumption than a cache of the same ...Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratch Pad Memory Optimization for ...",
			"url": "https://users.eecs.northwestern.edu/~choudhar/Publications/CompilerDirectedScratchPadMemoryOptimEmbeddedMultiprocessors.pdf",
			"excerpts": [
			  "Abstract—This paper presents a compiler strategy to optimize data accesses in regular array-intensive applications running on embedded multiprocessor ..."
			]
		  },
		  {
			"title": "Adaptive Scratch Pad Memory Management for Dynamic ...",
			"url": "https://www.engr.colostate.edu/~sudeep/wp-content/uploads/j10.pdf",
			"excerpts": [
			  "Guided by compiler analysis for generating scratch pad data layouts and hardware components for tracking dynamic memory accesses, the scratch pad data."
			]
		  },
		  {
			"title": "TMS320C64x EDMA Architecture",
			"url": "https://www.ti.com/lit/pdf/spra994",
			"excerpts": [
			  "• The CPU accesses memory in the L2 SRAM space. This access goes directly to L2 within the cache memory system; refer to TMS320C64x DSP Two Level Internal ...Read more"
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch- ...",
			"url": "https://scispace.com/pdf/compiler-decided-dynamic-memory-allocation-for-scratch-pad-1eaxs54tgy.pdf",
			"excerpts": [
			  "In static management of scratch-pad memory, the contents of the scratch-pad\nare decided at compile time itself. A simple way of doing this is based on programmer\nhints. Thus, the programmer provides annotations that tell if a variable is to be put\ninto the scratch-pad address space or not. In such a scheme, it is the programmer\nresponsibility to ensure that a particular variable fits completely inside the scratch-\npad address space. To make this process efficient, the selection strategy can be based\non compiler analysis of the application. Examples of such static allocation strategies\ninclude [12,13,73,77]. The advantage of a static scheme is that it does not involve\nany additional per-access overhead like that of software caching as no translation",
			  "Software caching [36,59] represents a dynamic allocation strategy. This class\nof methods emulate the behavior of a hardware cache in software. In particular, a\ntag consisting of the high-order bits of the address is stored for each cache line in\nsoftware. Before each load or a store, additional instructions are compiler-inserted\nto mask out the high-order bits of the address, access the tag, compare the tag with\nthe high-order bits and then branch conditionally to hit or miss code. On a hit, in\nthe hit code, using a mapping table the address is mapped to a new address in the\nscratch-pad where the data resides. Otherwise it is a cache miss and new data is\nbrought into the scratch-pad. At that point, the index tables are updated to reflect\nthe presence of this new data in the scratch-pad. Being a runtime strategy like\ncaching, the strategy can adapt to runtime conditions. This makes it particularly\nsuited for workloads whose runtime conditions continuously vary.",
			  "We propose a dynamic allocation methodology for global and stack data and\nprogram code that (i) accounts for changing program requirements at runtime (ii)\nhas no software-caching tags (iii) requires no run-time checks (iv) has extremely low\noverheads, and (v) yields 100% predictable memory access times.",
			  "data that is about to be accessed frequently is copied into the scratch-pad using\ncompiler-inserted code at fixed and infrequent points in the program. Earlier data\nis evicted if necessary.",
			  "When compared to an existing static allocation scheme,\nresults show that our scheme reduces runtime by up to 39.8% and energy by up to\n31.3% on average for our benchmarks, depending on the SRAM size used.",
			  "The actual\ngain depends on the SRAM size, but our results show that close to the maximum\nbenefit in run-time and energy is achieved for a substantial range of small SRAM\nsizes commonly found in embedded systems.",
			  "Our comparison with a direct mapped\ncache shows that our method performs roughly as well as a cached architecture in\nruntime and energy while delivering better real-time benefits.",
			  "Compiler-Decided Dynamic Memory Allocation for Scratch-Pad\nBased Embedded Systems"
			]
		  },
		  {
			"title": "Hexagon™ V73 HVX",
			"url": "https://docs.qualcomm.com/doc/80-N2040-54/80-N2040-54_REV_AB_Qualcomm_Hexagon_V73_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "HVX supports a local memory called vector TCM (VTCM) for scratch buffers and scatter/gather\noperations.",
			  "The size of the memory is implementation-defined. The size is discoverable from the\nconfiguration table.",
			  "VTCM needs normal virtual to physical translation just like other memory.",
			  "This memory has higher performance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer. It serves as the input or output\nof the scatter/gather instructions.",
			  "The following are advantages of using VTCM as the\nintermediate buffer:\n\nGuarantees no eviction (vs. L2 if the set is full)\n\nFaster than L2$ (does not have the overhead of cache management, like association)\n\nReduces L2$ pressure\n\nLower power than L2$\n\nSupports continuous read and write for every packet without contention",
			  ".9.4 L2FETCH**\nUse the L2FETCH instruction to prepopulate the L2 cache with data prior to using VMEM loads.\nL2FETCH is best performed in sizes less than 8 KB and issued at least several hundred cycles prior\nto using the data. If the L2FETCH instruction is issued too early, data can be evicted before use. In\ngeneral, prefetching and processing on image rows or tiles works best",
			  ".9.5 Access data contiguously**\nWhenever possible, arrange data in memory so that it is accessed contiguously. For example,\ninstead of repeatedly striding through memory, data might be first tiled, striped, or decimated to\n ... \nin L1, followed by scalar load instructions. Execute the DCFETCH anytime after the VMEM store,\nhowever, software should budget at least 30 cycles before issuing the scalar load instructi",
			  ".9.8 Avoid scatter/gather stalls**\nScatter and gather operations compete for memory and can result in long latency, therefore take\ncare to avoid stalls. The following techniques improve performance around scatter and gather:\n\nDistribute accesses across the intra-vector address range (lower address bits). Even\ndistribution across the least significant inter-vector address bits can also be beneficial. For\nV73, address bits [10:3] are important to avoid conflicts. Ideally this applies per vector\ninstruction, but distributing these accesses out between vector instructions can help absorb\nconflicts within a vector instruction.\n\nMinimize the density of scatter and gather instructions. Spread out these instructions in a\nlarger loop rather than concentrating them in a tight loop. The hardware can process a small\nnumber of these instructions in parallel. If it is difficult to spread these instructions out, limit\nbursts to four for a given thread (for "
			]
		  },
		  {
			"title": "Hexagon V79 HVX Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-61/memory.html",
			"excerpts": [
			  "HVX supports a local memory called vector tightly coupled memory\n(VTCM) for scratch buffers and scatter/gather operations.",
			  "The size of\nthe memory is implementation-defined.",
			  "VTCM needs normal virtual to physical\ntranslation just like other memory.",
			  "This memory has higher\nperformance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer.",
			  "It\nserves as the input or output of the scatter/gather instructions.",
			  "The\nfollowing are advantages of using VTCM as the intermediate buffer:",
			  "Guarantees no eviction (vs. L2 if the set is full)",
			  "Faster than L2$ (does not have the overhead of cache management, like\nassociation)",
			  "Reduces L2$ pressure",
			  "Lower power than L2$",
			  "Supports continuous read and write for every packet without\ncontention",
			  "In addition to HVX VMEM access, normal Hexagon memory access\ninstructions can access this memory."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies.",
			  "Decoupled software pipelining (DSWP) solves this by preferentially fetching and executing instructions from a program's critical path (CP). This allows DSWP to achieve high IPC with fast simple execution cores.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "Special microarchitectural support, called the\nsynchronization array (SA), provides low-latency inter-thread\nsynchronization and value communication and acts as a decoupling\nbuffer between the threads.",
			  "Decoupled execution ensures that stalls\nin one thread do not affect the other.",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Automatic Multithreading for DSWP: Unlike prior attempts at automatically multithreading sequential programs, DSWP does\nnot try to partition programs into totally independent threads.",
			  "We have a working compiler implementation that\nautomatically generates DSWP code."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization.",
			  " partitions loops\ninto long-running threads that communicate via inter-core queues.\nDSW",
			  "Parallel-Stage Decoupled Software Pipelining [16] (PS-DSWP)\nis an extension to combine pipeline parallelism with some stages\nexecuted in a DOALL, data-parallel fashion.",
			  "This paper presents the preliminary design of a new PS-DSWP method capable of handling arbitrary structured control flow, a slightly better ...Read more"
			]
		  },
		  {
			"title": "Phloem: Automatic Acceleration of Irregular Applications with ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2023.phloem.hpca.pdf",
			"excerpts": [
			  "Recent work has proposed hardware support for fine-grain pipeline parallelism, hiding long latencies by decoupling irregular applications into pipeline stages.Read more"
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation.",
			  "We evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP.",
			  "The base model used is a 6-core Itanium 2 CMP developed using\nthe Liberty Simulation Environment (LSE). The cores are validated\n(IPC and constituent error components accurate to within 6% of\nreal hardware for benchmarks measured [15]), and the details of\neach core in the model are given in Table 1. To this base model, a\nsynchronization array with 512 queues, each with 32 entries, was\nadded. The produce , consume\nand queue.set\ninstructions\nare also modeled in the pipeline of the Itanium 2 cores.",
			  "We have implemented PS-DSWP in the VELOCITY research\ncompiler [22]. The applications are first compiled into a low-level\nintermediate representation, and then transformed by a round of\nclassical optimizations. PS-DSWP was then automatically applied\nto the selected loops. Translation to Itanium 2 instructions is fol-\nlowed by another round of classical optimizations, scheduling, reg-\nister allocation, and a final scheduling phase.",
			  "t DSWP in combination\nwith speculation and parallel stages has the potential to unlock par-\nallelism among all the programs in the SPEC2000 integer bench-\nmark suite.",
			  "This paper presented a new non-speculative compiler transfor-\nmation called Parallel-Stage Decoupled Software Pipelining (PS-\nDSWP). PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformation. This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators.",
			  "For other programs, however, these tools yield poor performance because they invoke computation units for instructions sequentially, without exploiting parallelism."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages.",
			  "Decoupled software pipelining\n(DSWP) [ 35 ] uses a synchronization array to facilitate com-\nmunication between cores or the threads of a multithreaded\ncore. But DSWP focuses on pipelining a single loop across\ndi ff erent threads, which is too limiting for irregular applications.\nFor example, BFS uses a 3-level nested loop, with stages",
			  " Pipette’s ISA fea-\ntures control mechanisms that allow e ff ective decoupling under\nirregular control flow. ",
			  "es.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers.",
			  "By time-multiplexing stages on the same\ncore, Pipette avoids load imbalance and achieves high core IPC.",
			  " it uses the\nphysical register file to implement queues cheaply, avoiding the\nstorage costs of prior techniques. Sec",
			  "Pipette decouples threads within\nthe core using architecturally visible queues."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more",
			  "This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual ...Read more"
			]
		  },
		  {
			"title": "Quantifying the Semantic Gap Between Serial and Parallel ...",
			"url": "https://par.nsf.gov/servlets/purl/10337772",
			"excerpts": [
			  "This measures the TLP limit of both cyclic multithreading (CMT). (e.g., DoAcross [20], HELIX [3]) and pipeline multithreading. (PMT) (e.g., DSWP [5], PS-DSWP [8]) ...Read more"
			]
		  },
		  {
			"title": "From Sequential Programs to Concurrent Threads - Liberty ...",
			"url": "https://liberty.princeton.edu/Publications/cal05_dswp.pdf",
			"excerpts": [
			  "Ottoni, E. Raman, and D. I.. August, “Practical and accurate low-level pointer analysis,” in Proc. of the Int'l. Symp. on Code Generation and Optimization, 2005 ..."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Since technology scaling no longer provides previously seen rates of performance improvements, microprocessor designers have adopted a number of other techniques to improve processor performance at reasonable energy costs. An entire class of techniques improves performance by offloading computation intensive parts of programs from a general-purpose processor to customized accelerators [15], [17], [18], [24], [31], [34]. One approach to build these customized accelerators is to manually design dedicated circuit modules for each application using hardware description languages (HDLs) [24], [26]. While this approach often yields the best result, it requires significant non-recurring engineering costs to convert algorithms to HDL specifications. Another less labor-intensive approach is to use high-level synthesis (HLS) tools to generate customized accelerators from programs written in C or other high-level programming languages [4], [12], [13], [16], [30], [31], [32], [37].",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Hexagon™ V73",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-53_REV_AB_Qualcomm_Hexagon_V73_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "L2FETCH is nonblocking. After the instruction is initiated, the program continues to the next\ninstruction while the prefetching is performed in the background.",
			  "When data cache prefetching is enabled, the Hexagon processor observes patterns of data cache\nmisses and attempts to predict future misses based on recurring patterns of misses where the\naddresses are separated by a constant stride. If such patterns are found, the processor attempts\nto automatically prefetch future cache lines."
			]
		  },
		  {
			"title": "Hexagon™ V69",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-50_REV_AA_Qualcomm_Hexagon_V69_ProgrammerS_Reference_Manual.pdf",
			"excerpts": [
			  "The l2fetch instruction is non-blocking: it initiates a prefetch operation that is performed in the\nbackground by the prefetch engine while the thread continues to execute Hexagon processor\ninstructions."
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "within a fixed time after execution starts. But, when the sources are required varies.",
			  "Instructions that need more pipelining require early sources. Only HVX registers are Early",
			  "Source registers. Early source operands include:",
			  "Input to the multiplier. For example “V3.h = vmpyh(V2.h, V4.h)”. V2 and V4 are",
			  "multiplier inputs. For multiply instructions with accumulation, the accumulator is",
			  "not considered an early source multiplier input.",
			  "Input to shift/bit count instructions. Only the register that is being shifted or",
			  "counted is considered early source. Accumulators are not early sources.",
			  "Input to permute instructions. Only registers that are being permuted are",
			  "considered early source (not an acccumulator).",
			  "Unaligned store data is an early source.",
			  "If an early source register is produced in the previous vector packet, an interlock stall can",
			  "be incurred. Software should strive to schedule an intervening packet between the",
			  "producer and an early source consumer.",
			  "Vector Instructions",
			  "The following example shows various interlock cases:",
			  "V8 = VADD(V0,V0)",
			  "V0 = VADD(V8,V9)",
			  "// NO STALL",
			  "V1 = VMPY(V0,R0)",
			  "// STALL due to V0",
			  "V2 = VSUB(V2,V1)",
			  "// NO STALL on V1",
			  "V5:4 = VUNPACK(V2) // STALL due to V2",
			  "V2 = VADD(V0,V4)",
			  "// NO STALL on V4"
			]
		  },
		  {
			"title": "Hexagon V60 HVX Programmer's Reference Manual",
			"url": "https://docs.qualcomm.com/doc/80-N2040-30/80-N2040-30_REV_C_Hexagon_V60_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Certain instructions require Early Sources. Early source registers include:\n■\nInput to the multiplier. For example “ V3.h = vmpyh(V2.h, V4.h) ”. Here V2\nand V4 are multiplier inputs. For multiply instructions with accumulation, the\naccumulator is not considered an Early Source multiplier input.\n■\nInput to Shift/Bit Count instructions. For shifts, all vector sources are Early\nSource except for accumulators.\n■\nInput to Permute instructions. Only registers that are being permuted are\nconsidered Early Source (not Accumulator).\n■\nUnaligned Store Data is an Early Source.\nIf an Early Source register is produced in the previous vector packet, an interlock stall will\noccur. The software should try to schedule an intervening packet between the producer of\nan Early Source register. For example, the following shows various interlock cases:\nV8 = VADD(V0,V0)\nV0 = VADD(V8,V9) // NO STALL\nV1 = VMPY(V0,R0) // STALL due to V0\nV2 = VSUB(V2,V1) // NO STALL on V1\nV5:4 = VUNPACK(V2) // STALL due to V2\nV2 = VADD(V0,V4) // NO ST"
			]
		  },
		  {
			"title": "Qualcomm Corporate Template",
			"url": "https://llvm.org/devmtg/2017-02-04/Halide-for-Hexagon-DSP-with-Hexagon-Vector-eXtensions-HVX-using-LLVM.pdf",
			"excerpts": [
			  "Halide also provides a scheduling directive to prefetch data into the L2 cache.",
			  "For example, “ my_func.prefetch (y, 2)” will prefetch into the L2 cache,\n2 iterations worth of data needed in the ‘y’ loop.\n"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V73 Programmer's Reference Manual ...",
			"url": "https://device.report/m/9617176bfefffcbc1efdcc55512335e567e0d9a1c8a2813a1538de55ed99cf3b",
			"excerpts": [
			  "... instruction scheduling. 1.3 Technical ... 5.10.6.3 Software-based l2fetch The l2fetch instruction provides powerful L2 prefetching of data or instructions.Read more"
			]
		  },
		  {
			"title": "Advanced Scheduling Guide",
			"url": "https://documentation-be.hexagon.com/bundle/Advanced-Scheduling-Notes---2021.0/raw/resource/enus/Advanced%20Scheduling%20Notes%20-%202021.0.pdf?save_local=true",
			"excerpts": [
			  "Advanced Scheduling Guide. Master>Scheduling>Advanced Scheduling Rules. Run No: Create different run numbers to enable comparison of different settings and ...Read more"
			]
		  },
		  {
			"title": "Hexagon V68 Programmer s Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-46_REV_B_Hexagon_V68_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "The Hexagon processor includes the instruction dcfetch . This instruction queries the L1\ndata cache based on the address specified in the instruction:",
			  "If the address is present in the cache, no action is taken.",
			  "If the cache line for the address is missing, the processor attempts to fill the cache\nline from the next level of memory. The thread does not stall, but rather continues\nexecuting while the cache line fill occurs in the background."
			]
		  }
		],
		"reasoning": "The core concept of Scratch-Pad Memory (SPM) is well-supported by multiple excerpts. Several sources define SPM as a fast, software-managed on-chip SRAM that replaces hardware-managed caches, offering better performance, power efficiency, area cost, and time predictability. For instance, it is stated that SPM is a \"fast compiler-managed SRAM memory that replaces the hardware-managed cache\" and is motivated by \"better real-time guarantees vs cache and by its significantly lower overheads in energy consumption, area and overall runtime\" (e.g., excerpts 3032, 3035, 3036, 3020, 3001, 3002, 2996, 2994). This directly aligns with the core concept described in the `finegrained_field_value`.\n\nThe key technique of Double-Buffering (ping-pong buffering) is also well-supported. Excerpts describe it as a method to overlap computations and data transfers, using DMA to fetch data into one buffer while the CPU processes another (e.g., excerpts 3040, 3041, 3042, 3045, 3049). This technique is explicitly referred to as \"software pipelining\" to hide memory latency (excerpt 3040).\n\nRegarding allocation algorithms, the excerpts cover static, dynamic, and profile-guided approaches, as well as memory coloring. Static allocation is described as assigning data to SPM at compile time, which is simple but inflexible (excerpt 3103, 3068). Dynamic allocation involves compiler-inserted code to move data between SPM and off-chip memory during execution, adapting to runtime conditions (excerpts 3104, 3069, 3033). Udayakumaran and Barua's work is cited for proposing a \"highly predictable, low overhead and yet dynamic, memory allocation strategy for embedded systems with scratch-pad memory\" (excerpts 3032, 3025, 3026, 3029, 3013, 3014, 3015, 3016, 3017, 3018, 3009, 3010, 3011, 3012, 3034, 3037, 3038). Profile-guided allocation is mentioned in the context of using profile data to place frequently used data in fast memory (excerpt 3078). Memory coloring is explicitly defined as a \"general-purpose compiler approach... to assign static data aggregates, such as arrays and structs, in a program to an SPM,\" adapting graph-coloring algorithms (excerpts 2990, 2991, 2992, 2994, 2997, 2998, 3070, 3072, 3073, 2988).\n\nThe DMA scheduling pattern is also detailed. Excerpts explain that DMA allows data transfer in the background of CPU operation (excerpt 3850, 3111, 3113), with the CPU initiating the transfer and then performing other operations (excerpt 3113). Advanced DMA controllers, like the TI C6000's EDMA, can use \"ping and pong\" parameter sets and linked addresses to automatically start the next transfer, maximizing overlap (excerpts 3048, 3049, 3050). This ensures that the CPU is not stalled waiting for data (excerpt 3112, 3114, 3116, 3117).\n\nOverall, the provided excerpts offer comprehensive and consistent support for all aspects of the `finegrained_field_value`, demonstrating a high level of confidence.",
		"confidence": "high"
	  },
	  {
		"field": "machine_learning_in_scheduling.primary_approach",
		"citations": [
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "In this thesis, a novel reinforcement learning (RL) based model for the\ninstruction scheduling problem is developed including modelling features\nof processors such as forwarding, resource utilisation and treatment of the\naction space.",
			  "an RL agent that interacts with\nthe model of the problem is presented using three different types of graph\nneural networks for the state processing: graph conventional networks, graph\nattention networks, and graph attention based on the work of Lee *et al* .",
			  "The architecture consists of a state encoder based on a GNN, thereafter, two\nseparate additional neural networks are used for the policy network and value\nfunction network. The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "The results showed a minimal improvement over the heuristic agent.",
			  "2.7\nGraph Neural Networks (GNNs)",
			  "2.7.3\nGraph Embeddings",
			  "2.7.4\nGraph Convolutional Networks . . . . . . .",
			  "2.7.5\nGraph Attention Networks . . . . . . . .",
			  "*\n**Relevant Modern Machine Learning Work",
			  "*\n**Relevant Modern Machine Learning Work"
			]
		  },
		  {
			"title": "Machine Learning in Compiler Optimization",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-2.pdf",
			"excerpts": [
			  "In this thesis, deep RL and other machine learning methods are applied to solve complex\ncompiler optimization tasks: phase ordering, vectorization, scheduling, and cache allocation.",
			  "AutoPhase uses deep RL to generate better compiler phase orderings, a problem\nknown to be NP-hard.",
			  "NeuroVectorizer uses deep RL in a contextual bandit scheme for\nauto-vectorization.",
			  "To generate better program schedules, ProTuner uses MCTS and Ansor\nuses evolutionary algorithms.",
			  "RLDRM uses deep RL to optimize the cache allocated resources\nfor different applications.",
			  "Unlike prior state-of-the-art methods, our approaches do not rely on hand-engineered\nfeatures, greedy algorithms, or hand-written optimization, and can find better solutions in less\ntime."
			]
		  },
		  {
			"title": "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/abs/10.1145/3721289",
			"excerpts": [
			  "oSchedule method improves the heuristic **list scheduling (LS)** method [ [7]() ] by perceiving operation relations for customizing scheduling pri",
			  "L-based method [ [13]() ] uses timing matrices to record the current schedule, and the available ones. This method uses a convolutional neural network to perceive resource requirements among CLCs, and conduct scheduling with RL. The perceived resource requirements provide global program information at some level, contributing to higher performance than heuristic me",
			  "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning",
			  "uling determines the execution order and time of program operations. To perform scheduling, the program is generally transformed into graph-structured IR, such as DFG and **control data flow graph (CDFG)** . DFG shows the data flows of the program, and is generally used to represent the **basic blocks (BB)** . CDFG shows additional control flows among BBs, and is generally used to represent the whole pro"
			]
		  },
		  {
			"title": "Applying Knowledge-Guided Deep Reinforcement Learning with Graph Neural Networks for Compiler Optimization | Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering",
			"url": "https://dl.acm.org/doi/10.1145/3727648.3727682",
			"excerpts": [
			  "The agent selects optimization passes as actions, applies them to the LLVM IR, and generates a new IR",
			  "Experiments show up to a 2.82× improvement over heuristic algorithms (SA and GA) and 1.007×–2.06x gains over similar Deep RL methods (CORL)"
			]
		  },
		  {
			"title": "From Profiling to Optimization: Unveiling the Profile Guided Optimization",
			"url": "https://arxiv.org/html/2507.16649v1",
			"excerpts": [
			  "Indirect call promotion is enabled by value profiling: if the profile shows that an indirect function call (e.g. via function pointer or virtual call) almost always calls a specific target function, GCC will introduce a direct call check for that target (with guard) to avoid the indirect call overhead. In AutoFDO, the sample profile format also retains information about likely indirect call targets and their frequencies. This achieves a similar result to instrumentation-based indirect call promotion, but using sampled data."
			]
		  },
		  {
			"title": "A Survey on Compiler Autotuning using Machine Learning",
			"url": "https://arxiv.org/pdf/1801.04405",
			"excerpts": [
			  "Reinforcement learning (RL) is an area of machine learning which can not be classifed as supervised\nor unsupervised. It is inspired by behaviorist psychology and uses the notion of rewards or penalties\nso that a software agent interacts with an environment and maximizes his cumulative reward.",
			  "Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a\nnumber of diferent compiler optimization problems. These techniques primarily enhance the quality of the\nobtained results and, more importantly, make it feasible to tackle two main compiler optimization problems:\noptimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of\napplying optimizations).",
			  "Moss et al. [ 180 ] showed a machine learning process to construct heuristics for instruction\nscheduling, more specifcally, scheduling a straight-line code. They used static and IR features\nof the basic block with the SPEC benchmark to experimentally evaluate their approach by using\nGeometric mean as ftness function and fold-cross-validation."
			]
		  }
		],
		"reasoning": "Several passages explicitly describe using reinforcement learning techniques to optimize compiler scheduling, including models that interact with compiler IRs and execute scheduling decisions as part of the optimization loop. For example, passages describe an RL-based model that interacts with a DSP/IR scheduling environment, outlining policy/value networks and graph-processing variants for state encoding, which aligns with DRL-driven learning of scheduling policies. Other excerpts discuss the use of Graph Neural Networks to represent program dependencies or scheduling state, enabling graph-aware decision making within optimization and scheduling tasks. Together, these sources demonstrate a trend where learning-based approaches (especially DRL) leverage graph-structured representations to improve scheduling quality beyond traditional heuristics. Additional passages broaden the context to ML-guided compiler optimization, including ML-driven phase ordering, DAG-based dependency representations, and graph-centric scheduling strategies, which collectively reinforce the idea that DRL+GNN-based methods are at the forefront of modern instruction scheduling research. Based on the content, the strongest supports come from explicit DRL/RL scheduling studies and GNN-enabled scheduling representations; surrounding discussions about ML in compiler optimization provide corroborating context and demonstrate the maturation of this approach across related scheduling problems.",
		"confidence": "medium"
	  },
	  {
		"field": "machine_learning_in_scheduling.key_features",
		"citations": [
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "an RL agent that interacts with\nthe model of the problem is presented using three different types of graph\nneural networks for the state processing: graph conventional networks, graph\nattention networks, and graph attention based on the work of Lee *et al* .",
			  "The architecture consists of a state encoder based on a GNN, thereafter, two\nseparate additional neural networks are used for the policy network and value\nfunction network. The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "The results showed a minimal improvement over the heuristic agent.",
			  "2.7\nGraph Neural Networks (GNNs)",
			  "2.7.3\nGraph Embeddings",
			  "2.7.4\nGraph Convolutional Networks . . . . . . .",
			  "2.7.5\nGraph Attention Networks . . . . . . . .",
			  "*\n**Relevant Modern Machine Learning Work",
			  "*\n**Relevant Modern Machine Learning Work",
			  "In this thesis, a novel reinforcement learning (RL) based model for the\ninstruction scheduling problem is developed including modelling features\nof processors such as forwarding, resource utilisation and treatment of the\naction space."
			]
		  },
		  {
			"title": "Machine Learning in Compiler Optimization",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-2.pdf",
			"excerpts": [
			  "In this thesis, deep RL and other machine learning methods are applied to solve complex\ncompiler optimization tasks: phase ordering, vectorization, scheduling, and cache allocation.",
			  "AutoPhase uses deep RL to generate better compiler phase orderings, a problem\nknown to be NP-hard.",
			  "NeuroVectorizer uses deep RL in a contextual bandit scheme for\nauto-vectorization.",
			  "To generate better program schedules, ProTuner uses MCTS and Ansor\nuses evolutionary algorithms.",
			  "Unlike prior state-of-the-art methods, our approaches do not rely on hand-engineered\nfeatures, greedy algorithms, or hand-written optimization, and can find better solutions in less\ntime."
			]
		  },
		  {
			"title": "A Survey on Compiler Autotuning using Machine Learning",
			"url": "https://arxiv.org/pdf/1801.04405",
			"excerpts": [
			  "Moss et al. [ 180 ] showed a machine learning process to construct heuristics for instruction\nscheduling, more specifcally, scheduling a straight-line code. They used static and IR features\nof the basic block with the SPEC benchmark to experimentally evaluate their approach by using\nGeometric mean as ftness function and fold-cross-validation.",
			  "Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a\nnumber of diferent compiler optimization problems. These techniques primarily enhance the quality of the\nobtained results and, more importantly, make it feasible to tackle two main compiler optimization problems:\noptimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of\napplying optimizations).",
			  "Reinforcement learning (RL) is an area of machine learning which can not be classifed as supervised\nor unsupervised. It is inspired by behaviorist psychology and uses the notion of rewards or penalties\nso that a software agent interacts with an environment and maximizes his cumulative reward."
			]
		  },
		  {
			"title": "Reinforcement Learning Strategies for Compiler ...",
			"url": "https://www.bu.edu/caadlab/Shahzad22.pdf",
			"excerpts": [
			  "One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is ...Read more",
			  " *Agents* to maximize their long-term, cumulative\nrewards.",
			  "The overall reinforcement learning framework for a\nhigh level synthesis compiler is illustrated in Figure 1.",
			  "Reinforcement learning in this case requires specifying\nand tuning several components:",
			  "i) an *action space* that specifies appropriate actions\nthat are suggested by the agent. Code optimization in\ncompilers such as LLVM, is implemented as Passes.",
			  "achine learning (ML) offers one popular approach**\n**to automate finding optimal compiler passes but requires**\n**selecting the right "
			]
		  },
		  {
			"title": "Learning to Optimize Compiler Passes with Deep ...",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F18/projects/reports/project2_report_ver3.pdf",
			"excerpts": [
			  "In this paper, we explore the benefits of using deep reinforcement learning to achieve a better ordering of optimization\npasses in a mainstream compiler (LLVM). P",
			  "\nWe implement a framework that takes any group of program and intelligently finds an optimal sequence of passes\nthat optimiz"
			]
		  },
		  {
			"title": "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/abs/10.1145/3721289",
			"excerpts": [
			  "oSchedule method improves the heuristic **list scheduling (LS)** method [ [7]() ] by perceiving operation relations for customizing scheduling pri",
			  "L-based method [ [13]() ] uses timing matrices to record the current schedule, and the available ones. This method uses a convolutional neural network to perceive resource requirements among CLCs, and conduct scheduling with RL. The perceived resource requirements provide global program information at some level, contributing to higher performance than heuristic me",
			  "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning",
			  "uling determines the execution order and time of program operations. To perform scheduling, the program is generally transformed into graph-structured IR, such as DFG and **control data flow graph (CDFG)** . DFG shows the data flows of the program, and is generally used to represent the **basic blocks (BB)** . CDFG shows additional control flows among BBs, and is generally used to represent the whole pro"
			]
		  },
		  {
			"title": "Applying Knowledge-Guided Deep Reinforcement Learning with Graph Neural Networks for Compiler Optimization | Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering",
			"url": "https://dl.acm.org/doi/10.1145/3727648.3727682",
			"excerpts": [
			  "this paper models the selection process of optimization passes as a Markov decision process and proposes an optimization passes selection method based on deep reinforcement learning (RL)",
			  "a hybrid representation approach is designed by graph neural networks to enhance the program feature representation, which combines static program information and the structure features extracted from program control flow graphs",
			  "The agent selects optimization passes as actions, applies them to the LLVM IR, and generates a new IR",
			  "Experiments show up to a 2.82× improvement over heuristic algorithms (SA and GA) and 1.007×–2.06x gains over similar Deep RL methods (CORL)"
			]
		  },
		  {
			"title": "From Profiling to Optimization: Unveiling the Profile Guided Optimization",
			"url": "https://arxiv.org/html/2507.16649v1",
			"excerpts": [
			  "Indirect call promotion is enabled by value profiling: if the profile shows that an indirect function call (e.g. via function pointer or virtual call) almost always calls a specific target function, GCC will introduce a direct call check for that target (with guard) to avoid the indirect call overhead. In AutoFDO, the sample profile format also retains information about likely indirect call targets and their frequencies. This achieves a similar result to instrumentation-based indirect call promotion, but using sampled data.",
			  "Similarly, LLVM PGO use pass can turn indirect calls into direct calls for hot targets. Additionally, there is a late pass in code generation that might insert conditional jumps to likely targets. Much of this is handled in the IR PGO use pass itself by adding direct-call fast paths. The result is that by the time codegen happens, many indirect call sites have been optimized based on profile. This can yield significant speedups in C++ programs with virtual calls or function pointer calls.",
			  "Section Title: From Profiling to Optimization: Unveiling the Profile Guided Optimization > 5. Tools Evaluation > 5.2. Experimental Setup > Compilers & Modes:\nContent:\nWe compile all benchmarks with two production compiler toolchains: GCC 12 and LLVM 15, using the latest stable releases as of October 2023. We evaluate two instrumentation-based and sampilng-based profile-guided optimization modes on AMD64 and instrumentation-based PGO only on ARM architectures.",
			  "Instrumentation-based PGO in LLVM requires compiling with -fprofile-instr-generate , executing the instrumented binary to produce a .profraw file, merging raw data via llvm-profdata merge , and recompiling with -fprofile-instr-use=<merged.profdata> . In GCC, users compile with -fprofile-generate , execute to obtain .gcda counters, and recompile with -fprofile-use so that precise basic-block and edge counts guide optimization.",
			  "Sample-based PGO (AutoFDO) in LLVM is enabled by compiling with -gline-tables-only , running the binary under perf record -b to collect profiles, converts profiles via create_llvm_prof and recompiling with -fprofile-sample-use=<sample.profdata> . In GCC AutoFDO, one compile the program with -g1 , records perf data using perf record -b , converts it via create_gcov into an .afdo file, and compiles with -fauto-profile=profile.afdo to leverage heuristic frequency estimates.",
			  "\nSection Title: ... > 6.3. Compiler Adaptation: Intelligent Cross-Architecture and Dynamic Feedback\nContent:\nExisting compilers (e.g., GCC, LLVM) often ignore microarchitectural differences when applying profile data. For example, LLVM’s AutoFDO supports context-sensitive inlining but uses a uniform branch-probability model, neglecting how pipeline depth affects misprediction costs, causing optimization efficacy to vary across platforms. Traditional dual-build workflows (train-then-optimize) also fail to adapt to dynamic environments such as elastic cloud deployments.\n",
			  "Section Title: ... > 6.3. Compiler Adaptation: Intelligent Cross-Architecture and Dynamic Feedback\nContent:\nFuture profile techniques must evolve toward adaptive, cross-architecture PGO. For each hardware target, hardware-aware policies—driven by a microarchitecture database (caches, branch predictors, pipeline depths)—should dynamically tune code layout and transformation decisions. For instance, on AMD Zen’s deep pipelines, hot loops could be prioritized for unrolling to reduce stall penalties. Optimizers themselves could form a real-time feedback loop: like JIT mechanisms, they would continuously collect performance data at runtime and trigger incremental re-optimizations. Java HotSpot’s tiered compilation could be extended to AOT scenarios via background threads that periodically reconstruct hot code. On the IR side, designing a hardware-agnostic annotation system, decoupling profile data from architecture parameters, would allow a single profile to drive multi-target optimizations. LLVM’s Machine IR (MIR) already supports multi-target optimization and could integrate Profile-to-IR mapping rules for “profile once, optimize everywhere”."
			]
		  }
		],
		"reasoning": "The strongest support comes from excerpts that discuss machine learning-based instruction scheduling approaches and the specific features these ML models rely on. For example, several studies explore reinforcement learning (RL) approaches to scheduling with explicit mention of state representations and resource considerations, suggesting the model learns from architectural and runtime features. In particular, references describe an RL agent that models processor features such as forwarding, resource utilization, and the action space, indicating the kinds of features that should be fed into a scheduling model. The same group of excerpts describe graph neural networks (GNNs) used to encode program structure, including a state encoder and embeddings for resource utilization, which align with the need to capture the dependency graph and per-instruction characteristics (successors, predecessors, and data dependencies). Additional excerpts discuss AutoPhase and other ML-driven compiler optimization workflows, which provide context on how ML models are trained and integrated into compilers to optimize scheduling, vectorization, and cache/resource allocation, implying features like DAG-based dependency representations, graph embeddings, and hardware-aware signals. There are also references to profile-guided optimization (PGO) signals such as branch bias, cache miss hotspots, and memory stride statistics, which map directly to runtime behavior features. Together, these sources indicate a framework for building an ML-based scheduler for a 12/6/2/2/1 VLIW by providing: architectural feature representations (how many ALUs/slots are available), program structure representations (per-instruction dependencies, DAGs/DFGs, and successor/predecessor counts), and runtime signals (PGO data) as inputs to the ML model. The most relevant parts address (a) architectural/resource feature handling, (b) graph-based program representations for scheduling, and (c) utilization of PGO/runtime signals to guide scheduling decisions. The remaining ML-oriented compiler optimization literature provides broader context and methods for integrating ML-based schedulers into toolchains, reinforcing the suitability of the features described above for the target field value.",
		"confidence": "high"
	  },
	  {
		"field": "machine_learning_in_scheduling.mechanism",
		"citations": [
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "In this thesis, a novel reinforcement learning (RL) based model for the\ninstruction scheduling problem is developed including modelling features\nof processors such as forwarding, resource utilisation and treatment of the\naction space.",
			  "2.7\nGraph Neural Networks (GNNs)",
			  "2.7.3\nGraph Embeddings",
			  "2.7.4\nGraph Convolutional Networks . . . . . . .",
			  "2.7.5\nGraph Attention Networks . . . . . . . .",
			  "*\n**Relevant Modern Machine Learning Work",
			  "*\n**Relevant Modern Machine Learning Work",
			  "The MDP can take place as an isolated instruction scheduling problem or it\ncan be integrated into the compiler directly.",
			  "The MDP can take place as an isolated instruction scheduling problem or it\ncan be integrated into the compiler directly.",
			  "The compiler in the loop MDP\nversion, however, is a far bigger engineering challenge.",
			  "The compiler in the loop MDP\nversion, however, is a far bigger engineering challenge."
			]
		  },
		  {
			"title": "Machine Learning in Compiler Optimization",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-2.pdf",
			"excerpts": [
			  "In this thesis, deep RL and other machine learning methods are applied to solve complex\ncompiler optimization tasks: phase ordering, vectorization, scheduling, and cache allocation.",
			  "AutoPhase uses deep RL to generate better compiler phase orderings, a problem\nknown to be NP-hard.",
			  "NeuroVectorizer uses deep RL in a contextual bandit scheme for\nauto-vectorization."
			]
		  },
		  {
			"title": "Learning to Optimize Compiler Passes with Deep ...",
			"url": "https://people.eecs.berkeley.edu/~kubitron/courses/cs262a-F18/projects/reports/project2_report_ver3.pdf",
			"excerpts": [
			  "In this paper, we explore the benefits of using deep reinforcement learning to achieve a better ordering of optimization\npasses in a mainstream compiler (LLVM). P",
			  "\nWe implement a framework that takes any group of program and intelligently finds an optimal sequence of passes\nthat optimiz"
			]
		  },
		  {
			"title": "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/abs/10.1145/3721289",
			"excerpts": [
			  "oSchedule method improves the heuristic **list scheduling (LS)** method [ [7]() ] by perceiving operation relations for customizing scheduling pri",
			  "L-based method [ [13]() ] uses timing matrices to record the current schedule, and the available ones. This method uses a convolutional neural network to perceive resource requirements among CLCs, and conduct scheduling with RL. The perceived resource requirements provide global program information at some level, contributing to higher performance than heuristic me",
			  "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning",
			  "uling determines the execution order and time of program operations. To perform scheduling, the program is generally transformed into graph-structured IR, such as DFG and **control data flow graph (CDFG)** . DFG shows the data flows of the program, and is generally used to represent the **basic blocks (BB)** . CDFG shows additional control flows among BBs, and is generally used to represent the whole pro"
			]
		  }
		],
		"reasoning": "The finegrained field value describes a scheduling mechanism where an RL agent uses rich DAG/GNN embeddings to decide the next instruction to schedule, optimizing the schedule quality in a compiler-in-the-loop or supervised setting that can imitate an optimal solver like TDCDP. Excerpts that explicitly discuss machine-learning-based instruction scheduling for DSPs or general architectures show the use of RL models, DAG/GNN representations, and encoder-based state processing, which directly support the claim of an ML-based MDPM-based scheduling paradigm. Excerpts that describe MDPlike formulations or the possibility of integrating ML schedulers into a compiler pipeline corroborate the mechanism of an environment that provides rewards and learns to schedule within the compiler. Additionally, several excerpts discuss graph neural networks or graph embeddings as core parts of the state representation for scheduling, which matches the idea of using embeddings to capture complex instruction relationships and dependencies. Other ML-centric papers show how ML-driven schedulers can be evaluated, compared to TDCDP or integrated into compilers, which further substantiates the described mechanism. Collectively, these excerpts establish that an RL agent uses DAG/GNN-derived embeddings to select the next instruction, with a reward signal tied to schedule quality, and with training options including fast simulators or compiler-in-loop setups.",
		"confidence": "high"
	  },
	  {
		"field": "key_architectural_feature_recommendations.description",
		"citations": [
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "The Itanium architecture\nallows the renaming of registers which provide every iteration with its own set of\nregisters, avoiding the need for unrolling. This kind of register renaming is called\nregister rotation."
			]
		  },
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "rotating registers",
			  "Registers rotate for each loop iteration",
			  "General, predicate, FP registers rotate",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers."
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed."
			]
		  },
		  {
			"title": "Class 12 Software Pipelining - Modulo Scheduling",
			"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
			"excerpts": [
			  " Rotating registers\n"
			]
		  },
		  {
			"title": "SMT/VLIW/EPIC, Statically Scheduled ILP",
			"url": "http://class.ece.iastate.edu/tyagi/cpre581/lectures/LecVLIWEPIC.pdf",
			"excerpts": [
			  "Cydrome Cydra-5 (1987)**\n **7 operations encoded in 256-bit instruction word**\n **rotating register file**"
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining"
			]
		  }
		],
		"reasoning": "The target fine-grained field value describes a hardware-based rotating register mechanism where a loop iteration counter (IR) causes a subset of the architectural register file to map to different physical registers each iteration. The most direct matches confirm this hardware behavior: a reference states that the Itanium architecture allows register rotation via a rotating-register mechanism, and that by adding a hardware base and performing rotation, an instruction can reference a different physical register each iteration. This supports the idea that the hardware itself renames registers across iterations, which aligns with using an Iteration Register to shift through a rotating set of registers. Additional excerpts explain that IA-64 provides rotating registers for general, FP, and predicate registers, and that rotation is implemented so a new physical register is presented for each loop iteration. There are also notes describing how modulo scheduling on IA-64 leverages rotating registers to simplify code layout and provide per-iteration renaming, which corroborates the hardware-driven nature of the rotation. Other excerpts discuss rotating register files and the concept of a rotating register per iteration, reinforcing the core idea that the hardware renames registers cyclically as iterations advance, facilitated by an IR-like mechanism. In sum, the cited materials collectively substantiate the existence and behavior of a hardware mechanism where a rotating subset of the register file is automatically renamed each iteration, yielding a unique physical register for each loop iteration as described in the field value.",
		"confidence": "high"
	  },
	  {
		"field": "key_architectural_feature_recommendations.relevance_to_user_problem",
		"citations": [
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register"
			]
		  },
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "The Itanium architecture\nallows the renaming of registers which provide every iteration with its own set of\nregisters, avoiding the need for unrolling. This kind of register renaming is called\nregister rotation."
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed."
			]
		  },
		  {
			"title": "Class 12 Software Pipelining - Modulo Scheduling",
			"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
			"excerpts": [
			  " Rotating registers\n"
			]
		  },
		  {
			"title": "SMT/VLIW/EPIC, Statically Scheduled ILP",
			"url": "http://class.ece.iastate.edu/tyagi/cpre581/lectures/LecVLIWEPIC.pdf",
			"excerpts": [
			  "Cydrome Cydra-5 (1987)**\n **7 operations encoded in 256-bit instruction word**\n **rotating register file**"
			]
		  }
		],
		"reasoning": "Rotating registers are described as renaming a set of registers in a rotating fashion across iterations, which effectively gives each iteration its own fresh registers and prevents cross-iteration write-after-read conflicts. This directly addresses the user’s concern about loop-carried dependencies in tree traversals by enabling the same instruction to operate on a new register per iteration, thereby removing false dependencies that would otherwise throttle instruction throughput. In practical terms, this mechanism compresses the prologue/kernel/epilogue of a pipelined loop and supports tighter initiation intervals, which is precisely the goal of hiding memory latency and improving ILP on a VLIW architecture. The Itanium/IA-64 literature consistently frames modulo scheduling and rotating registers as core architectural support for software-pipelined loops, illustrating how the hardware provides a fresh logical register for each iteration and how that facilitates high-performance, compact code. Likewise, discussions of register rotation in IA-64 notes, manuals, and lectures reinforce that rotating registers enable one new register per loop iteration, making it easier to reference a different physical register each cycle and hence reduce hazards and code bloat. Extended sources also articulate how modulo scheduling and rotating registers work together with predication to further optimize loops, particularly when the loop body includes long dependency chains or irregular memory access patterns—precisely the “hard” cases the user wants to optimize. In summary, rotating registers are portrayed as the practical, hardware-assisted form of register renaming that directly mitigates cross-iteration hazards, enables lower II, and improves throughput in software-pipelined, VLIW-like loops; this aligns with the user’s objective of interleaving independent computations to fill idle slots and overcome a long dependency chain bottleneck.",
		"confidence": "high"
	  },
	  {
		"field": "key_architectural_feature_recommendations.benefit_for_vliw",
		"citations": [
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  "The algorithm of modulo variable expansion is as follows.\nFirst, we identify those variables that are redefined at the\nbeginning of every iteration.\nNext, we pretend that every",
			  "Modulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. The small set of values can even reside in register\nfiles, cutting down on both the memory traffic and the latency\nof the computation.",
			  "odulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. ",
			  "e i&a of modulo variable expansion can be illustrated by\nthe following code fragment, where a value is written into a\nregister and used two cycles later:\nDef(Rl)\nOP\nuse (Rl)\nIf the same register is used by all iterations, then the write\noperation of an iteration cannot execute before the read opera-\ntion in the preceding iteration.\nTherefore, the optimal\nthroughput is limited to one iteration every two cycles. This\ncode can be sped up by using different registers in alternating\niterations:\nDef (Rl)\nOP\nDef (R2)\nL : u*r (Rl)\nop\nDef (Rl)\nUae(R2)\nop\nDef(R2)\nCJ'ump L\nUes(R1)\nop\nUse (R2)\nWe call this optimization of allocating multiple registers to\na variable in the loop module variable eqansion.",
			  "Without modulo variable expansion, the length of the\nsteady state of a pipelined loop is simply the initiation inter-\nval. #en\nmodulo variable expansion is applied, code se-\nquences for consecutive iterations differ in the registers used,\nthus lengthening the steady state. If there are n repeating code\nsequences, the steady state needs to be unrofled n times."
			]
		  }
		],
		"reasoning": "Rotating registers are described as an architectural feature that renames registers across iterations, enabling the compiler or hardware to reference a new logical register each cycle without rewriting code. This directly supports software-pipelined loops by providing a mechanism to recycle registers across iterations, which is a core aspect of modulo scheduling and SWP without incurring extra copy or unrolling overhead. The Itanium/IA-64 sources explicitly state that registers rotate and that modulo scheduling is directly supported on architectures with rotating registers, predicates, and loop-count constructs, creating compact, high-performance code for pipelined loops. This establishes rotating registers as the hardware-side enabler for SWP in VLIW contexts and clarifies that they can reduce or eliminate the need for more invasive transformations like modulo variable expansion (which reuses registers across iterations but is more invasive to implement). Additional discussions corroborate that software pipelining is a practical technique for VLIW machines and that modulo variable expansion is an optimization that leverages such hardware features, further tying rotating registers to effective VLIW scheduling. Direct statements about rotating registers allowing data to move to adjacent registers across iterations illustrate the exact lifetime management benefit that underpins efficient modulo scheduling. The combination of explicit rotation mechanics and the documented use of rotating registers to simplify SWP loops provides strong, coherent support for the stated field value.",
		"confidence": "high"
	  },
	  {
		"field": "machine_learning_in_scheduling.safeguards",
		"citations": [
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "In this thesis, a novel reinforcement learning (RL) based model for the\ninstruction scheduling problem is developed including modelling features\nof processors such as forwarding, resource utilisation and treatment of the\naction space.",
			  "an RL agent that interacts with\nthe model of the problem is presented using three different types of graph\nneural networks for the state processing: graph conventional networks, graph\nattention networks, and graph attention based on the work of Lee *et al* .",
			  "The architecture consists of a state encoder based on a GNN, thereafter, two\nseparate additional neural networks are used for the policy network and value\nfunction network. The architecture also includes an additional smaller model\nto handle the resource utilisation stages embeddings which are inputs to the\nstate encoder.",
			  "The results showed a minimal improvement over the heuristic agent."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/full/10.1145/3643135",
			"excerpts": [
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling.",
			  "This article proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling.",
			  "The TDCDP approach achieves near-optimal solutions within an acceptable time overhead."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two ...",
			"url": "https://dl.acm.org/doi/10.1145/3643135",
			"excerpts": [
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling.",
			  "Post-RA Scheduling can effectively reduce the impact of load/store instructions. Additionally, it enhances target code by increasing ILP, optimizing code layout, and eliminating instruction conflicts.",
			  "During the Pre-RA Scheduling, the scheduler aims to optimize the utilization of functional units while ensuring that the width of the long instruction word remains within the designated limit.",
			  "This article proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling.",
			  "The optimization workflow, as shown in Figure [5]() , involves several steps. Firstly, the input programs are divided into multiple basic blocks. The analyzer extracts information from instructions and establishes dependency relationships among them. These dependencies can be effectively represented using a DAG."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3643135",
			"excerpts": [
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling.",
			  "When the required number of registers exceeds the available allocation limit, the compiler must insert load/store instructions to spill code.",
			  "However, it is important to note that increasing ILP also raises the risk of code spills.",
			  "The Pre-RA Scheduling focuses on reordering instructions to maximize ILP, while the Post-RA Scheduling primarily addresses issues related to physical register contention and data dependencies, with the goal of further enhancing the quality of the generated code.",
			  "Post-RA Scheduling can effectively reduce the impact of load/store instructions. Additionally, it enhances target code by increasing ILP, optimizing code layout, and eliminating instruction conflicts.",
			  "To mitigate this issue, we adopt the architecture of modern compilers, which employs two phases for instruction scheduling: Pre-RA Scheduling and Post-RA Scheduling."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | Request PDF",
			"url": "https://www.researchgate.net/publication/377718811_Optimizing_VLIW_Instruction_Scheduling_via_a_Two-Dimensional_Constrained_Dynamic_Programming",
			"excerpts": [
			  "The TDCDP approach achieves near-optimal solutions within an acceptable time overhead."
			]
		  }
		],
		"reasoning": "The strongest support for ML-driven scheduling with explicit resource-awareness comes from excerpts describing reinforcement-learning-based instruction scheduling where the model’s state encodes resource utilization and the policy/value networks drive decisions. This aligns with the idea of including the current resource load (such as register pressure) as a feature to guide scheduling, enabling pressure-aware decisions and helping manage multi-objective trade-offs such as cycle reduction versus register usage. Additionally, several excerpts discuss using ML and learned models within compiler optimization, including how graphs and embeddings of resource utilization inform scheduling decisions, which directly underpins the concept of multi-objective rewards that penalize excessive register pressure or code growth. The rationale for a safeguard that operates after register allocation (Post-RA scheduling) is backed by explicit notes that modern schedules are implemented with two-phase preprocessing and post-RA stages, and that Post-RA scheduling can reduce register-related issues and improve ILP while avoiding register-contention disorders. Beyond ML-based safeguards, there is substantial material on traditional TDCDP/ILP-style scheduling and the integration of time-slot and functional-unit decisions, with emphasis on constraint-driven approaches that reduce local optima and keep time overhead acceptable; such techniques provide a practical foundation for implementing safeguard policies (e.g., constrain scheduling to respect physical registers after allocation). The combination of ML-based state representations that include resource utilization, the explicit ML-driven or hybrid approaches to scheduling, and the established practice of Post-RA scheduling together support a safeguarding framework that uses multi-objective rewards, register-pressure awareness, and post-allocation constraints to prevent adverse side effects such as register spills and code bloat, while still pursuing aggressive instruction-level parallelism.",
		"confidence": "medium"
	  },
	  {
		"field": "executive_summary",
		"citations": [
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "The hyperblock is an efficient structure to utilize predicated execution for both compiletime optimization and scheduling and preliminary experimental ...",
			  "TLDR",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "(PDF) Hierarchical Pipelining of Nested Loops in High-Level Synthesis",
			"url": "https://www.researchgate.net/publication/268578277_Hierarchical_Pipelining_of_Nested_Loops_in_High-Level_Synthesis",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code. This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained. The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete.",
			  "The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined.",
			  "It also diminishes the start-up cost of loops with small number of iterations.",
			  "The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors.",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining ...",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "e authors would like to thank Wen-mei Hwu, Paul\nRyan, Jeff Baxter, and all members of the IMPACT\nre-\nsearch group for their comments and suggestions. The au-\nthors would also like to acknowledge Scott Mahlke, David\nLin, and William\nChen for their contributions\nof com-\npiler support for predicated execut",
			  "cated Execution is an architecture**\n**feature that supports conditional execution by providing**\n**predicate registers that hold the condition for execution**\n**of the oper",
			  "he Modulo Scheduling technique first determines a**\n**tight lower bound on II based on the resource usages and**\n**cross-iteration dependence ",
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  "n the approxi-\nmately equivalent performance of these two techniques, fu-\nture designers can determine the more cost-effective hard-\nware implementatio",
			  "The EMS algorithm consists of five steps:",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  "*like Hierarchical Reduction, the code must be regenerated**\n**by inserting conditional branches after modulo scheduling.*",
			  "MS), combines\nthe benefits of the previous Modulo Scheduling techniques,\nIf-conversion with Predicated Execution and Hierarchical\nReduction, to remove the limitations of each technique",
			  "f-\nconversion eliminates the need for prescheduling the paths\nof the conditional construct, which is essential to Hierar-\nchical Reductio",
			  "ll three Modulo Scheduling techniques and GURPR*\nhave been implemented in a prototype compiler.",
			  "r exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reductio"
			]
		  },
		  {
			"title": "Improving Scalability of Exact Modulo Scheduling with ...",
			"url": "https://www.csl.cornell.edu/~zhiruz/pdfs/sdsp-dac2019.pdf",
			"excerpts": [
			  "To handle\nresource constraints, SDC-based modulo scheduling [ 4 , 15 ] rely on\nstepwise legalization of the non-resource-constrained SDC schedule\nagainst the MRT to heuristically derive a solution, resulting in no\nguarantee on optimality.",
			  "ILP-based Formulation",
			  "In place of heuristics, integer linear programming (ILP) can be ap-\nplied to exactly model the modulo scheduling proble",
			  "Eichenberger\nand Davidson [ 9 ] leverages binary variable *a* *i* , *r* to indicate whether\noperation *i* is scheduled in time slot *r* in order to encode the modul",
			  "Oppermann et al. [ 11 ] improves upon the resource handling\ncapability of Eichenberger and Davidson by using binary variables to\nrepresent resource and time slot overlap instead of the actual modulo\nschedule.",
			  "ticular, they propose to use binary overlap variable\n*ϵ* *ij* to denote whether operation *i* ’s resource instance index is strictly\nless than *j* ’s index, and *µ* *ij* to denote whether operation *i* is executed\nin a time slot strictly earlier than the time slo",
			  " vari-\nables are in turn constrained with resource index variable *r* *i* , which\ndenotes the index of resource instance used by operation *i* , and time\nslot variable *s* *i* , which denotes the modulo time slot of operation *i* , as\nin Eichenberger and David"
			]
		  },
		  {
			"title": "Register-sensitive software pipelining",
			"url": "https://ieeexplore.ieee.org/document/669910/",
			"excerpts": [
			  "We propose an integrated approach for register-sensitive software pipelining. In this approach, the heuristics proposed in the stage scheduling method of A.E. Eichenberge... Show More",
			  "We propose an integrated approach for register-sensitive software pipelining. In this approach, the heuristics proposed in the stage scheduling method of A.E. Eichenberger and E.S. Davidson (1995) are integrated with the iterative scheduling method to obtain schedules with high initiation rate and low register requirements. The performance of our integrated software pipelining method was analyzed for a large number of loops taken from a variety of scientific benchmark programs.",
			  "Published in:** [Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing](/xpl/conhome/5485/proceedin",
			  "ate of Conference:** 30 March 1998 - 03 April 199",
			  "Date Added to IEEE *Xplore* :** 06 August 2002\n*",
			  "Print ISBN:** 0-8186-8404-6\n*",
			  "Print ISSN:** 1063-7133",
			  "DOI:** [10.1109/IPPS.1998.669910](https://doi.org/10.1109/IPPS.1998.669910)",
			  "Conference Location:** Orlando, FL, USA",
			  "Authors"
			]
		  },
		  {
			"title": "of Full and Partial Predicated Execution Support for ILP ...",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15745-s06/web/handouts/mahlke-isca95.pdf",
			"excerpts": [
			  "Scott A. Mahlke*",
			  "Richard\nE. Hank",
			  "James E. McCormick",
			  "David\nI. August",
			  "Wen-mei\nW. Hwu",
			  "Center for Reliable\nand High-Performance\nComputing",
			  "University\nof Illinois\nUrbana-Champaign,\nIL 61801",
			  "One can effectively\nutilize\npredicated\nexecution\nto improve\nbranch handling\nin instruction-level\nparallel\nprocessors.",
			  "though\nthe\npotential\nbenefits\nof predicated\nexecution\nare\nhigh, the tradeoffs\ninvolved\nin the design of an instruction\nset\nto support\npredicated\nexecution\ncan be difficult.\nO",
			  "partial\npredicated\nexecution\nsupport,\nsuch as conditional\nmoves, requires\nvery\nlittle\nchange to existing\narchitectures.",
			  "With\nour current\ncompiler\ntechnology,\nwe show that\nthe\ncompiler\ncan use both\npartial\nand full\npredication\nto\nachieve speedup in large control-intensive\nprograms.",
			  "In this section, the hyperblock compilation techniques for full predicate support are first summarized. Then, the trans- formation techniques to generate.Read more",
			  "Abstract"
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "wu93] the superblock is proposed as a mechanism for facilitating trace scheduling compilation.\nA superblock is simply a trace with all joins removed via tail duplication (fig",
			  "It is argued that\nthe primary benefit of superblocks is avoidance of the “complex bookkeeping” involved with\nmoving code past joins.",
			  "Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates.\nFor example, the branch in an if-then-else structure can be replaced with an instruction which\ndefines",
			  "ted by branches.\nAn alternate approach is to attempt to eliminate branches altogether. **Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates",
			  "**Predicated Execution and Enhanced Modulo Scheduling**",
			  "**\nOne of the objections to hierarchical reduction of conditional constructs is its rigidity. Since\nconditionals are scheduled separately without any regard to the resource requirements of the rest of\nthe loop, the resulting reduced nodes may have complex resource usages that create artificial\nconflicts and may prevent a schedule from being found. In [Warter93a] another approach is\nsuggested wherein conditional branches are replaced by predicated execution, as described in section\n"
			]
		  },
		  {
			"title": "\nAn If-Conversion Algorithm Based on Predication Execution",
			"url": "https://scialert.net/fulltext/?doi=itj.2010.984.988",
			"excerpts": [
			  "A hyperblock is a structure created to facilitate optimization and scheduling for predicated architectures.",
			  "A hyperblock is a set of predicated basic blocks in which control may only enter from the top, but may exit from one or more locations.",
			  "Hyperblocks are formed by applying tail duplication and if-conversion over a set of carefully selected paths.",
			  "Inclusion of a path into a hyperblock is done by considering its profitability.",
			  "The profitability is determined by the following factors: resource utilization, invalid instructions, branch instructions inherent instruction dependence, branch execution frequency, the balance of branch execution and so on.",
			  "Not only can if-conversion eliminate the branch and provide continuous instruction stream, it can also extend the length of basic block and improve the effective parallelism of code."
			]
		  },
		  {
			"title": "the-partial-reverse-if-conversion-framework-for-balancing- ...",
			"url": "https://scispace.com/pdf/the-partial-reverse-if-conversion-framework-for-balancing-3439qyi908.pdf",
			"excerpts": [
			  "14] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, R. A. Bringmann, and W. W. Hwu, “Effective compiler support for\npredicated execution using the hyperblock,” in *Proceedings of the 25th International Symposium on Microarchitecture* ,\npp. 45–54, December 1992",
			  "[15] S. A. Mahlke, R. E. Hank, J. McCormick, D. I. August, and W. W. Hwu, “A comparison of full and partial predicated\nexecution support for ILP processors,” in *Proceedings of the 22th International Symposium on Computer Architecture* ,\npp. 138–150, June 1995",
			  "13] J. C. Dehnert, P. Y. Hsu, and J. P. Bratt, “Overlapped loop support in the Cydra 5,” in *Proceedings of the Third*\n*International Conference on Architectural Support for Programming Languages and Operating Systems* , pp. 26–38,\nApril 1989"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol1.pdf",
			"excerpts": [
			  "Predication is the conditional execution of instructions. Conditional execution is implemented\nthrough branches in traditional architectures. IA-64 implements this function through the use of\npredicated instructions. Predication removes branches used for conditional execution resulting in\nlarger basic blocks and the elimination of associated mispredict penalties.",
			  "\nThe IA-64 pseudo-code shown below implements the C expressions without branches:\ncmp.ne p1,p2=a,r0\n// p1 <- a != 0\ncmp.ne p3,p4=e,r0 ;;// p3 <- e != 0\n(p1)add\nb=c,d\n ... ",
			  "Predication is the conditional execution of an instruction based on a qualifying predicate. A\nqualifying predicate is a predicate register whose value determines whether the processor commits\nthe results computed by an instruction.",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction. The check instruction must specify the same register number as the\ncorresponding advanced load.",
			  "When a check instruction is executed, the ALAT is searched for an entry with the same target\nphysical register number and type. If an entry is found, execution continues normally with the next\ninstruction.",
			  "cating and Checking ALAT Entries**\nAdvanced loads perform the following actions:\n1.\nThe ALAT register tag for the advanced load is computed. (For ldfp.a , a tag is computed\nonly for the first target register.)\n2.\nIf an entry with a matching ALAT register tag exists, it is removed.\n3.\nA new entry is allocated in the ALAT which contains the new ALAT register tag, the load\naccess size, and a tag derived from the physical memory address.\n4.\nThe value at the address specified in the advanced load is loaded into the target register and,\nif specified, the base register is updated and an implicit prefetch is performed.",
			  "Data speculation allows loads to be moved above possibly conflicting memory references.",
			  ") forms. When an advanced load is executed, it allocates an entry in a structure called the\nAdvanced Load Address Table (ALAT).",
			  "Later, when a corresponding check instruction is executed,\nthe presence of an entry indicates that the data speculation succeeded; otherwise, the speculation\nfailed and one of two kinds of compiler-generated recovery is performed:",
			  "The check instruction must specify the same register number as the\ncorresponding advanced load.",
			  "The advanced load check ( chk.a ) is used when an advanced load and several instructions\nthat depend on the loaded value are scheduled before a store that is ambiguous relative to the\nadvanced load.",
			  ". In both control and data speculation, the\ncompiler exposes ILP by issuing an operation early and removing the latency of this operation from\ncritical path. T",
			  "The compiler will issue an operation speculatively if it is reasonably sure that the\nspeculation will be beneficial.",
			  "To be beneficial two conditions should hold: it must be\n(1) statistically frequent enough that the probability it will require recovery is small, and (2) issuing\nthe operation early should expose further ILP-enhancing optimization.",
			  "Speculation is one of the\nprimary mechanisms for the compiler to exploit statistical ILP by overlapping, and therefore\n ..",
			  "Special mechanisms are provided to allow for compiler-directed speculation. This speculation takes\ntwo forms, control speculation and data speculation, with a separate mechanism to support each.",
			  "If no matching entry is found, the speculative results need to be recomputed:",
			  "ata speculation in IA-64 uses a special load instruction ( ld.a ) called an *advanced load*\ninstruction and an associated check instruction ( chk.a or ld.c ) to validate data-specul",
			  "When the ld.a instruction is executed, an entry is allocated in a hardware structure called the\nAdvanced Load Address Table (ALAT).",
			  "The ALAT is indexed by physical register number and\nrecords the load address, the type of the load, and the size of the load.",
			  "The check detects a deferred exception or a conflict with an intervening store and provides a\nmechanism to recover from failed speculation.",
			  "A new entry is allocated in the ALAT which contains the new ALAT register tag, the load\naccess size, and a tag derived from the physical memory address.",
			  "The value at the address specified in the advanced load is loaded into the target register and,\nif specified, the base register is updated and an implicit prefetch is performed.",
			  "Support for this is enabled by special NaT bits that are attached to integer registers and by special\nNatVal values for floating-point registers.",
			  "When a speculative load causes an exception, it is not\nimmediately raised.",
			  "Instead, the NaT bit is set on the destination register (or NatVal is written into\n ..",
			  "Data speculation in IA-64 uses a special load instruction ( ld.a ) called an *advanced load*\ninstruction and an associated check instruction ( chk.a or ld.c ) to validate data-speculated\nresult",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and floating-point pair\n( ldfp.a ) forms.",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction.",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks to operate\ncorrectly."
			]
		  },
		  {
			"title": "chapter 15 - the ia-64 architecture",
			"url": "https://www.cs.helsinki.fi/u/kerola/tikra/ch15.pdf",
			"excerpts": [
			  "With **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (true).\nThis enables the processor to speculatively execute both branches of an if\nstatement and only commit after the condition is determined.\n�",
			  " With **control speculation** , a load instruction is moved earlier in the program\nand its original position replaced by a check instruction. The early load saves\ncycle time; if the load produces an exception, the exception is not activated\nuntil the check instruction determines if the load should have been taken.\n",
			  " With **data speculation** , a load is moved before a store instruction that might\n ",
			  "**CHAPTER 15 / THE IA-64 ARCHITECTURE**\n**KEY POINTS**\n� The IA-64 instruction set architecture is a new approach to providing hard-\nware support for instruction-level parallelism and is significantly different that\nthe approach taken in superscalar architectures.\n� The most noteworthy features of the IA-64 architecture are hardware support\nfor predicated execution, control speculation, data speculation, and software\npipelining.\n� With **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (true).\nThis enables the processor to speculatively execute both branches of an if\nstatement and only commit after the condition is determined.\n�",
			  "**Figure 15.3**\nIA-64 Predication and Speculative Loading\n**552**\n**CHAPTER 15 / THE IA-64 ARCHITECTURE**",
			  "\nThe basic concepts underlying IA-64 are as follows:\n• Instruction-level parallelism that is explicit in the machine instructions rather\nthan being determined at run time by the processor\n• Long or very long instruction words (LIW/VLIW)\n• Branch predication (not the same thing as branch prediction)\n• Speculative loading\nIntel and HP refer to this combination of concepts as explicitly parallel instruc-\ntion computing (EPIC). ",
			  "oftware pipelining** is a technique in which instructions from multiple itera-\ntions of a loop are enabled to execute in paralle",
			  "ntel and HP refer to this combination of concepts as explicitly parallel instruc-\ntion computing (EPIC).",
			  "Itanium organization blends superscalar features with support for\nthe unique EPIC-related IA-64 features.",
			  "he Ita-\nnium includes nine execution units: two integer, two floating-point, two memory,\nand three branch execution units",
			  "ith **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (tru",
			  "ith **predicated execution** , every IA-64 instruction includes a reference to a\n1-bit predicate register and only executes if the predicate value is 1 (tru",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier; advanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nld8.c r6 = [r8]\n// Cycle 0; check load\nadd r5 = r6, r7 ;;\n// Cycle 0\nst8 [r18] = r5\n// Cycle 1\nHere we have moved the ld instruction earlier and converted it into an\nadvanced load. In addition to performing the specified load, the ld8.a instruction\nwrites its source address (address contained in r8) to a hardware data structure\nknown as the Advanced Load Address Table (ALAT). Each IA-64 store instruc-\ntion checks the ALAT for entries that overlap with its target address; if a match is\nfound, the ALAT entry is removed. When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c. When the check load is executed, it checks the ALAT\nfor a matching address. If one is found, no store instruction between the advanced\nload and the check load has altered the source address of the load, and no action is\ntaken. However, if the check load instruction does not find a matching ALAT entry,\n ...",
			  "A-64 overcomes this problem with the use of a tech-\nnique known as advanced load",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier; advanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0",
			  "add r5 = r6, r7 ;;\n// Cycle 0",
			  "st8 [r18] = r5\n// Cycle 1",
			  "When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c. When the check load is executed, it checks the ALAT\nfor a matching address. If one is found, no store instruction between the advanced\nload and the check load has altered the source address of the load, and no action is\ntaken. However, if the check load instruction does not find a matching ALAT entry,",
			  "ld8.c r6 = [r8]\n// Cycle 0; check load"
			]
		  },
		  {
			"title": "THE IA-64 ARCHITECTURE",
			"url": "https://www2.seas.gwu.edu/~mlancast/cs211al/reference/21-IA64.pdf",
			"excerpts": [
			  "With **predicated execution** , every IA-64 instruction includes a refer-\nence to a 1-bit predicate register, and only executes if the predicate\nvalue is 1 (true). This enables the processor to speculatively execute\nboth branches of an if statement and only commit after the condition\nis determined.",
			  "The IA-64 instruction set architecture is a new approach to providing\nhardware support for instruction-level parallelism and is significantly\ndifferent than the approach taken in superscalar architectures.",
			  "The most noteworthy features of the IA-64 architecture are hardware\nsupport for predicated execution, control speculation, data speculation,\nand software pipelining.",
			  "ith **data speculation** , a load is moved before a store instruction that\nmight alter the memory location that is the source of the load.A subse-\nquent check is made to assure that the load receives the proper memory\nval",
			  "A speculative load (ld.s) executes the memory fetch, performs exception de-\ntection, but does not deliver the exception (call the OS routine that handles\nthe exception).This ld.s instruction is hoisted to an appropriate point earlier in\nthe program.",
			  "A checking instruction (chk.s) remains in the place of the original load and de-\nlivers exceptions. This chk.s instruction may be predicated so that it will only\nexecute if the predicate is true.",
			  "If the ld.s detects an exception, it sets a token bit associated with the target\nregister, known as the *Not a Thing* (NaT) bit.",
			  "The term *hoist*\nis used to refer to the movement of a load instruction to a point earlier in the\ninstruction stream.",
			  "Another key innovation in IA-64 is control speculation, also known as speculative\nloading. This enables the processor to load data from memory before the program\nneeds it, to avoid memory latency delays.",
			  "IA-64 guarantees that if a branch is\ntaken, later instructions, even in the same instruction group, are not executed.",
			  "With data speculation, a load is moved before a store instruction that might alter the memory location that is the source of the load. A subse- quent check is ...Read more",
			  "With the Pentium 4, the microprocessor family that began with the 8086 and that\nhas been the most successful computer product line ever appears to have come\nto an end. Intel has teamed up with Hewlett-Packard (HP) to develop a new\n64-bit architecture, called IA-64. IA-64 is not a 64-bit extension of Intel’s 32-bit\nx86 architecture, nor is it an adaptation of Hewlett-Packard’s 64-bit PA-RISC\narchitecture. Instead, IA-64 is a new architecture that builds on years of research\nat the two companies and at universities. The architecture exploits the vast cir-\ncuitry and high speeds available on the newest generations of microchips by a\nsystematic use of parallelism. IA-64 architecture represents a significant depar-\nture from the trend to superscalar schemes that have dominated recent proces-\nsor development.",
			  "Let us look at a simple example, taken from [INTE00a,Volume 1]. Here is the\noriginal program:\n(p1) br some_label\n// Cycle 0\nld8 r1 = [r5] ;;\n// Cycle 1\nadd r2 = r1, r3\n// Cycle 3\nThe first instruction branches if predicate p1 is true (register p1 has value 1). Note\nthat the branch and load instructions are in the same instruction group,even though the\nload should not execute if the branch is taken. IA-64 guarantees that if a branch is\ntaken, later instructions, even in the same instruction group, are not executed. IA-64\nimplementations may use branch prediction to try to improve efficiency but must as-\nsure against incorrect results. Finally, note that the add instruction is delayed by at least",
			  "4 defines a 128-bit **bundle** that contains three instructions, called **syllables** , and\na template f",
			  "The processor can fetch instructions one or more\nbundles at a time; each bundle fetch brings in three instructions.",
			  "predicated execution** , every IA-64 instruction includes a refer-\nence to a 1-bit predicate register, and only executes if the predicate\nvalue is 1 (true).",
			  "ontrol Speculation**\nAnother key innovation in IA-64 is control speculation, also known as speculative\nloading. This enables the processor to load data from memory before the program\nneeds it, to avoid memory latency delays.",
			  "Intel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC).",
			  "he CFM register describes the state of the current general register stack frame, as-\nsociated with the currently active procedure. It includes the following fields",
			  "sof:** size of stack frame",
			  "sol:** size of locals portion of stack frame",
			  "sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining",
			  "The Ita-\nnium includes nine execution units: two integer, two floating-point, four memory,\nand three branch execution unit",
			  "ersal, the processor views at most two\ninstruction bundles at a time.The processor can issue a maximum of six instructions\nper clock cycle.\n",
			  "tanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.",
			  "EPIC related hardware includes support for predicated execution,\ncontrol and data speculation, and software pipelining.",
			  " architecture. Next, we look at the general organization to support the architecture.We\nthen examine in some detail the key features of the IA-64 architecture that promote\ninstruction-level parallelism. Finally, we look at the IA-64 instruction set architecture\nand the Itanium organization.\n**21.1 / MOTIVATION** **21-3**\n**21.1 MOTIVATION**\nThe basic concepts underlying IA-64 are:\n• Instruction-level parallelism that is explicit in the machine instructions rather\nthan being determined at run time by the processor\n• Long or very long instruction words (LIW/VLIW)\n• Branch predication (not the same thing as branch prediction)\n• Speculative loading\nIntel and HP refer to this combination of concepts as explicitly parallel\ninstruction computing (EPIC). Intel and HP use the term **EPIC** to refer to the tech-\nnology, or collection of techniques. **IA-64** is an actual instruction set architecture\nthat is intended for implementation using the EPIC technology.The first Intel prod-\nuct based on this architecture is referred to as **Itanium** . Other products will follow,\nbased on the same IA-64 architecture.\nTable 21.1 summarizes key differences between IA-64 and a traditional super-\nscalar approach.\nFor Intel,the move to a new architecture that is not hardware compatible with the\nx86 instruction architecture was a momentous decision.But it was driven by the dictates\nof the technology.When the x86 family began, back in the late 1970s, the processor chip\n ... \nbe restored through successive returns. This is a function of the alloc instruction,\nwhich designates a general register to save the current value of the PFS fields before\nthey are overwritten from the CFM fields.\n**Current Frame Marker and Previous Function State**\nThe CFM register describes the state of the current general register stack frame, as-\nsociated with the currently active procedure. It includes the following fields:\n• **sof:** size of stack frame\n• **sol:** size of locals portion of stack frame\n• **sor:** size of rotating portion of stack frame; this is a subset of the local portion\nthat is dedicated to software pipelining\n• **register rename base values:** Values used in performing register rotation gen-\neral, floating-point and predicate registers\nThe PFS application register contains the following fields:\n• **pfm:** Previous frame marker; contains all of the fields of the CFM\n• **pec:** Previous epilog count\n• **ppl:** Previous privilege level\n**21.5 ITANIUM ORGANIZATION**\nIntel’s Itanium processor is the first implementation of the IA-64 instruction set ar-\nchitecture.The first version of this implementation, known as Itanium, was released\nin 2001, followed in 2002 by the Itanium 2. The Itanium organization blends super-\nscalar features with support for the unique EPIC-related IA-64 features.Among the\nsuperscalar features are a six-wide, ten-stage-deep hardware pipeline, dynamic\nprefetch, branch prediction, and a register scoreboard to optimize for compile time\nnondeterminism. EPIC related hardware includes support for predicated execution,\ncontrol and data speculation, and software pipelining.\nFigure 21.10 is a general block diagram of the Itanium organization. The Ita-\nnium includes nine execution units: two integer, two floating-point, four memory,\nand three branch execution units. Instructions are fetched through an L1 instruction\ncache and fed into a buffer that holds up to eight bundles of instructions. When de-\nciding on functional units for instruction dispersal, the processor views at most two\ninstruction bundles at a time.The processor can issue a maximum of six instructions\nper clock cycle.\nThe organization is in some ways simpler than a conventional contemporary\nsuperscalar organization. The Itanium does not use reservation stations, reorder\nbuffers, and memory ordering buffers, all replaced by simpler hardware for specula-\ntion.The register remapping hardware is simpler than the register aliasing typical of\nsuperscalar machines. Register dependency-detection logic is absent, replaced by\nexplicit parallelism directives precomputed by the software.\n**21.5 / ITANIUM ORGANIZATION** **21-29**\nUsing branch prediction, the fetch/prefetch engine can speculatively load\nan L1 instruction cache to minimize cache misses on instruction fetches. The\nfetched code is fed into a decoupling buffer that can hold up to eight ",
			  "Speculative loading",
			  "A-64** is an actual instruction set architecture\nthat is intended for implementation using the EPIC technology",
			  "With predicated execution, every IA-64 instruction includes a refer- ence to a 1-bit predicate register, and only executes if the predicate value is 1 (true). ...",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier;\nadvanced load\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nld8.c r6 = [r8]\n// Cycle 0; check load\nadd r5 = r6, r7 ;;\n// Cycle 0\nst8 [r18] = r5\n// Cycle 1\nHere we have moved the ld instruction earlier and converted it into an ad-\nvanced load. In addition to performing the specified load, the ld8.a instruction\nwrites its source address (address contained in r8) to a hardware data structure\nknown as the Advanced Load Address Table (ALAT). Each IA-64 store instruc-\ntion checks the ALAT for entries that overlap with its target address; if a match is\nfound, the ALAT entry is removed.",
			  "We may also want to speculatively execute instructions that are data depen-\ndent on a load instruction, together with the load itself. Starting with the same\noriginal program, suppose we move up both the load and the subsequent add\ninstruction:\nld8.a r6 = [r8] ;;\n// Cycle -3 or earlier;\nadvanced load\n// other instructions\nadd r5 = r6, r7\n// Cycle -1; add that uses r6\n// other instructions\nst8 [r4] = r12\n// Cycle 0\nchk.a r6, recover\n// Cycle 0; check\nback:\n// return point from jump to\nrecover\nst8 [r18] = r5\n// Cycle 0\nHere we use a chk.a instruction rather than an ld8.c instruction to validate\nthe advanced load. If the chk.a instruction determines that the load has failed, it\ncannot simply reexecute the load; instead, it branches to a recovery routine to\nclean up:\nRecover:\nld8 r6 = [r8] ;;\n// reload r6 from [r8]\nadd r5 = r6, r7 ;;\n// re-execute the add\nbr back\n// jump back to main code",
			  "ld8.a r6 = [r8] ;;\n// Cycle -2 or earlier;\nadvanced load",
			  "// other instructions\nst8 [r4] = r12",
			  "advanced load",
			  "When the original ld8 is converted to an ld8.a\ninstruction and moved, the original position of that instruction is replaced with a\ncheck load instruction, ld8.c.",
			  "ld8.c r6 = [r8]\n// Cycle 0; check load",
			  "// Cycle 0; check load",
			  "If one is found, no store instruction between the\nadvanced load and the check load has altered the source address of the load, and\nno action is taken.",
			  "owever, if the check load instruction does not find a match-\ning ALAT entry, then the load operation is performed again to assure the correct\nresult",
			  "Here we use a chk.a instruction rather than an ld8.c instruction to validate\nthe advanced load.",
			  "Recover:",
			  "ld8 r6 = [r8] ;;",
			  "// reload r6 from [r8]",
			  "add r5 = r6, r7 ;;",
			  "add r5 = r6, r7 ;;",
			  "// re-execute the add",
			  "Control Speculation"
			]
		  },
		  {
			"title": "An Automated OpenCL FPGA Compilation Framework ...",
			"url": "https://repository.lboro.ac.uk/articles/An_automated_OpenCL_FPGA_compilation_framework_targeting_a_configurable_VLIW_chip_multiprocessor/9539657/files/17169290.pdf",
			"excerpts": [
			  "Predicated execution was used to reduce the negative effect of ... Code size can affect the performance of the instruction cache as a larger instruction.Read more"
			]
		  },
		  {
			"title": "Optimal vs. Heuristic Methods in a Production Compiler",
			"url": "https://www.cs.utexas.edu/~less/reading/spring00/ruttenberg.pdf",
			"excerpts": [
			  "a, if-conversion to convert loops with internal branches to a form using ... These two fundamental heuristics are modified in one or both of two ways to derive ..."
			]
		  },
		  {
			"title": "METHODOLOGIES FOR MANY-INPUT FEEDBACK-DIRECTED ...",
			"url": "https://central.bac-lac.gc.ca/.item?id=NR89287&op=pdf&app=Library&oclc_number=1019490294",
			"excerpts": [
			  "When a BB is selected for inclusion in the hyperblock, if-conversion is applied to remove the side exit and merge the block into the hyperblock. Tail ..."
			]
		  },
		  {
			"title": "Hyperblock formation of seemingly incompatible paths with...",
			"url": "https://www.researchgate.net/figure/Hyperblock-formation-of-seemingly-incompatible-paths-with-positive-results-due-to-code_fig1_3728133",
			"excerpts": [
			  "Hyperblock formation of seemingly incompatible paths with positive results due to code transformations. The T and F annotations in (a) indicate the taken ..."
			]
		  },
		  {
			"title": "CCIS 95 - Contemporary Computing - Springer Link",
			"url": "https://link.springer.com/content/pdf/10.1007/978-3-642-14825-5.pdf",
			"excerpts": [
			  "The control path relationship is also obvious if the control flow graph for the hyperblock before if-conversion is examined. In fig 2(b), it is clear that ..."
			]
		  },
		  {
			"title": "Hyperblock formation of seemingly compatible paths with...",
			"url": "https://www.researchgate.net/figure/Hyperblock-formation-of-seemingly-compatible-paths-with-negative-results_fig2_220091347",
			"excerpts": [
			  "In this paper we present simulation of optimization of branch prediction in parallel register sharing architecture to show high degree of ILP. The main idea ..."
			]
		  },
		  {
			"title": "Predicate Analysis and If-Conversion in an Itanium Link- ...",
			"url": "https://www2.cs.arizona.edu/~debray/Publications/ifcvt.pdf",
			"excerpts": [
			  "he Itanium contains multiple functional units and uses pro-\ngrammer specified instruction-level parallelism. Moreover, every\ninstruction is *predicated* : It specifies a one-bit predicate register,\nand if the value of that register is true (1), then the instruction is\nexecuted; otherwise, the instruction usually has no effect",
			  "The Ita-\nnium has 64 predicate registers; register p0 has constant value true\n(assignments to it are ignored",
			  "Instructions that specify a predicate register other\nthan p0 are said to be *guarded* ",
			  "Predicate registers are set by compare instructions.",
			  "A predicated instruction is guarded\nby a Boolean source operand; the instruction is executed only if this\nguard evaluates to true.",
			  " order to make effective use of the capabilities of such archi-\ntectures, a compiler must selectively eliminate conditional branches\nin favor of predicated instructions that are conditionally executed.",
			  "process, known as *if-conversion* , must be carried out judi-\nciously: if it is too aggressive, it leads to contention for system\nresources and a concomitant degradation in performance; if it is not\naggressive enough, it results in insufficient instruction-level paral-\nlelism, which also leads to a loss in performanc"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "The Effects of Predicated Execution on Branch Prediction",
			  "Gary Scott Tyson",
			  "Department of Computer Science",
			  "University of California, Davis",
			  "Davis, CA 95616",
			  "tyson@cs.ucdavis.edu",
			  "Abstract",
			  "**\nHigh performance architectures have always had to\ndeal with the performance-limiting impact of branch\noperations. Microprocessor designs are going to have to\ndeal with this problem as well, as they move towards\ndeeper pipelines and support for multiple instruction\nissue. Branch prediction schemes are often used to allevi-\nate the negative impact of branch operations by allowing\nthe\nspeculative\nexecution\nof\ninstructions\nafter\nan\nunresolved branch.\n",
			  " form.\nThis paper analyzes a variety of existing predica-\ntion models for eliminating branch operations, and the\neffect that this elimination has on the branch prediction\nschemes in existing processors, including single issue\narchitectures with simple prediction mechanisms, to the\nnewer multi-issue designs with correspondingly more\nsophisticated branch predictors.\n",
			  "The effect on branch\nprediction accuracy, branch penalty and basic block size\nis studied.",
			  "Predicated Execution Models",
			  "entioned in the introduction, predicated execu-\ntion refers to the conditional execution (or completion) of\ninstructions based on the result of a boolean condition.\nSeveral different approaches to providing predicated exe-\ncution have been proposed and implemented. They fall\ninto two broad categories referred to as **restricted** and\n**unrestricted** .",
			  "\nIn the restricted model a limited number of new\npredicate instructions are introduced which are used to\nexplicitly delay the effect of executing a statement on\nprogram variables. This is achieved by moving the state-\nment in question up to before the branch, modifying it to\nwrite to a free register instead of a program variable, and\nthen using one of the special predicate operations to con-\nditionally update the active program variables. This is the\napproach used in the Alpha and HP-RISC processors.\nIn t",
			  "In the unrestricted predication model, *all* instruc-\ntions can be predicated. This can be accomplished in a\nnumber of ways. One way is to include an additional\noperand field for each instruction, as was done in the\nCydra 5. Another way is to introduce a special instruction\n�\n�\nwhich controls the conditional execution of following\n(non-predicated)\ninstructions.\nAn\nexample\nof\nthis\napproach is seen in the *guarded execution model* pro-\nposed by Pnevmatikatos and Sohi [PnSo94], which\nincludes special instructions whose execution specify\nwhether following instructions should or should not be",
			  "This section will present 4 different existing predi-\ncation models, those used in the Alpha processor, the\nHP-RISC, guarded execution model and the Cydra ",
			  " Alpha processor supports the restricted model\nof predication via a *conditional move* instruction. A stan-\ndard move instruction only requires 2 operands (source\nand destination), leaving one field free to specify the con-\nditional value in the conditional move. If the condition is\nsatisfied then the register movement is allowed, otherwise\na state change is prevented",
			  "e Alpha compiler uses the conditional move\ninstruction in the following way: An expression calcula-\ntion is moved up to before a conditional branch, and is\nmodified to write to a free register instead of a live pro-\ngram variable. A conditional move is then used in place\nof the original branch instruction to transfer the temporary\nvalue into the live register. If the condition is satisfied,\nthe original destination of the expression will contain the\nresult; if the condition is not satisfied, then the conditional\nmove operation is not performed, and the active state of\nthe application being executed is unchange",
			  "There are several restrictions on this form of predi-\ncation. First, the compiler must ensure that no exceptions\nwill be generated by boosting the expression code (e.g.\ndivision by zero must be excluded). Other instructions\nthat cannot be predicated include some memory access\ninstructions and flow control operation",
			  "Second, the\ncompiler must allocate free registers to store any results\nprior to the conditional move instruction(s). This may not\nbe feasible if it results in register spilling.",
			  "The HP Precision Architecture uses *instruction*\n*nullification* to provide a less restricted form of predica-\ntion. In this architecture, control flow (branch) and arith-\nmetic instructions can specify whether the *following*\ninstruction should execute. The ability of arithmetic\ninstructions to nullify the following instruction allows the\ncompiler to remove branch instructions. A transformation\nsimilar to that of the Alpha’s can be performed with the\nadded capacity to include additional *skip* instructions over\nexception producing instructi",
			  "Pnevmatikatos and Sohi propose the use of a **guard**\ninstruction to control the execution of a sequence of\ninstructions. A guard instruction specifies two things - a\ncondition register and a mask value to indicate which of\nthe following instructions are dependent on the contents\nof that condition register. The processor hardware then\nuses this information to create a dynamic *scalar mask*\nwhich is used to determine whether a given instruction\nshould be allowed to modify the state of the processor.",
			  "5.3. Effects of Predication on Basic Block Size",
			  "ntrol dependencies restrict the ability of multi-\nissue architectures to fill instruction slots. Branch predic-\ntion schemes are used to help alleviate this problem by\nproviding a set of candidate instructions with a high pro-\nbability of execution that can be used to fill vacant slots.",
			  "Unfortunately, many compiler transformation are still\nprecluded.",
			  "y removing the control dependency entirely via\npredication, the compiler has more flexibility in reorder-\ning the code sequence. This can achieve a more efficient\ncode schedule. An average of 30% of branch instructions\nare short forward branches and are good candidates for\npredicate transformation. The removal of these branches\nleads to a significant increase in basic block size, and thus\nan increase in the efficiency of the scheduler",
			  "e 7 shows the results of the *Aggressive*\ntransformation on the integer SPEC benchmarks. This\nfigure shows that as the predication distance increases, the\nbasic block size increases as well, and that a branch dis-\ntance of 12 provides a 40% increase in basic block size.\nThis is almost half of the total increase achievable if **all**\nforward branches were removed. The *Restricted* predica-\ntion scheme provides a much more limited improvement\nin basic block size (4% to 6% increas",
			  "It is also possible to use predication in conjunction\nwith other transformations to remove even a wider range\nof branches. For example, a loop unrolling transforma-\ntion removes backward branches by duplicating the body\nof the loop and iterating (and therefore branching) fewer\ntimes. If the number of iterations can be determined at\ncompile time, then the unrolling is a simple duplicatio",
			  "6. Conclusions",
			  "Supporting the conditional execution of instructions\nis a technique that can have a significant impact on the\nperformance of most high-performance architectures. It\n ... \ninstruction slots lost due to branch misprediction. How-\never, aggressive predication also increases average basic\nblock size by up to 45%, providing the code scheduler\nwith more instruction to fill the execution pipeline and\nsignificantly reducing pipeline sta",
			  "In addition to the aggressive predication scheme,\nwe examined two other more restrictive approaches.",
			  "Results indicate that current predication schemes that\nallow for expression boosting but not full instruction set\npredication show dramatically less ability to reduce\nbranch misprediction penalty and increase basic block\nsize than the more aggressive approach.",
			  "While these\nschemes can be useful in certain applications (e.g. tuned\nOS routines), they provide only a limited\nbenefit in\nimproving more general applications.",
			  "e feel adding architectural support for the condi-\ntional execution of instructions is going to continue to\ngrow in importance for the following reasons",
			  "Branch misprediction penalties will continue rising as\npipeline depths and issue widths increase",
			  "The predicate cost is in terms of instructions, and\ntherefore not dependent on issue width. As issue\nwidths increase, the performance penalty for branch\nprediction becomes greater, where the predication\ncost remains constant.",
			  "Branches transformed by predication have worse than\naverage prediction accuracy",
			  "Predication allows for a significant increase in basic\nblock size.",
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for",
			  "This paper analyzes a variety of existing predica-\ntion models for eliminating branch operations, and the\neffect that this elimination has on the branch prediction\nschemes in existing processors, including single issue\narchitectures with simple prediction mechanisms, to the\nnewer multi-issue designs with correspondingly more\nsophisticated branch predictor",
			  "..."
			]
		  },
		  {
			"title": "Machine Learning-Based Instruction Scheduling for a DSP ...",
			"url": "https://kth.diva-portal.org/smash/get/diva2:1834162/FULLTEXT01.pdf",
			"excerpts": [
			  "Abstract"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "The main purpose of predication is to avoid jumps over very small sections of program code, increasing the effectiveness of [pipelined](/wiki/Pipeline_(computing) \"Pipeline (computing)\") execution and avoiding problems with the [cache](/wiki/CPU_cache \"CPU cache\") . It also has a number of more subtle benefits:",
			  "Functions that are traditionally computed using simple arithmetic and [bitwise operations](/wiki/Bitwise_operation \"Bitwise operation\") may be quicker to compute using predicated instructions.",
			  "Predicated instructions with different predicates can be mixed with each other and with unconditional code, allowing better [instruction scheduling](/wiki/Instruction_scheduling \"Instruction scheduling\") and so even better performance.",
			  "Elimination of unnecessary branch instructions can make the execution of necessary branches, such as those that make up loops, faster by lessening the load on [branch prediction](/wiki/Branch_prediction \"Branch prediction\") mechanisms.",
			  "Elimination of the cost of a branch misprediction which can be high on deeply pipelined architectures.",
			  "Predication's primary drawback is in increased encoding space. In typical implementations, every instruction reserves a bitfield for the predicate specifying under what conditions that instruction should have an effect. When available memory is limited, as on [embedded devices](/wiki/Embedded_system \"Embedded system\") , this space cost can be prohibitive. However, some architectures such as [Thumb-2](/wiki/Thumb-2 \"Thumb-2\") are able to avoid this issue (see below).",
			  "Other detriments are the following: [[ 3 ]]()\nPredication complicates the hardware by adding levels of [logic](/wiki/Control_unit \"Control unit\") to critical [paths](/wiki/Datapath \"Datapath\") and potentially degrades clock speed.",
			  "A predicated block includes cycles for all operations, so shorter [paths](/wiki/Control-flow_graph \"Control-flow graph\") may take longer and be penalized.",
			  "An extra register read is required. A non-predicated ADD would read two registers from a register file, where a Predicated ADD would need to also read the predicate register file. This increases Hazards in [Out-of-order execution](/wiki/Out-of-order_execution \"Out-of-order execution\") .",
			  "Predication is not usually speculated and causes a longer dependency chain. For ordered data this translates to a performance loss compared to a predictable branch. [[ 4 ]]()",
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true.",
			  "The use of predication is essential in IA-64's implementation of [software pipelining](/wiki/Software_pipelining \"Software pipelining\") because it avoids the need for writing separated code for prologs and epilogs.",
			  "The use of predication is essential in IA-64's implementation of software pipelining because it avoids the need for writing separated code for prologs and ...Read more"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://pages.cs.wisc.edu/~fischer/cs701/meta-optimization.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, the IM-\nPACT predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock. A hyperblock is a predi-\ncated single-entry, multiple-exit regi",
			  "Though predication effectively exposes ILP, simply pred-\nicating everything will diminish performance by saturating\nmachine resources with useless instruction",
			  "Figure 3 highlights the difference between control-flow\nand predicated execution.",
			  "Here, p2 and p3 are mutually exclusive predicates that are\nset according to the branch condition in part (a).",
			  " predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [1",
			  "h frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively affect performance.",
			  "Path ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels o",
			  "r of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc-\ntion in a hyperblock executes, long paths effectively\ndelay the time to completion of short paths. The cost\nof misprediction is relatively high for short paths.",
			  "mber of branches in path** :\nPaths of control\nthrough several branches have a greater chance of mis-\npredicting. Therefore, it may be worthwhile to predi-\ncate such paths. On the other hand, including several\nsuch paths may produce large hyperblocks that satu-\nrate resou",
			  "ompiler optimization considerations** : Paths that\ncontain hazard conditions ( *i.e.* , pointer dereferences\nand procedure calls) limit the effectiveness of many\ncompiler optimizations. In the presence of hazards, a\ncompiler must make conservative assump",
			  "chine-specific considerations** : A heuristic should\naccount for machine characteristics. For instance, the\nbranch delay penalty is a decisive factor",
			  "learly, there is much to consider when designing a heuris-\ntic for hyperblock selection",
			  "MPACT compiler begins by transforming the code\nso that it is more amenable to hyperblock formation [15].\nIMPACT’s algorithm then identifies acyclic paths of con-\ntrol that are suitable for hyperblock inclusio",
			  "A pri-\nority function— which is the critical calculation in the predi-\ncation decision process— assigns a value to each of the paths\nbased on characteristics such as the ones just described [15].\nSome of these characteristics come from runtime profiling.\nIMPACT uses the priority function shown below:\n*h* *i* =\n�\n0 *.* 25\n:\nif *path* *i* contains a hazard.\n1\n:\nif *path* *i* is hazard free.\n*d ratio* *i* =\n*dep height* *i*\nmax *j* =1 *→* *N* *dep height* *j*\n*o ratio* *i* =\n*num ops* *i*\nmax *j* =1 *→* *N* *num ops* *j*\n*priority* *i* = *exec ratio* *i* *·* *h* *i* *·* (2 *.* 1 *−* *d ratio* *i* *"
			]
		  },
		  {
			"title": "Optimize Options (Using the GNU Compiler Collection (GCC))",
			"url": "https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html",
			"excerpts": [
			  "RTL if-conversion tries to remove conditional branches around a block and",
			  "replace them with conditionally executed instructions.",
			  "These parameters\ngive the maximum permissible cost for the sequence that would be generated\nby if-conversion depending on whether the branch is statically determined\nto be predictable or not.",
			  "lso use other heuristics to decide whether if-conversion is likely to be\nprofitable.",
			  "The tracer-dynamic-coverage-feedback parameter\nis used only when profile\nfeedback is available.",
			  "The real profiles (as opposed to statically estimated\nones) are much less balanced allowing the threshold to be larger value.",
			  "`tracer-max-code-growth`",
			  "Stop tail duplication once code growth has reached given percentage.",
			  "he maximum number of instructions in a block which should be\nconsidered for if-conversion. ",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "Set to 0\nif either vectorization ( -ftree-vectorize ) or if-conversion\n( -ftree-loop-if-convert ) is disabled.",
			  "`case-values-threshold`",
			  "The smallest number of different values for which it is best to use a\njump-table instead of a tree of conditional branches."
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region.",
			  ". The processor\nnullifies all instructions in the incorrect path. I",
			  "redication can effectively expose ILP, simply predi-\ncating everything will diminish performance by saturating machine resources with useless\ninstructions",
			  "f p2 is true all of the time for instance, the instructions guarded by p3 con-\nsume valuable execution slots"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.eecg.utoronto.ca/~steffan/carg/readings/metaopt_talk.pdf",
			"excerpts": [
			  "***Hyperblock Formation***\nUses If-Conversion to convert control dependences into data\ndependences.\nSingle entry multiple exit block with predicated instructions.",
			  "There is a limit to the effectiveness of predicating instructions\nand forming large hyperblocks.",
			  "Path predictability - unpredictable branches",
			  "Path frequency - frequent paths",
			  "**Page 13**\n***Hyperblock Formation (2)***\nNumber of instructions in path - shorter paths\nNumber of branches in path - more branches but not too many\nCompiler optimization considerations (e.g. exceptions)\nMachine-specific considerations (e.g. branch delay penalty)",
			  "Experiment performed with Trimaran's IMPACT compiler\napproximating the Intel Itanium architecture (EPIC).",
			  "Tested with selections from SpecInt, SpecFP, Mediabench and\nother miscellaneous benchmark programs.",
			  "**Page 14**\n***Hyperblock Formation (3)***\n**Trimaran's Heuristic**\n**GP-generated Heuristic**",
			  "**Page 15**\n***Hyperblock Formation (4)***",
			  "***Hyperblock Formation (5)***",
			  "Quantitative Results:\nSingle benchmark results are very impressive: (1.54/1.23)\nMultiple benchmark results: (1.44/1.25)"
			]
		  },
		  {
			"title": "(PDF) A novel heuristic for selection of hyperblock in If-conversion",
			"url": "https://www.researchgate.net/publication/261459597_A_novel_heuristic_for_selection_of_hyperblock_in_If-conversion",
			"excerpts": [
			  "Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow.",
			  "Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions.",
			  "In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied.",
			  "On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution.",
			  "This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time",
			  "A hyperblock [10] is predicted region of code that contains straight-line sequence of instruction with a single entry point and possibly multiple exit points.",
			  "The navigation bandwidth of prediction mechanism depends upon the degree of ILP. It can be increased by increasing control flow prediction in procedural languages at compile time.",
			  "The main idea behind this concept is to use a step beyond the prediction of common branch and permitting the architecture to have the information about the CFG (Control Flow Graph) components of the program to have better branch decision for ILP.",
			  "on the ItaniumTM processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ].",
			  "We therefore implement the first formally verified implementation of a general if-conversion algorithm, with support for heuristics for branch prediction."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Most of these\nschemes group sequential basic blocks into larger entities, thereby\nreducing control dependencies, increasing the fetch rate and allow-\ning more opportunities for optimizatio",
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "imilar to\nsuperblocks, successive sets of instructions are combined and op-\ntimized to form a single hyperblock, however, hyperblocks may\ninclude multiple paths if the bias cannot be determined",
			  "predicated execution using hyper-\nblocks [1"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "Mathematical foundation of trace scheduling | ACM Transactions on Programming Languages and Systems",
			"url": "https://dl.acm.org/doi/10.1145/1961204.1961206",
			"excerpts": [
			  "Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations.",
			  "The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then *prove* that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained.",
			  "We also give a very simple proof that the trace scheduling algorithm always terminates."
			]
		  },
		  {
			"title": "Avoidance and Suppression of Compensation Code in ... - DTIC",
			"url": "https://apps.dtic.mil/sti/tr/pdf/ADA265612.pdf",
			"excerpts": [
			  "Trace scheduling is an optimization technique that has been used for VLIW, superscalar, and pipelined pro-\ncessors. Whereas a conventional instruction scheduler considers the operations in a single basic block when\nscheduling instructions, a trace scheduler employs a more global view. The key idea is that the trace scheduler\nselects a group of operations from a sequence of basic blocks, either based on heuristics or actual frequency\ninformation, and schedules these operations as if they were in a single basic block [91. If operations are moved\nacross basic block boundaries, such optimizations require that the overall program be fixed up to account for\nthe code movements. For example, if an operation that appeared above a branch is moved below this branch,\nthen the code outside of the trace must be fixed up by inserting a *compensation* *copy* into the off-trace code.\nA frequent criticism of trace scheduling is the potential for code explosion due to compensation copies, and\nthe problem of code growth was noted by Fisher in his original presentation of the algorithm [9]. In this paper\nwe show that this is not a practical concern. This paper examines the issue of compensation copies in depth\nand discusses various techniques for limiting the amount of compensation code generated.\nThere are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant). These techniques are successful; we show that the amount of compensation code generated\nis small over a variety of programs (the SPEC89 suite [27] and the Livermore Fortran Kernels [201, using the\nMultiflow Trace 7/300 as the target machine [5]). To evaluate the impact on the execution of programs, we\nmeasure the dynamic effects of trace scheduling with three types of code motion:\n1. Code motion restricted to basic blocks. This approximates most RISC compilers.\n2. Code motion restricted so that no compensation code is required. (This is essentially scheduling over\nextended basic blocks.')\n3. Full trace scheduling.\nThe performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%). This result implies that a compiler\ncan get much of the benefit of trace scheduling without the complication of compensation code. However,",
			  "There are two techniques to deal with compensation code: avoidance (restricting code motion so that no\ncompensation code is required) and suppression (ana!yzing the global flow of the program to detect when a\ncopy is redundant).",
			  "The performance gain of full trace scheduling over basic block scheduling is large (30%); the performance\ngain of trace scheduling with no compensation code is also large (25%)."
			]
		  },
		  {
			"title": "Formally Verified Superblock Scheduling",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3497775.3503679",
			"excerpts": [
			  "**In addition,** we provide a certified checker for path du-\nplications, which we use to prove tail duplication and loop\nunrolling optimizations. These optimizations increase sched-\nuling opportunities (at the price of code size increase). Our\nchecker of path duplications illustrates the interest of trans-\nlation validation designs: a simple formally verified checker\nenables to certify the correctness of an important class of\ntransformations, modulo small hints provided by oracles.",
			  "The\nSuperblock: An Effective Technique for VLIW and Superscalar Com-\npilation. *The Journal of Supercomputing* 7 (05 1993), 229–248. h",
			  "Section 3 recalls the specificities of superblock (vs\nbasic-block) scheduling.",
			  "**In addition,** we provide a certified checker for path du-\nplications, which we use to prove tail duplication and loop\nunrolling optimization"
			]
		  },
		  {
			"title": "Superblock Scheduler for Code-Size Sensitive Applications",
			"url": "https://llvm.org/devmtg/2021-02-28/slides/Arun-Superblock-sched.pdf",
			"excerpts": [
			  "TraceList = Construct Traces(MF);",
			  "TraceList = Construct Traces(MF);\n2.\nForm Superblocks from TraceList;\n3.\nFor each Superblock S in TraceList {\n1.\nConstruct Data Dependency Graph for S\n2.\nSchedule\n}",
			  "\nEmbedded Applications: Concerns\n• Code Size – Paramount Significance\n• Can’t afford code size increase due to tail duplication (Superblock Scheduling) or fixup code insertion (Trace Scheduling)",
			  "Remedies and Limitations:\n• Traces may be terminated immediately before side entries\n•\nObviates the need for tail duplication\n•\nNo need for fixup code Insertion (Trace Scheduling)\n• Downward Code Motion across branches is Prohibited\n•\nNo Code duplication on side exit edges\n• Load/Store Movement across branches is Prohibited",
			  "References\n[1] The Superblock: An Effective Technique for VLIW and Superscalar\nCompilation. W.W.Hwu et al. The Journal of Supercomputing, 7, 229-248 (1993)"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.classes.cs.uchicago.edu/archive/2017/fall/32001-1/papers/meta-optimization.pdf",
			"excerpts": [
			  "perblock for-\nmation. Here we introduce the optimization, and then we\ndiscuss factors that might be important when creating a pri-\nority function for it. We conclude the section by presenting\nexperimental results for hyperblock formatio",
			  "chitects have proposed two noteworthy methods for de-\ncreasing the costs associated with control transfers 3 : im-\nproved branch prediction, and predication. Improved branch\nprediction algorithms would obviously increase processor uti-\nlization. Unfortunately, some branches are inherently unpre-\ndictable, and hence, even the most sophisticated algorithm\nwould fail. For such branches, predication may be a fruitful\nalternative.",
			  "In the following list we give a brief overview of several cri-\nteria that are useful to consider when forming hyperblocks.\nSuch criteria are often referred to as features. In the list,\na path refers to a path of control flow ( *i.e.* , a sequence of\nbasic blocks that are connected by edges in the control flow\ngraph):",
			  "*Path predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [15]",
			  "ath frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively a ff ect performance",
			  "th ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels of ILP.",
			  "*Number of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc"
			]
		  },
		  {
			"title": "An Overview of IA-64 Architectural Features and Compiler ...",
			"url": "https://cseweb.ucsd.edu/classes/fa99/cse240/lee_ucsd_ia64.PDF",
			"excerpts": [
			  "Setting and propagation of deferred exceptions. – Speculative loads (ld.s) set the NaT bit or NaTVal if a deferrable exception occurs.Read more",
			  "❒ Parallel compares allow compound conditionals to be executed in a single instruction group. – Permit WAW in an instruction group. ❒ Example: if ( A && B && C ) ...Read more",
			  "ALAT - Advanced load address table",
			  "– HW structure containing information about outstanding advanced\nload addresses",
			  "– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "– ld.a - advanced load",
			  "– ld.c - check load",
			  "– ld.sa - combined control and data speculation.",
			  "Architectural Features",
			  "Compiler Optimization",
			  "Control and data speculation,\nPredication,\nMulti-way branch",
			  "Global scheduling with\ncontrol and data speculation,\nPredicate promotio",
			  "Modulo scheduling",
			  "cality hints,\nPrefetch,\nPost-increment",
			  "Memory hierarchy control",
			  "The compiler should take full advantage of IA-64 architectural\nfeatures to generate optimized code",
			  "O riginal\nR e schedule\nuse\nload\ncheck addressing conflict\nTransform\nstore\nuse\nstore\nload ignoring conflict\ncheck addressing conflict\nuse\nstore\nload ignoring conflict\naliased",
			  "chitectural Support for**\n**Data Speculati",
			  "ALAT - Advanced load address table\n– HW structure containing information about outstanding advanced\nload addresses\n– “Snoop” on stores and other memory writes to delete overlapping\nadvanced load addresses",
			  "Compiler Directed Data Speculation",
			  "Separate load behavior from overlap detection\n– ld8.a can be scheduled passing aliased stores\n– chk.a detects conflict\n",
			  "ld8.a r1 = r2 ;;",
			  "add r3 = r1,r4",
			  "ld8.a r1 = r2",
			  "Data Speculation",
			  "chk.a r1, recovery",
			  "chk.a r1, recovery",
			  "Instructions",
			  "– chk.a - advance load checks"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.0 User's Guide ( ...",
			"url": "https://www.ti.com/litv/pdf/spru187q",
			"excerpts": [
			  "Memory Alias Disambiguation ... speculative execution of loads with bounded address ranges. --speculate_unknown_loads. Allows speculative execution of loads ...Read more",
			  "currently specified by the --speculate_loads option. You must increase the --speculate_loads threshold as recommended in the software-pipeline feedback ...",
			  "--speculate_unknown_loads. Allows speculative execution of loads with ... When the --speculate_loads=n option is used, the speculative threshold is ...Read more",
			  "**The MUST_ITERATE Pragma Syntax**",
			  "The syntax of the pragma for C and C++ is:\n**\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);\n",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple.",
			  "\\ MUST_ITERATE (** *min* , *max* , *multiple* **);*",
			  "The arguments *min* and *max* are programmer-guaranteed minimum and maximum trip counts. The trip\ncount is the number of times a loop iterates.",
			  "The trip count of the loop must be evenly divisible by *multiple* .",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5);",
			  "However, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5); /* Note the blank field for max */\nI",
			  "When specifying a multiple via the MUST_ITERATE pragma, results of the program are undefined if the\ntrip count is not evenly divisible by multiple. Also, results of the program are undefined if the trip count is\nless than the minimum or greater than the maximum specified.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. You guarantee that\nthese properties are always true.",
			  "Through the use of the MUST_ITERATE pragma, you can guarantee that a loop executes a certain\nnumber of times.",
			  "Anytime the UNROLL pragma is applied to a loop,\nMUST_ITERATE should be applied to the same loop.",
			  "For loops the MUST_ITERATE pragma's third\nargument, multiple, is the most important and should always be specified.",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size."
			]
		  },
		  {
			"title": "Architecture Software Developer's Manual",
			"url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/itanium-architecture-software-developer-rev-2-3-vol-1-manual.pdf",
			"excerpts": [
			  "hen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly.",
			  "The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags.",
			  "The Not A Thing (NaT) bit is an extra bit on each of the general registers.",
			  "e check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Data Speculation and Instructions",
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and\nfloating-point pair ( ldfp.a ) forms.",
			  "Data speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated result",
			  "When the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT).",
			  "The check instruction must specify the same\nregister number as the corresponding advanced load.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Control speculation allows loads and their dependent uses to be safely moved above\nbranches.",
			  "Support for this is enabled by special NaT bits that are attached to integer\nregisters and by special NatVal values for floating-point registers.",
			  " When a speculative\nload causes an exception, it is not immediately raised. Instead, the NaT bit is set on the\ndestination register (or NatVal is written into the floating-point register). Subsequent\nspeculative instructions that use a register with a set NaT bit propagate the setting until\na non-speculative instruction checks for or raises the deferred exception.",
			  "The advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "The advanced load check works like the\n**Figure 4-",
			  "Recovery code may use either a normal or advanced load to obtain the correct value for\nthe failed advanced load.",
			  "An advanced load is used only when it is advantageous to\nhave an ALAT entry reallocated after a failed speculation.",
			  " an advanced load is executed, it allocates an\nentry in a structure called the Advanced Load Address Table (ALAT). Later, when a\ncorresponding check instruction is executed, the presence of an entry indicates that the\ndata speculation succeeded; otherwise, the speculation failed and one of two kinds of\ncompiler-generated recovery is perfo",
			  "1.\nThe check load instruction ( ld.c , ldf.c , or ldfp.c ) is used for recovery when\nthe only instruction scheduled before a store that is ambiguous relative to the\nadvanced load is the advanced load itself.",
			  "2.\nThe advanced load check ( chk.a ) is used when an advanced load and several\ninstructions that depend on the loaded value are scheduled before a store that is\nambiguous relative to the advanced load.",
			  "g (NaT) bit is an extra bit on each of the general registers. A register\nNaT bit indicates whether the content of a register is valid. If the NaT bit is set to one,\nthe register contains a deferred exception token due to an earlier speculation fault.",
			  "**Architecture**\nData speculation in the Itanium architecture uses a special load instruction ( ld.a )\ncalled an *advanced load* instruction and an associated check instruction ( chk.a or ld.c )\nto validate data-speculated results.\nW",
			  "\nWhen the ld.a instruction is executed, an entry is allocated in a hardware structure\ncalled the Advanced Load Address Table (ALAT). The ALAT is indexed by physical\nregister number and records the load address, the type of the load, and the size of the\nload.\n",
			  "\nA check instruction must be executed before the result of an advanced load can be used\nby any non-speculative instruction. The check instruction must specify the same\nregister number as the corresponding advanced load.\n",
			  "\nWhen a check instruction is executed, the ALAT is searched for an entry with the same\ntarget physical register number and type. If an entry is found, execution continues\nnormally with the next instruction.\n",
			  "\nIf no matching entry is found, the speculative results need to be recomputed:\n• Use a chk.a if a load and some of its uses are speculated. The chk.a jumps to\ncompiler-generated recovery code to re-execute the load and dependent\ninstructions.\n• Use a ld.c if no uses of the load are speculated. The ld.c reissues the load.\nEntries are removed from the ALAT due to:\n• Stores that write to addresses overlapping with ALAT entries.\n• Other advanced loads that target the same physical registers as ALAT entries.\n• Implementation-defined hardware or operating system conditions needed to\nmaintain correctness.\n• Limitations of the capacity, associativity, and matching algorithm used for a given\nimplementation of the ALAT.\n",
			  "\nThe ALAT is the structure that holds the state necessary for advanced loads and checks\nto operate correctly. The ALAT is searched in two different ways: by physical addresses\nand by ALAT register tags. An ALAT register tag is a unique number derived from the\nphysical target register number and type in conjunction with other\nimplementation-specific state. Implementation-specific state might include register\nstack wraparound information to distinguish one instance of a physical register that\nmay have been spilled by the RSE from the current instance of that register, thus\navoiding the need to purge the ALAT on all register stack wraparounds.\n",
			  "The Itanium architecture provides extensive support for software-pipelined loops,\nincluding register rotation, special loop branches, and application registers. When\ncombined with predication and support for speculation, these features help to reduce\ncode expansion, path length, and branch mispredictions for loops that can be software\npipelined."
			]
		  },
		  {
			"title": "IA64 Architecture and Compilers",
			"url": "https://old.hotchips.org/wp-content/uploads/hc_archives/hc11/1_Sun/hc99.t2.s1.IA64tut.pdf",
			"excerpts": [
			  "Control Speculation",
			  "Original:",
			  "(p1)",
			  "br.cond",
			  "ld8.s r1 = [ r2 ]",
			  "chk.s r1, recovery",
			  "Data Speculation",
			  "original",
			  "ld8 r1 = [ r2 ]",
			  "ld8 r1 = [ r2 ]",
			  "Transformed:",
			  "Transformed:",
			  "ld8.a r1 = [ r2 ]",
			  "st4 [ r3 ] = r7",
			  "st4 [ r3 ] = r7",
			  "chk.a r1, recovery",
			  "Separates loads into 2 parts: speculative\nloading of data and detection of conflicts/faults .",
			  "Hoisting Uses",
			  "Uses can be hoisted, but then chk.a needed for recovery",
			  "ld.c r1=",
			  "ld8.a r1=",
			  "ld8.a r1=",
			  "instr 1",
			  "uses =r1",
			  "uses =r1",
			  "instr 2",
			  "instr 2",
			  "st8",
			  "st8",
			  "chk.a r1,rec",
			  "br home",
			  "Recovery code",
			  "No hoisted uses",
			  "With hoisted uses",
			  "Hot Chips 1999",
			  "Architectural Support for Data Speculation",
			  "ALAT - HW structure containing information about\noutstanding advanced loads",
			  "Instructions",
			  "– ld.a - advanced loads",
			  "– ld.c - check loads",
			  "– chk.a - advance load checks",
			  "Speculative Advanced loads - ld.sa - is an control\nspeculative advanced load with fault deferral\n(combines ld.a and ld.s)",
			  "Almost all instructions can have a qualifying predicate (qp). – Ex: (p1) add r1 = r2, r3. – if qp is true, instruction executed normally. – if qp is false ...Read more",
			  "Two kinds of instructions in IA-64 programs. – Non-Speculative Instructions -- known to be useful/needed. • would have been executed in the original program.Read more",
			  "...",
			  "..."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 6.1 User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/spru187o/spru187o.pdf",
			"excerpts": [
			  "**--speculate_loads** = *n*\nSpecifies speculative load byte count threshold. Allows speculative\nexecution of loads with bounded addresses. See Section 3.2.3.1 .",
			  "Specifies speculative load byte count threshold. Allows speculative",
			  ".com\n*Optimizing Software Pipelining*\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "**--speculate_unknown_loads**",
			  "**--speculate_unknown_loads**\nAllows speculative execution of loads with bounded addresses.",
			  "Alias disambiguation is a technique that determines when two pointer expressions cannot point to the\nsame location, allowing the compiler to freely optimize such expressions.",
			  ";*\nMinimum required memory pad\n: 8 bytes",
			  "**3.2.3.2**",
			  "When you are writing your linear assembly, your code does *not* need to indicate the following:\n–\nPipeline latency\n–\nRegister usage\n–\nWhich unit is being used",
			  "e size reduction optimizations.\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.\nThe software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running. Example 3-1 illustrates the\ninformation that is generated for each loop.\nT",
			  "\nThe macro works regardless of which C6x device you build for, or if you port the code to another target.\nA more rare case is to pass the address of an offset from an array, for example:\nf(&buffer[3]);\nThis code passes an unaligned address to ptr, thus violating the presumption coded in the _nassert().\nThere is no direct remedy for this case. Avoid this practice whenever possible.",
			  "**Selecting the Best Threshold Value**",
			  "When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop",
			  "Collapsing can usually reduce the minimum safe trip count.",
			  "If the minimum known trip count is less than\nthe minimum safe trip count, a redundant loop is required.",
			  "Minimum required memory pad",
			  ": 8 bytes",
			  "Known Minimum Trip Count",
			  "Known Maximum Trip Count",
			  "Loop Carried Dependency Bound(^)",
			  "Unpartitioned Resource Bound",
			  "Partitioned Resource Bound(*)",
			  "5",
			  "Resource Partition:",
			  "A-side",
			  ".L units",
			  ".S units",
			  ".D units",
			  ".M units",
			  ".X cross paths",
			  ".T address paths",
			  "Long read paths",
			  "Long write paths",
			  "0",
			  "0",
			  "0",
			  "The\n--disable_software_pipelining option affects both compiled C/C++ code and assembly optimized code.",
			  "**Software Pipelining May Increase Code Size**",
			  "**Turn Off Software Pipelining (--disable_software_pipelining Option)**",
			  "If you enter comments on instructions in your linear assembly input file, the compiler moves the comments\nto the output file along with additional information. It attaches a 2-tuple <x, y> to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the software pipeline.",
			  "Figure 3-2. Software-Pipelined Loop*",
			  "This chapter provides an overview of these tools and introduces the features of the optimizing C/C++\ncompiler.",
			  "1",
			  "1",
			  "1",
			  "2",
			  "2",
			  "2",
			  "**3.2.3.1**",
			  "**Speculative Execution**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops.",
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "4",
			  "4",
			  "4",
			  "Optimizing Software Pipelining"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "0",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "1",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "2",
			  "load",
			  "**ld**"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/sprui04b",
			"excerpts": [
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous.",
			  "Memory alias disambiguation is the process of determining when such\nambiguity is not possible. When you cannot determine whether two memory references are ambiguous,\nyou presume they are ambiguous. This is the same as saying the two instructions have a memory\ndependence between them.",
			  "The assembly optimizer assumes memory references are aliased, unless it can prove otherwise.",
			  "ne side** . The C6000 has predicate, or conditional, registers available\nfor use with conditional instructions. There are six predicate registers. There are three on the A side\nand three on the B side. ",
			  "peculative threshold exceeded** . It would be necessary to speculatively load beyond the threshold\ncurrently specified by the --speculate_loads option. You must increase the --speculate_loads threshold\nas recommended in the software-pipeline feedback located in the assembly file",
			  "The resource breakdown\ndisplayed in the software pipelining information in the asm file, is computed after the compiler has\npartitioned instructions to either the A-side or the B-side. If the resources are imbalanced (i.e.; some\nresources on one side are used more than resources on the other) software pipelining is resource-bound,\nand the loop cannot be efficiently scheduled. If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining. You can give loop trip-\ncount information to the compiler using the \"MUST_ITERATE\" pragma",
			  "advice #30011: Consider adding assertions to indicate n-byte alignment\nof variables input1, input2, output if they are actually n-byte\naligned: _nassert((int)(input1) % 8 == 0).\nMost loops have memory access instructions. The compiler attempts to use wider load instructions, and\naligned memory accesses instead of non-aligned memory accesses to reduce/balance out resources used",
			  "**LDDW**",
			  "**STDW**",
			  "Allows aligned loads and stores of 8 bytes to memory. The pointer\nmust be aligned to an eight-byte boundary. This is defined as a\nmacro. You must include c6x.h. (2)(1)",
			  "Allows aligned loads of 8 bytes from memory. The pointer must be\naligned to an eight-byte boundary.",
			  "*Example 2",
			  "_nassert (((int)(x) & 0x3) == 0);",
			  "\\ MUST_ITERATE(20, , 4);",
			  "The C66x DSP generation comprises floating-point devices in the C6000 DSP platform. SPRUFE8— TMS320C674x CPU and Instruction Set Reference Guide. Describes ...Read more",
			  ".2.2***\n***Software Pipelining Information***\nThe compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "**4.2.3.1**\n**Speculative Execution**\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "collapsing which can be performed.\nWhen the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop. If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited. To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.\nCo",
			  "**4.3**\n**Redundant Loops**\nEvery loop iterates some number of times before the loop terminates. The number of iterations is called\nthe *trip count* . The variable used to count each iteration is the *trip counter* . When the trip counter reaches\na limit equal to the trip count, the loop terminates. The C6000 tools use the trip count to determine\nwhether or not a loop can be pipelined. The structure of a software pipelined loop requires the execution\nof a minimum number of loop iterations (a minimum trip count) in order to fill or prime the pipeline.",
			  "*4.4**\n**Utilizing the Loop Buffer Using SPLOOP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory",
			  "ized code.\n**Software Pipelining May Increase Code Size**\n**NOTE:**\nSoftware pipelining without the use of SPLOOP can lead to significant increases in code\nsize. To control code size for loops that get software pipelined, the --opt_for_space option is\nrecommended over the --disable_software_pipelining option. The --opt_for_space option is\ncapable of disabling non-SPLOOP software pipelining if necessary to achieve code size\nsavings, but it does not affect the SPLOOP capability (). SPLOOP does not significantly\nincrease code size, but can greatly speed up a loop. Using the --disable_software_pipelining\noption disables all software pipelining including SPLOOP.\n"
			]
		  },
		  {
			"title": "Register Allocation on the Intel® Itanium® Architecture",
			"url": "https://ssw.jku.at/Teaching/PhDTheses/Hoflehner/Diss_GerolfHoflehner_2010.pdf",
			"excerpts": [
			  "7.2 Data Speculation. IA-64 provides an advanced load (ld.a) and two advanced load check (chk.a, ld.c) instructions for data speculation. This enables the ...Read more",
			  "On IA-64 there are 64 predicate registers. Therefore the encoding of the qualifying predicate consumes 6 bits, which is one of the reasons why operations. ( ...Read more"
			]
		  },
		  {
			"title": "\n            BibTeX bibliography intel-ia-64.bib\n        ",
			"url": "http://ftp.math.utah.edu/pub/tex/bib/intel-ia-64.html",
			"excerpts": [
			  "\", URL = \"http://ieeexplore.ieee.org/iel5/6577/17552 ... data dependence speculation, and predication have individually been shown to be very effective.Read more"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531901.pdf",
			"excerpts": [
			  "The result of a control- or data-speculative calculation is checked for success or failure. If the check\nfails, a branch to *target* *25* is taken.",
			  "In the control_form, success is determined by a NaT indication for the source register. If the\nNaT bit corresponding to GR *r* *2* is 1 (in the gr_form), or FR *f* *2* contains a NaTVal (in the fr_form),\nthe check fails.",
			  "An entry is added to the ALAT. This allows later\ninstructions to check for colliding stores. If the referenced\ndata page has a non-speculative attribute, the target\nregister and NaT bit is cleared, and the processor\nensures that no ALAT entry exists for the target register.",
			  "The ALAT is queried using the general\nregister specifier *r* *1* (in the gr_form), or the floating-point register specifier *f* *1* (in the fr_form). If no\n ..",
			  "Regardless of ALAT hit or miss, base register updates\nare performed, if specified. An implementation may\noptionally cause the ALAT lookup to fail independent of\nwhether an ALAT entry matches. If not found, a load is\nperformed, and an entry is added to the ALAT (unless the\nreferenced data page has a non-speculative attribute, in\nwhich case no ALAT entry is allocated).",
			  "chk.a. *aclr r* *1* , *target* *25*",
			  "chk.a. *aclr f* *1* , *target* *25",
			  "**Table 2-30. sz Completers**",
			  "**sz Completer**",
			  "**Bytes Accessed**",
			  "1",
			  "1 byte",
			  "2",
			  "2 bytes",
			  "4 bytes",
			  "8",
			  "8 bytes",
			  "**Table 2-31. Load Types**",
			  "**ldtype**",
			  "**Completer**",
			  "**Interpretation**",
			  "**Special Load Operation**",
			  "none",
			  "Normal load",
			  "S",
			  "load",
			  "Certain exceptions may be deferred rather than\ngenerating a fault. Deferral causes the target register’s\nNaT bit to be set. The NaT bit is later used to detect\ndeferral.",
			  "**ld**",
			  "For the non-speculative load types, if NaT bit associated with GR *r* *3* is 1, a Register NaT\nConsumption fault is taken. For speculative and speculative advanced loads, no fault is raised, and\nthe exception is deferred. For the base-update calculation, if the NaT bit associated with GR *r* *2* is 1,\nthe NaT bit associated with GR *r* *3* is set to 1 and no fault is raised.",
			  "The value of the *ldhint* completer specifies the locality of the memory access.",
			  "speculative",
			  "4"
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | Proceedings of the 31st annual international symposium on Computer architecture",
			"url": "https://dl.acm.org/doi/10.5555/998680.1006735",
			"excerpts": [
			  "ISCA 2004Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including\npredication and explicitcontrol speculation, intended to enhance the compiler'sability\nto expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface. This study presents an empirical evaluation of the use of the ALAT and data ...",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC research results in Itanium 2 | ACM SIGARCH Computer Architecture News",
			"url": "https://dl.acm.org/doi/10.1145/1028176.1006735",
			"excerpts": [
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.",
			  "Explicitly-Parallel Instruction Computing (EPIC) providesarchitectural features, including predication and explicitcontrol speculation, intended to enhance the compiler'sability to expose instruction-level parallelism (ILP) incontrol-intensive programs",
			  "An Empirical Study of Data Speculation Use on the Intel Itanium 2 Processor",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface.",
			  "The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced\nLoad Address Table (ALAT) to support data speculation via an instruction set interface.",
			  "Using the Intel Itanium 2 microprocessor,the SPECint2000 benchmarks and the IMPACT Compilerfor IA-64, a research compiler competitive with thebest commercial compilers on the platform, we providean in situ evaluation of code generated using aggressive,EPIC-enabled techniques in a reality-constrained microarchitecture.Our work shows a 1.13 average speedup(up to 1.50) due to these compilation techniques, relativeto traditionally-optimized code at the same inlining andpointer analysis levels, and a 1.55 speedup (up to 2.30) relativeto GNU GCC, a solid traditional compiler.",
			  "We also demonstratethe increasing significance of runtime effects, such as datacache and TLB, in determining end performance and theinteraction of these effects with control speculation.",
			  "ALAT"
			]
		  },
		  {
			"title": "THE INTEL IA-64 COMPILER CODE GENERATOR",
			"url": "https://www.cs.virginia.edu/~skadron/cs854_uproc_survey/spring_2001/cs854/m5044.pdf",
			"excerpts": [
			  "The ECG consists of multiple phases occur-\nring in the order shown in Figure 1. The first\nphase, translation, converts the optimizer’s\nintermediate representation (IL0) of the pro-\ngram into the ECG IR. Predicate region for-\nmation, if conversion, and compare\ngeneration occur in the predication ",
			  "he ECG contains two schedulers: the soft-\nware pipeliner for targeted cyclic regions and\nthe global code scheduler for all remaining\nregions. Both schedulers make use of control\nand data speculation",
			  "e ECG’s register allocator must handle\nseveral IA-64 specific issues. These include\nnot-a-thing (NaT) bit maintenance during\nspill/fill, advanced load address table (ALAT)\nawareness for data-speculative registers, cor-\nrect rotating register allocation for the soft-\nware pipeliner, and predicate awarenes",
			  "aT\nbits are associated with a control speculation\nmechanism. The ALAT is the hardware mech-\nanism enabling data speculation. 1\nThe predicator\nPredication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features",
			  "Predication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features.",
			  "Data spec-\nulation allows unlikely memory dependencies\nto be broke",
			  "GCS selects the best candidate from all the\nspeculative and nonspeculative candidates based\non a cost-benefit analysis.",
			  "ulation check instruc-\ntions and recovery code 8 are generated as a\nbyproduct of speculation; load safety informa-\ntion 9 avoids unnecessary control speculat",
			  "Figure 6. Comparing data speculation usage:\nwithout (a) and with data speculation (b)."
			]
		  },
		  {
			"title": "Instruction Level Parallelism - the static scheduling approach",
			"url": "http://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/2007-08/Lectures/pdfs/ACA-Ch05-EPICandThreaded-CurrentVersion-WithoutHiddenSlides-4up.pdf",
			"excerpts": [
			  "IA64: Speculative, Non-Faulting Load",
			  ".s** fetches speculatively from memory",
			  "e. any exception due to **ld.s** is suppress",
			  "NOP, else a branch is taken to some compensation code",
			  "ld.a** starts the monitoring of any store to the sa",
			  "address as the advanced load",
			  "no aliasing has occurred since **ld.a, ld.c** is a NO",
			  "aliasing has occurred, **ld.c** re-loads from memor"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v7.4 - User's Guide",
			"url": "https://www.ti.com/lit/pdf/spru187",
			"excerpts": [
			  "Memory Alias Disambiguation*\n***4.5.4***\n***Memory Bank Conflict Algorithm***\nThe assembly optimizer uses the following process to determine if two memory access instructions might\nhave a memory bank conflict:",
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous. Memory alias disambiguation is the process of determining when such\nambiguity is not possible. When you cannot determine whether two memory references are ambiguous,\nyou presume they are ambiguous. This is the same as saying the two instructions have a memory\ndependence between them.",
			  "Every loop iterates some number of times before the loop terminates. The number of iterations is called",
			  "the *trip count* . The variable used to count each iteration is the *trip counter* . When the trip counter reaches",
			  "a limit equal to the trip count, the loop terminates. The C6000 tools use the trip count to determine",
			  "**Redundant Loops**",
			  "**Redundant Loops**",
			  "Beginning with v7.4.0 of the C6000 Code Generation Tools, the compiler and linker can provide automatic\nload speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load\noptimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the\ncompiler can prove to be allocated.\nIn",
			  "However, you can also specify the speculative loads threshold explicitly via the *--speculate_loads=n*",
			  "option, where *n* is at least the minimum required pad (as explained earlier), but you also need to consider",
			  "whether a larger threshold value would facilitate additional collapsing. This information is also provided, if",
			  "applicable. For example, in the above comment block, a threshold value of 14 might facilitate further",
			  "collapsing. If you choose the *auto* argument to --speculate_loads, the compiler will consider the larger",
			  "threshold value automatically.",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing",
			  "loads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler",
			  "cannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the",
			  "cannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the",
			  "compiler can predicate these loads to prevent over execution. However, this can increase register",
			  "compiler can predicate these loads to prevent over execution. However, this can increase register",
			  "pressure and might decrease the total amount collapsing which can be performed.",
			  "pressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0",
			  "to *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory",
			  "to *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory",
			  "location it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "location it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "**3.2.3.1**",
			  "**3.2.3.1**",
			  "Beginning with v7.4.0 of the C6000 Code Generation Tools, the compiler and linker can provide automatic",
			  "load speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load",
			  "load speculation via the *auto* argument to the --speculate_loads option (i.e. *--speculate_loads=auto* or *-*\n*mh=auto* ). Use of the *auto* argument makes it easier to use and benefit from speculative load",
			  "optimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the",
			  "optimizations. This option can generate speculative loads of up to 256 bytes beyond memory that the",
			  "compiler can prove to be allocated.",
			  "compiler can prove to be allocated.",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required",
			  "pre- and post-padding:",
			  "pre- and post-padding:",
			  "If the symbol of the speculatively loaded buffer is known during compile time, the linker will ensure the",
			  "If the symbol of the speculatively loaded buffer is known during compile time, the linker will ensure the",
			  "object pointed to by the symbol has the required padding to let the speculative load access legal",
			  "object pointed to by the symbol has the required padding to let the speculative load access legal",
			  "memory.",
			  "memory.",
			  "•",
			  "•",
			  "•",
			  "•",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of",
			  "data sections will allow legal accessing beyond the boundaries of the data sections.",
			  "data sections will allow legal accessing beyond the boundaries of the data sections.",
			  "The syntax of the pragma in C is:\n**\\ DATA_ALIGN (** *symbol* **,** *constant* **);**\n",
			  "The DATA_ALIGN pragma aligns the *symbol* in C, or the next symbol declared in C++, to an alignment\nboundary. The alignment boundary is the maximum of the symbol's default alignment value or the value of\nthe *constant* in bytes. The constant must be a power of 2.",
			  "count is the number of times a loop iterates. The trip count of the loop must be evenly divisible by *multiple* .",
			  "When compiling for C6600 devices, such an array is automatically aligned to a 16-byte boundary. When\ncompiling for C6400, C6400+, C6740, and C6600 devices, such an array is automatically aligned to an 8-\nbyte boundary. When compiling for C6200 or C6700, such an array is automatically aligned to 4-byte\nboundary, or, if the base type requires it, an 8-byte boundary",
			  "argument list as follows:",
			  "\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 16 == 0)",
			  "However, if the trip count could be any nonzero multiple of 5, the pragma would look like this:",
			  "\\ defined(_TMS320C6200) || defined(_TMS320C6700)\n\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 4 == 0)\n\\",
			  "If multiple\nMUST_ITERATE pragmas are specified for the same loop, the smallest max and largest min are used.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. You guarantee that",
			  "these properties are always true. Through the use of the MUST_ITERATE pragma, you can guarantee",
			  "that a loop executes a specific number of times.",
			  ")\n\\ defined(_TMS320C6400)\n\\ ALIGNED_ARRAY(ptr) _nassert((int) ptr % 8 == 0)",
			  "for(i = 0; i < trip_count; i++)",
			  "{ ...",
			  "This example tells the compiler that the loop executes between 8 and 48 times and that the trip_count",
			  "variable is\na multiple of 8 (8, 16, 24, 32, 40, 48). The compiler can now use all this information to generate the best",
			  "loop possible by unrolling better even when the --interrupt_threshold *n* option is used to specify that",
			  "interrupts do occur every *n* cycles.",
			  "the MUST_ITERATE pragma's third\nargument, multiple, is the most important and should always be specified.",
			  "9.20.2**\n**Using MUST_ITERATE to Expand Compiler Knowledge of Loo",
			  "\\ ALIGNED_ARRAY(ptr) /* empty */\n",
			  "MUST_ITERATE can specify a range for the trip count as well as a factor of the trip count.",
			  "**Using MUST_ITERATE and _nassert to Enable SIMD and Expand Compiler***\n***Knowledge of Loo",
			  "Through the use of MUST_ITERATE and _nassert, you can guarantee that a loop executes a certain\nnumber of times.",
			  "Through the use of MUST_ITERATE and _nassert, you can guarantee that a loop executes a certain\nnumber of times.",
			  "This example tells the compiler that the loop is guaranteed to run exactly 10 times:",
			  "This example tells the compiler that the loop is guaranteed to run exactly 10 times:",
			  "\\ MUST_ITERATE(10,10);",
			  "for (I = 0; I < trip_count; I++) { ...",
			  "for (I = 0; I < trip_count; I++) { ...",
			  "MUST_ITERATE can also be used to specify a range for the trip count as well as a factor of the trip count.",
			  "MUST_ITERATE can also be used to specify a range for the trip count as well as a factor of the trip count.",
			  "For example:",
			  "For example:",
			  "For example:",
			  "\\ MUST_ITERATE(8,48,8);",
			  "\\ MUST_ITERATE(8,48,8);",
			  "for (I = 0; I < trip; I++) { ...",
			  "for (I = 0; I < trip; I++) { ...",
			  "This example tells the compiler that the loop executes between 8 and 48 times and that the trip variable is\na multiple of 8 (8, 16, 24, 32, 40, 48).",
			  "The compiler can now use all this information to generate the best\nloop possible by unrolling better even when the --interrupt_threshold *n* option is used to specify that\ninterrupts do occur every *n* cycles.",
			  "The *TMS320C6000 Programmer's Guide* states that one of the ways to refine C/C++ code is to use word\naccesses to operate on 16-bit data stored in the high and low parts of a 32-bit register. Examples using\ncasts to int pointers are shown with the use of intrinsics to use certain instructions like _mpyh. This can be\nautomated by using the _nassert(); intrinsic to specify that 16-bit short arrays are aligned on a 32-bit\n(word) boundary.",
			  "The following examples generate the same assembly code:",
			  "The following examples generate the same assembly code:",
			  "The C6400+, C6740, and C6600 ISAs have a loop buffer which improves performance and reduces code\nsize for software pipelined loops. The loop buffer provides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.",
			  "u can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C6400/C6400+ CPU*\n ... \nway that improves performance while maintaining the semantics of the original order. Instruction\nscheduling is used to improve instruction parallelism and hide pipeline latencies. It can also be used to\nreduce code size.",
			  "The assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code",
			  "Like the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\n ... \nope",
			  "An LD(B/BU)(H/HU)(W) or ST(B/H/W) operation in linear assembly can have memory bank information\nassociated with it implicitly, by using the .mptr directive. The .mptr directive associates a register with the\ninformation that allows the assembly optimizer to determine automatically whether two memory operations\nhave a bank conflict. If the assembly optimizer determines that two memory operations have a memory\nbank conflict, then it does not schedule them in parallel within a software pipelined loop.",
			  "For example:\n.mptr a_0,a+0,16\n.mptr a_4,a+4,16\nLDW *a_0++[4], val1\n; base=a, offset=0, stride=16\nLDW *a_4++[4], val2\n; base=a, offset=4, stride=16\n.mptr dptr,D+0,8\nLDH *dptr++, d0\n; base=D, offset=0, stride=8\nLDH *dptr++, d1\n; base=D, offset=2, stride=8\nLDH *dptr++, d2\n; base=D, offset=4, stride=8",
			  "The goal is to find a software pipeline in which the following instructions are in parallel:\nLDW *a0++[2],val1 ; load a[0-1]\nbankx\n||\nLDW *a2++[2],val2 ; load a[2-3]\nbankx+2\nLDW *b0++[2],val1 ; load b[0-1]\nbanky\n||\nLDW *b2++[2],val2 ; load b[2-3]\nbanky+2",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute\nin parallel. See Section 3.2 for more information.",
			  "The assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in",
			  "There are five predicate registers on the C6200 and C6700, and\nsix predicate registers on the C6400, C6400+, and C6700+.",
			  "NOTE:**\nRefer to the *TMS320C6000 Programmer’s Guide* for details on the information and\nmessages that can appear in the Software Pipelining Information comment block before each\nloo",
			  "Example 3-1. Software Pipelining Information",
			  "Known Minimum Trip Count\n: 2",
			  "Known Maximum Trip Count\n: 2",
			  "Known Max Trip Count Factor\n: 2",
			  "Loop Carried Dependency Bound(^) : 4",
			  "Unpartitioned Resource Bound\n: 4",
			  "Partitioned Resource Bound(*)\n: 5",
			  "Resource Partition:\n;*\nA-side\nB-s",
			  "ii = 5",
			  "Register is live too long",
			  "ii = 6",
			  "Did not find schedule",
			  "ii = 7",
			  "Schedule found with 3 iterations in parallel",
			  "Prolog not removed",
			  "Collapsed prolog stages\n: 0",
			  "Minimum required memory pad : 2 bytes",
			  "Minimum safe trip count\n: 2",
			  "Turn Off Software Pipelining (--disable_software_pipelining Option)",
			  "TE:**\nSpecifying any --opt_for_space option turns off redundant loops.",
			  "3.4",
			  "Utilizing the Loop Buffer Using SPLOOP on C6400+, C6740, and C6600",
			  "The C6400+, C6740, and C6600 ISAs have a loop buffer which improves performance and reduces code\nsize for software pipelined loops. The loop buffer provides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.",
			  "nterrupt latency. Loops executing out of the loop buffer are interruptible.",
			  "You can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C6400/C6400+ CPU*",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "**Speculative Execution**",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "Turning Off Redundant Loops",
			  "Improves performance for loops with unknown trip counts and eliminates redundant loops.",
			  "Reduces or eliminates the need for speculated loads.",
			  "Reduces power usage.",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.2.x User's Guide ( ...",
			"url": "https://www.ti.com/lit/ug/sprui04b/sprui04b.pdf",
			"excerpts": [
			  "**Speculative Execution**",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "Collapsing can usually reduce the minimum safe trip count. If the minimum known trip count is less than\nthe minimum safe trip count, a redundant loop is required.",
			  "he compiler and linker can provide automatic load speculation via the *auto* argument to the --\nspeculate_loads option (i.e. *--speculate_loads=auto* or *-mh=auto* ). Use of the *auto* argument makes it\neasier to use and benefit from speculative load optimizations. This option can generate speculative loads\nof up to 256 bytes beyond memory that the compiler can prove to be allocated.",
			  "If the symbol of the speculatively loaded buffer is known at compile time, the linker ensures the object\npointed to by the symbol has the required padding to let the speculative load access legal memory.",
			  "If the symbol information is not known during compile time, the linker will ensure that the placement of\ndata sections will allow legal accessing beyond the boundaries of the data sections.",
			  "d.\nHowever, you can also specify the speculative loads threshold explicitly via the *--speculate_loads=n*\noption, where *n* is at least the minimum required pad (as explained earlier)",
			  "**Memory Alias Disambiguation**",
			  "Memory aliasing occurs when two instructions can access the same memory location. Such memory\nreferences are called ambiguous. Memory alias disambiguation is the process of determining when such\nambiguity is not possible.",
			  "Dependencies between instructions constrain the instruction schedule, including the software pipeline\nschedule. In general, the fewer the Dependencies, the greater freedom you have in choosing a schedule\nand the better the final schedule performs.",
			  "***5.6.1***",
			  "he Assembly Optimizer Handles Memory References (Default)***\nThe assembly optimizer assumes memory references are aliased, unless it can prove otherwis",
			  "Because alias analysis is very limited in the assembly optimizer, this presumption is often overly\nconservative.",
			  "In such cases, the extra instruction Dependencies, due to the presumed memory aliases,",
			  "The syntax of the pragma for C and C++ is:\n**\\ MUST_ITERATE (** *min* , *max* , *multiple* **)",
			  ".\nAll arguments are optional. For example, if the trip count could be 5 or greater, you can specify the\nargument list as follows:\n\\ MUST_ITERATE(5)",
			  "***Advice #30011 Improve Loop; Add _nasssert()***",
			  "\nYou may be able to help the compiler avoid producing redundant loops with the use of --\nprogram_level_compile --opt_level=3 (see Section 4.7 ) or the use of the MUST_ITERATE pragma (see\nSection 7.9.21 ).\n",
			  "The loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.\nYou can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C64x/C64x+ CPU and*\n*Instruction Set Reference Guide* for information on SPLOOP.",
			  "Most loops have memory access instructions. The compiler attempts to use wider load instructions, and\naligned memory accesses instead of non-aligned memory accesses to reduce/balance out resources used",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5) /* Note the blank field for max */\n",
			  "\nHowever, if the trip count could be any nonzero multiple of 5, the pragma would look like this:\n\\ MUST_ITERATE(5, , 5) /* Note the blank field for max */\n",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "It is sometimes necessary for you to provide min and multiple in order for the compiler to perform\nunrolling. This is especially the case when the compiler cannot easily determine how many iterations the\nloop will perform (that is, the loop has a complex exit condition).",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. When you use this\npragma, you are guaranteeing to the compiler that a loop executes a specific number of times or a\nnumber of times within a specified range.",
			  "The MUST_ITERATE pragma specifies to the compiler certain properties of a loop. When you use this\npragma, you are guaranteeing to the compiler that a loop executes a specific number of times or a\nnumber of times within a specified range.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "No statements are allowed between the MUST_ITERATE pragma and the for, while, or do-while loop to\nwhich it applies. However, other pragmas, such as UNROLL and PROB_ITERATE, can appear between\nthe MUST_ITERATE pragma and the loop.",
			  "For loops the MUST_ITERATE pragma's third argument, multiple, is the most important and should\nalways be specified.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "the MUST_ITERATE pragma should be applied to any other loops as often as possible. This\nis because the information provided via the pragma (especially the minimum number of iterations) aids the\ncompiler in choosing the best loops and loop transformations (that is, software pipelining and nested loop\ntransformations). It also helps the compiler reduce code size.",
			  "***4.14.17***",
			  "Turning Off Redundant Loops",
			  "OTE:**\nSpecifying any --opt_for_space option turns off redundant loops",
			  "Utilizing the Loop Buffer Using SPLOOP",
			  "OP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. The loop buffer\nprovides the following benefits:\n•\nCode size. A single iteration of the loop is stored in program memory.\n•\nInterrupt latency. Loops executing out of the loop buffer are interruptible.\n•\nImproves performance for loops with unknown trip counts and eliminates redundant loops.\n•\nReduces or eliminates the need for speculated loads.\n•\nReduces power usage.\nYou can tell that the compiler is using the loop buffer when you find SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end. Refer to the *TMS320C64x/C64x+ CPU and*\n*Instruction Set Reference Guide* for information on SPLOOP.\nWhen",
			  "When the --opt_for_space option is not used, the compiler will not use the loop buffer if it can find a faster\nsoftware pipelined loop without it. When using the --opt_for_space option, the compiler will use the loop\nbuffer when it can.",
			  "he compiler does not generate code for the loop buffer (SPLOOP/D/W) when any of the following occur:\n•\nii (initiation interval) > 14 cycle",
			  "About the Assembly Optimizer",
			  "**\nIf you are not satisfied with the performance of your C/C++ code after you have used all of the C/C++\noptimizations that are available, you can use the assembly optimizer to make it easier to write assembly\ncode for the C6000.\nThe assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\nparallel. The code generation tools attempt to software pipeline your code with inputs from you and with\n ... \nBa",
			  "\n*Avoiding Memory Bank Conflicts With the Assembly Optimizer",
			  "The internal memory of the C6000 family varies from device to device. See the appropriate device data\nsheet to determine the memory spaces in your particular device. This section discusses how to write code\nto avoid memory bank conflicts.\nMost C6000 devices use an interleaved memory bank scheme, as shown in Figure 5-1 . Each number in\nthe diagram represents a byte address. A load byte (LDB) instruction from address 0 loads byte 0 in\nbank 0. A load halfword (LDH) from address 0 loads the halfword value in bytes 0 and 1, which are also in\nbank 0. A load word (LDW) from address 0 loads bytes 0 through 3 in banks 0 and 1.\nBecause each bank is single-ported memory, only one access to each bank is allowed per cycle. Two\naccesses to a single bank in a given cycle result in a memory stall that halts all pipeline operation for one",
			  "When prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* . When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop. If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited. To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "The following messages appear if the loop is completely disqualified for software pipelining:",
			  "ad loop structure** . This error is very rare and can stem from the following",
			  "An asm statement inserted in the C code inner loop",
			  "Parallel instructions being used as input to the Linear Assembly Optimizer",
			  "Complex control flow such as GOTO statements and breaks",
			  "op contains a call** . Sometimes the compiler may not be able to inline a function call that is in a\nloop.",
			  "Because the compiler could not inline the function call, the loop could not be software pipelined.",
			  "oo many instructions** . There are too many instructions in the loop to software pipeline",
			  "tware pipelining disabled** . Software pipelining has been disabled by a command-line option, such\nas when using the --disable_software_pipelining option, not using the --opt_level=2 (or -O2) or --\nopt_level=3 (or -O3) option, or using the --opt_for_space=2 or --opt_for_space=3 option.",
			  "Specifying any --opt_for_space option turns off redundant loops.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read.",
			  "Sometimes, the\ncompiler can predicate these loads to prevent over execution.",
			  "However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.",
			  "When the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* .",
			  "hen the threshold is *n* , the compiler can allow a load to be speculatively executed as the memory\nlocation it reads will be no more than *n* bytes before or after some location explicitly read within the loop.",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "If\nthe *n* is omitted, the compiler assumes the speculative threshold is unlimited.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "To specify this in Code\nComposer Studio, select the Speculate Threshold check box and leave the text box blank in the Build\nOptions dialog box on the Compiler tab, Advanced category.",
			  "Otherwise, pipelining must be suppressed.",
			  "Both\nthese values can be found in the comment block preceding a software pipelined loop.",
			  "**Redundant Loops**\nEvery loop iterates some number of times before the loop terminates. The number of iterations is called\nthe *trip count",
			  "The C6000 tools use the trip count to determine\nwhether or not a loop can be pipelined.",
			  "The structure of a software pipelined loop requires the execution\nof a minimum number of loop iterations (a minimum trip count) in order to fill or prime the pipeline.",
			  "*Utilizing the Loop Buffer Using SPLOOP**\nThe loop buffer improves performance and reduces code size for software pipelined loops. ",
			  "The loop buffer\nprovides the following benefits:",
			  "Code size. A single iteration of the loop is stored in program memory.",
			  "Interrupt latency. Loops executing out of the loop buffer are interruptible.",
			  "Improves performance for loops with unknown trip counts and eliminates redundant loops.",
			  "Reduces or eliminates the need for speculated loads.",
			  "Reduces power usage.",
			  "SPLOOP(D/W) at the beginning of a\nsoftware pipelined loop followed by an SPKERNEL at the end.",
			  "When the --opt_for_space option is not used, the compiler will not use the loop buffer if it can find a faster\nsoftware pipelined loop without it.",
			  "When using the --opt_for_space option, the compiler will use the loop\nbuffer when it can.",
			  "The compiler does not generate code for the loop buffer (SPLOOP/D/W) when any of the following occur:",
			  "ii (initiation interval) > 14 cycles",
			  "When a loop is software pipelined, the comment block preceding the loop provides the following\ninformation:",
			  "Required memory pad for this loop",
			  "The minimum value of *n* needed to achieve this software pipeline schedule and level of collapsing",
			  "Suggestion for a larger value of *n* to use which might allow additional collapsing",
			  "This information shows up in the comment block as follows:",
			  "Minimum required memory pad : 5 bytes",
			  "Minimum threshold value\n: --speculate_loads=7",
			  "For further improvement on this loop, try option --speculate_loads=14",
			  " you choose the *auto* argument to --speculate_loads, the compiler will consider the larger\nthreshold value automatically.",
			  "he compiler and linker can provide automatic load speculation via the *auto* argument to the --\nspeculate_loads option (i.e. *--speculate_loads=auto* or *-mh=a",
			  "e of the *auto* argument makes it\neasier to use and benefit from speculative load optimizations.",
			  "This option can generate speculative loads\nof up to 256 bytes beyond memory that the compiler can prove to be allocated.",
			  "In addition, the compiler communicates information to the linker to help automatically ensure the required\npre- and post-padding:",
			  "It attaches a 2-tuple <x, y> to the comments to specify\nthe iteration and cycle of the loop an instruction is on in the software pipeline.",
			  "At optimization levels --opt_level=2 (or -O2) and -O3, the compiler attempts to software pipeline your\nloops.",
			  "Software-pipelined\nloops are sometimes difficult to debug because the code is not presented serially.",
			  "TE:**\nSoftware pipelining without the use of SPLOOP can lead to significant increases in code\nsize. To control code size for loops that get software pipelined, the --opt_for_space option is",
			  "An asm statement inserted in a C code loop will disqualify the loop for software pipelining. Software-\npipelining is a key optimization for achieving good performance. You may see reduced performance\nwithout software pipelining",
			  "Replace the asm() statement with native C, or an intrinsic function call to prevent this from happening.",
			  "he assembly optimizer performs several tasks including the following:\n•\nOptionally, partitions instructions and/or registers\n•\nSchedules instructions to maximize performance using the instruction-level parallelism of the C6000\n•\nEnsures that the instructions conform to the C6000 latency requirements\n•\nOptionally, allocates registers for your source code\nLike the C/C++ compiler, the assembly optimizer performs software pipelining. *Software pipelining* is a\ntechnique used to schedule instructions from a loop so that multiple iterations of the loop execute in\nparallel. The code generation tools attempt to software pipeline your code with inputs from you and wit",
			  "he --\ndisable_software_pipelining option affects both compiled C/C++ code and assembly optimized code.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute\nin parallel. See Section 4.2 for more information.",
			  "The C6000 architecture is partitioned into two nearly symmetric halves. The resource breakdown\ndisplayed in the software pipelining information in the asm file, is computed after the compiler has\npartitioned instructions to either the A-side or the B-side.",
			  "If the resources are imbalanced (i.e.; some\nresources on one side are used more than resources on the other) software pipelining is resource-bound,\nand the loop cannot be efficiently scheduled.",
			  "You can give loop trip-\ncount information to the compiler using the \"MUST_ITERATE\" pragma",
			  "*Advice #30010 Improve Loop; Add MUST_ITERATE pragma(2)",
			  "advice #30010: If you know that this loop will always execute at least\n<2> times, try adding \" MUST_ITERATE(2)\" just before the loop.",
			  "Advice #30011 Improve Loop; Add _nasssert()*",
			  "advice #30011: Consider adding assertions to indicate n-byte alignment\nof variables input1, input2, output if they are actually n-byte\naligned: _nassert((int)(input1) % 8 == 0).",
			  "The loop buffer improves performance and reduces code size for software pipelined loops.",
			  "If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining.",
			  "If the compiler has information about the trip-count for the\nloop, it can unroll the loop to balance resource usage, and get better pipelining.",
			  "***Turn Off Software Pipelining (--disable_software_pipelining Option)***",
			  "Software pipelining schedules instructions from a loop so that multiple iterations of the loop execute in\nparallel.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "Speculative Execution"
			]
		  },
		  {
			"title": "Itanium™ Processor Microarchitecture Reference",
			"url": "https://parallel.ru/sites/default/files/ftp/computers/intel/ia64/itanium.pdf",
			"excerpts": [
			  "When a compiler uses IA-64 control speculation, it is responsible for inserting chk.s instructions to check if recovery code needs to be executed. When a chk.s.Read more"
			]
		  },
		  {
			"title": "Rapid Development of a Flexible Validated Processor Model",
			"url": "https://liberty.princeton.edu/Publications/tech04-03_i2.pdf",
			"excerpts": [
			  "Since IA-64 is an EPIC ISA, data dependences are explicitly marked. In IA-64 this is done via stop bits that appear in a bundle. The encoding supports ..."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v 7.3 - User's Guide",
			"url": "https://coecsl.ece.illinois.edu/se423/datasheets/OMAP138Ref_Guides/C6000CCompiler7.3.pdf",
			"excerpts": [
			  "Optimizing Software Pipelining ... For more information about software pipelining, see the TMS320C6000 Programmer's Guide.Read more",
			  "Software pipelining schedules instructions from a loop so that multiple iterations of the loop execute in\nparallel.",
			  "At optimization levels --opt_level=2 (or -O2) and --opt_level=3 (or -O3), the compiler usually\nattempts to software pipeline your loops.",
			  "The --opt_for_space option also affects the compiler's decision to\nattempt to software pipeline loops.",
			  "In general, code size and performance are better when you use the\n--opt_level=2 or --opt_level=3 options.",
			  "The software pipelining information appears as a comment in the .asm file before a loop and for the\nassembly optimizer the information is displayed as the tool is running.",
			  "Example 3-1 illustrates the\ninformation that is generated for each loop.",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each\ncycle of the loop kernel and displays the instruction ordering of a single iteration of the software pipelined\nloop.",
			  "er to the *TMS320C6000 Programmer* ’ *s Guide* for details on the information and\nmessages that can appear in the Software Pipelining Information comment block before each\nloop",
			  "The terms defined below appear in the software pipelining information. For more information on each\nterm, see the *TMS320C6000 Programmer's Guide* ",
			  "**Loop unroll factor.** The number of times the loop was unrolled specifically to increase performance\nbased on the resource bound constraint in a software pipelined loop.",
			  "**Known minimum trip count.** The minimum number of times the loop will be executed.",
			  "**Known maximum trip count.** The maximum number of times the loop will be executed.",
			  "**Known max trip count factor.** Factor that would always evenly divide the loops trip count. This\ninformation can be used to possibly unroll the loop.",
			  "**Loop carried dependency bound.** The distance of the largest loop carry path. A loop carry path\noccurs when one iteration of a loop writes a value that must be read in a future iteration.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to\noptimize C/C++ code or linear assembly code.",
			  "The --debug_software_pipeline option places additional software pipeline feedback in the generated\nassembly file. This information includes a single scheduled iteration view of the software pipelined loop.",
			  "The number of times the loop was unrolled specifically to increase performance\nbased on the resource bound constraint in a software pipelined loop.",
			  "The minimum number of times the loop will be executed.",
			  "The maximum number of times the loop will be executed.",
			  "The maximum number of times the loop will be executed.",
			  "**3.2.3.1**\n**Speculative Execution**\nWhen prologs and epilogs are collapsed, instructions might be speculatively executed, thereby causing\nloads to addresses beyond either end of the range explicitly read within the loop. By default, the compiler\ncannot speculate loads because this could cause an illegal memory location to be read. Sometimes, the\ncompiler can predicate these loads to prevent over execution. However, this can increase register\npressure and might decrease the total amount collapsing which can be performed.\nWhen the --speculate_loads= *n* option is used, the speculative threshold is increased from the default of 0\nto *n* "
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler v8.1.x",
			"url": "https://www.ti.com/lit/ug/sprui04a/sprui04a.pdf",
			"excerpts": [
			  "3. Using the C/C++ Compiler ................................................................................................... 20. 3.1. About the Compiler.Read more"
			]
		  },
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Advanced loads are available in integer ( ld.a ), floating-point ( ldf.a ), and floating-point pair\n( ldfp.a ) forms.",
			  "When an advanced load is executed, it allocates an entry in a structure called the\nAdvanced Load Address Table (ALAT).",
			  "A check instruction must be executed before the result of an advanced load can be used by any\nnon-speculative instruction.",
			  "If no matching entry is found, the speculative results need to be recomputed:\n• Use a chk.a if a load and some of its uses are speculated. The chk.a jumps to\ncompiler-generated recovery code to re-execute the load and dependent instructions.\n",
			  "• Use a ld.c if no uses of the load are speculated. The ld.c reissues the load.\n",
			  "\nEntries are removed from the ALAT due to:\n• Stores that write to addresses overlapping with ALAT entries.\n• Other advanced loads that target the same physical registers as ALAT entries.\n• Implementation-defined hardware or operating system conditions needed to maintain\ncorrectness.\n•",
			  "The ALAT is the structure that holds the state necessary for advanced loads and checks to operate\ncorrectly.",
			  "Control speculation and data speculation are not mutually exclusive; a given load may be both\ncontrol and data speculative.",
			  "Predication is the conditional execution of an instruction based on a qualifying predicate.",
			  "4**\n**Combining Control and Data Speculation**\nControl speculation and data speculation are not mutually exclusive; a given load may be both\ncontrol and data speculative. Both control speculative ( ld.sa , ldf.sa , ldfp.sa ) and non-control\nspeculative ( ld.a , ldf.a , ldfp.a ) variants of advanced loads are defined for general and\nfloating-point registers",
			  "2.4.4**\n**Data Speculation**\nData speculation allows loads to be moved above possibly conflicting memory references.\n*Advanced loads* exclusively refer to data speculative load",
			  "The Itanium architecture provides extensive support for software-pipelined loops, including register\nrotation, special loop branches, and application registers.",
			  "The Itanium architecture enables the compiler to exploit\nspeculative information without sacrificing the correct execution of an application",
			  "The Itanium architecture allows issuing of independent instructions in bundles (three instructions\nper bundle) for parallel execution",
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register",
			  "The br.cloop instruction does not have a branch predicate. Instead, the branching\ndecision is based on the value of the LC register.",
			  "the Itanium architecture explicitly supports software\npipelining, large register files, and compiler-controlled speculation."
			]
		  },
		  {
			"title": "The Itanium (I64) family",
			"url": "https://users.nik.uni-obuda.hu/sima/letoltes/Processor_families_Knowledge_Base_2019/Intels_Itanium_(IA64)_family_2018_10_29.pdf",
			"excerpts": [
			  "ontrol speculation\n[ 13\n]\n� IA\n- 64 provides a new group of loads, called\nspeculative loads (\nld.s\n)\nwhich can\nsafely be scheduled before one or more prior branches",
			  "In the block where the programer originally placed the load, the compiler\nschedules a\nspeculation check (\nchk.s\n),\nas shown in the next Figure.",
			  " runtime, if a speculative load results in an exception, the exception is defered\nand a\ntoken (\nNaT\n)\nis written to the target register.",
			  "he\nchk.s\ninstruction checks then the target register for a\nNaT\ntoken\n, and if\na token is present the execution branches to a special",
			  "execute the load non\n- speculatively.",
			  "ta speculation substitutes a conventional load by a\nadvanced load (\nld.a\n)\nand a\ndata\n-\nverifying load (\nld.",
			  "The compiler can move a data\n- speculative load above potentialy aliasing stores\nto alow the load an early schedule",
			  "The compiler schedules the data verifying load in place of the conventional load.",
			  "he\nprocesor uses hardware to detect whether an alias has ocured.\nWhen no alias has ocured, the data\n- speculative load has already loaded the\ncorect value.",
			  "the oposite case, the data\n- verifying load re\n- executes the load and stals the\nprocesor to ensure that\nthe corect data returns\nin time\nfor subsequent uses\nof the load",
			  "ALAT\nis\nthe Advanced Load Adress Table in Itanium architectures.",
			  "It\nis implemented with an asociative\nmemory and\nis used to store information\nrelated\nto advance load instructions,\nto implement data speculation [\n20\n].",
			  "There are two types of speculation; control speculation and data speculation, as\nindicated below and will be discussed subsequently.",
			  "Control speculation",
			  "Data speculation",
			  "Example for control speculation"
			]
		  },
		  {
			"title": "Itanium processor microarchitecture",
			"url": "https://www.cs.ucr.edu/~bhuyan/cs162/LECTURE7b.pdf",
			"excerpts": [
			  "he advanced\nload address table (ALAT) to support data\nspeculation and, with hardware, to manage\nthe deferral of exceptions on speculative exe-\ncution",
			  "In the common case, there’s no match, the\nALAT state is unchanged, and the advanced\nload result is used normally.",
			  "In the case of an\noverlap, all address-matching advanced loads\nin the ALAT are invalidated.",
			  "hen only the load and no depen-\ndent instructions were boosted, a load-check\n(ld.c) instruction is used, and the load\ninstruction is reissued down the pipeline, this\ntime retrieving the updated memory data",
			  "he ld.c\ninstruction can be issued in parallel with\ninstructions dependent on the load result\ndata. ",
			  "e advanced load and its\ndependent uses were boosted, an advanced\ncheck-load (chk.a) instruction traps to a user-\nspecified handler for a special fix-up code that\nreissues the load instruction and the opera-\ntions dependent on t",
			  "The hardware control for speculation\nis quite simple: adding an extra bit to the data\npath supports deferred exception tokens.",
			  "The hardware control for speculation\nis quite simple: adding an extra bit to the data\npath supports deferred exception tokens.",
			  "32-entry ALAT",
			  "The execution control is augmented with a bookkeeping structure called the advanced load address table (ALAT) to support data speculation and, with hardware, to ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "http://bitsavers.informatik.uni-stuttgart.de/components/ti/TMS320xx/320C6000/SPRU198D_TMS320C6000_Programmers_Guide_Mar2000.pdf",
			"excerpts": [
			  "Use –pm program level optimization to reduce memory pointer aliasing.",
			  "Add restrict declarations to all pointers passed to a function whose objects do\nnot overlap.",
			  "Use the .mdep and .no_mdep assembly optimizer directives.",
			  "The compiler does a much better job of alias disambiguation than the assem-\nbly optimizer. C source provides much more informat",
			  "The command line options:\ncl6x –pm –o3\nhave several effects, of which improved alias disambiguation is only one.",
			  "e\ndefault presumption on aliases which cannot be disambiguated is the pessi-\nmistic one: they are aliases.",
			  "C/C++ Compiler and Alias Disambiguation",
			  "rrect use of the const and restrict keywords helps the alias disambi-\nguation problem. The const keyword tells the compiler that the data object\nwill not be modified, even by an alias. So, any const qualified memory read\ncannot possibly be an alias to a memory write.",
			  "he restrict keyword tells the compiler that\nwithin the scope of a particular pointer, only the object pointed to can be ac-\ncessed by that pointer",
			  "Memory Alias Disambiguation versus Memory Bank Conflict Detection",
			  "f a ’C6000 execute packet (a set of instructions which execute in parallel) in-\ncludes two references to memory, and both of those references are to the\nsame memory bank, because each bank is single-ported memory, the pipeline\nstalls for one cycle while the second memory word is accessed. This is called\na bank conflict, and it is obviously worth avoiding",
			  "he assembly optimizer pro-\nvides a directive called .mptr for the purpose of solving this problem. See the\nTMS320C6000 Optimizing C/C++ Compiler User’s Guide for all the details",
			  "Memory Alias Disambiguation",
			  "Memory alias disambiguation analyzes whether a dependence, through a\nmemory location, exists between a given pair of instructions. Dependences\nbetween instructions are then used to determine the fastest legal schedule of\nthose instructions.",
			  "wo final sections discuss how the C/C++ compiler handles memory alias dis-\nambiguation, and the differences between memory alias disambiguation and\nmemory bank conflict detection",
			  "The compiler does a much better job of alias disambiguation than the assem-\nbly optimizer. C source provides much more information to work wit",
			  "cl6x -ma",
			  "he command line option:\ncl6x -mt\nmeans something different to the compiler than it does to the assembly opti-\nmizer. As presented already, this option reverses the assembly optimizer’s\npessimistic assumption that memory references it cannot disambiguate must\nbe aliases. To the compiler, this same option means several specific instances\nof memory aliases do not occur in your C/C++ code"
			]
		  },
		  {
			"title": "(PDF) EPIC: An Architecture for Instruction-Level Parallel Processors",
			"url": "https://www.researchgate.net/publication/213894488_EPIC_An_Architecture_for_Instruction-Level_Parallel_Processors",
			"excerpts": [
			  "itecture,\nVLIW architecture,\ninstruction - level\npa rallelism, MultiOp, non -\nunit assumed latencies,\nNUAL, rotating register\nfiles, unbundled branches,\ncontrol speculation,\nspeculative opcodes,\nexception tag,\npredicated execution,\nfully - resolved predicates,\nwired - OR and wired - AND\ncompare opcodes,\nprioriti zed loads and\nstores, data speculation,\ncache specifiers,\nprecise interrupts,\nNUAL - freeze and NUAL -\ndrain semantics, delay\nbuffers, replay buffers,\nEQ and LEQ semantics,\nlatency stalling,\nMultiOp - P and MultiOp -\nS\nsemantics,\ndynamic translatio n,\nMultiTemplate and\nVariOp instruction\nformats, history of "
			]
		  },
		  {
			"title": "Integrated Predicated and Speculative Execution in ... - IMPACT",
			"url": "http://impact.crhc.illinois.edu/shared/papers/isca-98-epic.pdf",
			"excerpts": [
			  "\nThe IMPACT EPIC Architecture exposes instruction-level par-\nallelism through predicated execution and compiler-directed con-\ntrol and data dependence speculation. This section of the paper\npresents the architectural features and semantics that enable these\ntechnologies. As",
			  " by additions to operation encodings and to the register file\nand by addition of the Memory Conflict Buffer, a device which\nchecks speculated loads for conflicts with subsequent stores.\n",
			  "sion**\nPredication, control speculation, and data speculation in a co-\nherent architecture can be effectively used by the compiler to en-\nhance the performance of wide-issue processors. Predication al-\nlows the compiler to rewrite the program control structure and to\noverlap separate execution paths. Control and data speculation al-\nlow the compiler to break dependences to reduce the dependence\nheight of computation. This paper shows that these techniques\ncomplement each other enabling the compiler to achieve additive\nperformance benefits.\nThe IMPACT"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol2.pdf",
			"excerpts": [
			  "Unaligned IA-64 semaphore references ( cmpxchg, xchg , fetchadd ) result in an Unaligned Data\nReference fault regardless of the state of PSR.ac.",
			  "When PSR.ac is 0, IA-64 data memory references that are not aligned may or may not result in an\nUnaligned Data Reference fault based on the implementation. The level of unaligned memory\nsupport is implementation specific. However, all implementations will raise an Unaligned Data\n**Table 4-16. ALAT Behavior on Non-faulting Advanced/Check Loads**",
			  "**Memory**",
			  "**Attribute**",
			  "**ld.sa**",
			  "**ld.a**",
			  "**ld.c.clr,**",
			  "**ld.c.clr.acq,**",
			  "**ldf.c.clr**",
			  "**ld.c.nc,**",
			  "**ldf.c.nc**",
			  "**Response**",
			  "**Response**",
			  "**Response**",
			  "**Response**",
			  "**no NaT**",
			  "**NaT**",
			  "**hit**",
			  "**hit**",
			  "**ALAT**",
			  "**ALAT**",
			  "**ALAT**",
			  "**ALAT**",
			  "**miss**",
			  "**miss**",
			  "speculative\nalloc\nremove\nalloc\nremove\nnop\nunchanged a\na. May allocate a new ALAT entry if size and/or address are different than the corresponding ld.a or ld.sa whose\nALAT entry was matched.",
			  "alloc\nnon-speculative\nn/a\nremove\nremove\nundefined\nnop\nundefined\nmust not\nalloc\nlimited speculation\nn/a\nremove\nalloc\nremove\nnop\nunchanged a\nalloc\n*IA-64 Addressing and Protection*",
			  "4-39",
			  "Reference fault if the IA-64 datum spans a 4K aligned boundary, and many implementations will\nraise an Unaligned Data Reference fault if the IA-64 datum spans a cache line. Implementations\nmay also raise an Unaligned Data Reference fault for any other unaligned IA-64 memory reference.",
			  "\n*About the IA-64 System Programmer* *’* *s Guide*",
			  "Chapter 15, “ Context Management ” describes how operating systems need to preserve IA-64\nregister contents. In addition to spilling and filling a register ’ s data value, IA-64 also requires\nsoftware to preserve control and data speculative state associated with that register, i.e. its NaT bit\nand ALAT state. This chapter also discusses IA-64 system architecture mechanisms that allow an\noperating system to significantly reduce the number of registers that need to be spilled/filled on\ninterruptions, system calls, and context switches. These optimizations improve the performance of\nan IA-64 operating system by reducing the amount of required memory traffic. This chapter is\nuseful for IA-64 operating system programmers.",
			  "Chapter 16, “ Memory Management ” introduces various IA-64 memory management strategies.",
			  "The IA-64 region register model, protection keys, and the virtual hash page table usage models are\ndescribed. This chapter is of interest to IA-64 virtual memory management software developers.",
			  "Chapter 17, “ Runtime Support for Control and Data Speculation ” describes the operating system\nsupport that is required for control and data speculation. This chapter describes various speculation\nsoftware models and their associated operating system implications. This chapter is of interest to\nIA-64 operating system developers and IA-64 compiler writers.",
			  "To support advanced loads in the presence of a virtual alias, the processor ensures that the\nAdvanced Load Address Table (ALAT) is resolved using physical addresses and is coherent with\nphysical memory.",
			  "Reference fault if the IA-64 datum spans a 4K aligned boundary, and many implementations will\nraise an Unaligned Data Reference fault if the IA-64 datum spans a cache line.",
			  "Table 4-16. ALAT Behavior on Non-faulting Advanced/Check Loads",
			  "memory",
			  "Attribute",
			  "ld.sa",
			  "ld.a",
			  "ld.c.clr,",
			  "ld.c.clr.acq,",
			  "ldf.c.clr",
			  "ld.c.nc,",
			  "ldf.c.nc",
			  "Response",
			  "Response",
			  "Response",
			  "Response",
			  "no NaT",
			  "NaT",
			  "hit",
			  "hit",
			  "ALAT",
			  "ALAT",
			  "ALAT",
			  "ALAT",
			  "miss",
			  "miss",
			  "speculative",
			  "a. May allocate a new ALAT entry if size and/or address are different than the corresponding ld.a or ld.sa whose\nALAT entry was matched.",
			  "non-speculative",
			  "undefined",
			  "undefined",
			  "must not",
			  "limited speculation",
			  "n/a",
			  "n/a",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "remove",
			  "nop",
			  "nop",
			  "nop",
			  "unchanged a",
			  "unchanged a",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "alloc",
			  "*IA-64 Addressing and Protection*",
			  "*IA-64 Addressing and Protection*",
			  "..."
			]
		  },
		  {
			"title": "HPL-PD Architecture Specification:Version 1.1 - shiftleft.com",
			"url": "https://shiftleft.com/mirrors/www.hpl.hp.com/techreports/93/HPL-93-80R1.pdf",
			"excerpts": [
			  "HPL-PD defines a new philosophy of ILP computing, called Explicitly\nParallel Instruction Computing (EPIC) [2-4], which represents an evolution of the VLIW\narchitecture 2 .",
			  "The\narchitecture\nadmits\nEPIC,\nVLIW\nand\nsuperscalar\nimplementations so as to provide a basis for understanding the\nmerits\nand\ndemerits\nof\nthese\ndifferent\nstyles\nof\nimplementation.",
			  "This report describes those parts of the architecture that are common to all machines in the\nfamily. It introduces basic concepts such as the structure of an instruction, instruction execution\nsemantics, the types of register files, etc. and describes the semantics of the operation repertoire."
			]
		  },
		  {
			"title": "Effective Compiler Support for Predicated Execution Using ...",
			"url": "https://safari.ethz.ch/architecture/fall2022/lib/exe/fetch.php?media=144965.144998.pdf",
			"excerpts": [
			  "If-conversion can eliminate all non- loop backward branches from a program. Predicated execution support has been used effectively for scheduling both numeric ...Read more"
			]
		  },
		  {
			"title": "Modulo Scheduling Aware Cluster Assignment",
			"url": "https://clei.org/proceedings_data/CLEI2008/Anales/pdf/CLEI/CLEI_2008_067.pdf",
			"excerpts": [
			  "The recurrence bound. (RecMII) is determined by loop-carried data dependences. The MII is thus determined as MAX(ResMII, RecMII). Methods for finding the ...Read more",
			  "VLIW compilers use\nsoftware pipelining to schedule loop body instructions such that multiple loop\niterations execute in parallel.",
			  "In this paper, we\ndescribe modulo scheduling, which is a method for implementing software\npipelining.",
			  "A critical phase in a VLIW compiler is cluster\nassignment for software pipelined loop body instructions.",
			  "Very long instruction word (VLIW) processors are well suited for\ntoday’s high performance processing applications, which are characterized by\nmathematically oriented loop kernels and abundant instruction level\nparallelism. VLIW processors have multiple functional units, which are often\npartitioned into clusters with local register files.",
			  "Cluster assignment is the\nprocess of partitioning instructions to clusters with the goals of maximizing\nparallelism and minimizing communication overhead.",
			  "We present a cluster assignment algorithm that is designed to run\nbefore modulo scheduling.",
			  "Finally, we present experimental results that\ndemonstrate the effectiveness of the cluster assignment algorithm for the Texas\nInstruments TMS320C6x 2-cluster VLIW processor."
			]
		  },
		  {
			"title": "Instruction Scheduling (Part 2) - Software Pipelining",
			"url": "https://www.cs.utexas.edu/~pingali/CS380C/2007fa/lectures/swp.pdf",
			"excerpts": [
			  "Lecture Outline",
			  "Previous Lecture",
			  "1 Introduction to instruction scheduling",
			  "2 Representation of data dependences and resource constraints",
			  "3 Acyclic scheduling: list scheduling",
			  "Loop scheduling: definition of software pipelining",
			  "Parameters of software-pipelined schedules",
			  "Heuristics: *modulo scheduling*",
			  "Modulo Scheduling",
			  "\nSchedule a single iteration such as it is valid repeted every *II*\ncycle\n",
			  "Need to fix *II* before scheduling one iteration",
			  "If not possible, then increase the value of targeted *II*",
			  "Iterative Modulo Scheduling (IMS)",
			  "Principle",
			  "Principle",
			  "Seminal work on modulo scheduling by Bob Rau, MICRO-27\n(1994)",
			  "Extension of list scheduling to loop\nNotion of budget\nCompiler\nImplemented in Intel’s compiler ICC",
			  "Lecture Overview\nIntroduction\nModulo Scheduling\nHardware Support\nConclusion",
			  "Itanium 2 Architecture",
			  "EPIC (Explicitly Parallel Instruction Computing)",
			  "VLIW-like architecture except for memory operations\n(out-of-order memory accesses)\nIPC up to 6 (Instructions Per Cycle)",
			  "Compiler has to expose parallelism (+ constraints on\ninstruction grouping: *bundling* )\nProvides fully support for software-pipelined schedules:\nrotating register files and predication",
			  "Conclusion\nInstruction Scheduling\nInput: set of instructions\nOutput: order on instructions\nMust respect both data dependences and resource constraints\nNeed a model to represent such constraints (DDG, reservation\ntables, automaton, . . . )\nSchedule Types\nBasic block: List scheduling\nLoop:\nList scheduling on the body ⇒ do not benefit from\nintra-iteration parallelism\nSoftware pipelining ⇒ Modulo scheduling heuristics (IMS,\nSMS)",
			  "Software: Modulo variable expansion. Hardware: Rotating register file. Predication. Small irregular control ⇒ If-conversion. Kernel-only loop.Read more",
			  "Provides fully support for software-pipelined schedules:",
			  "VLIW-like architecture except for memory operations",
			  "Instruction is commited iff the predicate is true",
			  "Add one bit (predicate) per instruction",
			  "Convert control dependence into data dependence",
			  "rotating register files and predication",
			  "rotating register files and predication",
			  "\nItanium 2 Architecture",
			  "Software-pipelined schedule with *II* = 1:",
			  "Predication"
			]
		  },
		  {
			"title": "Probabilistic predicate-aware modulo scheduling",
			"url": "https://ieeexplore.ieee.org/document/1281671/",
			"excerpts": [
			  "Predicated execution enables the removal of branches by converting segments of branching code into sequences of conditional operations.",
			  "However, a resource is only put to productive use when the predicate associated with an operation evaluates to True.",
			  "To reduce this superfluous commitment of resources, we propose probabilistic predicate-aware scheduling to assign multiple operations to the same resource at the same time, thereby over-subscribing its use.",
			  "Assignment is performed in a probabilistic manner using a combination of predicate profile information and predicate analysis aimed at maximizing the benefits of over-subscription in view of the expected degree of conflict.",
			  "Conflicts occur when two or more operations assigned to the same resource have their predicates evaluate to True.",
			  "A predicate-aware VLIW processor pipeline detects such conflicts, recovers, and correctly executes the conflicting operations.",
			  "By increasing the effective throughput of a fixed set of resources, probabilistic predicate-aware scheduling provided an average of 20% performance gain in our evaluations on a 4-issue processor, and 8% gain on a 6-issue processor.",
			  "**Published in:** [International Symposium on Code Generation and Optimization, 2004. CGO 2004.](/xpl/conhome/9012/proceeding)"
			]
		  },
		  {
			"title": "Probabilistic Predicate-Aware Modulo Scheduling - CCCP",
			"url": "https://cccp.eecs.umich.edu/papers/msmelyan-cgo04.pdf",
			"excerpts": [
			  "he scheduler chooses its\ninitial II to be the maximum of two lower bounds. The resource-\nconstrained lower bound ( **ResMII** ) is equal to the number of cycles\nthat the most heavily used resource is busy during a single iteration\nof the",
			  "The recurrence-constrained lower bound ( **RecMII** ) is\ndetermined by longest cycle in the dependence ",
			  "Iterative Modulo Scheduling (IMS) [16] is a software pipelining\ntechnique that interleaves successive iterations of a loop.",
			  " IMS is to find a valid schedule for an innermost loop that can be\noverlapped with itself multiple times so that a constant interval be-\ntween successive iterations ( **Initiation Interval** ( **II** )) is minimize",
			  "he II-cycle code region that achieves the maximum overlap be-\ntween iterations is called the **kern",
			  "MS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time",
			  "signment is performed in a probabilistic manner using a\ncombination of predicate profile information and predicate analy-\nsis aimed at maximizing the benefits of over-subscription in view\nof the expected degree of conflict.",
			  "nflicts occur when two or\nmore operations assigned to the same resource have their predicates\nevaluate to True. A predicate-aware VLIW processor pipeline de-\ntects such conflicts, recovers, and correctly executes the conflicting\noperations.",
			  "predicate-aware VLIW processor pipeline de-\ntects such conflicts, recovers, and correctly executes the conflicting\noperations.",
			  "In this work we apply this technique to modulo\nscheduled loop regions which are generally resource constrained.",
			  " DPAMS\nschedule is shown in Figure 2(b). Disjoint operations *A2* and *A5*\nare scheduled in the same slot of the M",
			  "s a result, the ALU is\nalways utilized at Time 8, and the achieved schedule length is 9 cy-\ncles, an 11% performance improvement over the 10 cycle baselin",
			  "PAMS is\na generalization of deterministic predicate-aware modulo schedul-\ning (DPAMS) [23]. DPAMS allows operations to share the same\nresource when their predicates are provably disjoint, i.e. at most\none will evaluate to True at run-time",
			  "Thus, PPAMS significantly increases the utilization of may-use re-\n ... \n",
			  "predicates",
			  "operation",
			  "Activation",
			  "execution",
			  "Frequencies Frequencies",
			  "p1:",
			  "0.2",
			  "A1: 1.0",
			  "p2:",
			  "0.8",
			  "M1: 1.0",
			  "p3:",
			  "0.09",
			  "C1: 1.0",
			  "p4:",
			  "0.01",
			  "C2: 1.0",
			  "p5:",
			  "0.11",
			  "C3: 1.0",
			  "A2: 0.2",
			  "A3: 0.09",
			  "A4: 0.01",
			  "C4: 1.0",
			  "M2: 0.11",
			  "B:",
			  "1.0",
			  "DPAMS guarantees that no conflicts will occur by combining\nonly disjoint operations to share the same resource [23].",
			  "DPAMS guarantees that no conflicts will occur by combining\nonly disjoint operations to share the same resource [23].",
			  "AMS\nschedule is shown in Figure 2(b). Disjoint operations *A2* and *A5*\nare scheduled in the same slot of the MRT. As a result, the ALU is\nalways utilized at Time 8, and the achieved schedule length is 9 cy-\ncles, an 11% performance improvement over the 10 cycle baseli",
			  "central idea of PPAMS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time.",
			  "central idea of PPAMS (probabilistic predicate-aware mod-\nulo scheduling) is to allow over-subscription of may-use resources\nwherein multiple operations are allowed to reserve the same re-\nsource at the same time.",
			  "s a consequence, it is possible for dy-\nnamic over-subscription of resources to take place so that two or\nmore resource-sharing operations will have their predicates evalu-\nate to True at runtime, resulting in a resource confli",
			  "PAMS is\na generalization of deterministic predicate-aware modulo schedul-\ning (DPAMS) [23]"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://www.researchgate.net/publication/305064866_Iterative_Modulo_Scheduling",
			"excerpts": [
			  "This paper presents iterative modulo scheduling, a practical algorithm that is capable of dealing with realistic machine models. The paper also characterizes ...Read more",
			  "Iterative modulo scheduling: An algorithm for software pipelining loops. December 1994. B. Ramakrishna Rau. Module scheduling is a framework within which a wide ...Read more"
			]
		  },
		  {
			"title": "Compiler Optimization-Space Exploration",
			"url": "https://liberty.princeton.edu/Publications/cgo03_ose.pdf",
			"excerpts": [
			  "Hyperblock formation and corresponding heuristics have been proposed to determine when and how to predicate code [16]. However, even with these techniques ...Read more"
			]
		  },
		  {
			"title": "An Implementation of Swing Modulo Scheduling with ...",
			"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
			"excerpts": [
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.\nE",
			  "\nEMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.\n",
			  "\nIf-conversion is performed to transform the loop body into straight line predicated code using\nthe RK algorithm [32].\n",
			  "While If-conversion does allow loops with conditional statements to be software pipelined, the\ndownside is that both execution path’s resources must be summed when determining the resource\nconstraints for the loop. This can lead to a pipelined loop that does not have an optimal II.\n3",
			  "Eichenberger and Edward S. Davidson. Stage scheduling: A technique to reduce the register requirements of a modulo schedule. In MICRO 28: Proceedings of the ...Read more",
			  "This thesis details the implementation of Swing Modulo Scheduling, a Software Pipelining tech- nique, that is both effective and efficient in terms of ...",
			  "Software Pipelining , reconstructs the loop\nsuch that each iteration of the loop is executed at a constant interval, producing an optimal schedule.",
			  "Modulo Scheduling is a family of Software Pipelining\ntechniques that uses a “modulo” technique (instead of maximal unrolling) to place instructions in\n ... ",
			  "Modulo Scheduling algorithms aim to create a schedule with\nan Initiation Interval (II) equal to MII, which is the smallest II possible and results in the most\noptimal schedule. The lower the II, the greater the parallelism.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,. Modulo Scheduling the loop, applying modulo variable expansion, ...Read more",
			  "For complex architectures, the iterative techniques (IMS and IRIS) both",
			  "The second group, Modulo Scheduling [27, 33, 12, 21, 4, 16, 28], aims to create a schedule with no\nresource or dependence conflicts that can be repeated at a constant interval. Since Swing Modulo\nScheduling (SMS) falls into the second category, this thesis will briefly describe a few of the other\nwell known algorithms in this category.",
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.",
			  "The best type of hardware support is one that provides Predicated Execution , such as IA64 [20].\nPredicated Execution allows instructions to be nullified during their execution in the pipeline. So if\nan instruction is speculatively moved above a branch and the branch is taken, even if the instruction\nis already in the processor’s pipeline, the instruction can be nullified",
			  "If-conversion replaces conditional branches with a compare instruction that sets a flag. Instruc-\ntions that were dependent upon the conditional branch are now instructions that only execute if\nthe flag is set. If-conversion typically requires hardware support, but some algorithms [41] have\nmade slight modifications to avoid this. Hardware support such as predicated execution, common\non VLIW and EPIC architectures (such as IA64) set a conditional flag per instruction and allow\ninstructions to execute only when the conditional flag is tru",
			  "tware Pipelining [9] is a group of techniques that aim to exploit Instruction Level Parallelism\n(ILP) by overlapping successive iterations of a loop.\nOver the years, two main approaches to\nSoftware Pipelining have developed: Move-then-Schedule , and Schedule-then-Move . The Move-\nthen-Schedule techniques [16, 30, 14, 22], which will not be discussed in this thesis, move instructions\nacross the back-edge of the loop in order to achieve a pipelined loop. The Schedule-then-Move\nalgorithms attempt to create a schedule that maximizes performance and constructs a new pipelined\nloop composed of instructions from current and previous iterations.",
			  "The second main contribution of this thesis is to extend the Swing Modulo Scheduling algorithm\nto handle more than single basic block loops. We have modified the algorithm to handle superblock\nloops (single-entry, multiple-exit, multiple basic block loops). These extensions allow the algorithm\nto be applied at various stages of compilation:\n• Statically: Just as the original Swing Modulo Scheduling algorithm was done statically, the\nextensions simply expand the number of loops acceptable for this transformation.\n• Offline: Using profile information, hot paths (subsets of the loop in the form of single entry,\nmultiple exit loops), can be recognized and be transformed by our extensions.\n• Runtime: Similar to the profile driven optimization, hot paths are found during program\n .."
			]
		  },
		  {
			"title": "Effective Cluster Assignment for Modulo Scheduling",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-98-modulo.pdf",
			"excerpts": [
			  "Llosa, et al., 'Swing Modulo Scheduling: A lifetime-. Sensitive Approach,' IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques,.Read more"
			]
		  },
		  {
			"title": "GCC Swing Modulo Scheduling | PDF",
			"url": "https://www.scribd.com/document/695649826/Swing-Modulo-Scheduling",
			"excerpts": [
			  "SMS is a technique for software pipelining that aims to reduce register pressure. The key steps are: 1. Calculate a Minimum Initiation Interval (MII) for the ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling",
			"url": "https://www.sigmicro.org/media/tot/retrospective2016.pdf",
			"excerpts": [
			  "Ra- makrishna (Bob) Rau's seminal paper entitled “Iterative. Modulo Scheduling: An Algorithm for Software Pipelining Loops,” which appeared at MICRO. 1994, 22 ..."
			]
		  },
		  {
			"title": "Topic 12: Cyclic Instruction Scheduling - cs.Princeton",
			"url": "https://www.cs.princeton.edu/courses/archive/spring15/cos320/lectures/12-CyclicScheduling.pdf",
			"excerpts": [
			  "• Each iteration is scheduled to start some fixed number of cycles later than the ... • Modulo reservation table solves this problem. • To schedule an op at time ...Read more"
			]
		  },
		  {
			"title": "[PDF] Lifetime-sensitive modulo scheduling",
			"url": "https://www.semanticscholar.org/paper/922b8e03182a201d36102f30924e419c8a176bfa",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time, and empirical results ..."
			]
		  },
		  {
			"title": "Lifetime-sensitive modulo scheduling",
			"url": "https://dl.acm.org/doi/10.1145/173262.155115",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time."
			]
		  },
		  {
			"title": "Lifetime-Sensitive Modulo Scheduling",
			"url": "https://iss.oden.utexas.edu/Publications/Papers/PLDI1993b.pdf",
			"excerpts": [
			  "This paper shows how to software pipeline a loop for mini- mal register pressure without sacrificing the loop's minimum execution time.Read more"
			]
		  },
		  {
			"title": "Figure 5 from Register allocation for software pipelined loops",
			"url": "https://www.semanticscholar.org/paper/Register-allocation-for-software-pipelined-loops-Rau-Lee/0568c568401a88fc03ff93bc1deb483b7e5294bb/figure/7",
			"excerpts": [
			  "Proceedings of MICRO-27. The 27th Annual IEEE/ACM… 1994. TLDR. A Register-Pressure-Sensitive (RPS) scheduling technique is developed and three algorithms-RPS ...Read more"
			]
		  },
		  {
			"title": "Learning basic block scheduling heuristics from optimal data | Proceedings of the 2005 conference of the Centre for Advanced Studies on Collaborative research",
			"url": "https://dl.acm.org/doi/10.5555/1105634.1105652",
			"excerpts": [
			  "The basic block instruction scheduling problem is to find a minimum length schedule ... Govindarajan. Instruction scheduling. In Y. N. Srikant and ...Read more"
			]
		  },
		  {
			"title": "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory | International Journal of Parallel Programming | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1023/A:1007564126785",
			"excerpts": [
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory",
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory > REFERENCES",
			  "Section Title: Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory > Author information > Authors and Affiliations",
			  "Enhanced Co-Scheduling: A Software Pipelining Method Using Modulo-Scheduled Pipeline Theory. *International Journal of Parallel Programming* **28** , 1–46 (2"
			]
		  },
		  {
			"title": "(PDF) Theory of Modulo-Scheduled Pipelines",
			"url": "https://www.researchgate.net/publication/2287790_Theory_of_Modulo-Scheduled_Pipelines",
			"excerpts": [
			  "In this paper w e propose CO-Scheduling, a framework f o r simultaneous design of hardware pipelines struc-tures and software-pipelined schedules. T w o important components of t h e Co-Scheduling framework are: (1) An extension t o t h e analysis of hardware pipeline design t h a t m e e t s t h e needs of periodic (or software pipelined) schedules. Reservation tables, forbidden la-tencies, collision vectors, and state diagrams f r o m classical pipeline theory are revisited and extended to solve t h e n e w problems. (2) An efficient method, based o n t h e above extension of pipeline analysis, t o p e r f o r m (a) software pipeline scheduling and (b) hardware pipeline reconfiguration which are mutually \"compatible \". T h e proposed method has been implemented and pre-liminary experimental results f o r 1008 kernel loops are reported. Co-scheduling successfully obtains a sched-ule f o r 95% of these loops. T h e median t i m e t o obtain these schedules i s 0.25 seconds o n a Sparc-20.",
			  "Reservation tables, forbidden la-tencies, collision vectors, and state diagrams f r o m classical pipeline theory are revisited and extended to solve t h e n e w problems.",
			  "h e proposed method has been implemented and pre-liminary experimental results f o r 1008 kernel loops are reported. Co-scheduling successfully obtains a sched-ule f o r 95% of these loops."
			]
		  },
		  {
			"title": "(PDF) Modulo scheduling without overlapped lifetimes",
			"url": "https://www.academia.edu/125636329/Modulo_scheduling_without_overlapped_lifetimes",
			"excerpts": [
			  "M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proc. of the ACM SIGPLAN '88 Conference on Programming Language Design and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for ...",
			"url": "https://dl.acm.org/doi/10.1145/989393.989420",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in ...Read more",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without complicated hardware support. Our ...Read more"
			]
		  },
		  {
			"title": "TOWARD A SOFTWARE PIPELINING FRAMEWORK FOR ... - CAPSL",
			"url": "https://www.capsl.udel.edu/pub/doc/theses/master/Ributzka-master.pdf",
			"excerpts": [
			  "rotating registers, modulo variable expansion (MVE) has to be applied to guarantee the correctness of the schedule. The following example shows why MVE is ..."
			]
		  },
		  {
			"title": "Software pipelining, VLIW, EPIC, instruction-set support",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch06-StaticSchedulingForILP.pdf",
			"excerpts": [
			  "Rotating register frame. Software-assisted branch prediction. Software-assisted memory hierarchy. Job creation scheme for compiler engineers. We will look at ...Read more",
			  "Monica S. Lam, Robert P. Wilson. 19th ISCA, May 1992, pages 19-21. Limits of ... To help trace scheduling and software pipelining, the Itanium.Read more",
			  "IA-64",
			  "EPIC = “2nd generation VLIW”?",
			  "VLIW: Very Large Instruction Word"
			]
		  },
		  {
			"title": "Space-Time Scheduling of Instruction-Level Parallelism on a ...",
			"url": "https://groups.csail.mit.edu/cag/raw/documents/Lee:ASPLOS:1998.pdf",
			"excerpts": [
			  "Cur- rently, the event scheduler runs before register allocation; it has no register consumption information and does not consider regis- ter pressure when ...Read more"
			]
		  },
		  {
			"title": "Register Pressure-Based Modulo Scheduling for Clustered ...",
			"url": "https://ece.northeastern.edu/groups/nucar/publications/jornadas.pdf",
			"excerpts": [
			  "We propose a new modulo scheduling algorithm for clustered architectures with distributed register file and functional units. A critical step of the ...Read more"
			]
		  },
		  {
			"title": "Modulo Scheduling with Reduced Register Pressure",
			"url": "https://upcommons.upc.edu/bitstreams/9de3a477-f5f8-4307-88c5-16ab35f9e150/download",
			"excerpts": [
			  "Software pipelining is a scheduling technique that is used by some product compilers in order to expose more instruction\nlevel parallelism out of innermost loops. Modulo scheduling refers to a class of algorithms for software pipelining.",
			  "The register pressure increases as\nthe instruction level parallelism increases.",
			  ". Section 3\ndescribes the proposed modulo scheduling algorithm that\nis called HRMS. ",
			  "er\nto [1].\n**2.1 Overview of Modulo Scheduling**\nIn a software pipelined loop, the schedule for an iteration is\ndivided into stages so that the execution of consecutive it-\nerations that are in distinct stages is overlapped. The num-\nber of stages in one iteration is termed **stage count** ( *SC* ). The\nnumber of cycles per stage is *II* .",
			  "Lam, “Software Pipelining: An Effective Scheduling Tech-\nnique for VLIW Machines,” *Proc. SIGPLAN ’88 Conf. Programming*\n*Language Design, and Implementation* , pp. 318-328, June 1988.",
			  "Software pipelining is a scheduling technique that is used by some product compilers in order to expose more instruction\nlevel parallelism out of innermost loops. Modulo scheduling refers to a class of algorithms for software pipelining. Most previous\nresearch on modulo scheduling has focused on reducing the number of cycles between the initiation of consecutive iterations (which\nis termed II ) but has not considered the effect of the register pressure of the produced schedules. The register pressure increases as\nthe instruction level parallelism increases. When the register requirements of a schedule are higher than the available number of\nregisters, the loop must be rescheduled perhaps with a higher II . Therefore, the register pressure has an important impact on the\nperformance of a schedule. This paper presents a novel heuristic modulo scheduling strategy that tries to generate schedules with\nthe lowest II , and, from all the possible schedules with such II , it tries to select that with the lowest register requirements.",
			  "e Scheduling: A Tech-\nnique to Reduce the Register Requirements of a Modulo Sched-\nule,” *Proc. 28th Ann. Int’l Symp. Microarchitecture (MICRO-28)* ,\npp. 338-349, Nov. 1"
			]
		  },
		  {
			"title": "Swing Modulo Scheduling: A Lifetime-Sensitive Approach",
			"url": "https://upcommons.upc.edu/bitstreams/a90677c0-9f85-4a0b-8573-a170b09f0555/download",
			"excerpts": [
			  "Swing Modulo Scheduling* (SMS",
			  "generates schedules that are near optimal in terms of\ninitiation interval, register requirements and stage count.",
			  "g* is an heuristic approach that has\na low computational cost.",
			  "For a set of small loops, SMS obtained the\noptimum initiation interval in all the cases and its schedules\nrequired only **5%** more registers and a 1% higher stage\ncount than the optimum.",
			  "technique that exploits instruction level parallelism out of\nloops by overlapping successive iterations of the loop and\nexecuting them in parallel.",
			  "The key idea is to find a pattern\nof operations (named the kernel code) so that when\nrepeatedly iterating over this pattern, it produces the effect\nthat an iteration is initiated before the previous ones have\ncompleted.",
			  "SMS is also\ncompared with an integer linear programming approach\nthat generates optimum schedules but with a huge\ncomputational cost, which makes it feasible only for very\nsmall loops.",
			  "The addition of spill code can degrade performance [16]\ndue to additional cycles in the schedule, or due to memory\ninterferences.",
			  "Some research groups have targeted their work towards\nexact methods based on integer linear programming, For\ninstance, the proposal in [ 111 search the entire scheduling\nspace to find the optimal resource-constrained schedule\nwith minimum buffer requirements, while the proposals in\n[10,6] find schedules with the actual minimum register\nrequirements.",
			  "Keywords:**\nFine\nGrain\nParallelism,\nInstruction\nScheduling, Loop Scheduling, Software Pipelining,\nRegister\nRequirements,\nVLIW\nand\nSuperscalar\nArchitectures.",
			  "load 2 cycles",
			  "**store:** 1 cycle",
			  "**Latencies:**",
			  "For loop-variants, a value is generated in each iteration\nof the loop and, therefore, there is a different lifetime\ncorresponding to each iteration.",
			  "Because of the nature of\nsoftware pipelining, lifetimes of values defined in an\niteration can overlap with lifetimes of values defined in\nsubsequent iterations.",
			  "This is the main reason why the\nregister requirements are increased.",
			  "In addition, for values\nwith a lifetime larger than the **ZZ** new values are generated\nbefore the previous ones are u",
			  "To fix this problem,\neither software solutions (modulo variable expansion [ 151)\nand hardware solutions (rotating register files [7]) have\nbeen proposed.",
			  "Some of the software pipelining approaches can be\nregarded as the sequencing of two independent steps: node\nordering and node scheduling.",
			  "These two steps are\nperformed assuming MZZ as the initial value for **Z",
			  "it is\nnot possible to obtain a schedule with this **ZZ,** the schedulin"
			]
		  },
		  {
			"title": "Modulo scheduling with reduced register pressure",
			"url": "https://ieeexplore.ieee.org/document/689643/",
			"excerpts": [
			  "Section Title: Modulo scheduling with reduced register pressure > Abstract:\nContent:\nIn addition, the proposed method is compared with some other existing methods. The results indicate that the proposed method performs better than other heuristic methods and almost as well as linear programming methods, which obtain optimal solutions but are impractical for product compilers because their computing cost grows exponentially with the number of operations in the loop body.",
			  "Section Title: Modulo scheduling with reduced register pressure > Metrics\nContent:\n[](https://innovate.ieee.org/interested-in-a-subscription-for-your-organization/?LT=XPLLG_XPL_2020_SUB_Journals300x250_Sub-NFT)"
			]
		  },
		  {
			"title": "C6000 Compiler: Tuning Software Pipelined Loops - TI E2E",
			"url": "https://e2e.ti.com/cfs-file/__key/communityserver-discussions-components-files/791/C6000-Compiler_5F00_-Tuning-Software-Pipelined-Loops-_2D00_-Texas-Instruments-Wiki.pdf",
			"excerpts": [
			  "the C6000\narchitecture is partitioned into two nearly symmetric halves. The resource breakdown displayed in the software pipelining information is computed after the compiler has\npartitioned instructions to either the A-side or the B-side.",
			  "The software-pipelined loop information includes the source lines from which the loop originates, a description of the resource and latency requirements for the loop, and whether\nthe loop was unrolled (among other information).",
			  "g factor is now the number of functional units as indicated by the resource bound:\n;* Loop Carried Dependency Bound(^) : 0\n;* Unpartitioned Resource Bound : 2\n;* Partitioned Resource Bound(*) : 2\n",
			  "Exploiting Wide Loads and Stores (SIMD)",
			  "The C64x and C64x+ processors support both aligned and non-aligned double-word loads and stores. (Recall that the C67x and C67x+ support only aligned double-word loads and\nno double-word stores. The C62x supports only word-wide loads and stores.) If it is known that the function parameters are double-word aligned, switching to aligned, double-\nword memory accesses saves both D units and T address paths",
			  "To tell the compiler that the memory accesses are aligned on double-word (64-bit) boundaries, use _nasserts() inside the function *just prior* to the loop of interest:",
			  ":\n_nassert((int) input1 % 8 == 0); // input1 is 64-bit aligned",
			  "d\n_nassert((int) input2 % 8 == 0); // input2 is 64-bit aligned",
			  "_nassert((int) output % 8 == 0); // output is 64-bit aligned\nT",
			  ";* ii = 2 Schedule found with 4 iterations in parallel",
			  ";* Loop Unroll Multiple : 4x",
			  ";* Known Minimum Trip Count : 1",
			  ";* Known Max Trip Count Factor : 1",
			  "...",
			  "...",
			  "\n}\n}\nThe software-pipelining information for the resulting assembly code, which yields one result every 0.75 cycles (or 4 results every 3 cycles), is as follows:\n;* SOF",
			  " is the number of cycles between starting iteration i and iteration i + 1. ii is equivalent to the\ncycle count for the software-pipelined loop body. s is the number of cycles for the first iteration to complete, or equivalently, the length of a *single scheduled iteration* of the\nsoftware-pipelined loop.",
			  "In a software-pipelined loop, even though a single loop iteration might take s cycles to complete, a new iteration is initiated every ii cycles.",
			  "Software\npipelining is only enabled at –o2 or –o3. On the C6000 variants C62x, C67x, and C64x, software pipelining is completely disabled when code size flags –ms2 and –ms3 (see C6000\nCompiler: Recommended Compiler Options) are used. On C64x + , C674x, and C66x cores, software pipelining is enabled if and only if the loop buffer (see section 2.1 of Hand\nTuning Loops and Control Code on the TMS320C6000) can be used.",
			  "The initiation interval (ii) is 7. This means that in the steady state, a result (equivalently, an original loop iteration) is computed every 7 CPU cycles. Therefore, the baseline CPU\nperformance is 7 cycles/result.",
			  "After adding the restrict keyword, rebuild the function. Observe that the loop-carried dependency bound has disappeared. This means that each iteration is independent. New\niterations are now initiated as soon as resources are available.",
			  "The edges from the load instructions to the add instructions are straightforward. The destination registers for the loads are the source registers for the add instruction. A load\ninstruction takes 5 cycles to populate its destination register.",
			  "The edge from the add to the store is also straightforward since the destination of the add is the source of the store. The result of the add is available after 1 cycle. Consequently, the\nedge between the add and the store is annotated with a 1.",
			  "he edges from the store back to the loads are less obvious. How does one know to put them in? Why are they there? The answer to these questions is determined by process of\nelimination. Since there is no register dependence, there is most likely a memory dependence. In this case, the compiler does not know whether the input arrays could reference the\n ...",
			  "Look for the machine resources with a (*) after them. Notice which ones are most congested. In this case, the bottleneck is on the D unit and the T address path.",
			  ";* SINGLE SCHEDULED ITERATION",
			  ";* C26:\n;* 0 LDW .D1T1 *A5++,A4 ; |9|",
			  ";* 1 LDW .D2T2 *B4++,B5 ; |9|",
			  ";* 2 [ B0] BDEC .S2 C26,B0 ; |8|",
			  ";* 3 NOP 3",
			  ";* 6 ADD .L1X B5,A4,A3 ; |9|",
			  ";* 7 STW .D1T1 A3,*A6++ ; |9|",
			  "_nassert((int) output % 8 == 0); // output is 64-bit aligned",
			  "The following diagrams show the concept of unrolling the loop for better (more balanced) utilization of the critical D unit resource (the situation would be analogous for the critical\nT address path). On the left side, four loop iterations are shown, as indicated by the double-arrows, producing eight results in four cycles. One D unit is unused every other cycle.\nThe right side shows performance after unrolling the loop by 2x. Both D units are executing useful instructions in every cycle. Of course, the order of the loads and stores must be\nrearranged, but the compiler takes care of this.",
			  ";* ii = 3 Schedule found with 4 iterations in parallel\n...\n;* SINGLE SCHEDULED ITERATION\n;*\n;* C26:\n;* 0 LDW .D2T2 *B5++(8),B6 ; |10|\n;* 1 NOP 1\n;* 2 LDW .D1T1 *A6++(8),A3 ; |10|\n;* || LDW .D2T2 *-B5(4),B4 ; |10|\n;* 3 LDW .D1T1 *-A6(4),A3 ; |10|\n;* 4 NOP 1\n;* 5 [ B0] BDEC .S2 C26,B0 ; |8|\n;* 6 NOP 1\n;* 7 ADD .S1X B6,A3,A4 ; |10|\n;* 8 ADD .L1X B4,A3,A5 ; |10|\n;* 9 NOP 1\n;* 10 STNDW .D1T1 A5:A4,*A7++(8) ; |10|\n;* 11 ; BRANCHCC OCCURS {C26} ; |8|",
			  "The loop has been sped up 9.3x compared to the original source code with no modifications other than the addition of restrict qualifiers, MUST_ITERATE pragmas and\n_nasserts().",
			  "void BasicLoop(int *restrict output,\nint *restrict input1,\nint *restrict input2,\nint n)\n{\nint i",
			  "Software pipelining is only enabled at –o2 or –o3. On the C6000 variants C62x, C67x, and C64x, software pipelining is completely disabled ...Read more"
			]
		  },
		  {
			"title": "Introduction to TMS320C6000 DSP Optimization",
			"url": "https://www.ti.com/lit/pdf/sprabf2",
			"excerpts": [
			  "– For example, the MUST_ITERATE pragma, when placed right before a loop operation, tells the compiler the minimum iteration count of the loop, ...",
			  "Software pipelining is a technique to schedule instructions from a loop so that multiple iterations of the\nloop can execute in parallel and the pipeline can be utilized as fully as possible.",
			  "In most cases, the C compiler can automatically utilize the SPLOOP buffer while performing software\npipelining.",
			  "However, the SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets",
			  "When the compiler option, – *o3* , is used, the C6000 C compiler is able to\nautomatically perform software pipelining on the loop code. F",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies.",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies.",
			  "the SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets\n(An execute packet consists of up to eight instructions that can be executed in parallel).",
			  "Therefore, it is recommended to avoid implementing long or\ncomplex loops in C.",
			  "In stage 3, based on the knowledge it has gained in the previous stages, the compiler tries to optimize the\nloop by finding the minimal value of ii that it can use to schedule a software pipelined loop.",
			  "One major drawback of software pipelining is that it increases the size of the generated assembly file and\ncan affect interruptibility.",
			  "ure 1 shows that the family consists of fixed point DSP cores such as C64x ™\nand C64x+ ™ , early floating/fixed point DSP cores such as C67x ™ and C67x+, and the newest\nfloating/fixed point DSP cores such as C674x and C66x.",
			  "All the DSP cores share the same high-level\narchitecture, with hardware enhancements made and additional instructions supported on the newer\ndevices.",
			  "Moreover, applications developed on older devices are 100% upward compatible.",
			  "The C674x core is capable to operate at 1.25 GHz, and perform\n10,000 million complex multiply-and-accumulate operations; the C66x core is capable to operate at 1.25",
			  "on C66x core is an\nenhanced version of the C674x core. ",
			  "The latest generation C66x core is an\nenhanced version of the C674x core.",
			  "The C6000 DSPs have many architectural advantages that make them ideal for computation-intensive\nreal-time applications.",
			  "The C6000 DSPs have many architectural advantages that make them ideal for computation-intensive\nreal-time applications.",
			  "Section 2.3 introduces the concept of software pipelining, which is an instruction scheduling method to ensure the full utilization of the ...Read more"
			]
		  },
		  {
			"title": "Hand-Tuning Loops and Control Code on the TMS320C6000",
			"url": "https://www.ti.com/lit/pdf/spra666",
			"excerpts": [
			  "A technique called software pipelining contributes the biggest boost to improving looped code performance. Software pipelining is only enabled at –o2 or –o3. On ...Read more",
			  "The C6000 architecture is an eight-way enhanced VLIW architecture. The C64x, C64x+, C67x+\ncontain 64 registers. The C62x and C67x devices contain 32 registers.",
			  "**M unit.** Multiplication",
			  " an efficient software-pipelined loop, where ii<s, ii is called the *initiation interval* ; it is the\nnumber of cycles between starting iteration i and iteration i+1.",
			  "**S unit.** Shifts, branches and compares",
			  "is called the *initiation interval* ; it is the\nnumber of cycles between starting iteration i and iteration i+1. ii is equivalent to the cycle count\nfor the software-pipelined loop body. s is the number of cycles for the first iteration to complete,\nor equivalently, the length of a *single scheduled iteration* of the software-pipelined loop",
			  "A particular area where the C6000 processor family shines is its ability to speed through looped\ncode. This is quite advantageous in digital signal processing, image processing and other\nmathematical routines that tend to be loop-centric. A technique called *software pipelining*\ncontributes the biggest boost to improving looped code performance. Software pipelining is only\nenabled at –o2 or –o3. On all C6000 variants except the C64x+, software pipelining is\ncompletely disabled when code size flags –ms2 and –ms3 (Section 3.1) are used. On the\nC64x+, software pipelining is enabled if and only if the loop buffer (Section 2.1) can be use",
			  "**X cross-path.** Path between functional units and opposite-side register file",
			  "In a software-pipelined loop, even though a single loop iteration might take s cycles to complete,\na new iteration is initiated every ii cycles.",
			  "Some operations can be done on more than one functional unit. For example, add instructions\ncan execute on the L, S, or D units. Several other operations can be done on either the L or S\nunits.",
			  "*SPRA666*",
			  "4",
			  "*Hand-Tuning Loops and Control Code on the TMS320C6000*",
			  "The C64x, C64x+, C67x+\ncontain 64 registers.",
			  "ithout software pipelining, loops are scheduled so that loop iteration i completes before\niteration i+1 begins. Software pipelining allows iterations to be overlapped. Thus, as long as\ncorrectness can be preserved, iteration i+1 can start before iteration i finishes. This generally\npermits a much higher utilization of the machine’s resources than might be achieved from non-\nsoftware-pipelined scheduling techniques.",
			  "ii cycles",
			  "s cycles",
			  "iteration i",
			  "iteration i+1",
			  "iteration i+2",
			  "Figure 2.",
			  "software pipelining",
			  ";* ii = 7 Schedule found with 1 iterations in parallel",
			  "Loop Unrolling",
			  "One possibility for achieving this is to unroll the loop manually:\nvoid BasicLoop(int *restrict output,\nint *restrict input1,\nint *restrict input2,\nint n)\n{\nint i;\n\\ MUST_ITERATE(1)\n**for (i=0; i<n; i+=2) {**\n**output[i] = input1[i] + input2[i];**\n**output[i+1] = input1[i+1] + input2[i+1];**\n}\n}\nRebuilding yields the following results:",
			  "When the loop is unrolled 2x, each iteration takes longer, but the loop now generates two results\nper iteration instead of one. Thus, the unrolled loop requires 1.5 cycles/result in the steady state,\nwhereas the non-unrolled version requires 2 cycles/result.",
			  "_nassert((int) input1 % 8 == 0); // *input1 is 8-byte aligned*",
			  "_nassert((int) input2 % 8 == 0); // *input2 is 8-byte aligned*",
			  "_nassert((int) output % 8 == 0); // *output is 8-byte aligned*",
			  "**\\ MUST_ITERATE(4,,4)** // *n >= 4, n % 4 = 0*",
			  "for (i=0; i<n; i++) {\noutput[i] = input1[i] + input2[i];\n}",
			  "The software-pipelining information for the resulting assembly code, which yields one result\nevery 0.75 cycles (or 4 results every 3 cycles), is as follows:",
			  "**ii = 3** Schedule found with 3 iterations in parallel\n",
			  "The T address paths are now balanced. There is one less D unit than expected because the\ncompiler chose to use a non-aligned double word store instead of two aligned single-word\nstores. Recall, non-aligned memory accesses use both T address paths but only one D unit."
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "https://spinlab.wpi.edu/courses/ece4703_2008/spru198g.pdf",
			"excerpts": [
			  "C6000 compiler also performs software pipelining on various MIPS intensive loops. This feature is important for any pipelined VLIW machine to perform. In.",
			  "In addition to performing many common DSP optimizations, the. C6000 compiler also performs software pipelining on various MIPS intensive loops. This feature ...",
			  "TMS320C6000 Optimizing C Compiler User's Guide (literature number. SPRU187) describes the 'C6000 C compiler and the assembly optimizer. This C compiler ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C Compiler Tutorial",
			"url": "https://www.ti.com/lit/pdf/spru425",
			"excerpts": [
			  "Through a technique called software pipelin- ing, the 'C6000 code generation tools use the multiple resources of the Veloci-. TI architecture efficiently and ...Read more",
			  "We can see that the Unpartioned Resource. Bound is 4 while the Partitioned Resource Bound is 5. When the Partitioned Resource Bound is higher, this usually ...Read more",
			  "Each time a particular iteration interval fails, the ii is increased and retried. This continues until the ii is equal to the length of a list scheduled loop ( ...Read more",
			  "One thing the compiler does when software pipelining is to allow reordering\nof all loads and stores occurring from the same array or pointer. This allows\nfor maximum flexibility in scheduling.",
			  "The C6000 has predicate, or conditional, registers available for use with condi-\ntional instructions. There are 5 predicate registers on the ’C62x and ’C67x, and\n6 predicate registers on the ’C64x. There are two or three on the A side and\nthree on the B sid"
			]
		  },
		  {
			"title": "Introduction to TMS320C6000 DSP Optimization",
			"url": "http://docs.ampnuts.ru/ti.com.datasheet/66AK2E05/Application_note_SPRABF2.PDF",
			"excerpts": [
			  "When the compiler option, –o3, is used, the C6000 C compiler is able to automatically perform software pipelining on the loop code. For more ...Read more",
			  "Section 2.3 introduces the concept of software pipelining, which is an instruction scheduling method to ensure the full utilization of the ...Read more",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the\ncorrect timing dependencies. The hardware selectively overlays copies of the single scheduled iteration in\na software pipelined manner to construct an optimized execution of the loop.",
			  "One major drawback of software pipelining is that it increases the size of the generated assembly file and\ncan affect interruptibility. The SPLOOP buffer is implemented on the C64x+, C674x, and C66x cores to\naddress these issues.",
			  "he SPLOOP buffer cannot be used to handle loops that exceed 14 execute packets\n(An execute packet consists of up to eight instructions that can be executed in parallel). ",
			  "When the situation becomes too complex, the\ncompiler might not be able to pipeline at all. Therefore, it is recommended to avoid implementing long or\ncomplex loops in C.",
			  "t is recommended to develop the application in C for its simplicity to develop and\nupdate, and use the C6000 code generation tools to generate highly optimized assembly source files ",
			  "Manual Unroll",
			  "As shown in Example 10 , loop unrolling can be accomplished manually. The number of iterations is\nhalved, and two of the previous single loop implementation is carried out in the new loop.",
			  "he MUST_ITERATE pragma gives the compiler the flexibility to\nautomatically unroll when it is deemed profitable, and the compiler often can perform this in an ideal way.\n",
			  "Two of the most frequently used SIMD instructions are the LDDW and STDW instructions, which perform\naligned 64-bit load and 64-bit store, respectively.",
			  "The LDDW and STDW instructions require that data be aligned on 8-byte boundary. If data\nis not aligned, the non-aligned version of these instructions (LDNDW and STNDW) will be\nused instead.",
			  "The SPLOOP buffer stores a single scheduled iteration of the loop in a specialized buffer, along with the correct timing dependencies. The ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing Compiler User's Guide",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/DSPs/Texas_Instrument_TMS320Cxx/DSP%20C6000/spru187.pdf",
			"excerpts": [
			  "The assembly optimizer also software pipelines loops. For more information\nabout the assembly optimizer, see Chapter 4. For more information about soft-\nware-pipelining, see the TMS320C6000 Programmer’s Guide.",
			  "n Off Software Pipelining (–mu Option)**\nAt optimization levels –o2 and –o3, the compiler attempts to software pipeline\nyour loops. You might not want your loops to be software-pipelined for debug-\nging reasons. Software-pipelined loops are sometimes difficult to debug be-\ncause the code is not presented serially. The –mu option affects both compiled\nC/C++ code and assembly optimized co",
			  "The terms defined below appear in the software pipelining information. For\nmore information on each term, see the TMS320C6000 Programmer’s Guide.\n�\n**Loop unroll factor.** The number of times the loop was unrolled specifi-\ncally to increase performance based on the resource bound constraint in\na software pipelined loop.\n�\n**Known minimum trip count.** The minimum number of times the loop will\nbe executed.\n�\n**Known maximum trip count.** The maximum number of times the loop will\nbe executed.\n�\n**Known max trip count factor.** Factor that would always evenly divide the\nloops trip count. This information can be used to possibly unroll the loop.\n�\n**Loop label.** The label you specified for the loop in the linear assembly\ninput file. This field is not present for C/C++ code.\n�\n**Loop carried dependency bound.** The distance of the largest loop carry\npath. A loop carry path occurs when one iteration of a loop writes a value\nthat must be read in a future iteration. Instructions that are part of the loop\ncarry bound are marked with the ^ symbol.\n�",
			  "Optimizing Software Pipelining",
			  "3-11",
			  "Optimizing Your Code",
			  "Speculative Execution",
			  "When prologs and epilogs are collapsed, instructions might be speculatively\nexecuted, thereby causing loads to addresses beyond either end of the range\nexplicitly read within the loop.",
			  "When the –mhn option is used, the speculative threshold is increased from the\ndefault of 0 to n.",
			  "When the threshold is n, the compiler can allow a load to be\nspeculatively executed as the memory location it reads will be no more than",
			  "e assembly optimizer also software pipelines loops. For more information\nabout the assembly optimizer, see Chapter 4. For more information about soft-\nware-pipelining, see the TMS320C6000 Programmer’s Guide.",
			  "By default, the compiler cannot speculate loads\nbecause this could cause an illegal memory location to be read.",
			  "Sometimes,\nthe compiler can predicate these loads to prevent over execution.",
			  "owever,\nthis can increase register pressure and might decrease the total amount col-\nlapsing which can be performed",
			  "e prolog and epilog of a software-pipelined loop consists of up to p–1 stages\nof length ii, where p is the number of iterations that are executed in parallel dur-\ning the steady state and ii is the cycle time for the pipelined loop body."
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C Compiler User's Guide",
			"url": "https://www.rose-hulman.edu/class/ee/yoder/ece581/ECE581%20documents/Old/spru187e.pdf",
			"excerpts": [
			  "- Unpartitioned resource bound. The best possible resource bound val- ues before the instructions in the loop are partitioned to a particular side ...Read more",
			  "**Speculative Execution**\nAn instruction is speculatively executed if it is executed before it is known\nwhether the result will be used. If the result is unused, it is discarded. To specu-\nlatively execute an instruction, the compiler moves it past a branch. This has\nthe same effect as moving a statement out of the body of an if statement.\nAn instruction can be legally speculatively executed only if the discarded result\ndoes not change the program semantics. In a speculatively executed instruc-\ntion, not only is the result discarded, but also the previous value in that location.",
			  "Load instructions might be able to be speculatively executed, but it is possible\nthat the address register is invalid if the result will not be used. This means\nthat a load instruction might access an address that is past the beginning or\nend of valid memory, causing a fault.",
			  "The 'C6000 C compiler is a full-featured optimizing compiler that translates standard ANSI C programs into 'C6000 assembly language source. The fol- lowing ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide (Rev. K)",
			"url": "https://www.ti.com/lit/pdf/spru198",
			"excerpts": [
			  "The _nassert intrinsic can convey information about the alignment of pointers\nand arrays.",
			  "_nassert(((int) a & 0x3) == 0); /* a is word aligned */",
			  "For C64x/C64x+ _amemd corresponds to\ndifferent assembly instructions than when used\nwith other C6000 devices; see Table 2 − 6.",
			  "Having determined that the minimum iteration interval is 1, you can initiate a\nnew iteration every cycle. You can schedule LDW (or LDDW) and MPY (or\nMPYSP) instructions on every cycle.",
			  "\nTable 5 − 7 shows a fully pipelined schedule for the fixed-point dot product",
			  "The rightmost column in Table 5 − 7 is a single-cycle loop that contains the\nentire loop. Cycles 0 − 6 are loop setup code, or loop prolog.",
			  "Typically, these loops achieve an optimal usage of the 8 functional units\navailable on the C6000 DSP by scheduling multiple iterations of the loop in\nparallel.",
			  "The SPLOOP mechanism includes an SPLOOP buffer large\nenough to store 14 execute packets and several dedicated registers.",
			  "The C64x+ improves code size and interruptibility of software-pipelined loops\nwith the hardware SPLOOP buffer and software support instructions.",
			  "ew packed data manipulation\ninstructions allow for improved reordering of packed data and register pairs.\n",
			  "The following books describe the TMS320C6000 devices and related support tools. To obtain a copy of any of these TI documents, call the Texas Instru-.Read more"
			]
		  },
		  {
			"title": "Example 1. Software Pipelining Information",
			"url": "https://downloads.ti.com/docs/esd/SPRUI04/software-pipelining-information-stdz0577313.html",
			"excerpts": [
			  "Initiation interval (ii). The number of cycles between the initiation of successive iterations of the loop. The smaller the initiation interval, the fewer ...Read more"
			]
		  },
		  {
			"title": "Global Instruction Scheduling for LLVM",
			"url": "https://llvm.org/devmtg/2014-10/Slides/Larin-GlobalInstructionScheduling.pdf",
			"excerpts": [
			  "Cyclic/Acyclic cases are handled rather differently. – Cyclic scheduling is currently performed rather “early”. • When bundling is not yet available.Read more",
			  "Global Instruction\nScheduling for LLVM",
			  "Sergei Larin , Qualcomm Innovation Center",
			  "Aditya Kumar , Qualcomm Innovation Center",
			  "A case study - Hexagon Global Scheduler",
			  "ce critical path is established we are free to utilize “holes” around it and shorten",
			  "It does not have to worry about increase in register pressure",
			  "Global Scheduling done after packetization (bundling)",
			  "without lengthening the critical path",
			  "Uses the DFA based Hexagon VLIW packetizer\n– To manage resource constraints in bundles",
			  " To manage resource constraints in bundles",
			  " Performing global instruction movement on *actual machine instructions*",
			  "and *allocated physical registers* offers several benefits",
			  "it when possible",
			  "It can perform very target specific optimizations",
			  "Like custom peephole opt and peculiar code layout or compaction",
			  "Power consumption related options etc.",
			  "\nYou have nothing to lose but your chains ",
			  "Cons include",
			  "Restricted scheduling region formation",
			  "Limited flexibility in code motion",
			  "Complex (if any) register scavenging",
			  "Multi-way branching",
			  "• Major goal is to fill in the gaps in the bundles by “pulling\nup” instructions from successor basic blocks\n– …without lengthening the critical path",
			  "Utilizes a specialized post-RA predicate-aware liveness\nanalysis pass",
			  "Performs speculative and predicative global scheduling of\ninstructions in a superblock",
			  "1.4% improvement on SPEC 2K Int"
			]
		  },
		  {
			"title": "How to get started with VLIW scheduler? - Project Infrastructure / LLVM Dev List Archives - LLVM Discussion Forums",
			"url": "https://discourse.llvm.org/t/how-to-get-started-with-vliw-scheduler/58139",
			"excerpts": [
			  "The way we do it for Hexagon is that we have our own specialization of the MachineScheduler pass, followed by a packetizer pass.",
			  "Actually, we run two schedulers: machine scheduler, and post-RA scheduler, but that doesn’t change much conceptually.",
			  "In the scheduler passes we don’t form instruction packets, at least not explicitly.",
			  "We do some sort of a simulation of “what if we were forming packets right now” type of thing, maximizing packetization opportunities ahead of time, before the packetization itself, hoping that enough of it will prevail.",
			  "Finally, near the end of the post-RA passes we do run the DFAPacketizer pass which forms the actual packets (i.e. “bundles” in LLVM terminology).",
			  "The DFA packetizer relies on tables that are generated out of your target’s td files describing your processor’s resources (some of which the standard scheduler uses as well).",
			  "If you’re starting from scratch, I’d get the packetizer working first."
			]
		  },
		  {
			"title": "[llvm-dev] RFC: Generalizing Hexagon's VLIWMachineScheduler",
			"url": "https://groups.google.com/g/llvm-dev/c/IXxgPZ5KdZI",
			"excerpts": [
			  "The VLIW-aware pre-allocation scheduler used by Hexagon is a mostly-generic alternative to the default MachineScheduler implementation that attempts to order instructions in such a way as to optimize the number of instructions issued per\ncycle. It does this by tracking available resources (DFAPacketizer) and balancing register pressure. This is a departure from the default list scheduler, which has no resource tracking and does not attempt to model instructions executing in parallel, instead\noptimizing the straight-line path length of the DAG."
			]
		  },
		  {
			"title": "Random Testing of Code Generation in Compilers",
			"url": "https://robcasloz.github.io/teaching/BevinHansson_2015.pdf",
			"excerpts": [
			  "As Hexagon is a VLIW architecture, the compiler is responsible for scheduling\nand bundling instructions to achieve the most efficient execution.",
			  "exagon instructions are\ncollected into groups called *bundles* during compilation. Each bundle can con-\ntain up to four instructions",
			  "The architecture possesses a 32-bit and 64-bit register set, as well as\na set of 1-bit predicate registers for conditional jumps and predicated instruc-\ntions [36]",
			  "In LLVM,\nthe slot requirements are encoded in the form of available resources, which\ninstructions must reserve in order to be bundled together.",
			  "If an instruction is\nto be added to a bundle but its resource has already been taken, it is not a\nviable candidate for bundling.",
			  "LLVM’s packetizer uses a state machine to determine if instructions can be\nbundled with each other.",
			  "ch state is a possible combination of reserved re-\nsources and each transition is a reservation of a certain resource.",
			  "e resource\nrequirements (called *itineraries* ) are encoded into the Hexagon instruction def-\ninitions in LLVM.",
			  "Figure 5.4 shows a toy example with two resources: *S* 0 and\n*S* 1; and three types of instructions: those which must use *S* 0, those which\nmust use *S* 1 and those which can use either.",
			  "e example demonstrates re-\nsource reservation; there is no transition from state *b* on *S* 0, as that resource\nhas already been taken by a previously bundled instruction.",
			  "VM’s packetizer uses a state machine to determine if instructions can be\nbundled with each other. Each state is a possible combination of reserved re-\nsources and each transition is a reservation of a certain resource."
			]
		  },
		  {
			"title": "New Release: Deeper DSP Customization with Qualcomm Hexagon SDK 2.0 (Part 2 of 2) | Qualcomm",
			"url": "https://www.qualcomm.com/news/onq/2015/04/new-release-deeper-dsp-customization-qualcomm-hexagon-sdk-20-part-2-2",
			"excerpts": [
			  "Hexagon is one of the first digital signal processors to support code generation using a low-level virtual machine (LLVM) compiler, and in this release of the SDK we’re expanding its accessibility beyond OEMs to developers.",
			  "The LLVM compiler is part of the effort by Qualcomm Technologies, Inc. to develop a simpler but more powerful code development approach for mobile, with more integrated tools to program the heterogeneous Snapdragon platform.",
			  "Hexagon LLVM compiler converts high-level-language code into Hexagon assembly code when the back end takes the intermediate language and generates Hexagon-specific assembly code.",
			  "The result is more flexible than a monolithic compiler because it decouples the compiler‘s front-end language support from the processor-specific, back-end target code generation. That way, the front-end compiler work can benefit all processors for that language, while the back-end, processor-specific code generator team can incorporate processor-specific optimizations."
			]
		  },
		  {
			"title": "[LLVMdev] VLIW code generation for LLVM backend",
			"url": "https://groups.google.com/g/llvm-dev/c/HLbNkuCFgmE",
			"excerpts": [
			  "\nHi,\nIt seems the only one VLIW target Hexagon in LLVM 3.2 devel uses a\nstraightforward way to emit its VLIW-style asm codes.\nIt uses a list scheduler to schedule on DAG and a simple packetizer to\nwrap the emitted asm instructions.\nBoth scheduling and packetizing work on basic blocks.\nso,",
			  "There is work currently underway to add SW pipelining and some sort of\nglobal scheduling to Hexagon, but if there is some interest to it from other\ntargets, it would be helpful to know. What is your involvement with this?\nSergei Larin\n--\nQualcomm Innovation Center, Inc. is a member of Code Aurora Forum.\n"
			]
		  },
		  {
			"title": "Qualcomm Corporate Template",
			"url": "https://llvm.org/devmtg/2017-02-04/Halide-for-Hexagon-DSP-with-Hexagon-Vector-eXtensions-HVX-using-LLVM.pdf",
			"excerpts": [
			  "Halide provides two execution environments for HVX.",
			  "Hardware model or the offload model.",
			  "Transparently dispatches Halide pipeline from the host CPU to the\nHexagon™ processor.\n",
			  "Very easy to use as a developer.",
			  "HVX supports unaligned loads, but they are less efficient than aligned loads.",
			  "Halide provides an abstraction to specify assumptions about the alignment of\nexternal memory buffers.",
			  "Halide also provides a scheduling directive to prefetch data into the L2 cache.",
			  "For example, “ my_func.prefetch (y, 2)” will prefetch into the L2 cache,\n2 iterations worth of data needed in the ‘y’ loop.\n",
			  " Halide for Hexagon™ DSP with\nHexagon Vector eXtensions (HVX)\nusing LLVM",
			  "ide programs / pipelines consist of two major components\n•\nAlgorithm\n•\nSchedul",
			  "Halide on Hexagon with HVX",
			  "\nCompiler targets include x86/SSE, ARM v7/NEON, CUDA, Hexagon™/HVX\nand OpenCL.",
			  "Halide enables rapid authoring and evaluation of optimized pipelines\nby separating the algorithm from the computational organization of the different\nstages of the pipeline (schedule).",
			  "Halide provides two execution environments for HVX.\n•\nHardware model or the offload mode",
			  "Halide provides two execution environments for HVX.\n•\nHardware model or the offload mode",
			  "\nHalide::Func f;\n// horizontal blur – Algorithm.\nf(x, y) = (input(x-1, y) + input(x, y) + input(x+1, y))/3;"
			]
		  },
		  {
			"title": "llvm/lib/Target/Hexagon/Hexagon.td at master · llvm-mirror/llvm · GitHub",
			"url": "https://github.com/llvm-mirror/llvm/blob/master/lib/Target/Hexagon/Hexagon.td",
			"excerpts": [
			  "HexagonVLIWPacketizer.cpp",
			  "HexagonSchedule.td",
			  "HexagonScheduleV66.td"
			]
		  },
		  {
			"title": "LLVM: lib/CodeGen/DFAPacketizer.cpp Source File",
			"url": "https://llvm.org/doxygen/DFAPacketizer_8cpp_source.html",
			"excerpts": [
			  " This class implements a deterministic finite automaton (DFA) based\n9 // packetizing mechanism for VLIW architectures. It provides APIs to\n10 // determine whether there exists a legal mapping of instructions to\n11 // functional unit assignments in a packet. The DFA is auto-generated from\n12 // the target's Schedule.td ",
			  "sists of 3 major elements: states, inputs, and transitions. For\n15 // the packetizing mechanism, the input is the set of instruction classes for\n16 // a target. The state models all possible combinations of functional unit\n17 // consumption for a given set of instructions in a packet. A transition\n18 // models the addition of an instruction to a packet. In the DFA constructed\n19 // by this class, if an instruction can be added to a packet, then a valid\n20 // transition exists from the corresponding state."
			]
		  },
		  {
			"title": "TableGen BackEnds — LLVM 22.0.0git documentation",
			"url": "https://llvm.org/docs/TableGen/BackEnds.html",
			"excerpts": [
			  "**Purpose** : This class parses the Schedule.td file and produces an API that\ncan be used to reason about whether an instruction can be added to a packet\non a VLIW architecture. The class internally generates a deterministic finite\nautomaton (DFA) that models all possible mappings of machine instructions\nto functional units as instructions are added to a packet.",
			  "**Output** : Scheduling tables for GPU back-ends (Hexagon, AMD).",
			  "**Usage** : Included directly on `<Target>InstrInfo.cpp` ."
			]
		  },
		  {
			"title": "The LLVM Target-Independent Code Generator — LLVM 23.0.0git documentation",
			"url": "https://llvm.org/docs/CodeGenerator.html",
			"excerpts": [
			  "In a Very Long Instruction Word (VLIW) architecture, the compiler is responsible\nfor mapping instructions to functional-units available on the architecture. To\nthat end, the compiler creates groups of instructions called *packets* or *bundles* . The VLIW packetizer in LLVM is a target-independent mechanism to\nenable the packetization of machine instructions.",
			  "The packetizer reads instruction classes from a target’s itineraries and creates\na deterministic finite automaton (DFA) to represent the state of a packet. A DFA\nconsists of three major elements: inputs, states, and transitions. The set of\ninputs for the generated DFA represents the instruction being added to a\npacket. The states represent the possible consumption of functional units by\ninstructions in a packet. In the DFA, transitions from one state to another\noccur on the addition of an instruction to an existing packet. If there is a\nlegal mapping of functional units to instructions, then the DFA contains a\ncorresponding transition.",
			  "The absence of a transition indicates that a legal\nmapping does not exist and that the instruction cannot be added to the packet.",
			  "To generate tables for a VLIW target, add *Target* GenDFAPacketizer.inc as a\ntarget to the Makefile in the target directory. The exported API provides three\nfunctions: `DFAPacketizer::clearResources()` , `DFAPacketizer::reserveResources(MachineInstr *MI)` , and `DFAPacketizer::canReserveResources(MachineInstr *MI)` . These functions allow\na target packetizer to add an instruction to an existing packet and to check\nwhether an instruction can be added to a packet. See `llvm/CodeGen/DFAPacketizer.h` for more information.",
			  "To generate tables for a VLIW target, add *Target* GenDFAPacketizer.inc as a\ntarget to the Makefile in the target directory.",
			  "The VLIW packetizer in LLVM is a target-independent mechanism to\nenable the packetization of machine instructions."
			]
		  },
		  {
			"title": "LLVM: llvm::VLIWPacketizerList Class Reference",
			"url": "https://llvm.org/doxygen/classllvm_1_1VLIWPacketizerList.html",
			"excerpts": [
			  "eimplemented in [llvm::HexagonPacketizerList](classllvm_1_1HexagonPacketizerList.html) .",
			  "llvm::VLIWPacketizerList Class Reference",
			  "` \" [llvm/CodeGen/DFAPacketizer.h](DFAPacketizer_8h_source.html) \"`",
			  "Definition at line [136](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Section Title: Detailed Description > [◆]() initPacketizerState()",
			  "Definition at line [182](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Section Title: Detailed Description > [◆]() PacketizeMIs()",
			  "Definition at line [153](DFAPacketizer_8cpp_source.html) of file [DFAPacketizer.cpp](DFAPacketizer_8cpp_source.html) ",
			  "Section Title: Detailed Description > [◆]() ResourceTracker",
			  "Definition at line [165](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) .",
			  "Definition at line [165](DFAPacketizer_8h_source.html) of file [DFAPacketizer.h](DFAPacketizer_8h_source.html) ."
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.h Source File",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8h_source.html",
			"excerpts": [
			  "3// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions. 4// See https://llvm.org/LICENSE.txt for license information."
			]
		  },
		  {
			"title": "Is there a way to convert between SchedMachineModel and ...",
			"url": "https://discourse.llvm.org/t/is-there-a-way-to-convert-between-schedmachinemodel-and-itineraries/38837",
			"excerpts": [
			  "I was trying to write a very simple VLIW packetizer (Hexagon was my starting point). It turns out that current DFAPacketizer is using itineraries, but my ..."
			]
		  },
		  {
			"title": "SchedMachineModel: Adding and Optimizing a Subtarget",
			"url": "https://llvm.org/devmtg/2014-10/Slides/Estes-MISchedulerTutorial.pdf",
			"excerpts": [
			  "Enable additional analysis / heuristics with -misched-cluster,. -misched-cyclicpath, -misched-regpressure, and -misched- fusion. Set scheduler (strategy) with ...Read more"
			]
		  },
		  {
			"title": "Scheduling Model in LLVM - Part I | Min Hsu's Homepage",
			"url": "https://myhsu.xyz/llvm-sched-model-1/",
			"excerpts": [
			  "Scheduling models are part of the target definition. They are associated with one or more target processors. Multiple processors can also share ...Read more"
			]
		  },
		  {
			"title": "LLVM: include/llvm/CodeGen/DFAPacketizer.h Source File",
			"url": "https://llvm.org/doxygen/DFAPacketizer_8h_source.html",
			"excerpts": [
			  " a deterministic finite automaton (DFA) based\n9 // packetizing mechanism for VLIW architectures. It provides APIs to\n10 // determine whether there exists a legal mapping of instructions to\n11 // functional unit assignments in a packet. The DFA is auto-generated from\n12 // the target's Schedule.td fil",
			  "of 3 major elements: states, inputs, and transitions. For\n15 // the packetizing mechanism, the input is the set of instruction classes for\n16 // a target. The state models all possible combinations of functional unit\n17 // consumption for a given set of instructions in a packet. A transition\n18 // models the addition of an instruction to "
			]
		  },
		  {
			"title": "LLVM: llvm::DFAPacketizer Class Reference",
			"url": "https://llvm.org/doxygen/classllvm_1_1DFAPacketizer.html",
			"excerpts": [
			  "llvm::DFAPacketizer Class Reference",
			  "|void llvm::DFAPacketizer::clearResources |( |) |\n",
			  "\n|void |[reserveResources]() ( [MachineInstr](classllvm_1_1MachineInstr.html) & [MI](IRTranslator_8cpp.html) ) |",
			  "l) |[canReserveResources]() ( [MachineInstr](classllvm_1_1MachineInstr.html) & [MI](IRTranslator_8cpp.html) ) |\n",
			  "ml) |[canReserveResources]() ( [const](AArch64PromoteConstant_8cpp.html) [MCInstrDesc](classllvm_1_1MCInstrDesc.html) *MID) |",
			  "void |[reserveResources]() ( [const](AArch64PromoteConstant_8cpp.html) [MCInstrDesc](classllvm_1_1MCInstrDesc.html) *MID) |\n|"
			]
		  },
		  {
			"title": "[llvm-dev] How to get started with VLIW scheduler?",
			"url": "https://groups.google.com/g/llvm-dev/c/U4JNhxz0IHI/m/atKaNI0dAgAJ",
			"excerpts": [
			  "The way we do it for Hexagon is that we have our own specialization of the MachineScheduler pass, followed by a packetizer pass.  Actually, we run two schedulers: machine scheduler, and post-RA scheduler, but that doesn’t change much conceptually.",
			  "In the scheduler passes we don’t form instruction packets, at least not explicitly.  We do some sort of a simulation of “what if we were forming packets right now” type of thing, maximizing packetization opportunities ahead of time, before the packetization\nitself, hoping that enough of it will prevail.",
			  "Finally, near the end of the post-RA passes we do run the DFAPacketizer pass which forms the actual packets (i.e. “bundles” in LLVM terminology).  The DFA packetizer relies on tables that are generated out of your target’s td files describing\nyour processor’s resources (some of which the standard scheduler uses as well).",
			  "If you’re starting from scratch, I’d get the packetizer working first."
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.cpp File Reference",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8cpp.html",
			"excerpts": [
			  "Gets the predicate register of a predicated instruction.",
			  "Returns true if an instruction is predicated on p0 and false if it's predicated on !p0."
			]
		  },
		  {
			"title": "RFC: Generalizing Hexagon's VLIWMachineScheduler - Project Infrastructure / LLVM Dev List Archives - LLVM Discussion Forums",
			"url": "https://discourse.llvm.org/t/rfc-generalizing-hexagons-vliwmachinescheduler/59178",
			"excerpts": [
			  "The VLIW-aware pre-allocation scheduler used by Hexagon is a mostly-generic alternative to the default MachineScheduler implementation that attempts to order instructions in such a way as to optimize the number of instructions issued per cycle. It does this by tracking available resources (DFAPacketizer) and balancing register pressure.",
			  "This is a departure from the default list scheduler, which has no resource tracking and does not attempt to model instructions executing in parallel, instead optimizing the straight-line path length of the DAG.",
			  "\nI intend to generalize these data structures…\nVLIWResourceModel\n",
			  "VLIWMachineScheduler"
			]
		  },
		  {
			"title": "llvm/include/llvm/Target/TargetSchedule.td at master · llvm-mirror/llvm · GitHub",
			"url": "https://github.com/llvm-mirror/llvm/blob/master/include/llvm/Target/TargetSchedule.td",
			"excerpts": [
			  "class SchedMachineModel {",
			  "int IssueWidth = -1; // Max micro-ops that may be scheduled per cycle.",
			  "int MicroOpBufferSize = -1; // Max micro-ops that can be buffered.",
			  "int LoopMicroOpBufferSize = -1; // Max micro-ops that can be buffered for",
			  "// optimized loop dispatch/execution.",
			  "int LoadLatency = -1; // Cycles for loads to access the cache.",
			  "int HighLatency = -1; // Approximation of cycles for \"high latency\" ops.",
			  "int MispredictPenalty = -1; // Extra cycles for a mispredicted branch.",
			  "// Per-cycle resources tables.",
			  "ProcessorItineraries Itineraries = NoItineraries;",
			  "bit PostRAScheduler = 0; // Enable Post RegAlloc Scheduler pass.",
			  "// Subtargets that define a model for only a subset of instructions",
			  "// that have a scheduling class (itinerary class or SchedRW list)",
			  "// and may actually be generated for that subtarget must clear this",
			  "// bit. Otherwise, the scheduler considers an unmodelled opcode to",
			  "// be an error. This should only be set during initial bringup,",
			  "// or there will be no way to catch simple errors in the model",
			  "// resulting from changes to the instruction definitions.",
			  "def NoSchedModel : SchedMachineModel {",
			  "let NoModel = 1;",
			  "let CompleteModel = 0;",
			  "// Define a kind of processor resource that may be common across",
			  "// similar subtargets.",
			  "class ProcResourceKind;",
			  "// Define a number of interchangeable processor resources. NumUnits",
			  "// determines the throughput of instructions that require the resource.",
			  "// An optional Super resource may be given to model these resources as",
			  "// a subset of the more general super resources. Using one of these",
			  "// resources implies using one of the super resources.",
			  "//",
			  "//",
			  "// ProcResourceUnits normally model a few buffered resources within an",
			  "// out-of-order engine. Buffered resources may be held for multiple",
			  "// clock cycles, but the scheduler does not pin them to a particular",
			  "// clock cycle relative to instruction dispatch.",
			  "class ProcResourceUnits<ProcResourceKind kind, int num> {",
			  "ProcResourceKind Kind = kind;",
			  "int NumUnits = num;",
			  "ProcResourceKind Super = ?;",
			  "int BufferSize = -1;",
			  "SchedMachineModel SchedModel = ?;",
			  "}",
			  "}",
			  "}"
			]
		  },
		  {
			"title": "1 TableGen Programmer's Reference",
			"url": "https://llvm.org/docs/TableGen/ProgRef.html",
			"excerpts": [
			  "This document describes the LLVM TableGen facility in detail. It is intended for the programmer who is using TableGen to produce code for a project.Read more"
			]
		  },
		  {
			"title": "LLVM: lib/Target/Hexagon/HexagonVLIWPacketizer.cpp Source File",
			"url": "https://llvm.org/doxygen/HexagonVLIWPacketizer_8cpp_source.html",
			"excerpts": [
			  "plements a simple VLIW packetizer using DFA. The packetizer works on\n10 // machine basic blocks. For each instruction I in BB, the packetizer consults\n11 // the DFA to see if machine resources are available to execute I. If so, the\n12 // packetizer checks if I depends on any instruction J in the current packet.\n13 // If no dependency is found, I is added to current packet and machine resource\n14 // is marked as taken. If any dependency is found, a target API call is made t",
			  "HexagonPacketizerList(MachineFunction &MF, MachineLoopInfo &MLI, AAResults *AA, const MachineBranchProbabilityInfo *MBPI, bool Minimal)"
			]
		  },
		  {
			"title": "Allocating Rotating Registers by Scheduling",
			"url": "https://microarch.org/micro46/files/paper5a3_slides.pdf",
			"excerpts": [
			  "Rotating Register Allocation ≡ Scheduling ! • It is a software pipelining problem. – A register allocation is a modulo schedule of lifetimes ... • Extended to ...Read more"
			]
		  },
		  {
			"title": "Lecture 12 - Computer Science - Harvard University",
			"url": "https://www.eecs.harvard.edu/cs146-246/cs146-lecture12.pdf",
			"excerpts": [
			  "Nearly all instructions are predicated",
			  " 64 1-bit predict registers\n",
			  "IA-64 instructions are encoded in bundles , which are 128 bits wide.",
			  "Each bundle consists of a 5-bit template field and 3 instructions, each 41 bits\nin length",
			  "5-bit template field",
			  "Compiler must explicitly indicate the boundary between one instruction\ngroup and another by placing a stop between 2 instructions that belong to\ndifferent groups",
			  "Stop bits",
			  "The integer registers designed to assist procedure calls using\na register stack",
			  "Register rotation eases the task of allocating\nregisters in software pipelined loops",
			  "All INT registers have a 1-bit NaT (Not A Thing)",
			  "Deferred exceptions",
			  "Chk.s instructions can detect and branch to recovery\ncode",
			  "Advanced Loads* allow speculative memory\nreference",
			  "LAT* table is allocated with register destination +\nmemory address",
			  "Stores associatively lookup the table when they execute",
			  "Before using the value of the advanced load\n– Explicit check is needed to see if ALAT entry is valid",
			  "Predicated execution (select 1 out of 64 1-bit flags)",
			  "Predicated execution (select 1 out of 64 1-bit flags)"
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "http://refspecs.linux-foundation.org/IA64-softdevman-vol4.pdf",
			"excerpts": [
			  "Profile-guided compilers that use advanced IA-64 architectural features such as predication\nand speculation benefit from run-time profile information to optimize instruction schedules.",
			  "architectural features such as speculation, predication, and more.",
			  "On the Itanium processor, WC loads are performed directly from memory and not from coalescing buffers. It has a separate 2-entry, 64-byte Write Coalesce Buffer ...Read more",
			  "IA-64 architecture features a revolutionary 64-bit instruction set architecture (ISA) which applies a\nnew processor architecture technology called EPIC, or Explicitly Parallel Instruction Computing.",
			  "Chapter 11, “ Software Pipelining and Loop Support ” provides a detailed discussion on optimizing\nloops through use of software pipelinin",
			  "Chapter 10, “ Predication, Control Flow, and Instruction Stream ” describes optimization features\nrelated to predication, control flow, and branch hint",
			  "The Itanium processor cycle accounting monitors account for all major single and multi-cycle stall\nand flush conditions.",
			  "This chapter outlines the targeted performance monitor usage\nmodels, defines the software interface and programming model, and lists the set of monitored\nevents.",
			  "The Itanium processor does not support a non-temporal buffer; instead, non-temporal accesses are\nallocated in L2 cache with biased replacement.",
			  "r 7, “ Performance Monitor Events ” summarizes the Itanium processor events and describes\nhow to compute commonly used performance metrics for Itanium processor event"
			]
		  },
		  {
			"title": "Intel® Itanium™ Processor Reference Manual for Software ...",
			"url": "https://people.freebsd.org/~marcel/refs/ia64/itanium/24532003.pdf",
			"excerpts": [
			  "On the Itanium processor, WC loads are performed directly from memory and not from coalescing buffers. It has a separate 2-entry, 64-byte Write Coalesce Buffer ...Read more"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"url": "https://www.intel.sg/content/dam/doc/manual/itanium-architecture-vol-1-2-3-4-reference-set-manual.pdf",
			"excerpts": [
			  "Itanium architecture incorporates features which enable high sustained performance\nand remove barriers to further performance increases. The Itanium architecture is\nbased on the following principles:",
			  "predication** section describes if-conversion, predicate usage, and code\nscheduling to reduce the affects of branching.\n•",
			  "• Mechanisms for synergy between the compiler and the processor",
			  "Memory latency is a major factor in determining the performance of integer\napplications. In order to help reduce the effects of memory latency, the Itanium\narchitecture explicitly supports software pipelining, large register files, and\ncompiler-controlled speculation.",
			  "• 128 integer and floating-point registers, 64 1-bit predicate registers, 8 branch\nregisters",
			  "• Support for many execution units and memory ports",
			  "• Features that enhance instruction level parallelism",
			  "• Speculation (which minimizes memory latency impact).",
			  "• Predication (which removes branches).",
			  "The Itanium architecture enables procedures to communicate register usage to\nthe processor. This allows the processor to schedule procedure register operations even\nwhen there is a low degree of ILP.",
			  "The Itanium architecture allows issuing of independent instructions in\nbundles (three instructions per bundle) for parallel execution and can issue multiple\nbundles per clock.",
			  "Supported by a large number of parallel resources such as large\nregister files and multiple execution units, the Itanium architecture enables the\ncompiler to manage work in progress and schedule simultaneous threads of\ncomputation.",
			  "The Itanium architecture\nallows the renaming of registers which provide every iteration with its own set of\nregisters, avoiding the need for unrolling. This kind of register renaming is called\nregister rotation.",
			  "Modulo scheduling of a loop is analogous to hardware pipelining of a functional unit\nsince the next iteration of the loop starts before the previous iteration has finished.",
			  "The Itanium\narchitecture enables the compiler to exploit speculative information without sacrificing\nthe correct execution of an application (see “Speculation” on page 1:16 ).",
			  "ting-point registers 32 through 127 are termed the **rotating floating-point**\n**registers** . These registers can be programmatically renamed to accelerate lo",
			  "The Itanium architecture provides mechanisms, such as instruction templates, branch\nhints, and cache hints to enable the compiler to communicate compile-time information\nto the processor.",
			  "Every memory load and store in the Itanium architecture has a 2-bit cache hint field in\nwhich the compiler encodes its prediction of the spatial and/or temporal locality of the\nmemory area being accessed.",
			  "The Itanium architecture provides extensive support for software-pipelined loops,\nincluding register rotation, special loop branches, and application registers.",
			  "When\ncombined with predication and support for speculation, these features help to reduce\ncode expansion, path length, and branch mispredictions for loops that can be software\npipelined.",
			  "Register Stack Engine",
			  "A key feature of the Itanium architecture is IA-32 instruction set compatibility. The Intel® Itanium® Architecture Software Developer's Manual ...Read more"
			]
		  },
		  {
			"title": "CS 211: Intel Itanium",
			"url": "https://www2.seas.gwu.edu/~narahari/cs211/materials/lectures/IA64-Roy.pdf",
			"excerpts": [
			  "**Architectural Support for Control Speculation**",
			  "• 65th bit (NaT bit) on each GR indicates if an exception\nhas occurred",
			  " Special speculative loads that set the NaT bit if a\ndeferrable exception occurs\n",
			  " Special chk.s instruction that checks the NaT bit and\nbranches to recovery, if set\n",
			  " Computational instructions propagate NaTs like IEEE\nNaN’s\n•",
			  "Compare operations propagate “false” when writing\npredicates",
			  "*Predication Concepts**",
			  "Predication Concepts",
			  " Branching causes difficulty to handle effects\n",
			  "IA-64 provides predication\n",
			  "� Allows some branches to be removed",
			  "� Allows some types of safe code motion beyond\nbranches",
			  "� Basis for branch architecture and conditional\nexecution",
			  "**Architectural Support for Data Speculation**",
			  "• ALAT - HW structure containing information about\noutstanding loads advanced across stores",
			  "• Compare and test instructions write predicates with results\nof comparison/test",
			  " ld.a - advanced loads\n",
			  "� ld.c - check loads",
			  "� chk.a - advance load checks",
			  " aliasing st invalidating entries in ALAT\n",
			  "• Speculative advanced loads - ld.sa - is a control\nspeculative advanced load with fault deferral\n(combines ld.a and ld.s)",
			  "**Predication Concepts**",
			  "• Move complexity of resource allocation, scheduling,\nand parallel execution to compiler\n",
			  "IA-64 provides predication",
			  "\n• Provide features that enable the compiler to\nreschedule programs using advanced features\n(predication, speculation)",
			  "• Enable wide execution by providing processor\nimplementations that the compiler can take\nadvantage of\n",
			  "Basis for branch architecture and conditional\nexecution",
			  "**Overview of IA-64 Compiler Technology**",
			  "predication",
			  "• If-conversion",
			  " Uses regular, unconditional and parallel compares\n",
			  " Software pipelining\n",
			  " Modulo scheduling and rotating register allocation\n",
			  "• Uses rotating registers, stage predicates, loop\nbranches",
			  " Global instruction scheduling\n",
			  " Uses control and data speculation, predication, post-\nincrements, multi-way branches",
			  "• Global register allocation",
			  "• Uses register stack",
			  "**IA-64 Strategies**",
			  "**Instruction Formats: Bundles**",
			  "• Instruction types",
			  "� M: Memory",
			  " I: Shifts, MM\n",
			  "� B: Branch",
			  "� F: Floating point",
			  "� L+X: Long",
			  "**Instruction 2**",
			  "**Instruction 1**",
			  "**Instruction 0**",
			  "**41 bits**",
			  "**41 bits**",
			  "**41 bits**",
			  "**template**",
			  "**5 bits**",
			  "**128 bits**",
			  "• Template types",
			  "� Regular: MII, MLX, MMI, MFI, MMF",
			  " Stop: MI_I M_MI\n",
			  " Instructions\n",
			  "**Architectural Support**",
			  "• 64 1-bit predicate registers (true/false)",
			  "� p0 - p63",
			  "� most compare/test write result and complement",
			  "� Ex: cmp.eq p1,p2 = r1,0",
			  "• Almost all instructions can have a qualifying predicate\n(qp)",
			  " Ex: (p1) add r1 = r2, r3\n",
			  "� if qp is true, instruction executed normally",
			  "� if qp is false, instruction is squashed",
			  " Allocate registers for predicated code\n"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration",
			  "tation provides a “new register” for**\n**each loop iterati",
			  "each loop iteration",
			  "General, predicate, FP registers rotate",
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "The Initiation Interval (ii) is essentially the length of one pipeline stage -\nthe number of clock cycles it takes to execute one pass through the\nloop instructions or the number of clock cycles between starting\niteration i and starting iteration i+1.",
			  "Modulo Loop Scheduling is just one software pipelining technique.",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.",
			  "The IA-64 architecture has special support for software pipelining.",
			  "ation provides a “new register” for**\n**each loop iteratio",
			  "Enabled with -O2 switch",
			  "Typical SWP",
			  "Disqualifiers",
			  "Support for software pipelining is included in the Intel C/C++ compiler.",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count regist",
			  "Predication",
			  "One predicate assigned to each stage",
			  "Modulo Loop Scheduling involves developing a schedule for one loop\niteration such that when the schedule is repeated at regular intervals,\nno intra- or inter-iteration dependency is violated, and no resource\nusage conflict arises.",
			  "Initiation interval (ii)",
			  "Constant interval between iterations"
			]
		  },
		  {
			"title": "ItaniumR Architecture for Programmers. Understanding 64-Bit ...",
			"url": "https://amp.flylib.com/books/en/2.376.1.124/1",
			"excerpts": [
			  "In order to make specific assignments of rotating registers, we must consider the number of stages throughout which every quantity is \"live.\" First, notice that ..."
			]
		  },
		  {
			"title": "\nProjects to improve performance on IA-64\n- GNU Project",
			"url": "https://www.gnu.org/software/gcc/projects/ia64.html",
			"excerpts": [
			  "Profile information could be used for if-conversion to decide which side of the branch should be predicated. Profile information is used for predication and ..."
			]
		  },
		  {
			"title": "IA-64 Architecture Innovations",
			"url": "https://heco.estue.nl/courses/ACA/IA64/dale.ppt",
			"excerpts": [
			  "Separate Rotating Register Base for each ... EC counts pipeline stages for drain. Software Pipelining. Overlapping execution of different loop iterations.Read more"
			]
		  },
		  {
			"title": "Constant Propagation on Predicated Code",
			"url": "https://www.jucs.org/jucs_9_8/constant_propagation_on_predicated/Knoop_J.pdf",
			"excerpts": [
			  "For architectures such as the IA-64, the importance of switching by means of predi- cated execution from (smaller) basic blocks to (larger) hyperblocks is that ...Read more"
			]
		  },
		  {
			"title": "(PDF) The Transmeta Code Morphing/spl trade/ Software: using speculation, recovery, and adaptive retranslation to address real-life challenges",
			"url": "https://www.academia.edu/28434567/The_Transmeta_Code_Morphing_spl_trade_Software_using_speculation_recovery_and_adaptive_retranslation_to_address_real_life_challenges",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, systemlevel implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation."
			]
		  },
		  {
			"title": "The Technology Behind Crusoe™ Processors",
			"url": "https://classes.engineering.wustl.edu/cse362/images/c/c7/Paper_aklaiber_19jan00.pdf",
			"excerpts": [
			  "The software layer is called Code Morphing™ software because it dynamically “morphs” x86 instructions into VLIW instructions. The Code Morphing software ...Read more"
			]
		  },
		  {
			"title": "Read a paper: The Transmeta Code Morphing Software",
			"url": "https://www.youtube.com/watch?v=FcFR2of_wsQ",
			"excerpts": [
			  "Transmeta Code Morphing™ Software: using speculation, recovery ... (CGO '03). IEEE Computer Society, USA, 15–24. Link to paper: https ..."
			]
		  },
		  {
			"title": "CS 6120: The Transmeta Code Morphing Software",
			"url": "https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/transmeta/",
			"excerpts": [
			  "The Transmeta CMS system is a compiler solution to a hardware problem. While implemnting a just-in-time compiler for x86 is subtle and difficult ...Read more",
			  "Today we are reading the 2003 paper on Transmeta CMS (Code Morphing SoftwareTM). The CMS layer ran x86 programs on the Transmeta ...Read more",
			  "In a VLIW instruction set like Crusoe's, each instruction is really several smaller instructions which are issued in parallel. In the ...Read more"
			]
		  },
		  {
			"title": "Two-pass MRET trace selection for dynamic optimization",
			"url": "https://patents.google.com/patent/US7694281B2/en",
			"excerpts": [
			  "When the counter exceeds a certain threshold, the candidate trace head becomes a hot trace head. Then the hot trace is simply selected as the execution path ..."
			]
		  },
		  {
			"title": "Binary Compatibility 1 The Transmetta Code Morphing ...",
			"url": "https://users.ece.utexas.edu/~merez/EE382V/lect03.pdf",
			"excerpts": [
			  "The paper discusses the software system and the hardware support for implementing x86, specifically exceptions and interrupts,. I/O, and self- ...Read more",
			  "Transmetta’s Crusoe microprocessor is a VLIW micro-\nprocessor that along with a software layer, the Code Morphing Software (CMS), provides\na full implementation of the x86 architecture."
			]
		  },
		  {
			"title": "The Transmeta Code Morphing™ Software | Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization",
			"url": "https://dl.acm.org/doi/10.5555/776261.776263",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the **Code Morphing Software** (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "In this paper we discuss some of the challenges raised by these issues, and present the techniques developed in Crusoe and CMS to meet those challenges.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects.",
			  "The wide range of PC workloads that CMS must handle gracefully in real-life operation, plus the need for full system-level x86 compatibility, expose several issues that have received little or no attention in previous literature, such as exceptions and interrupts, I/O, DMA, and self-modifying code."
			]
		  },
		  {
			"title": "(PDF) The Transmeta Code Morphing Software: Using Speculation, Recovery, and Adaptive Retranslation to Address Real-Life Challenges",
			"url": "https://www.researchgate.net/publication/220799118_The_Transmeta_Code_Morphing_Software_Using_Speculation_Recovery_and_Adaptive_Retranslation_to_Address_Real-Life_Challenges",
			"excerpts": [
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system.",
			  "S must ef ficiently handle memor y-m apped I/O and\nother system-level operations,without penalizing\nnormal (non-I/O) memory references",
			  "elf-modifying code. Similar\nproblems resultfrom pages containing bothcode and\ndata, common in Windows/ 9X device drivers, BIOSs,\nand embedded systems running a real-time operating\nsystem such as QNX.\n",
			  "S mu st faith fully reproduce the precise exception\nbehavior of the x86 target, without overlyconstraining\nthe scheduling of its translations.",
			  "MS mu st respond to int errupts at precise x86\ninstruction boundaries,wherethe system possesses a\nconsistent target state.",
			  "The Transmeta native VLIW processors provide\nhardware assistance for various kinds of speculation and\nsubsequent recovery; we describe thismechanism in\nsubsection 3.1.",
			  "Normal atoms only update theworking copy of the\nregister.",
			  "If execution reachesthe end of a translation, a\nspecial commit operation copies all work ing registers in to\ntheir corresponding shadow registers, committingthe\nwork done in the translation",
			  "On the other hand, if any\nexceptional condition, such as the failure of one of CMS ’ s\ntranslation assumptions, occurs inside the translation, the\nruntimesystem undoes the effects of allmolecules\nexecuted since the last commit via a rollback operation\nthat copies the shadowregister values (committed at the\nend of the previous translation) back into the working\nregisters",
			  "Store data are held in a gated store buffer , fromwhich\nthey are onlyreleased to the memory systemat the time\nof a commit.",
			  "On a rollback, stores notyet committed can\nsimplybe dropped from thestore buffer.",
			  "Commit and rollback also apply to memory operations.",
			  "the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  ".g., memory conflict buffers,\nGallagher et al. [14], or the Intel IA-64 ",
			  "CMS monitors recurring\nfailures andgenerates a more conservativetranslation\nwhen it deemsthe rate of failure to be excessive.",
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects."
			]
		  },
		  {
			"title": "The Transmeta Code Morphing Software",
			"url": "https://safari.ethz.ch/digitaltechnik/spring2018/lib/exe/fetch.php?media=dehnert_transmeta_code_morphing_software.pdf",
			"excerpts": [
			  "CMS is structured like many other dynamic translation\nsystems. Initially, an interpreter decodes and executes\nx86 instructions sequentially, with careful attention to\nmemory access ordering and precise reproduction of\nfaults, while collecting data on execution frequency,\nbranch directions, and memory-mapped I/O operations.\nWhen the number of executions of a section of x86 code\nreaches a certain threshold, its address is passed to the\ntranslator.",
			  "The translator selects a region including that address,\nproduces native code to implement the x86 code from the\nregion identified, and stores the translation with various\nrelated information in the translation cache . From then\non, until something invalidates the translation cache entry,\nCMS executes the translation when the x86 flow of\ncontrol reaches the translated x86 code region.",
			  "Initially, the exits of a translation branch to a lookup\nroutine (the “ no chain ” path in Figure 1) that transfers\ncontrol either to an existing translation for the next\n ... \ninclude up to 200 x86 instructions. This provides an\nextended scope for optimization. The optimizer performs\na number of traditional and Crusoe-specific optimizations\non the region, and schedules the final native VLIW code\nas a set of single-entry, multiple-exit traces.",
			  "The wide range\nof PC workloads that CMS must handle gracefully in\nreal-life operation, plus the need for full system-level x86\ncompatibility, expose several issues that have received\nlittle or no attention in previous literature, such as excep-\ntions and interrupts, I/O, DMA, and self-modifying code.",
			  "Speculation in this context refers to making and\nexploiting assumptions — unproven at translation time —\nabout the code being translated. (For example, the\ntranslator might assume that two specific load and store\ninstructions reference non-overlapping memory.) This\ntype of speculation enables generation of much more\nefficient translations, but should one or more assumptions\nprove to be false, incorrect results may be produced.\nHence, the assumptions must somehow be verified at\nruntime, with appropriate action taken when a violation is\ndetected.",
			  "The requirement for CMS to deliver an utterly faithful\nyet high-performance implementation of a legacy\ncommercial\nmicroprocessor\narchitecture\nposes\na\nsignificant challenge. A key paradigm that allows us to\naddress many technical obstacles is that of speculation,\nrecovery, and adaptive retranslation.",
			  "CMS guarantees correct operation by careful\nscheduling,\ninserting\nno-ops\nif\nnecessary.\nOnly\nunpredictably long-latency operations such as loads that\nmiss in the caches have their additional latency handled\nautomatically by the hardware.",
			  "In fact, the current TM5000 family evolved\nsignificantly from the first TM3000 family Crusoe\nprocessors, adding atoms to more efficiently implement\nx86 segmentation, 16-bit operations, and indirect\nbranches, all without a change in the target ISA.",
			  "CMS can make no assumptions about the operating\nsystem running on the processor and cannot depend on\ninformation or other assistance from the system. It is\na system-level implementation, not application-level,\nand even executes the BIOS code.",
			  "The choice of translation regions is beyond the scope\nof this paper, but they may be fairly large and complex,\ncontain long traces, IF statements, and nested loops, and\ninclude up to 200 x86 instructions.",
			  "The\nparadigm of speculating aggressively, rolling back to a\nconsistent state for recovery when exceptions are\ndetected, and adaptively retranslating to deal with\nrecurring exceptions is a powerful part of the CMS\nsolution.",
			  "the Intel IA-64 ALAT [18]",
			  "to detect overlapping memory operations\nscheduled out of sequence, and to branch to the\ncompensation code (e.g., memory conflict buffers,\nGallagher et al. [14], or the Intel IA-64 ALAT [18]).",
			  "All registers\nholding x86 state are shadowed ; that is, there exist two\ncopies of each register, a working copy and a shadow\ncopy."
			]
		  },
		  {
			"title": "Dynamo: A Transparent Dynamic Optimization System",
			"url": "https://cseweb.ucsd.edu/classes/sp00/cse231/dynamopldi.pdf",
			"excerpts": [
			  "Dynamo\nfirst selects a “hot” trace, then optimizes it, and finally emits\nrelocatable code for it into the fragment cache.",
			  "The emitted\nrelocatable code is contiguous in the fragment cache memory, and\nbranches that exit this code jump to corresponding exit stubs at the\nbottom of the code. This code is referred to as a *fragment* . The\ntrace is a unit of the application’s dynamic instruction stream (i.e.,\na sequence of application instructions whose addresses are\napplication binary addresses) whereas the fragment is a Dynamo\ninternal unit, addressed by fragment cache addresses. The\nfollowing subsections outline the trace selection, trace optimization\nand fragment code generation mechanisms of Dynamo.",
			  "No profiling is done on the code generated into Dynamo’s\nfragment cache. This allows the cached code to run directly on the\nprocessor at full native speed without any Dynamo introduced\noverheads. ",
			  "the fragment cache is designed to tolerate periodic\nflushes. Periodically flushing some of the traces in the fragment\ncache helps remove unused traces, and also forces re-selection of\nactive traces.",
			  "e selected hot trace is prepared for optimization by\nconverting it into a low-level intermediate representation (IR) that\nis very close to the underlying machine instruction s",
			  "The first task of trace optimization is to transform the\nbranches on the trace so that their fall-through direction remains on\nthe trace. Loops are only allowed if the loop-back branch targets\nthe start-of-trace. Otherwise the loop-back branch is treated as a\ntrace exit. Unconditional direct branches are redundant on the trace\nand can be removed."
			]
		  },
		  {
			"title": "Dynamo TR",
			"url": "https://homes.cs.washington.edu/~bodik/ucb/cs703-2002/papers/dynamo-full.pdf",
			"excerpts": [
			  "**3.3 Innovative features**",
			  "e first, and probably the most important, is Dynamo’s hot trace selection scheme. It is\nhighly speculative in nature, and uses very minimal profiling. The essence of the idea is that when\na program address gets hot, the sequence of addresses executed immediately following it are\nlikely to be a hot trace. Thus, instead of using complicated path profiling, Dynamo only\nassociates counters with certain special program addresses (such as targets of backward taken\nbranches). When such an address gets hot, the interpreter emits a copy of the interpreted native\ninstructions that immediately follow the one at the hot add",
			  "The second innovation is the use of a dynamic trace as the granularity of runtime\noptimization. The instructions comprising a dynamic trace are often non-contiguous in memory,\nand thus may not have been part of the static program scope that was available to the generator of\nthe program binary. A dynamic trace is therefore likely to expose new optimization opportunities,\neven in a statically optimized program binary.",
			  "e traces can only be entered at the top,\n14\nand they have no control join points within them. This simplicity of control flow allowed us to\ndesign a very fast, non-iterative, trace optimizer, that turned out to be highly effective. Dynamo\nonly optimizes each trace once, at the time it is emitted into the software code cache, and does not\n .",
			  "**7 Trace Selector**",
			  "Dynamic optimization is based on identifying hot spots in the executing program. The Trace\nSelector extracts hot code regions around the detected hot spots and passes them to the Fragment\nOptimizer for optimization and eventual placement in the Fragment Cache. The arrangement of\nthe extracted hot code regions in the Fragment Cache leads to a new code layout, which has direct\nimpact on future performance of the hot spot code.",
			  "Thus, the goal of trace selection\nis twofold: to improve the *code layout* and to expose *dynamic re-optimization* opportunities.",
			  "he code region of choice in Dynamo is a *partial execution trace* (trace for short). Traces\nfall into the dynamic scope of the native program and directly represent dynamic control flow\ninformation. A trace is a dynamic sequence of executed basic blocks. The sequence may not be\ncontiguous in the program memory; it may even be *interprocedural* , spanning one or several\nprocedure boundaries, including dynamically linked modules.",
			  "Some profiling is necessary to differentiate hot code region from cold ones. Profile\ninformation is collected as a side effect of interpretation, and trace selection is triggered only\nwhen a hot program address is interpreted. After selecting a hot trace, the Fragment Optimizer\ntransforms the trace into a location-independent, contiguous code sequence called a *fragment* ,\nwhich is later placed in the Fragment Cache for execution",
			  "Trace\nSelector extracts hot code regions around the detected hot spots and passes them to the Fragment\nOptimizer for optimization and eventual placement in the Fragment Cache.",
			  "The Trace Selector attempts to reduce or mitigate taken-branch penalties by converting taken\nbranches on a trace into fall through branches."
			]
		  },
		  {
			"title": "US20120260067A1 - Microprocessor that performs x86 isa and arm isa machine language program instructions by hardware translation into microinstructions executed by common execution pipeline \n        - Google Patents",
			"url": "https://patents.google.com/patent/US20120260067A1/en",
			"excerpts": [
			  "Transmeta Corporation of Santa Clara, Calif., coupled VLIW core hardware and “a pure software-based instruction translator [referred to as “Code Morphing ...Read more"
			]
		  },
		  {
			"title": "Transmeta - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Transmeta",
			"excerpts": [
			  "Code Morphing Software (CMS) consisted of an interpreter, a runtime system and a dynamic binary translator. x86 instructions were first interpreted one ...Read more"
			]
		  },
		  {
			"title": "YETI: a graduallY Extensible Trace Interpreter",
			"url": "http://www.cs.toronto.edu/~matz/dissertation/matzThesisProposal.pdf",
			"excerpts": [
			  "2.3.1 HP Dynamo. HP Dynamo [2, 16, 1] is a system for trace-based runtime optimization of statically optimized binary code. Dynamo initially interprets a ...Read more"
			]
		  },
		  {
			"title": "Top secret chip less secret now",
			"url": "https://www.cnet.com/tech/tech-industry/top-secret-chip-less-secret-now/",
			"excerpts": [
			  "\"Code morphing\" software, assisted by hardware on the chip itself, translates the instructions \"on the fly,\" then stores the translated instructions so they can ...Read more"
			]
		  },
		  {
			"title": "Dynamic Optimization of IA-32 Applications Under DynamoRIO",
			"url": "https://groups.csail.mit.edu/cag/rio/tim-meng-thesis.pdf",
			"excerpts": [
			  "Only code in the fragment cache is allowed to execute, never code in the original image of the binary. As the number of fragments increases, performance is ...Read more"
			]
		  },
		  {
			"title": "Transmeta Awarded Another Patent",
			"url": "https://tech.slashdot.org/story/99/09/28/1531230/transmeta-awarded-another-patent",
			"excerpts": [
			  "This is a hardware method of rolling back stores to get the exact state at the time of exception without tracking state on an instruction by ...Read more"
			]
		  },
		  {
			"title": "The Transmeta Code Morphing/spl trade/ Software: using ...",
			"url": "https://ieeexplore.ieee.org/document/1191529/",
			"excerpts": [
			  "The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.",
			  "Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the Code Morphing Software (CMS), that combines an interpreter, dynamic binary translator, optimizer, and run-time system.",
			  "In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects.",
			  "The wide range of PC workloads that CMS must handle gracefully in real-life operation, plus the need for full system-level x86 compatibility, expose several issues that have received little or no attention in previous literature, such as exceptions and interrupts, I/O, DMA, and self-modifying code."
			]
		  },
		  {
			"title": "US7111096B1 - Fast look-up of indirect branch destination in a dynamic translation system \n        - Google Patents",
			"url": "https://patents.google.com/patent/US7111096",
			"excerpts": [
			  "The morph host processor executes the code morphing software which translates the target application programs dynamically into host processor instructions that are able to accomplish the purpose of the original software. A",
			  ", the code morphing software carries out a significant portion of the functions of digital processors in software, reducing the hardware required for processing, and, hence, reducing power consumption.",
			  " As the instructions are translated, they are stored in a translation buffer where they may be subsequently accessed and executed, as needed, during continued program execution without further translation.\n",
			  "anslation memory **7** , sometimes referred to as the translation buffer, holds all of the translations produced during the run of the target application program.",
			  "e method makes use of a cache **5** that is reserved (defined) in the memory of the host processor by the code morphing software for storage of a fraction of the translations of target addresses and the respective accompanying tags for each respective translation."
			]
		  },
		  {
			"title": "US6594821B1 - Translation consistency checking for modified target instructions by comparing to original copy \n        - Google Patents",
			"url": "https://patents.google.com/patent/US6594821",
			"excerpts": [
			  "The morph host processor includes a number of hardware enhancements which allow sequences of target instructions spanning known states of the target processor to be translated into host instructions, stored for further use in the translation buffer, and tested to determine if the translated instructions will execute correctly.",
			  "Memory stores and target processor state are updated upon successful execution in a process referred as “committing.” These hardware enhancements allow the rapid and accurate handling of exceptions which occur during the execution of the sequences of host instructions by returning execution to the beginning of a sequence of instructions at which known state of the target processor exists. Returning the operations to a point in execution at which target state is known is called “rollback.”"
			]
		  },
		  {
			"title": "US5832205A - Memory controller for a microprocessor for detecting a failure of speculation on the physical nature of a component being addressed \n        - Google Patents",
			"url": "https://patents.google.com/patent/US5832205A/en",
			"excerpts": [
			  "the code morphing software combined with the enhanced morph host translates target instructions into instructions for the morph host on the fly and caches those host instructions in a memory data structure (referred to in this specification as a \"translation buffer\").",
			  "a translation buffer to hold translated instructions allows instructions to be recalled without rerunning the lengthy process of determining which primitive instructions are required to implement each target instruction, addressing each primitive instruction, fetching each primitive instruction, optimizing the sequence of primitive instructions, allocating assets to each primitive instruction, reordering the primitive instructions, and executing each step of each sequence of primitive instructions involved each time each target instruction is executed. O",
			  "Once a target instruction has been translated, it may be recalled from the translation buffer and executed without the need for any of these myriad of steps.",
			  "the translator In addition to simply translating the instructions, optimizing, reordering, rescheduling, caching, and executing each translation so that it may be rerun whenever that set of instructions needs to be executed, the translator also links the different translations to eliminate in almost all cases a return to the main loop of the translation process.",
			  "the translator portion of the code morphing software in accomplishing this linking process. It will be understood by those skilled in the art that this linking operation essentially eliminates the return to the main loop for most translations of instructions, which eliminates this overhead.",
			  "Prior art emulators have also struggled with what is generally referred to as self modifying code. Should a target program write to the memory that contains target instructions, this will cause translations that exist for these target instructions to become \"stale\" and no longer valid. It is necessary to detect these stores as they occur dynamically.",
			  "The present invention deals with this problem by another enhancement to the morph host. A translation bit (T bit) which may also be stored in the translation look-aside buffer is used to indicate target memory pages for which translations exist. The T bit thus possibly indicates that particular pages of target memory contain target instructions for which host translations exist which would become stale if those target instructions were to be overwritten."
			]
		  },
		  {
			"title": "Design and Implementation of a Lightweight Dynamic ...",
			"url": "https://jilp.org/vol6/v6paper5.pdf",
			"excerpts": [
			  "amic optimization has been presented in the past in frameworks such as Dynamo [1]\nand Continuous Profiling and Optimization (CPO) [2]. Dynamo is a transparent dynamic\nnative-to-native optimization system. Dynamo starts running a statically compiled exe-\ncutable by interpretation, waiting for hot traces to show up. Once hot traces are detected,\nDynamo stops the program and generates code fragments for these traces. Subsequent ex-\necution on the same trace will be redirected to the newly optimized code in the fragment\ncache. Since interpretation is expensive, Dynamo tries to avoid it by translating as many\nhot traces as possible to the fragment cache. To achieve this goal, it uses a small threshold\nto quickly determine whether a trace is hot. This approach often ends up with generating\ntoo much code and less effective traces. In the recent work Dynamo RIO [6], this feature\nhas been changed",
			  "[2] presents a model closer to traditional PBO where the original code is instru-\nmented, and the profile information is used to compile optimized versions of code. In CPO,\nprofiled information is used to drive PBO while the program is running and the compiled\nresult is hot-swapped into the program. The advantage of this scheme is that the IR infor-\nmation makes application of many optimizations easier.",
			  "There are a lot of additional research topics on dynamic optimization. Many of them\nare often seen in dynamic translation [8],[9],[12] and binary transformation [15].",
			  "In this paper we present a lightweight runtime optimizer implemented on a real system\nwith existing hardware and software support. We discuss much detail of the design and\nimplementation including profiling, hot trace selection, phase detection, optimization and\ncode patching. The overhead of this system is very low due to the use of hardware per-"
			]
		  },
		  {
			"title": "The Performance of Runtime Data Cache Prefetching in a ...",
			"url": "https://www.microarch.org/micro36/html/pdf/lu-PerformanceRuntimeData.pdf",
			"excerpts": [
			  "Dynamo starts running a statically\ncompiled executable by interpretation, waiting for hot\ntraces to show up.",
			  "Once hot traces are detected, Dynamo\nstops the program and generates code fragments for these\ntraces.",
			  "bsequent execution on the same trace will be redi-\nrected to the newly optimized code in the fragment cache.",
			  " *trace* is a single entry, multi-exit code\nsequence. ",
			  " Dynamo, a trace is selected once the reference\ncount of the target block of a backwards taken branch ex-\nceeds a threshold.",
			  "s implemented as a shared library on Linux\nfor IA64 that can be automatically linked to the applica-\ntion at startup. It is also a runtime trace optimizer like Dy-\nnamo [3], but ADORE relies on the Itanium hardware per-\nformance monitor (HPM) to identify hotspots instead of us-\ning interpretation"
			]
		  },
		  {
			"title": "Improving Region Selection in Dynamic Optimization Systems | Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.1109/MICRO.2005.22",
			"excerpts": [
			  "The performance of a dynamic optimization system depends heavily on the code it selects to optimize.",
			  "Many current systems follow the design of HP Dynamo and select a single interprocedural path, or trace, as the unit of code optimization and code caching.",
			  "In particular, we propose two new region-selection algorithms and evaluate them against Dynamo's selection mechanism, Next-Executing Tail (NET).",
			  "Our first algorithm, Last-Executed Iteration (LEI), identifies cyclic paths of execution better than NET, improving locality of execution while reducing the size of the code cache.",
			  "Our second algorithm allows overlapping traces of similar execution frequency to be combined into a single large region.",
			  "This second technique can be applied to both NET and LEI, and we find that it significantly improves metrics of locality and memory overhead for each."
			]
		  },
		  {
			"title": "Dynamo: a transparent dynamic optimization system",
			"url": "https://dl.acm.org/doi/10.1145/358438.349303",
			"excerpts": [
			  "Dynamo: a transparent dynamic optimization system. PLDI '00: Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation.Read more"
			]
		  },
		  {
			"title": "[PDF] The Transmeta Code Morphing/spl trade/ Software: using speculation, recovery, and adaptive retranslation to address real-life challenges | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/The-Transmeta-Code-Morphing-spl-trade-Software%3A-and-Dehnert-Grant/5c17b90a86ab37488215f6a060ecd3350a994ebf",
			"excerpts": [
			  "The Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation are presented."
			]
		  },
		  {
			"title": "EP1240582B1 - Fine grain translation discrimination \n        - Google Patents",
			"url": "https://patents.google.com/patent/EP1240582B1",
			"excerpts": [
			  "the morph host processor includes a number of hardware enhancements which allow sequences of target instructions spanning known states of the target processor to be translated into host instructions, stored for further use in the translation buffer, and tested to determine if the translated instructions will execute correctly.",
			  "T bit is stored with a physical page address in a translation lookaside buffer (TLB).",
			  "The exception handler invalidates these translations by turning off the T bit protection for the TLB entry.",
			  "the method is referred to as a \"T bit exception.”",
			  "One problem which can arise with the new processor is that it is possible with some operating systems and applications for a target processor to write to target instructions stored in memory. If this happens, the host instructions which are translations of the target instructions which have been overwritten are no longer valid.",
			  "fine-grain T bit protection eliminates a significant portion of the T bit exceptions generated by the new processor.",
			  "Consequently, a new process has been devised by which the code morphing software may determine whether an attempted write to a memory page is to translated instructions or to data and allow writes to data without invalidating translations of instructions on the page."
			]
		  },
		  {
			"title": "a compiler approach for scratchpad memory management",
			"url": "http://ieeexplore.ieee.org/document/1515604/",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast software-managed on-chip SRAM, is now widely used in modern embedded processors.",
			  "Compared to hardware-managed cache, it is more efficient in performance, power and area cost, and has the added advantage of better time predictability.",
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "An integrated scratch-pad allocator for affine and non-affine code | Proceedings of the conference on Design, automation and test in Europe: Proceedings",
			"url": "https://dl.acm.org/doi/10.5555/1131481.1131740",
			"excerpts": [
			  "Scratch-Pad memory (SPM) allocators that exploit the presence of affine references to arrays are important for scientific benchmarks.",
			  "On the other hand, such allocators have so far been limited in their general applicability.",
			  "In this paper we propose an integrated scheme that for the first time combines the specialized solution for affine program allocation with a general framework for other code.",
			  "We find that our integrated framework does as well or outperforms other allocators for a variety of SPM sizes."
			]
		  },
		  {
			"title": "An Integrated Scratch-Pad Allocator for Affine and Non- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-DATE-2006.pdf",
			"excerpts": [
			  " memory (SPM) allocators that exploit the*\n*presence of affine references to arrays are important for sci-*\n*entific benchmarks. On the other hand, such allocators have*\n*so far been limited in their general applicability. In this pa-*\n*per we propose an integrated scheme that for the first time*\n*combines the specialized solution for affine program alloca-*\n*tion with a general framework for other code. We find that*\n*our integrated framework does as well or outperforms other*\n*allocators for a variety of SPM sizes",
			  "In this paper we present an integrated algorithm to al-\nlocate both affine and non affine programs into SPM. Our\nintegrated algorithm is an extension of a previously pro-\nposed general framework [10]. Besides the advantages of\nan non-affine algorithm like general applicability, the algo-\nrithm can also places parts of variables when they accessed\nusing affine functions. Integration also allows for selecting\nthe transfer point using a cost-model. Our results show that\nthe integrated algorithm combines the benefits of an affine\nand non affine allocator. For a wide range of sizes, the in-\ntegrated algorithm does as well or better than either affine\nonly or non-affine only allocators",
			  "The prevalence of SPM in embedded systems is evident\nfrom the large variety of chips with SPM available today in\nthe market.( *e.g.,* [1,4]). Such memory is typically software\nmanaged. Software management, apart from avoiding ex-\ntra caching hardware, also allows for more predictable pro-\ngram execution than when caches are present. Studies have\nfurther shown other benefits of using SPM including reduc-\ntions in power consumption, memory latency, area and even\nsignificant runtime gains for embedded applications. Trends\nin embedded designs indicate that this dominance of SPM’s\nwill continue in the future [9].",
			  "The rest of the paper is organized as follows. Section 2\noverviews related work. Section 3 describes our integrated\nmethod for affine and non-affine code. Section 4 presents an\nevaluation of our methodology. Section 5 concludes."
			]
		  },
		  {
			"title": "Scratch-pad memory allocation without compiler support for java applications | Proceedings of the 2007 international conference on Compilers, architecture, and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/1289881.1289899",
			"excerpts": [
			  "S. Tomar, S. Kim, N. Vijaykrishnan, M. Kandemir, and M. J. Irwin. Use of local memory for efficient java execution. In Proceedings of the International Conference on Computer Design: VLSI in Computers & Processors (ICCD). IEEE, 2001.",
			  "S. Udayakumaran and R. Barua. Compiler-decided dynamic memory allocation for scratch-pad based embedded systems. In Proceedings of the international conference on Compilers, architectures and synthesis for embedded systems (CASES), pages 276--286. ACM Press, 2003.",
			  "S. Udayakumaran, A. Dominguez, and R. Barua. Dynamic Allocation for Scratch-Pad Memory using Compile-Time Decisions. To appear in the ACM Transactions on Embedded Computing Systems (TECS), 5(2), 2006. http://www.ece.umd.edu/~barua/udayakumaran-TECS-2006.pdf."
			]
		  },
		  {
			"title": "Dynamic management of scratch-pad memory space | Proceedings of the 38th annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/378239.379049",
			"excerpts": [
			  "Section Title: Dynamic management of scratch-pad memory space",
			  "Content:\nAuthors : [M. Kandemir](# \"M. Kandemir\") M. Kandemir\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100186744) , [J. Ramanujam](# \"J. Ramanujam\") J. Ramanujam\nDepartment of ECE, Louisiana State University, Baton Rouge, LA\n[View Profile](/profile/81100351630) , [J. Irwin](# \"J. Irwin\") J. Irwin\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81350589577) , [N. Vijaykrishnan](# \"N. Vijaykrishnan\") N. Vijaykrishnan\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81331506265) , [I. Kadayif](# \"I. Kadayif\") I. Kadayif\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100417786) , [A. Parikh](# \"A. Parikh\") A. Parikh\nMicrosystems Design Lab, The Pennsylvania State University, University Park, PA\n[View Profile](/profile/81100298924) [Authors Info & Claims](#) To view Author Info & Claims, you must have Premium access.\n[Learn more](/about/upgrade) [Sign in](/action/showLogin?redirectUri=%2Fdoi%2F10.1145%2F378239.379049)\n[DAC '01: Proceedings of the 38th annual Design Automation Conference](/doi/proceedings/10.1145/378239)\nPages 690 - 695\nhttps://doi.org/10.1145/378239.379049\nPublished : 22 June 2001 [Publication History](#) To view Publication History, you must have Premium access.\n ... \nSection Title: Dynamic management of scratch-pad memory space > Comments > Affiliations"
			]
		  },
		  {
			"title": "Dynamic Management of Scratch-Pad Memory Space",
			"url": "https://www.researchgate.net/profile/Ismail-Kadayif-2/publication/3904523_Dynamic_management_of_scratch-pad_memory_space/links/0a85e53a977b05a602000000/Dynamic-management-of-scratch-pad-memory-space.pdf",
			"excerpts": [
			  "We propose a compiler-controlled dynamic\non-chip scratch-pad memory (SPM) management framework that uses\nboth loop and data transformations.",
			  "perimental results obtained us-\ning a generic cost model indicate significant reductions in data transfer\nactivity between SPM and off-chip memory.",
			  "Embedded systems are often designed as a *system-on-chip* (SoC) archi-\ntecture to cater to the demands of small form factor"
			]
		  },
		  {
			"title": "Compiler-Decided Dynamic Memory Allocation for Scratch- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-CASES-2003.pdf",
			"excerpts": [
			  "Existing scratch-pad allocation methods are of two types. First,\nsoftware-caching schemes emulate the workings of a hardware\ncache in software. Instructions are inserted before each load/store\nto check the software-maintained cache tags.",
			  "scratch-pad* is a fast compiler-managed\nSRAM memory that replaces the hardware-managed cache. It is\nmotivated by its better real-time guarantees vs cache and by its sig-\nnificantly lower overheads in energy consumption, area and overall\nruntime, even with a simple allocation scheme [4]",
			  "lexCache: A Framework for Flexible\nCompiler Generated Data Caching. In *The 2nd Workshop on*\n*Intelligent Memory Systems* , Boston, MA, November 12\n20",
			  "Arun Iyengar. Design and performance of a general-purpose\nsoftware cache. *Journal of Parallel and Distributed*\n*Computing* , 38(2):248–255, ",
			  "PU12 Reference Manual* . Motorola Corporation, 2000. (A\n16-bit processor). http://e-www.motorola.com/brdata/-\nPDFDB/MICROCONTROLLERS/-\n16 BIT/68HC12 FAMILY/REF MAT/CPU12RM.pdf.",
			  "demir, J.Ramanujam, M.J.Irwin, N.Vijaykrishnan,\nI.Kadayif, and A.Parikh. Dynamic Management of\nScratch-Pad Memory Space. In *Design Automation*\n*Conference* , pages 690–695, 2001",
			  "This paper presents a highly predictable, low overhead and yet\ndynamic, memory allocation strategy for embedded systems with\nscratch-pad memory."
			]
		  },
		  {
			"title": "[PDF] Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/31aab4326d7f7d3bd5e792fbf886ff3a44d4414b",
			"excerpts": [
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems · Sumesh Udayakumaran, R. Barua · Published in International Conference on… 30 ..."
			]
		  },
		  {
			"title": "A compiler approach for scratchpad memory management",
			"url": "https://www.researchgate.net/publication/4178715_Memory_coloring_A_compiler_approach_for_scratchpad_memory_management",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "https://www.researchgate.net/publication/220170090_Compiler-Directed_Scratchpad_Memory_Management_via_Graph_Coloring",
			"excerpts": [
			  "graph coloring algorithm for scalars that is generalized as described in [Smith et al. 2004] to deal with interchangeable and aliased registers.Read more"
			]
		  },
		  {
			"title": "Dynamic Allocation for Scratch-Pad Memory using Compile ...",
			"url": "https://terpconnect.umd.edu/~barua/udayakumaran-TECS-2005.pdf",
			"excerpts": [
			  "Dynamic Allocation for Scratch-Pad Memory using\nCompile-Time Decisions",
			  "Sumesh Udayakumaran and Angel Dominguez and Rajeev Barua",
			  "University of Maryland",
			  "In this research we propose a highly predictable, low overhead and yet dynamic, memory allocation strategy\nfor embedded systems with scratch-pad memory. A *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  "Scratch-pad allocation primarily methods are of two types. First, software-caching schemes emulate the work-\nings of a hardware cache in software. Instructions are inserted before each load/store to check the software-\nmaintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption and\nSRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of\nalgorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation\nschemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that\nnever changes at runtime cannot achieve the full locality benefits of a cache.",
			  "We propose a dynamic allocation methodology for global and stack data and program code that, (i) accounts",
			  " *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache",
			  "It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme."
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems | Proceedings of the 2003 international conference on Compilers, architecture and synthesis for embedded systems",
			"url": "https://dl.acm.org/doi/10.1145/951710.951747",
			"excerpts": [
			  " Compiler-decided dynamic memory allocation for scratch-pad based embedded systems\n",
			  " Section Title: Compiler-decided dynamic memory allocation for scratch-pad based embedded systems"
			]
		  },
		  {
			"title": "Optimizing DMA Data Transfers for Embedded Multi-Cores",
			"url": "http://www-verimag.imag.fr/~maler/Papers/thesis-selma.pdf",
			"excerpts": [
			  "**Explicitly Managed Memory**",
			  "Double buffering or more generally multi-buffering is a well known programming technique\nreferred to as *software pipelining* used to hide memory latency. The main idea is to overlap com-\nputations and data transfers and it is expressed at the software level.",
			  "This takes advantage from the fact that DMA can work in parallel with a processor. Pro-\ngram 2.4.2 uses *double buffering* for input blocks *B* *x* [0] , *B* *x* [1] and output blocks *B* *y* [0] , *B* *y* [1] . At\neach iteration before the processor starts working on the current buffer, it issues an *asynchronous*\nDMA call to fetch the next buffer. Therefore, the processor can work on a super block *j* residing\nin one buffer while the DMA brings *in parallel* the super block *j* + 1 to the other buffer. Note that\n*comp* ( *j* ) is used as a shorthand for the inner double loop in Program 2.4.1. In practice, events are\nassociated with each *dma_get* and *dma_put* command to ensure the synchronization between each\ndata transfer and its corresponding computation.",
			  "Hence, program 2.4.2 defines a *software pipeline* with 3 stages: input of super block ( *j* + 1) ,\ncomputation on super block *j* and output of super block ( *j* *−* 1) , see Figure 2.10. Reading the first\nblock and writing back the last block are, respectively, the *prologue* and *epilogue* of the pipeline.\nNote that Figure 2.10 only describes the obvious precedences between the computations and data\ntransfers but tells us nothing about their relative durations.",
			  "ile for general purpose multiprocessor architectures, the design choice of using caches is\nobvious, it is not the case anymore for MPSoCs. The main reason is that MPSoCs are resource\nconstrained systems and managing cache coherence is very expensive both in terms of area and\npower [LPB06]. Therefore Scratchpad Memories (SPMs) have been proposed as an alternative\nto caches where data (and sometimes code) transfers through the memory hierarchy are *explic-*\n*itly* managed by the software. Furthermore, unlike a general purpose processor where a standard\ncache-based memory hierarchy is employed to guarantee good average performance over a wide\nrange of applications, in MPSoCs, the overall memory organization has to be tailored in order\nto fit at best the needs of the target class of applications. Indeed, according to the memory ac-\ncess pattern of the target applications, the use of scratchpad memories can be more efficient t"
			]
		  },
		  {
			"title": "Applications Using the TMS320C6000 Enhanced DMA",
			"url": "https://www.ti.com/lit/pdf/spra636",
			"excerpts": [
			  "Ping-Pong Buffering for McBSP Data",
			  "While the EDMA is transferring\ndata in to and out of the ping buffers, the CPU is manipulating the data in the pong buffers.",
			  "The ping–pong scheme for this example is shown in\nFigure 27.",
			  "The EDMA channels may be configured to access any location in the device’s memory map."
			]
		  },
		  {
			"title": "TMS320C6000 DSP Enhanced Direct Memory Access ( ...",
			"url": "https://www.ti.com/lit/pdf/spru234",
			"excerpts": [
			  "Each channel has two parameter sets, ping and pong.",
			  "Ping-pong buffering is a simple technique that allows the CPU activity to be\ndistanced from the EDMA activity. This means that there are multiple (usually\ntwo) sets of data buffers for all incoming and outgoing data streams.",
			  "The link address for the\nping entry is set to the PaRAM offset of the pong parameter set (Figure 1−27)."
			]
		  },
		  {
			"title": "Optimizing Data Distribution for Loops on Embedded ...",
			"url": "https://utd-ir.tdl.org/server/api/core/bitstreams/cb4d3e16-7abb-4de8-8fc4-77d727d707f1/content",
			"excerpts": [
			  "472–511, May 2006. [13] S. Udayakumaran and R. Barua, “Compiler-decided dy- namic memory allocation for scratch-pad based embed ...Read more"
			]
		  },
		  {
			"title": "Compiler-directed scratch pad memory hierarchy design ...",
			"url": "https://pure.psu.edu/en/publications/compiler-directed-scratch-pad-memory-hierarchy-design-and-managem/",
			"excerpts": [
			  "One of the primary challenges in embedded system design is designing the memory hierarchy and restructuring the application to take advantage of it.Read more"
			]
		  },
		  {
			"title": "(PDF) Dynamic management of scratch-pad memory space",
			"url": "https://www.academia.edu/78529842/Dynamic_management_of_scratch_pad_memory_space",
			"excerpts": [
			  "We propose a compiler-controlled dynamic on-chip scratch-pad memory (SPM) management framework that uses both loop and data transformations. Experimental ..."
			]
		  },
		  {
			"title": "Scratchpad Memory : A Design Alternative for Cache On-chip ...",
			"url": "https://robertdick.org/aeos/reading/banakar-scratchpad.pdf",
			"excerpts": [
			  "In this paper we address the problem of on-chip mem- ory selection for computationally intensive applications, by proposing scratch pad memory as an ...Read more"
			]
		  },
		  {
			"title": "Qualcomm® Hexagon™ DSP SDK",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-88500-4/153_Hexagon_SDK.html",
			"excerpts": [
			  "The Qualcomm Hexagon DSP SDK provides a set of tools, software, and documentation that assist in running applications on Qualcomm devices and leveraging the ...Read more"
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques for low-power and -energy",
			"url": "https://www.researchgate.net/publication/29649252_A_Survey_of_Scratch-Pad_Memory_Management_Techniques_for_low-power_and_-energy",
			"excerpts": [
			  "In the dynamic approach, program blocks can be transferred between SPM and the offchip memory during the application execution [12] . In addition to performance and energy consumption, SPM as an SRAM-based on-chip memory plays a major role in the reliability of embedded systems.",
			  "The main idea underlying the proposed mechanism includes four stages: 1) to use parity codes for error detection in the SPM, 2) to keep an address matching table in the main memory to store the address of the copy of SPM blocks in the main memory, in the case of dynamic SPM, 3) to allocate a specific segment of the main memory as an SPM backup, in the case of static SPM, and 4) to recover from errors using an interrupt service routine (ISR).",
			  "The memory subsystem accounts for a significant portion of the aggregate energy budget of contemporary embedded systems. Moreover, there exists a large potential for optimizing the energy consumption of the memory subsystem. Consequently, novel memories as well as novel algorithms for their efficient utilization are being designed.",
			  "An Optimal Memory Allocation Scheme for Scratch-Pad Based Embedded Systems",
			  "Assigning program and data objects to scratchpad for energy reduction",
			  "Dynamic overlay of scratchpad memory for energy minimization",
			  "Data partitioning for maximal scratchpad usage",
			  "The set of selected program parts are determined with an optimal algorithm using integer linear programming.",
			  "Scratchpad memory: design alternative for cache on-chip memory in embedded systems.",
			  "Compiler-decided dynamic memory allocation for scratch-pad based embedded systems",
			  "A minimal increase in code size is also reported.",
			  "ed which analyzes the application, partitions an array variable whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Res"
			]
		  },
		  {
			"title": "Scratchpad Memory - an overview | ScienceDirect Topics",
			"url": "https://www.sciencedirect.com/topics/computer-science/scratchpad-memory",
			"excerpts": [
			  "tatic-allocation) methods assign data to SPM at compile time but cannot adapt to changing program behavior or reallocate memory at runtime.",
			  "ynamic-allocation) methods, implemented via compiler-inserted code, allow variables to move between SPM and off-chip memory during execution, supporting more flexible memory usage."
			]
		  },
		  {
			"title": "Compiler-directed scratchpad memory management via graph coloring | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1582710.1582711",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
			  "Section Title: Compiler-directed scratchpad memory management via graph coloring > Abstract",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM.",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file.",
			  "Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications.",
			  "In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator."
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques ...",
			"url": "https://inria.hal.science/inria-00170210v1/document",
			"excerpts": [
			  "e SPM Manage-\nment Policy at Runtime can be Static which means that data could be over-\nwritten but not moved to another place. With Moves some existing variables\nin SPM could be evicted to make space for incoming ones. In this way, data is\nnever lost. On the one hand, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM",
			  "type. These approaches try to answer the question of which program variables\nshould be allocated to which memory or memory bank. In these techniques,\nbecause of the reduced size of SRAM, the lesser-used variables are first allocated\nto slower memory banks (DRAM), while the most frequently used variables are\nkept in fast memory (SRAM) as much as possible. These methods use profile\ndata to gather access frequency information in order to place frequently used\ndata in fast memory, and other data in slower memory. To do so, most authors\nmodel the problem as a 0/1 integer linear programming (ILP) problem and then\nuse an available IP solver to solve it.",
			  "These methods use profile\ndata to gather access frequency information in order to place frequently used\ndata in fast memory, and other data in slower memory.",
			  "nd, Fixed Moves always place data at the same offset\nin the SPM or DRAM. On the other hand, with Free Moves the SPM allocation\ncan be dynamically adapted at runtime by placing most frequently used data at\nany free location in the SPM or DRAM.\n",
			  "[Steinke et al. , 2002b] S. Steinke, L. Wehmeyer, B. Lee, and P. Marwedel. Assigning\nprogram and data objects to scratchpad for energy reduction. In DATE , page 409.\nIEEE Computer Society, 2002.",
			  "\n[Steinke et al. , 2002a] S. Steinke, N. Grunwald, L. Wehmeyer, R. Banakar, M. Balakr-\nishnan, and P. Marwedel. Reducing Energy Consumption by Dynamic Copying of\nInstructions onto Onchip Memory. In ISSS , 2002.\n",
			  "\n[Udayakumaran and Barua, 2003] S. Udayakumaran and R. Barua. Compiler-decided\ndynamic memory allocation for scratch-pad based embedded systems. In CASES ,\npages 276–286. ACM Press, 2003.\n",
			  "[Udayakumaran and Barua, 2006] S. Udayakumaran and R. Barua.\nAn integrated\nscratch-pad allocator for affine and non-affine code. In DATE , pages 925–930, 2006.\n[",
			  "[Verma et al. , 2003] M. Verma, S. Steinke, and P. Marwedel. Data Partitioning for\nMaximal Scratchpad Usage. In ASPDAC , 2003.\n[",
			  "\n[Verma et al. , 2004] M. Verma, L. Wehmeyer, and P. Marwedel. Dynamic Overlay of\nScratchpad Memory for Energy Minimization. In CODES+ISSS , 2004.\n"
			]
		  },
		  {
			"title": "Hexagon V79 HVX Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-61/memory.html",
			"excerpts": [
			  "HVX supports a local memory called vector tightly coupled memory\n(VTCM) for scratch buffers and scatter/gather operations.",
			  "The size of\nthe memory is implementation-defined.",
			  "VTCM needs normal virtual to physical\ntranslation just like other memory.",
			  "This memory has higher\nperformance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer.",
			  "It\nserves as the input or output of the scatter/gather instructions.",
			  "The\nfollowing are advantages of using VTCM as the intermediate buffer:",
			  "Guarantees no eviction (vs. L2 if the set is full)",
			  "Faster than L2$ (does not have the overhead of cache management, like\nassociation)",
			  "Reduces L2$ pressure",
			  "Lower power than L2$",
			  "Supports continuous read and write for every packet without\ncontention",
			  "In addition to HVX VMEM access, normal Hexagon memory access\ninstructions can access this memory."
			]
		  },
		  {
			"title": "A Scratchpad Memory Allocation Scheme for Dataflow Models",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-104.pdf",
			"excerpts": [
			  "In [3], Banakar et al show that on average a scratchpad memory has 34% smaller area and 40% lower power consumption than a cache of the same ...Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratch Pad Memory Optimization for ...",
			"url": "https://users.eecs.northwestern.edu/~choudhar/Publications/CompilerDirectedScratchPadMemoryOptimEmbeddedMultiprocessors.pdf",
			"excerpts": [
			  "Abstract—This paper presents a compiler strategy to optimize data accesses in regular array-intensive applications running on embedded multiprocessor ..."
			]
		  },
		  {
			"title": "Adaptive Scratch Pad Memory Management for Dynamic ...",
			"url": "https://www.engr.colostate.edu/~sudeep/wp-content/uploads/j10.pdf",
			"excerpts": [
			  "Guided by compiler analysis for generating scratch pad data layouts and hardware components for tracking dynamic memory accesses, the scratch pad data."
			]
		  },
		  {
			"title": "TMS320C64x EDMA Architecture",
			"url": "https://www.ti.com/lit/pdf/spra994",
			"excerpts": [
			  "• The CPU accesses memory in the L2 SRAM space. This access goes directly to L2 within the cache memory system; refer to TMS320C64x DSP Two Level Internal ...Read more"
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch- ...",
			"url": "https://scispace.com/pdf/compiler-decided-dynamic-memory-allocation-for-scratch-pad-1eaxs54tgy.pdf",
			"excerpts": [
			  "In static management of scratch-pad memory, the contents of the scratch-pad\nare decided at compile time itself. A simple way of doing this is based on programmer\nhints. Thus, the programmer provides annotations that tell if a variable is to be put\ninto the scratch-pad address space or not. In such a scheme, it is the programmer\nresponsibility to ensure that a particular variable fits completely inside the scratch-\npad address space. To make this process efficient, the selection strategy can be based\non compiler analysis of the application. Examples of such static allocation strategies\ninclude [12,13,73,77]. The advantage of a static scheme is that it does not involve\nany additional per-access overhead like that of software caching as no translation",
			  "Software caching [36,59] represents a dynamic allocation strategy. This class\nof methods emulate the behavior of a hardware cache in software. In particular, a\ntag consisting of the high-order bits of the address is stored for each cache line in\nsoftware. Before each load or a store, additional instructions are compiler-inserted\nto mask out the high-order bits of the address, access the tag, compare the tag with\nthe high-order bits and then branch conditionally to hit or miss code. On a hit, in\nthe hit code, using a mapping table the address is mapped to a new address in the\nscratch-pad where the data resides. Otherwise it is a cache miss and new data is\nbrought into the scratch-pad. At that point, the index tables are updated to reflect\nthe presence of this new data in the scratch-pad. Being a runtime strategy like\ncaching, the strategy can adapt to runtime conditions. This makes it particularly\nsuited for workloads whose runtime conditions continuously vary.",
			  "We propose a dynamic allocation methodology for global and stack data and\nprogram code that (i) accounts for changing program requirements at runtime (ii)\nhas no software-caching tags (iii) requires no run-time checks (iv) has extremely low\noverheads, and (v) yields 100% predictable memory access times.",
			  "data that is about to be accessed frequently is copied into the scratch-pad using\ncompiler-inserted code at fixed and infrequent points in the program. Earlier data\nis evicted if necessary.",
			  "When compared to an existing static allocation scheme,\nresults show that our scheme reduces runtime by up to 39.8% and energy by up to\n31.3% on average for our benchmarks, depending on the SRAM size used.",
			  "The actual\ngain depends on the SRAM size, but our results show that close to the maximum\nbenefit in run-time and energy is achieved for a substantial range of small SRAM\nsizes commonly found in embedded systems.",
			  "Our comparison with a direct mapped\ncache shows that our method performs roughly as well as a cached architecture in\nruntime and energy while delivering better real-time benefits.",
			  "Compiler-Decided Dynamic Memory Allocation for Scratch-Pad\nBased Embedded Systems"
			]
		  },
		  {
			"title": "Automated Scratchpad Mapping and Allocation for Embedded ...",
			"url": "https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=4035&context=etd",
			"excerpts": [
			  "Direct memory access (DMA) is a feature of modern computers that allows certain\nhardware subsystems within the computer to access system memory independently\nof the central processing unit (CPU).",
			  "Without DMA, when the CPU is using pro-\ngrammed input/output, it is typically fully occupied for the entire duration of the\nread or write operation, and is thus unavailable to perform other work.",
			  "With DMA,\nthe CPU initiates the transfer, does other operations while the transfer is in progress,\nand receives an interrupt from the DMA controller when the operation is done.",
			  "If the per-core on-chip memory or the share memory (MSMC) are programmat-\nically configured to behave partially or fully as a scratchpad memory (SPM) space,\nintegrated DMA controller allows data to be exchanged between on- and off-chip\nmemories in parallel to operations being performed on the DSP.",
			  "The Enhanced Di-\nrect Memory Access (EDMA) controller can also be programmed to perform complex\n3-dimensional data access patterns on both the source and destination memories to\nallow highly flexible transaction",
			  "With EDMA, data accessed in a predictable, regular access pattern can be con-\ncurrently prefetched using EDMA to exchange data between scratchpad and DRAM.",
			  "Data accessed irregularly in a data-dependent pattern can be cached in a separate\nregion of scratchpad to take advantage of locality."
			]
		  },
		  {
			"title": "ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing. | Request PDF",
			"url": "https://www.researchgate.net/publication/221569161_ROS-DMA_A_DMA_Double_Buffering_Method_for_Embedded_Image_Processing_with_Resource_Optimized_Slicing",
			"excerpts": [
			  "This paper presents our work on PfeLibÃ¢Â€Â”a high performance software library for image processing and computer vision algorithms for an embedded system. The main target platform for PfeLib is the TMS320C6000 series of digital signal processors (DSPs) from Texas instrume",
			  "ROS-DMA can yield substantially better performance compared to using L2 cache.",
			  "Section Title: ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing."
			]
		  },
		  {
			"title": "5.1. Software Pipelining — TI C7000 C/C++ Optimization Guide",
			"url": "https://software-dl.ti.com/codegen/docs/c7000/optimization_guide/5_understand_opt/compopt_under_pipelining.html",
			"excerpts": [
			  "the concept of software pipelining, which\nis a compiler optimization that is essential for achieving\nhigh-performance on a VLIW architecture like the C7000 DSP.\nSection Title: 5.1. Software Pipelining [¶]( \"Permalink to this headline\") > 5.1.1. Software Pipelining Motivation [¶]( \"Permalink to this headline\")\nContent:\nAs described in [C7000 Digital Signal Processor CPU Architecture](../2_architecture/compopt_arch_overview.html) , the C7000 DSP has a\nvery-long instruction word (VLIW) architecture that contains 13\nfully-pipelined functional units on which instructions can execute.\nUp to 13 instructions can start on any cycle. Keeping these\nfunctional units busy executing instructions is key to achieving performance\non the C7000 DSP core.\nKeeping all of these functional units busy can be challenging. *Software pipelining* is a technique used on inner loops by a compiler to attempt to keep the\nfunctional units as busy as possible. The C7000 compiler performs software\npipelining automatically on inner loops, when appropriate compiler options\nare used.\nWithout software pipelining, inner loops will likely suffer from poor performance\non a VLIW architecture. This is because without software pipelining, only a\nsingle iteration of the loop is executed at a time.\n[Figure 5.1]() shows a conceptual representation of an inner loop that is not software pipelined. The loop in this example has five equal sized sections, A through E. The arrow represents repeated execution of the loop body (via a conditional branch instruction at the end of s"
			]
		  },
		  {
			"title": "(PDF) Modulo scheduling for the TMS320C6x VLIW DSP architecture",
			"url": "https://www.academia.edu/130276962/Modulo_scheduling_for_the_TMS320C6x_VLIW_DSP_architecture",
			"excerpts": [
			  "We discuss how we adapted modulo scheduling to implement software pipelining for the C6x.",
			  "ments ThIS32OC6x (C6x) is a Very Long Instruction Word (VLIW) DSP architecture capable of issuing eight operations in parallel. ",
			  "In this paper, we present the results of implementing a software pipelining algorithm for the C6x."
			]
		  },
		  {
			"title": "Architecture Support for Bitslicing - Cryptology ePrint Archive",
			"url": "https://eprint.iacr.org/2021/1236.pdf",
			"excerpts": [
			  "Bitslicing, first introduced by Biham [1], is a technique orig-\ninally proposed to increase the throughput of a program by\nrunning multiple instances of a code in paralle",
			  "In bitslicing,\nall the variables are transposed so that each register contains\nonly one bit of the variable.",
			  "A fully-bitsliced program needs to be flattened (no\nbranches).",
			  "itslicing provides a proper base to combine our masking\nand redundant computation schemes as described in the\nnext section.\n",
			  "oday, many digital circuits have a System-on-Chip\n(SoC) architecture. In such systems, hardware support for\nbitslicing can be in the form of instruction extension in the\nprocessor implementation or it can be a hardware module\naccessible by the processor through a",
			  "Our goal in this\nwork is to integrate both of these types of hardware support\nfor bitslicing into an SoC.",
			  "The proposed 14 new custom instructions use 1.5 *×* fewer registers\ncompared to the equivalent functionality expressed using RISC-V instruction",
			  "e present architecture support for bitslicing in a System-on-Chip. Our hardware extensions are of two types; internal to the\nprocessor core, in the form of custom instructions, and external to the processor, in the form of direct memory access module with\nsupport for data transposition. ",
			  "bitslicing provides a proper base to combine our masking\nand redundant computation schemes as described in the\nnext section."
			]
		  },
		  {
			"title": "High-throughput block cipher implementations with SIMD - ScienceDirect",
			"url": "https://www.sciencedirect.com/science/article/abs/pii/S2214212622001788",
			"excerpts": [
			  "Bitslicing is one of the most efficient software implementation techniques for block ciphers while providing resistance to cache-timing attacks.",
			  "It was first proposed by Biham to improve the software performance of DES (Data Encryption Standard) ",
			  "By converting the encryption/decryption process into an equivalent logical representation, the entire cipher algorithm can be implemented as a series of atomic Boolean operations.",
			  " Then Matsui and Nakajima showed that a Bitsliced implementation of AES combined with SIMD technology could be faster than any other implementations reported before",
			  "most efficient implementation was presented by Käsper and Schwabe in 2009 [11], with a newly designed state which allows processing eight blocks in parallel on Intel Core platforms.",
			  "Intel introduced AES-NI (AES New Instructions) instruction set in 2008 to accelerate the encryption and decryption of AES.",
			  "Thus, multiple blocks can be encrypted simultaneously with a cost equivalent to that of encrypting just a single block.",
			  "This multi-block parallelizable feature allows to fully utilize the benefits offered by SIMD technology.",
			  "\nChester et al. first presented a Bitsliced implementation of AES in 2006",
			  "In this paper, we present an implementation of AES S-box only using 76 logical operations with SIMD instruction sets.",
			  "Note that this implementation is synthesized with the *vpternlogd* instruction, which to the best of our knowledge is the first time that the *vpternlogd* instruction has been introduced to accelerate implementations of block ciphers.",
			  "We also present an implementation of AES S-box with 113 standard logical operations, which reduces 4 logical operations compared to the previous best results.",
			  "we show that the bit-wise sliced state representation could be more efficient than before by introducing the linear layer optimization based on the matrix decomposition.",
			  "As a result, the throughput of our AES implementation is significantly improved and can exceed that of AES-NI on the same platform.",
			  "Bitsliced technique operates the data in a bit-wise level.",
			  "To collect equivalent bits from multiple blocks into a single register, a rearrangement of state bits needs to be performed both before and after the encryption.",
			  "The implementation of a block cipher with a block size ofbits can encrypt 256 (512) blocks of data simultaneously, using256-bit YMM registers (512-bit ZMM registers). ",
			  "All the source code of this paper is available at https://github.com/xurq5953/High-Throughput-Block-Cipher-Implementations .",
			  "The remaining of this paper is structured as follows: In Section 2, we review the relevant backgrounds. Then we take AES as an example to illustrate implementations of the S-box, the linear layer and the data schedule in Sections 3, 4 and 5 respectively. In Section 6, we compare the performance of the optimized AES implementations with previous results. Section 7 presents an application to SM4 to illustrate the generality of our techniques.",
			  "In this paper, we derive the most compact implementations of AES S-box for Bitsliced implementations.",
			  "Through our search strategies the number of the logical operations can be reduced to 64.9% of the previous best result.",
			  "Then we show that our implementations of the linear layer is also more efficient than before. Meanwhile, these techniques can be applied to multiple ciphers and the implementations enjoy the resistance to cache-timing attack."
			]
		  },
		  {
			"title": "A software pipelining based VLIW architecture and optimizing compiler | Proceedings of the 23rd annual workshop and symposium on Microprogramming and microarchitecture",
			"url": "https://dl.acm.org/doi/pdf/10.5555/255237.255244",
			"excerpts": [
			  "This paper introduces a VLIW architecture and its optimizing compiler which are now under development. Based on the URPR software pipelining approach, the architecture integrates nine PEs with the same structure on a single-chip. In addition, a pipeline register file is used to reduce the inter-body dependent distance to enhance the overlapping of the adjacent loop iterations, furthermore to shorten the length of the optimized loop body. The pipeline register file also increases the bandwidth between PEs. The optimizing compiler is also based on the URPR software pipelining approach. It uses a two-level software pipelining method to implement phase-coupled resource allocation and code optimization, and obtains good time and space optimal results. A compilation example of an FFT innermost loop is discussed. The simulation results indicate that the architecture could reach high performance with the aid of the optimizing compiler."
			]
		  },
		  {
			"title": "implementation - How is bitslicing faster? - Cryptography Stack Exchange",
			"url": "https://crypto.stackexchange.com/questions/30140/how-is-bitslicing-faster",
			"excerpts": [
			  "The basic idea of bitslicing, or SIMD within a register, involves two parts: expressing the cipher in terms of single-bit logical operations ...Read more"
			]
		  },
		  {
			"title": "Code Compaction and Parallelization for VLIW/DSP Chip ...",
			"url": "https://commit.csail.mit.edu/papers/1999/Petrov-MEng.pdf",
			"excerpts": [
			  "The linear program as defined above does not capture data dependencies or delay slots and thus is often too low and inacurate to be useful. In addition, the ...Read more"
			]
		  },
		  {
			"title": "Distilbart",
			"url": "https://www.kaggle.com/code/aryadhorajiya/distilbart",
			"excerpts": [
			  "Use of a hash function to index a hash table is called hashing or ... reduction from Merkles Hash Tree signature to the identity of the hash function.Read more"
			]
		  },
		  {
			"title": "Fundamental Data Structures | PDF",
			"url": "https://www.scribd.com/document/199819816/Fundamental-Data-Structures",
			"excerpts": [
			  "Associative array Association list Hash table Linear probing Quadratic probing Double hashing Cuckoo hashing Hopscotch hashing Hash function Perfect hash ...Read more"
			]
		  },
		  {
			"title": "Usuba | Proceedings of the 2018 4th Workshop on Programming Models for SIMD/Vector Processing",
			"url": "https://dl.acm.org/doi/10.1145/3178433.3178437",
			"excerpts": [
			  "Bitslicing is a programming technique commonly used in cryptography that consists in implementing a combinational circuit in software.",
			  "It results in a massively parallel program immune to cache-timing attacks by design.",
			  "This paper introduces Usuba, a synchronous dataflow language producing bitsliced C code.",
			  "Usuba is both a domain-specific language -- providing syntactic support for the implementation of cryptographic algorithms -- as well as a domain-specific compiler -- taking advantage of well-defined semantics invariants to perform various optimizations before handing the generated code to an (optimizing) C compiler.",
			  "On the Data Encryption Standard (DES) algorithm, we show that Usuba outperforms a reference, hand-tuned implementation by 15% (using Intel's 64 bits general-purpose registers and depending on the underlying C compiler) whilst our implementation also transparently supports modern SIMD extensions (SSE, AVX, AVX-512), other architectures (ARM Neon, IBM Altivec) as well as multicore processors through an OpenMP backend."
			]
		  },
		  {
			"title": "This thesis has been submitted in fulfilment of the ... - ERA",
			"url": "https://era.ed.ac.uk/bitstream/handle/1842/39718/Vasiladiotis2022.pdf?sequence=1&isAllowed=y",
			"excerpts": [
			  "Our *key insight* is that some of the detected dependences\nare not, in fact, fatal to parallelization, and therefore cause some loops to be needlessly\ndiscarded. Instead, *liveness-based commutativity* analysis focuses on the more crucial\nissue of whether any such dependences have a detectable effect on the eventual result.",
			  "The objective of automatic parallelization is to automate the task of transforming a\nsequential application into an equivalent parallel version.",
			  "In a nutshell, commutativity analysis detects\ncode regions whose order of execution can be exchanged without altering the program\noutcome.",
			  "Figure 4.2: A simple loop performing a reduction operation over an array (left) and its\ncorresponding DDG (right).",
			  "Reductions are closely related to scans since the latter can be expressed in terms\nof the former, by applying the combiner operator successively over all ordered subsets\nof the input (i.e., over one element, over two elements, and so on) and keeping the\nintermediate results.",
			  "he associativity of the reduction operator\nenables different execution orderings. ",
			  "n of Symbolic Execution*\n63\nthe same after we permute the loop’s iteration order. The key idea is that if the live-outs\nremain unaffected, in other words, the loop is found to be commutative, then we can\n ..",
			  "liveness-based commutativity* as implemented in DCA, described in the\nprevious chapter, is not able to detect the loop in Figure 6.1 as commutative"
			]
		  },
		  {
			"title": "A Pattern Language for writing Efficient Kernels on GPU ...",
			"url": "https://ghostshark.it/cuda/Barbieri-phdthesis-format-b5.pdf",
			"excerpts": [
			  "The parallel reduction is also interesting because it demonstrates the principle\nthat it is usually better to redesign your algorithm rather than to add synchronization\nprimitives to an existing algorithm. In this section, we present how this is achieved\non a SIMT architecture.",
			  "Since *⊕* is associative, we can reformulate the problem in a way that is schema-\ntized in Figure 7.3 using the *PRAM* model. The resulting dependecy graph could\nbe considered as a binary tree, in which the leaves are operations that read operands\ndirectly from the input vector and the result of the root node is the algorithm’s out-\n7.2 Reduce pattern",
			  "6\nFig. 7.3 Binary tree reduction",
			  "Forces**\n• the solution should take care that *⊕* is *associative* or both *associative* and\n*commutative* ;\n• the elements of *A* could be loaded from memory or generated on the device\nfrom computation",
			  "The number of operations, in this case, is ∑\nlog 2 *N*\n*i* = 1\n*N*\n2 *i* ."
			]
		  },
		  {
			"title": "upgrade of ac to ada reasearch compiler",
			"url": "https://dspace.uowm.gr/xmlui/bitstream/handle/123456789/3635/%20%20%20%20%20%20%20%20,%20%20%20%20%20%20%20%20%20%20%20.pdf?sequence=1",
			"excerpts": [
			  "2.3.2.5.1**\n**Full loop unrolling**\nIt is possible to fully unroll a loop. If an index is used, it can be replaced by the\nactual constant values it will have for every instruction. The loop itsel"
			]
		  },
		  {
			"title": "Instruction Scheduling for Clustered VLIW DSPs",
			"url": "http://www.cs.cmu.edu/afs/cs/academic/class/15745-s05/www/papers/leupers00instruction.pdf",
			"excerpts": [
			  "Recent digital signal processors (DSPs) show a homo- geneous VLIW-like data path architecture, which allows C compilers to generate efficient code.Read more",
			  "ues.\nThe contributionof this paper is an *integrated instruction*\n*scheduling and partitioningtechnique* specifically designed\nfor clustered VLIW data paths. As a demonstrator we will\nuse the TI C6201 VLIW DSP. T",
			  "Instruction scheduling for processors with a clustered\ndata path is more difficult than for an orthogonal VLIW ar-\nchitecture. In case of the TI C6201, for instance, the limited\ncommunication capabilities between clusters A and B have\nto be taken into account. Computations executed on A and\nB in general need to exchange values, but the transport of\nvalues between A and B must take place via a *restrictive*\n*interconnection network* , which allows the transfer of only\na single value from A to B (and vice versa) within each in-\nstruction cyc",
			  "e, the TI C6201 data\npath is divided into two identical halves, called A and B.\nBoth A and B have their local RF, and there is full orthog-\n2 Orthogonality means that each FU has random access to each register.\no",
			  "ters. The data path consists of two symmetric clusters A\nand B. Each cluster has a local 16\n�\n16 bit RF and 4 FUs\n(called L, S, M, and D) working in parallel. Each FU type is\ncapable of executing a certain subset of instructions. T",
			  "The contributionof this paper is an *integrated instruction*\n*scheduling and partitioningtechnique* specifically designed\nfor clustered VLIW data paths. As a demonstrator we will\nuse the TI C6201 VLIW DSP. T",
			  "n fact, the TI C6201 is a very interesting ex-\nample for this purpose, because its capability of using cros"
			]
		  },
		  {
			"title": "Minimization of Bitsliced Representation of 4×4 S-Boxes ...",
			"url": "https://ceur-ws.org/Vol-3421/paper2.pdf",
			"excerpts": [
			  "Bit-slicing [3] is one of the promising approaches that provide a high-performance constant-time implementation of a CA with immunity to time ...Read more"
			]
		  },
		  {
			"title": "ds.data structures - Associative hash mixing - Theoretical Computer Science Stack Exchange",
			"url": "https://cstheory.stackexchange.com/questions/1684/associative-hash-mixing",
			"excerpts": [
			  "For the hash caching to work here, the hash mixing function must be associative in order to respect the degrees of freedom a tree has in ...Read more"
			]
		  },
		  {
			"title": "Implementation of Bitsliced AES Encryption on CUDA-Enabled GPU | Request PDF",
			"url": "https://www.researchgate.net/publication/318678631_Implementation_of_Bitsliced_AES_Encryption_on_CUDA-Enabled_GPU",
			"excerpts": [
			  "Nishikawa et al. 6 achieved 605.9 Gbps throughput for AES-128 on a Tesla P100 using an optimized bitsliced implementation.",
			  "Our implementation achieved 2.6% (ECB) and 9% (CTR) faster than the state-of-the-art table-based implementation on a RTX3080 GPU."
			]
		  },
		  {
			"title": "Bitsliced High-Performance AES-ECB on GPUs",
			"url": "https://cetinkayakoc.net/docs/b09.pdf",
			"excerpts": [
			  "By using a bitslicing\napproach, our algorithm is also resistant to timing side channels. All operations\nthat involve key or data use bitwise operations whose execution time does not\ndepend on the values of the data.",
			  "the bitsliced implementations\nare also inherently immune to the cache-timing attacks, as discussed in [ 1 , 2 , 26 ].",
			  "This step requires only xor operations. Our GPU version loads the 10 round keys\ninto shared memory to improve performance when processing multiple blocks.",
			  "By loading the round keys into shared memory, we avoid having to read the\nround keys from GPU global memory repeatedly.",
			  "The 2 factor is explained in the SubBytes section.",
			  "Resistance to Timing-Attack**\nThe CPU-based algorithm of Kasper and Schwabe is resistant to timing side\nchannels due to the use of constant time operations [ 8 ]. By using a bitslicing\napproach, our algorithm is also resistant to timing side channel"
			]
		  },
		  {
			"title": "GPU accelerated 3DES encryption",
			"url": "https://open.metu.edu.tr/bitstream/handle/11511/91393/%C4%B1ndex.pdf",
			"excerpts": [
			  "Bitslicing can be performed in two different ways on GPUs:\nthread-level parallelization and variable-level parallelization. ",
			  "Inthevariable-levelbitslicingmethod,eachbitoftheblockisstoredinadifferentvariable.Ifoneuses *n* -bitvariables,inthisapproach *n* plaintext\nblocks can be encrypted in parallel."
			]
		  },
		  {
			"title": "HashFusion – a method for combining cryptographic hash ...",
			"url": "https://www.labs.hpe.com/techreports/2017/HPE-2017-08.pdf",
			"excerpts": [
			  "The Merkle hash tree provides a way of hashing large number of data blocks by hashing adjacent elements\ntogether in a fixed binary tree-structure organization (see [1,2]).",
			  "The primary advantage over standard hash\nchaining is that the primitive data blocks can be independently hashed and do not thereafter need to be retained.",
			  "The construction proceeds by building full-size binary subtrees as soon as they arrive, and then finalizing the tree\nwhen a \"no-more-input\" signal is received, typically by \"promoting\" any rightmost subtrees as high as possible.",
			  "This protocol avoids needing to know the total number of blocks, but it clearly\nneeds to retain incomplete subtrees.",
			  "the Merkle hash-tree technique\nimposes strict constraints on how the hashes of data blocks are combined.",
			  "By using HashFusion and its associative property, we can implement a more flexible tree structure than the\nMerkle hash tree which is efficient for both the update of existing hashes and arbitrary insertion/deletion of\nentries.",
			  "The basic idea is to exploit the way that HashFusion’s associativity allows data to be *regrouped* – this means that\neach of the following different shaped structures compute identical values, V, at root:",
			  "However, where these topologies do differ is in average update times – the fastest update time is for panel 1\nabove (the most balanced), the slowest update times for panels 3 and 4 (the most skewed), and panel 2 is\nbetween these.",
			  "Now, the key observation is to realise that we need the structure to be near balanced for an efficient update (i.e.\nall hashes are (almost) at the same distance from the root) – but because HashFusion is associative, we can\n*tolerate* different shapes of tree."
			]
		  },
		  {
			"title": "Real-Time Parallel Hashing on the GPU",
			"url": "https://www.cs.ucdavis.edu/~amenta/pubs/ParallelHash.pdf",
			"excerpts": [
			  "Our major contribution in this work is a parallel, GPU-friendly hash\ntable design that allows construction as well as lookup at interactive\nrates.",
			  "he classical FKS\nperfect hashing scheme [Fredman et al. 1984], which is simple and\nfast but not space-efficient. The second is the recently developed\n*cuckoo hashing* method [Pagh and Rodler 2001]. ",
			  "Hashing is, of course, not the only way to solve the problem of ran-\ndom access to sparse data. Another possible solution is to represent\nthe sparse elements as a dense arra",
			  "Hash tables with real-time construction and access times are useful\nin a variety of applications that require sparse data.",
			  "exercise our\nhashing scheme in two graphics applications: spatial hashing for in-\ntersection detection of moving data, and geometric hashing for fea-\nture matching",
			  "e demonstrate that a spatial hash on a virtual 128 3\ngrid can be constructed at each frame at around 27 fps. Geometric\nhashing is a GP-GPU application requiring intensive random access\nto a table of pairs of feature points."
			]
		  },
		  {
			"title": "Maximizing Performance with Massively Parallel Hash Maps on GPUs | NVIDIA Technical Blog",
			"url": "https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/",
			"excerpts": [
			  "\nHash maps are *associative* containers, meaning they store *key* – *value* pairs where a *key* is mapped to an associated *value* , enabling the retrieval of values by looking up their keys. For example, you could use a hash map to implement a phone book by using an individual’s name as the key and their phone number as the associated value.",
			  "Hash maps are different from other associative containers in that operations, such as insertion or retrieval, have a constant cost on average.",
			  " [`std::map`](https://en.cppreference.com/w/cpp/container/map) in the C++ Standard Template Library is not a hash table, but is typically implemented as a binary search tree.",
			  " [`std::unordered_map`](https://en.cppreference.com/w/cpp/container/unordered_map) is more akin to the kind of hash tables relevant to this discussion.",
			  "For the purposes of this post, there is no difference between a hash table and a hash map. Both terms will be used interchangeably throughout."
			]
		  },
		  {
			"title": "Developing High-Performance Applications for HP-UX on IA-64",
			"url": "http://www.3kranger.com/Interact/Features/0101devIA64.shtm",
			"excerpts": [
			  "The IA-64 architecture provides special branches, along with rotating registers (including predicates), which allow the compiler to generate software pipelined ...Read more"
			]
		  },
		  {
			"title": "Intel® 64 and IA-32 Architectures Optimization Reference Manual Volume 1",
			"url": "https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual-volume-1.html",
			"excerpts": [
			  "Section Title: Intel® 64 and IA-32 Architectures Optimization Reference Manual Volume 1 > Description\nContent:\nThe Intel® 64 and IA-32 Architectures Optimization Reference Manual describes how to optimize software to take advantage of the performance characteristics of IA-32 and Intel 64 architecture processors."
			]
		  },
		  {
			"title": "Hexagon DSP SDK Collection: Landing Page",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-77512-1/hexagon-dsp-sdk-introduction-to-npu.html",
			"excerpts": [
			  "The ISA features a VLIW-style static grouping of instructions.",
			  "Multi-threading and\nVLIW are complementary technologies.",
			  "The Hexagon ISA is\na hybrid DSP/CPU that features a 4-issue VLIW comprised of dual load/store slots and\ndual 64-bit vector execution slots.",
			  "All instructions operate on a shared 32-entry\nper-thread register file.",
			  "Vector operations use register pairs from the general\nregister file.",
			  "The ISA features a rich set of DSP arithmetic support including\n16-bit and 32-bit fractional and complex data types, 32-bit floating-point, and full\n64-bit integer arithmetic support."
			]
		  },
		  {
			"title": "355308-Optimization-Reference-Manual- ...",
			"url": "https://cdrdv2-public.intel.com/814201/355308-Optimization-Reference-Manual-049-Changes-Doc.pdf",
			"excerpts": [
			  "The design guidelines discussed in this manual for developing high-performance software generally apply\nto current and future IA-32 and Intel 64 processors."
			]
		  },
		  {
			"title": "Hexagon V79 Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-60",
			"excerpts": [
			  "Introduction. The Qualcomm Hexagon™ processor is a general-purpose digital signal processor designed for high performance and low power.Read more"
			]
		  },
		  {
			"title": "Hexagon V79 HVX Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/doc/80-N2040-61/topic",
			"excerpts": [
			  "Introduction. This document describes the Qualcomm® Hexagon™ Vector eXtensions (HVX) instruction set architecture.Read more"
			]
		  },
		  {
			"title": "Utilizing the C64x+ Compiler to Optimize Performance and ...",
			"url": "https://e2e.ti.com/cfs-file/__key/communityserver-discussions-components-files/791/C64plus_5F00_cgt_5F00_overview.pdf",
			"excerpts": [
			  "◇ Use/non-use of CALLP instructions. ◇ In addition to traditional optimizations like adjusting unrolling, software pipelining, etc. Page 24. How does the ...Read more"
			]
		  },
		  {
			"title": "Hexagon V79 Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-60/program-flow.html",
			"excerpts": [
			  "The Hexagon processor includes hardware loop instructions that perform loop branches with zero overhead.Read more"
			]
		  },
		  {
			"title": "Hexagon V5x Programmer's Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-8_REV_H_Hexagon_V5X_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Software pipelined* loops are common for VLIW architectures such as the Hexagon\nprocessor. They offer increased code performance in loops by overlapping multiple loop\niterations",
			  " software pipeline has three sections:\n■\nA *prologue* in which the loop is primed\n■\nA *kernel* (or steady-state) portion\n■\nAn *epilogue* which drains t",
			  "ble 7-2 the kernel section of the pipelined loop performs three iterations of the loop\nin parallel:\n■\nThe load for iteration N+2\n■\nThe multiply for iteration N+1\n■\nThe store for iteration ",
			  "he Hexagon processor has only two load units, a packet\nwith three load instructions is invalid. The behavior of such a packet is undefined. "
			]
		  },
		  {
			"title": "Hexagon V68 Programmer s Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-46_REV_B_Hexagon_V68_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Packets have a single PC address, which is the address of the start of the packet. Branches\ncannot be performed into the middle of a packet.",
			  "Architecturally, packets execute to completion – including updating all registers and\nmemory – before the next packet begins. As a result, application programs are not exposed\nto any pipeline artifacts.",
			  "esource constraints**\nA packet cannot use more hardware resources than are physically available on the\nprocessor. For instance, because the Hexagon processor has only two load units, a packe",
			  "The Hexagon processor includes the instruction dcfetch . This instruction queries the L1\ndata cache based on the address specified in the instruction:",
			  "If the address is present in the cache, no action is taken.",
			  "If the cache line for the address is missing, the processor attempts to fill the cache\nline from the next level of memory. The thread does not stall, but rather continues\nexecuting while the cache line fill occurs in the background.",
			  "From the program perspective, however, both load operations must complete\nbefore the packet can complete.\nThus, the packet is atomic from the program perspective.",
			  "software pipeline has three sections:\n\nA *prologue* in which the loop is primed\n\nA *kernel* (or steady-state) portion\n\nAn *epilogue* which drains th",
			  "This is best illustrated with a simple example, as shown in Table 8-2 .\nIn Table 8-2 the kernel section of the pipelined loop performs three iterations of the loop\nin parallel:"
			]
		  },
		  {
			"title": "Clustered Modulo Scheduling in a VLIW Architecture with ...",
			"url": "https://jilp.org/vol3/sanchez-jilp.pdf",
			"excerpts": [
			  "The proposed algorithm is called RMCA (which stands for Register and Memory Communication-Aware) modulo scheduling. It is an evolution of the algorithm ...Read more",
			  "For a clustered VLIW architecture, both II and SC can be affected by inter-cluster register communica-\ntions. If the communication buses become saturated, a higher II is required. Moreover, communication\noperations may increase the length of the schedule, and therefore the SC may be increased. Thus, the IPC\nof a clustered VLIW architecture will be lower than that of an equivalent unified VLIW architecture with\nthe same resources in general. On the other hand, a clustered architecture may reduce the critical delays\nsuch as the register file access time and the bypass latency (Palacharla, Jouppi and Smith, 1997), and allow\nfor faster clock rates.",
			  "The two main parameters that statically characterize a modulo scheduled loop are the *initiation interval*\n(II) and the *stage count* (SC).",
			  "The former reflects the number of cycles that a kernel iteration takes (assum-\ning no stalls), whereas the latter shows how many iterations are overlapped, and determines the length of\nthe prolog and epil",
			  "NCYCLE Total = NCYCLE Compute + NCYCLE Stall",
			  "NCYCLE Compute = NTIMES * ((NITER + SC -1) * II)",
			  "NCYCLE Stall represents the number of cycles where the processor is stalled and depends on several factors as\nwe detail below.",
			  "The proposed algorithm produces schedules with very low communication requirements and outperforms previous cluster-oriented schedulers. 1. Introduction.Read more"
			]
		  },
		  {
			"title": "Minimal Unroll Factor for Code Generation of Software ...",
			"url": "https://inria.hal.science/hal-00764521/document",
			"excerpts": [
			  "Computing an adequate unroll factor and allocating registers to the separated live range instances is called periodic register allocation. In ...Read more"
			]
		  },
		  {
			"title": "Minimizing Register Requirements of a Modulo Schedule via Optimum Stage Scheduling | International Journal of Parallel Programming | Springer Nature Link",
			"url": "https://link.springer.com/content/pdf/10.1007/BF03356744.pdf?pdf=button",
			"excerpts": [
			  "Content:\n[Register-sensitive modulo scheduling](/search?query=Register-sensitive%20modulo%20scheduling&facet-discipline=%22Computer%20Science%22)\n[software pipelining](/search?query=software%20pipelining&facet-discipline=%22Computer%20Science%22)\n[instruction level parallelism](/search?query=instruction%20level%20parallelism&facet-discipline=%22Computer%20Science%22)\n[VLIW](/search?query=VLIW&facet-discipline=%22Computer%20Science%22)\n[superscalar](/search?query=superscalar&facet-discipline=%22Computer%20Science%22)",
			  "Section Title: Minimizing Register Requirements of a Modulo Schedule via Optimum Stage Scheduling > About this article > Key Words"
			]
		  },
		  {
			"title": "Modulo Scheduling with Reduced Register Pressure | IEEE Transactions on Computers",
			"url": "https://dl.acm.org/doi/10.1109/12.689643",
			"excerpts": [
			  "This paper presents a novel heuristic modulo scheduling strategy that tries to generate schedules with the lowest II, and, from all the possible schedules with such II, it tries to select that with the lowest register requirements.",
			  "The proposed method has been implemented in an experimental compiler and has been tested for the Perfect Club benchmarks.",
			  "The results show that the proposed method achieves an optimal II for at least 97.5 percent of the loops and its compilation time is comparable to a conventional top-down approach, whereas the register requirements are lower.",
			  "The register pressure increases as the instruction level parallelism increases.",
			  "When the register requirements of a schedule are higher than the available number of registers, the loop must be rescheduled perhaps with a higher II.",
			  "Software pipelining is a scheduling technique that is used by some product compilers in order to expose more instruction level parallelism out of innermost loops.",
			  "Modulo scheduling refers to a class of algorithms for software pipelining."
			]
		  },
		  {
			"title": "SDC-Based Modulo Scheduling for Pipeline Synthesis",
			"url": "https://www.csl.cornell.edu/~zhiruz/pdfs/sdcmod-iccad2013.pdf",
			"excerpts": [
			  "Swing modulo scheduling [20] is able to reduce the regis-\nter requirements of the resulting schedule by placing each\noperation close to either its predecessors or successors.",
			  "Register Pressure**\nReducing the register pressure is one of the main optimiza-\ntion objectives of a modern modulo scheduler.",
			  "odulo scheduling is a popular technique to enable pipelined\nexecution of successive loop iterations for performance im-\nprovement",
			  ".\nSwing modulo scheduling [20] is able to reduce the regis-\nter requirements of the resulting schedule by placing each\noperation close to either its predecessors or successors. A",
			  "Modulo scheduling [27] is a popular compilation tech-\nnique to enable loop pipelinin"
			]
		  },
		  {
			"title": "Stage scheduling: a technique to reduce the register ... - dblp",
			"url": "https://dblp.dagstuhl.de/rec/conf/micro/EichenbergerD95a.html",
			"excerpts": [
			  "Alexandre E. Eichenberger, Edward S. Davidson: Stage scheduling: a technique to reduce the register requirements of a modulo schedule. MICRO 1995: 338-349."
			]
		  },
		  {
			"title": "Region-based Register Allocation for EPIC Architectures",
			"url": "https://cs.nyu.edu/media/publications/hansoo.pdf",
			"excerpts": [
			  "The stage scheduler moves operations across stages in order to reduce register usage in the loop. When a loop is modulo scheduled, some of the virtual registers ...Read more"
			]
		  },
		  {
			"title": "Profile-based global live-range splitting | Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation",
			"url": "https://dl.acm.org/doi/10.1145/1133981.1134007",
			"excerpts": [
			  "Live-range splitting is a technique to split the live range of a given variable into multiple subranges, each of which can be assigned to a different ...Read more"
			]
		  },
		  {
			"title": "\n\tLive range splitting optimization\n",
			"url": "https://community.ibm.com/community/user/blogs/ajit-kumar-agarwal/2025/08/19/live-range-splitting-optimization",
			"excerpts": [
			  "If the live range L2 is contained inside the live range L1 then L1 is split at the start and death of L2 and there is store at the start of L2 ...Read more"
			]
		  },
		  {
			"title": "Register Spilling and Live-Range Splitting for SSA-Form ...",
			"url": "https://bernsteinbear.com/assets/img/register-spilling-range-splitting-ssa.pdf",
			"excerpts": [
			  "It is sensitive to the structure of the program by splitting live-ranges around loops. Our experiments show a reduction of executed reload instructions by.Read more"
			]
		  },
		  {
			"title": "Live-range Unsplitting for Faster Optimal Coalescing",
			"url": "http://web4.ensiie.fr/~blazy/Blazy-Robillard.pdf",
			"excerpts": [
			  "Extreme live-range splitting helps in finding optimal and efficient solutions for spilling. But, it generates programs with huge interference graphs. Each re-.Read more"
			]
		  },
		  {
			"title": "Minimizing Register Requirements of a Modulo Schedule via Optimum Stage Scheduling | International Journal of Parallel Programming | Springer Nature Link",
			"url": "https://link.springer.com/article/10.1007/BF03356744",
			"excerpts": [
			  "Section Title: Minimizing Register Requirements of a Modulo Schedule via Optimum Stage Scheduling > About this article > Key Words",
			  "[Register-sensitive modulo scheduling](/search?query=Register-sensitive%20modulo%20scheduling&facet-discipline=%22Computer%20Science%22)",
			  "[software pipelining](/search?query=software%20pipelining&facet-discipline=%22Computer%20Science%22)",
			  "[instruction level parallelism](/search?query=instruction%20level%20parallelism&facet-discipline=%22Computer%20Science%22)",
			  "[VLIW](/search?query=VLIW&facet-discipline=%22Computer%20Science%22)",
			  "[superscalar](/search?query=superscalar&facet-discipline=%22Computer%20Science%22)",
			  "tage scheduling: A technique to reduce the register requirements of a modulo schedule, *Proc. of the 28th Ann. Int. Symp. on Microarchitecture* , pp. 338–349 (December 1995)"
			]
		  },
		  {
			"title": "(PDF) Stage Scheduling: A Technique to Reduce the Register Requirements of a Modulo Schedule",
			"url": "https://www.researchgate.net/publication/2288857_Stage_Scheduling_A_Technique_to_Reduce_the_Register_Requirements_of_a_Modulo_Schedule",
			"excerpts": [
			  "Measurements on a benchmark suite of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels shows that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler. ",
			  "These heuristics address a common\nshortcoming of curren t mo dulo sc hedulers whic h tends\nto result in sc hedules with increased register require-\nmen ts due to p o or placemen t of the op erations not on\nthe critical pat",
			  " e presen t a set of\nstage sche duling\nheuristics that\nreduce the register requiremen ts of a mo dulo sc hedule\nb y reassigning some op erations to dieren t stages, i.e.\nb y shifting op erations b y m ultiples of",
			  "Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements.",
			  "We present a set of low computational complexity stage-scheduling heuristics that reduce the register requirements of a given modulo schedule by shifting operations by multiples of II cycles."
			]
		  },
		  {
			"title": "[PDF] Stage scheduling: a technique to reduce the register requirements of a modulo schedule | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Stage-scheduling%3A-a-technique-to-reduce-the-of-a-Eichenberger-Davidson/c3dcb5eda4553cdcc94cba21ba5a905f58fbfb1e",
			"excerpts": [
			  "TLDR\nILP speedups of factors in the ten's are demonstrated with constrained resources and Disjoint Eager Execution is shown both theoretically and experimentally to yield more parallelism than both branch prediction and eager execution when the same, finite, execution resources are assumed.",
			  ": Stage scheduling: a technique to reduce the register requirements of a modulo schedule",
			  "A. Eichenberger](/author/A.-Eichenberger/1985764) , [E. Davidson](/author/E.-Davidson/144274347)",
			  "A set of heuristics to improve the spilling process and to better decide between adding spill code or directly decreasing the execution rate of iterations are proposed and a novel approach for simultaneous instruction scheduling and register spilling in modulo scheduling is proposed: MIPS (modulo scheduling with integrated register spilling).",
			  "View on ACM",
			  "researcher.ibm.com"
			]
		  },
		  {
			"title": "978-1-4613-1705-0.pdf",
			"url": "https://link.springer.com/content/pdf/10.1007/978-1-4613-1705-0.pdf",
			"excerpts": [
			  "The algorithm of modulo variable expansion is as follows. ... software pipelining, suitable only for machines ... pipelining into a complete algorithm that is based ..."
			]
		  },
		  {
			"title": "AN IMPLEMENTATION OF SWING MODULO ...",
			"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis-book.ps",
			"excerpts": [
			  "paths can be scheduled in the same slot provided there is not control path between them. Modulo Variable Expansion is a technique first developed by Lam [25] to ...Read more"
			]
		  },
		  {
			"title": "Software pipelining - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Software_pipelining",
			"excerpts": [
			  "Lam showed that special hardware is unnecessary for effective modulo scheduling. Her technique, *modulo variable expansion* is widely used in practice.",
			  "The trick is to replicate the body of the loop after it has been scheduled, allowing different registers to be used for different values of the same variable when they have to be live at the same time.",
			  "ca_Lam \"Monica Lam\") presents an elegant solution to this problem in her thesis, *A Systolic Array Optimizing Compiler* (1989) ( [ISBN](/wiki/ISBN_(identifier) \"ISBN (identifier)\") [0-89838-300-5](/wiki/Special:BookSources/0-89838-300-5 \"Special:BookSources/0-89838-300-5\") ). She calls it *modulo variable expansion* . "
			]
		  },
		  {
			"title": "Lifetime-sensitive modulo scheduling in a production ...",
			"url": "https://upcommons.upc.edu/bitstreams/64597290-7fb5-43ac-84fe-4d46c514961f/download",
			"excerpts": [
			  "In this paper, we present a novel scheduling strategy,. Swing Modulo Scheduling (SMS1), which considers the criticality of the nodes. It is a heuristic ...Read more"
			]
		  },
		  {
			"title": "(PDF) Resource‐Constrained Modulo Scheduling",
			"url": "https://www.researchgate.net/publication/230455412_Resource-Constrained_Modulo_Scheduling",
			"excerpts": [
			  "This formulation was improved by Eichenberger and Davidson [13] in order to solve practical cases with a commercial solver. Dupont de Dinechin developed ...Read more"
			]
		  },
		  {
			"title": "(PDF) Stage scheduling: a technique to reduce the register requirementsof a module schedule",
			"url": "https://www.researchgate.net/publication/3618062_Stage_scheduling_a_technique_to_reduce_the_register_requirementsof_a_module_schedule",
			"excerpts": [
			  "Stage Scheduling: A Technique to Reduce the Register Requirements of a Modulo Schedule",
			  "Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements.",
			  "We present a set of low computational complexity stage-scheduling heuristics that reduce the register requirements of a given modulo schedule by shifting operations by multiples of II cycles.",
			  "ow full abstract] benchmark suite of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels shows that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling with Enhanced ...",
			"url": "https://ieeexplore.ieee.org/iel8/11245191/11244915/11245326.pdf",
			"excerpts": [
			  "Exact methods (e.g., constraint programming [11], integer linear programming [12]) guarantee optimal schedules but incur prohibitive computational overhead ...Read more",
			  "VLIW architectures are typically equipped with mul- tiple heterogeneous FUs, such as integer arithmetic logic units(ALUs), multipliers (MAC), and load/store ..."
			]
		  },
		  {
			"title": "Optimal Integrated VLIW Code Generation with Integer ...",
			"url": "https://opendl.ifip-tc6.org/db/conf/europar/europar2006/BednarskiK06.pdf",
			"excerpts": [
			  "We give an Integer Linear Programming (ILP) solution that fully in- tegrates all steps of code generation, i.e. instruction selection, register allocation and ..."
			]
		  },
		  {
			"title": "SMT Solver Z3",
			"url": "https://ece.uwaterloo.ca/~agurfink/stqam.w20/assets/pdf/W04-UsingZ3.pdf",
			"excerpts": [
			  "Satisfiability is the problem of determining wither a formula F has a model. • if F is propositional, a model is a truth assignment to Boolean variables.Read more"
			]
		  },
		  {
			"title": "Optimal instruction scheduling using integer programming",
			"url": "https://www.researchgate.net/publication/220752198_Optimal_instruction_scheduling_using_integer_programming",
			"excerpts": [
			  "Wilken, Liu and Heffernan [66] and Shobaki and Wilken [60] showed that through various modeling and algorithmic techniques, integer linear programming and enumeration techniques could be used to produce optimal instruction schedules for large basic blocks and superblocks targeted to a multi-issue processor.",
			  "We have selected integer linear programming to solve instruction scheduling because ILP algorithms are sufficiently general to solve virtually any combinatorial optimization problem.",
			  "A major challenge when developing an exact approach to an NP-complete problem is to develop a solver that scales and is robust in that it rarely fails to find a solution in a timely manner on a wide selection of real problems.",
			  "n et al. [66] presented an integer programming approach for basic block scheduling. They tested their model on SPEC95 floating point benchmark.",
			  "Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler.",
			  "In this article, we propose an Integer Linear Programming (ILP)-based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program.",
			  "May 2000\n[ACM SIGPLAN Notices](journal/ACM-SIGPLAN-Notices-1558-1160) 35(5):121-133\nDOI: [10.1145/358438.349318]"
			]
		  },
		  {
			"title": "Optimal instruction scheduling using integer programming | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/358438.349318",
			"excerpts": [
			  "This paper presents a new approach to local instruction scheduling based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks.",
			  "The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule.",
			  "The simplified graph results in a simplified integer program which can be solved much faster.",
			  "The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks.",
			  "Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14%."
			]
		  },
		  {
			"title": "Constraint-Based Register Allocation and Instruction ...",
			"url": "https://robcasloz.github.io/publications/TRITA-EECS-AVL-2018-48.pdf",
			"excerpts": [
			  "aint programming (CP) has several characteristics that make it a partic-\nularly suitable combinatorial technique to model and solve the integrated register\nallocation and instruction scheduling problem. On the modeling side, global con-\nstraints can be used to capture the main structure of different subproblems such as\n8\nregister packing and scheduling with limited processor resources. Thanks to these\nhigh-level abstractions, complete yet compact CP models can be formulated that\navoid an explosion in the number of variables and constraints. On the solving side,\nglobal constraints can reduce the search space exponentially by increasing the effect\nof propagation. Also, CP solvers are highly customizable, which enables the use of\nproblem-specific solving methods to improve scalability.",
			  "els according to its scope: local, regional, and global.\nThe early approaches to local instruction scheduling (using IP [7,98], CP [43],\nand special-purpose enumeration techniques [29]) focus on handling highly irregular\nprocessors and do not scale beyond some tens of instructions. In 2000, the semi-\nnal IP approach by Wilken *et al.* [141] demonstrates that basic blocks of hundreds\nof instructions can be scheduled optimally with problem-specific solving methods.\nS",
			  "sequent research [67,136] culminates with the CP approach by Malik *et al.* [103],\n26\nwhich solves optimally basic blocks of up to 2600 instructions for a more realistic\nand complex processor than the one used originally by Wilken *et al.* A particu-\nlar line of research in local instruction scheduling is to minimize the register need\nof the schedule, which is the typical optimization criterion of the first instruction\nscheduling stage in a conventional compiler back-end (see Figure 1.2). Multiple\napproaches have been proposed [63,84,101,129] that solve medium-sized problems\noptimally and demonstrate moderate code quality improvements over their conven-\ntional count",
			  "he integration of register allocation and instruction scheduling using constraint\nprogramming is practical and effective.\nThe proposed approach is *practical* as it is *complete* , *scales* to medium-sized\nproblems of up to 1000 instructions (including the vast majority of problems ob-\nserved in typical benchmark suites), and generates *executable* code. It is *effective* as\nit yields better code than the conventional approach for different processors, bench-\nmark suites, and optimization criteria, delivering on the promise of combinatorial",
			  "nstruction scheduling can target *in-order* processors (which issue instructions\nin the order given by the compiler’s schedule) or *out-of-order* processors (which\n ...",
			  "The subproblem of grouping\ninstructions into bundles is referred to as *instruction bundling* ",
			  "The model and its implementation have been evolved incrementally, by increas-\ning in each iteration the scope of the problem modeled (that is, solving more sub-\nproblems in integration) and the complexity of the targeted processors. This process\ninvolves six major iterations:\n1. build a simple model of local instruction scheduling for a simple general-\npurpose processor;\n2. extend the model with local register allocation for a more complex VLIW\ndigital signal processor;\n3. increase the scope of the model to entire functions;\n4. extend the model with the subproblems of load-store optimization, multi-\nallocation, and a refined form of coales",
			  "Load-store optimization removes\nunnecessary memory access instructions inserted during register allocation while\nultimate coalescing removes unnecessary register-to-register copy instructions.",
			  "nstraint programming on a dedicated program representation enables a compact\nmodel that captures the complete set of global register allocation subproblems in in-\ntegration with instruction scheduling. The model has been extended with additional\nsubproblems beyond the usual scope of conventional compilers. These subproblem\nextensions illustrate the ease with which the approach can be adapted to specific\nprocessor features such as operand forwarding or double load instructions.",
			  "The approach is implemented by Unison, an open-source software tool [26] that\ncomplements the state-of-the-art conventional compiler LLVM."
			]
		  },
		  {
			"title": "Constraint program solvers - IBM CPLEX",
			"url": "https://www.ibm.com/products/ilog-cplex-optimization-studio/cplex-cp-optimizer",
			"excerpts": [
			  "Use constraint programming techniques to compute solutions for detailed scheduling problems and combinatorial optimization problems.",
			  "nt:\nIBM® ILOG® CP Optimizer is a necessary and important complement to the optimization specialist’s toolbox for solving real-world operational planning and scheduling problems. ILOG C",
			  "For pure academic problems such as job-shop, open-shop and flow-shop, it finds solutions that are comparable to solutions found by state-of-the-art specialized algorithms.",
			  "Use modeling features specialized to scheduling like intervals (for activities) and cumul functions (for resources).",
			  "Model finite capacity resources and reservoirs.",
			  "Model setup times to compute schedules that define the best possible sizes for batches."
			]
		  },
		  {
			"title": "OPL CP Optimizer in a nutshell - IBM Documentation",
			"url": "https://www.ibm.com/docs/en/icos/22.1.1?topic=programming-opl-cp-optimizer-in-nutshell",
			"excerpts": [
			  "Section Title: OPL CP Optimizer in a nutshell",
			  "Content:\nLast Updated : 2022-12-09\nProvides the basics of using constraint programming\nin an OPL model.\nA CP model must be declared as such.\nIt uses only discrete decision variables for which you must define\na domain. The product of all domain sizes makes up the search space.\nCP models are further characterized by specific constraints and expressions,\nand parameterizable propagation, and search.\nHere are the basics of using constraint programming in an OPL model.\nRead the CP Optimizer documentation for details.\n ...",
			  "Section Title: OPL CP Optimizer in a nutshell > Related information",
			  "Content:\n[Scheduling with IBM ILOG CPLEX Studio](/docs/en/SSSA5P_22.1.1/ilog.odms.ide.help/OPL_Studio/scheduling_gs/topics/opl_gs_sched_with_cplex_studio.html \"Introduces the basic building blocks of a scheduling model.\")\n[Scheduling](/docs/en/SSSA5P_22.1.1/ilog.odms.ide.help/OPL_Studio/opllangref/topics/opl_langref_scheduling.html \"Describes how to model scheduling problems in OPL.\")\n[Constraints](/docs/en/SSSA5P_22.1.1/ilog.odms.ide.help/OPL_Studio/opllangref/topics/opl_langref_constraints.html \"Specifies the constraints supported by OPL and discusses various subclasses of constraints to illustrate the support available for modeling combinatorial optimization applications.\")",
			  " Copyright IBM Corporation 2022"
			]
		  },
		  {
			"title": "Mapping and Scheduling on Multi-core Processors using ...",
			"url": "https://theses.hal.science/tel-01087271/document",
			"excerpts": [
			  "We encode this problem in form of logical constraints and present it to\n*satisfiability modulo theory* (SMT) solver",
			  "SMT solvers, solve the encoded problem by using\na combination of search techniques and constraint propagation to find an assignment to the\nproblem variables satisfying the given cost constraints.",
			  "dataflow applications, the design space explodes with increased number of tasks and pro-\n ... \nIn this chapter we present some basic facts on SMT solvers which we apply to optimize\nmapping and scheduling for many-core processors. We explain briefly the theory behind them.",
			  "Then we introduce multi-criteria optimization, for which we would like to apply the SMT\nsolvers. Multi-criteria problems need specialized algorithms in order to track their solutions.",
			  "e present an example of SMT constraints for scheduling a split-join graph shown in Fig-\nure  .  for *�* = 3 . The derived task graph, *T* = ( *U* *,* E *,�,�* ) , will have tasks *A* 0 *,* *B* 0 *,* *B* 1 *,* *B* 2 and *C* 0 ",
			  "Let *s* ( *u* ) and *e* ( *u* ) denote the start and end time of a task *u* , and *�* ( *u* ) be the processor on which\nit will execute.",
			  "The constraints which express the task graph for scheduling to the solver are given as:",
			  "Non-negative start times : indicating that there are no negative start times of the task.",
			  "End times : The task always finishes when the time equal to its execution time has\nelapsed after the starting time.",
			  "Precedence relations : No task can start before the finish of its predecessors.",
			  "Non-overlapped execution on a processor : Tasks running on same processor should\nnot overlap in time.",
			  "Processor cost : The tasks should use only *M* processors or less.",
			  "Thus the scheduling of task graph can be encoded as an SMT problem with linear arithmetic\ntheory.",
			  ". A more detailed encoding of the problem is presented in Section  .  .\n",
			  "*Non-retractable and retractable constraints*\nA constraint solver can be used for optimization by adding to the constraint satisfaction\nproblem a condition of form *x* ≤ *C* that can be used for cost optimization, where *x* is a tight\nupper bound on the cost and *C* is a constant."
			]
		  },
		  {
			"title": "SAT solving: a sample scheduler - Javier Casas",
			"url": "https://www.javiercasas.com/articles/sat-solver-scheduler/",
			"excerpts": [
			  "SAT/SMT solving has been so far weird from my point of view. It strongly feels like a Prolog on steroids that can solve many problems without actually telling it how to do it.",
			  "The question is: in what order do we execute the steps so that the total time is minimised? Let's work a solution with Z3 and Python.",
			  "We can imagine this arbitrary restriction as the step requiring a special machine, and we only have a single of each special machine in the workshop."
			]
		  },
		  {
			"title": "Optimal Basic Block Instruction Scheduling for Multiple-Issue ...",
			"url": "https://cs.uwaterloo.ca/~vanbeek/Publications/ictai06.pdf",
			"excerpts": [
			  "Heffernan and Wilken [8] present a set of graph trans- formations for dependency DAGs for basic blocks and show that optimally scheduling the transformed DAGs ...Read more",
			  "van Beek and K. Wilken. Fast optimal instruction schedul- ing for single-issue processors with arbitrary latencies. In. Proc. of the 7th International Conf.Read more"
			]
		  },
		  {
			"title": "Cumulative functions",
			"url": "https://www.ibm.com/docs/en/icos/22.1.1?topic=scheduling-cumulative-functions",
			"excerpts": [
			  "CP Optimizer introduces the notion of the cumulative function expression, which is a function that represents the sum of individual contributions of intervals.Read more"
			]
		  },
		  {
			"title": "12 Solving SAT and SMT Problems Using Z3",
			"url": "https://www.cs.umd.edu/class/fall2025/cmsc433/Solving_SAT_and_SMT_Problems_Using_Z3.html",
			"excerpts": [
			  "The solver is telling us that there is no possible choice of a, b, c that makes the current constraints true. Z3 can find solutions to more than just SAT ...Read more"
			]
		  },
		  {
			"title": "Optimal Basic Block Instruction Scheduling for Multiple-Issue ...",
			"url": "https://cs.uwaterloo.ca/research/tr/2005/CS-2005-19.pdf",
			"excerpts": [
			  "Previous work on optimal basic block schedulers has taken several approaches, including:\nbranch-and-bound enumeration [5, 13, 14, 21, 29], dynamic programming [19], integer linear\nprogramming [1, 4, 18, 20, 31], and constraint programming [10, 30].",
			  "Wilken, Liu, and Heffernan [31] were the first to develop a robust optimal\nscheduler that scaled up to large basic blocks.",
			  "\n**3.6**\n**Dominance constraints**\nHeffernan and Wilken [14] present a set of graph transformations for dependency DAGs for\nbasic blocks and show that optimally scheduling the transformed DAGs using branch-and-\nbound enumeration is faster and more robust. The DAG transformations reduce the search\nspace while preserving optimality. We found that these transformations also worked well\nin our constraint programming approach. In our context, the transformations add simple\nconstraints to the model of the form *i* *≥* *j* , which we call dominance constraints."
			]
		  },
		  {
			"title": "Cumul functions in CP Optimizer",
			"url": "https://www.ibm.com/docs/en/icos/22.1.0?topic=c-cumul-functions-in-cp-optimizer",
			"excerpts": [
			  "CP Optimizer introduces the notion of *cumul function expression* which is a function that\nrepresents the sum of individual contributions of intervals.",
			  "In scheduling problems involving cumulative resources (also known as renewable resources),\nthe cumulated usage of the resource by the activities is usually represented by a function of\ntime.",
			  "CP Optimizer provides several\nconstraints over cumul function expressions. These constraints allow restriction of the possible\nvalues of the function over the complete horizon or over some fixed or variable interval (see [Constraints on cumul function expressions](cumul_functions.html) ).",
			  "Letdenote the set of all functions fromto. A cumul function expression *f* is an\nexpression whose value is a function ofand thus, whose domain *dom(f)* is a subset of.",
			  "Cumul function expressions",
			  "For instance, let *a* and *b* be two interval variables and a cumul function expression *f* defined\nby: *f = pulse(a, 3)* + *pulse(a, 2)* - *stepAtEnd(a, 1)* + *stepAtStart(b, 2)* - *stepAtEnd(b, 3)* .",
			  "The contribution of *a* to *f* is the function *pulse(a, 3)* + *pulse(a, 2)* - *stepAtEnd(a, 1)* and the\ncontribution of *b* to *f* is the function *stepAtStart(b, 2)* - *stepAtEnd(b, 3)* ."
			]
		  },
		  {
			"title": "(PDF) An Application of Constraint Programming to Superblock Instruction Scheduling",
			"url": "https://www.researchgate.net/publication/221632951_An_Application_of_Constraint_Programming_to_Superblock_Instruction_Scheduling",
			"excerpts": [
			  "constraint programming brought added value in two ways.",
			  "The ﬁrst value added by constraint programming is that it allowed us to\nachieve the primary goal of our project, which was to develop a superblock\ninstruction scheduler that was realisti c yet fast enough to be incorporated into\na productioncompiler. ",
			  "Using constraint programming, it was relatively easy to\nadd additional constraints to model realistic architecturesand it is not clear how\nto similarly extend previously proposed enumeration and integer programming\napproaches. ",
			  "In our constraint programming ap-\nproach, we remove these assumptions and present the ﬁrst optimal superblock\nscheduling approach for realistic architectures",
			  ". Further, even though our target\narchitectures are realistic, our approach scales up to more diﬃcult and larger\nsuperblocks than in previous work.",
			  " On this benchmark suite, the optimal scheduler was very robust -all but a handful of the hundreds of thousands of basic blocks in our benchmark suite were solved optimally within a reasonable time limit - and scaled to the largest basic blocks, including basic blocks with up to 2600 instructions.",
			  "This compares favorably to the best previous exact approaches.",
			  "Dependingon the architectural model,between\n98.23% to 99.98% of allsuperblocks were solved to optimality. Th"
			]
		  },
		  {
			"title": "A Constraint Programming Approach for Instruction Assignment",
			"url": "https://cs.uwaterloo.ca/~vanbeek/Publications/interact11.pdf",
			"excerpts": [
			  " In a constraint programming approach, a problem is mod-\neled by stating constraints on acceptable solutions, where a con-\nstraint defines a relation among variables, each taking a value in a\ngiven domain.",
			  "The novelty of our approach lies in the decomposi-\ntion of the problem and our improvements to the constraint model,\nwhich reduces the effort needed in the search for the optimal so-\nlution.",
			  "Our approach is applicable when larger compile times are\nacceptable."
			]
		  },
		  {
			"title": "logic - Representing temporal constraints in SMT-LIB - Stack Overflow",
			"url": "https://stackoverflow.com/questions/35592724/representing-temporal-constraints-in-smt-lib",
			"excerpts": [
			  "It's possible that my assumption of being in QF-LIA is wrong and it's also possible that Z3 struggles with this type of constraints.",
			  "When I tried simpler constraints Z3 managed to reach a resolution but I noticed that the models it generated were very complicated with lots of internal structures.",
			  "Z3's online prover with my code can be found [here",
			  "If you are already familiar with CLTL (Constraint Linear Temporal Logic) and able to model your problem in CLTL, you can use Zot and my plugin ae2bvzot.",
			  "It automatically translates the CLTL formula into smt2, runs Z3 and renders the result with assignment for each variable in each time instant.",
			  "You can restrict Z3 to a particular logic, for instance by using `(set-logic QF_LIA)` Z3 will complain if your problem does not fit quantifier-free linear integer arithmetic.",
			  " in addition you use arrays, then `(set-logic QF_AUFLIA)` . However your use of arrays *may* be part of the problem. If you can write an uglier SMT problem without using arrays, I am pretty sure that Z3 will solve it much faster.",
			  "Adding more theories to your spec can only make things worse, so try to express your problem using just `QF_LIA` ",
			  "Finite ranges can be enforced either using assertions, or by replacing `(check-sat)` with `(check-sat-using...` plus the appropriate options."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/full/10.1145/3643135",
			"excerpts": [
			  "ent Wilken, Jack Liu, and Mark Heffernan. 2000. Optimal instruction scheduling using integer programming. *SIGPLAN Not.* 35, 5 (may2000), 121–133",
			  "This article proposes a two-dimensional constrained dynamic programming (TDCDP) approach and a quantitative model for instruction scheduling.Read more",
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling.",
			  "Utilizing these details, we formulate an optimization problem and derive the optimal solution utilizing the GUROBI [ [27]() ] optimization solver.",
			  "Subsequently, the scheduler component applies the TDCDP algorithm to reorder instructions, culminating in a schedule solution.",
			  "VLIW architecture holds great significance in the field of DSP and finds wide application in chip design for domains such as audio, video, image processing, and communication.",
			  "Instruction scheduling in VLIW is a static procedure that assigns time slots and functional units to instructions during compilation."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two ...",
			"url": "https://dl.acm.org/doi/10.1145/3643135",
			"excerpts": [
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling."
			]
		  },
		  {
			"title": "Optimizing VLIW Instruction Scheduling via a Two-Dimensional Constrained Dynamic Programming | ACM Transactions on Design Automation of Electronic Systems",
			"url": "https://dl.acm.org/doi/abs/10.1145/3643135",
			"excerpts": [
			  "In modern compilers (i.e., LLVM), instructions are usually scheduled in two phases, which can be concluded as Pre-RA (register allocation) Scheduling and Post-RA Scheduling.",
			  "Utilizing these details, we formulate an optimization problem and derive the optimal solution utilizing the GUROBI [ [27]() ] optimization solver.",
			  "Subsequently, the scheduler component applies the TDCDP algorithm to reorder instructions, culminating in a schedule solution."
			]
		  },
		  {
			"title": "\nBit Twiddling Hacks\n",
			"url": "https://graphics.stanford.edu/~seander/bithacks.html",
			"excerpts": [
			  "if (v & 0x00FF00FF) c -= 8;",
			  "if (v & 0x0F0F0F0F) c -= 4;",
			  "if (v & 0x33333333) c -= 2;",
			  "if (v & 0x55555555) c -= 1;",
			  "unsigned int c;     // c will be the number of zero bits on the right,",
			  "// so if v is 1101000 (base 2), then c will be 3",
			  "// NOTE: if 0 == v, then c = 31.",
			  "if (v & 0x1)",
			  "// special case for odd v (assumed to happen half of the time)",
			  "c = 0;",
			  "else",
			  "else",
			  "c = 1;",
			  "if ((v & 0xffff) == 0)",
			  "v >>= 16;",
			  "c += 16;",
			  "if ((v & 0xff) == 0)",
			  "v >>= 8;",
			  "c += 8;",
			  "if ((v & 0xf) == 0)",
			  "v >>= 4;",
			  "c += 4;",
			  "if ((v & 0x3) == 0)",
			  "v >>= 2;",
			  "c += 2;",
			  "c -= v & 0x1;",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "}",
			  "unsigned int v;            // find the number of trailing zeros in v",
			  "int r;                     // the result goes here",
			  "float f = (float)(v & -v); // cast the least significant bit in v to a float",
			  "r = (*(uint32_t *)&f >> 23) - 0x7f;",
			  "Section Title: Bit Twiddling Hacks > &                                                                                       1111 1111",
			  "Content:",
			  "unsigned int v;  // find the number of trailing zeros in v",
			  "int r;           // put the result in r",
			  "static const int Mod37BitPosition[] = // map a bit value mod 37 to its position",
			  "32, 0, 1, 26, 2, 23, 27, 0, 3, 16, 24, 30, 28, 11, 0, 13, 4,",
			  "7, 17, 0, 25, 22, 31, 15, 29, 10, 12, 6, 0, 21, 14, 9, 5,",
			  "20, 8, 19, 18",
			  "r = Mod37BitPosition[(-v & v) % 37];",
			  "int r;           // result goes here",
			  "static const int MultiplyDeBruijnBitPosition[32] =",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "{",
			  "0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8,",
			  "31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9",
			  "};",
			  "};"
			]
		  },
		  {
			"title": "Qualcomm Hexagon DSP: An architecture optimized for ...",
			"url": "https://pages.cs.wisc.edu/~danav/pubs/qcom/hexagon_hotchips2013.pdf",
			"excerpts": [
			  "}",
			  "{",
			  "{",
			  "{",
			  "{"
			]
		  },
		  {
			"title": "The Forgotten Art of Binary Manipulation: 6 Bit-Twiddling Tricks That Will Make You Look Like a Wizard | by Sohail Saifi | Medium",
			"url": "https://medium.com/@sohail_saifi/the-forgotten-art-of-binary-manipulation-6-bit-twiddling-tricks-that-will-make-you-look-like-a-b7d46f7a4b67",
			"excerpts": [
			  "Counting the number of bits set to 1 in a value (known as the “population count” or “Hamming weight”) is a deceptively powerful operation with applications ranging from cryptography to machine learning.",
			  "This works because `num & (num - 1)` always clears the rightmost set bit. The loop runs exactly once for each set bit, rather than once for each bit position.",
			  "For fixed-width integers, there’s an even faster approach using the “SWAR” (SIMD Within A Register) technique:\nThis algorithm is mind-bending at first glance, but it works by essentially doing a parallel reduction. It counts bits in parallel within 2-bit fields, then 4-bit fields, then 8-bit fields, and so on.",
			  "The Forgotten Art of Binary Manipulation: 6 Bit-Twiddling Tricks That Will Make You Look Like a Wizard > Learning and Practicing Bit Manipulation",
			  "The naive approach iterates through each bit:",
			  "Hacker’s Delight” by Henry S. Warren Jr.** — The definitive reference on bit manipulation technique"
			]
		  },
		  {
			"title": "GitHub - ianbrayoni/bithacks: Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			"url": "https://github.com/ianbrayoni/bithacks",
			"excerpts": [
			  "Hats off to Sean Eron Anderson, [seander@cs.stanford.edu](mailto:seander@cs.stanford.edu) , for creating https://graphics.stanford.edu/~seander/bithacks.html",
			  "I **DO NOT** own any of this. The following contents comes directly from the above link.",
			  "Section Title: Acknowledgement",
			  "Section Title: Contributing > About",
			  "Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			  "Bit Twiddling Hacks By Sean Eron Anderson: code snippets in Python",
			  "[graphics.stanford.edu/~seander/bithacks.html](https://graphics.stanford.edu/~seander/bithacks.html \"https://graphics.stanford.edu/~seander/bithacks.html\")",
			  "[graphics.stanford.edu/~seander/bithacks.html](https://graphics.stanford.edu/~seander/bithacks.html \"https://graphics.stanford.edu/~seander/bithacks.html\")"
			]
		  },
		  {
			"title": "GitHub - gibsjose/BitHacks: Markdown/PDF Versions of ...",
			"url": "https://github.com/gibsjose/BitHacks",
			"excerpts": [
			  "Sean Anderson's wonderful compilation of Bit Twiddling Hacks, converted to a more readable Markdown version and accompanying PDF.Read more"
			]
		  },
		  {
			"title": "Templatized branchless int max/min function - c++",
			"url": "https://stackoverflow.com/questions/514435/templatized-branchless-int-max-min-function",
			"excerpts": [
			  "I'm trying to write a branchless function to return the MAX or MIN of two integers without resorting to if (or ?:)."
			]
		  },
		  {
			"title": "How can I make branchless code and how does it work?",
			"url": "https://stackoverflow.com/questions/32107088/how-can-i-make-branchless-code-and-how-does-it-work",
			"excerpts": [
			  "I want to show you a more general (and much clearer) way to write branchless code. You can just use ? : operator: int t = data[c]; sum += (t >= 128 ? t : 0);"
			]
		  },
		  {
			"title": "Java Math.min/max performance - ...",
			"url": "https://stackoverflow.com/questions/22752198/java-math-min-max-performance",
			"excerpts": [
			  "Here's a branchless min operation, max can be implemented by replacing diff=a-b with diff=b-a . Copy. public static final long min(final long ..."
			]
		  },
		  {
			"title": "c++ max & min without if or else [closed]",
			"url": "https://stackoverflow.com/questions/72316958/c-max-min-without-if-or-else",
			"excerpts": [
			  "I want a way to cout << maximum & minimum of two numbers with cin >> without if() or else(). I tried before and I just found the if and else way."
			]
		  },
		  {
			"title": "Fast branchless max for unsigned integers - c++",
			"url": "https://stackoverflow.com/questions/17947718/fast-branchless-max-for-unsigned-integers",
			"excerpts": [
			  "I found a trick from the AGGREGATE Magic for fast computing max values. The only problem that this is for integers, and however I have tried some things."
			]
		  },
		  {
			"title": "Transform random integers into range [min,max] without ...",
			"url": "https://stackoverflow.com/questions/65561520/transform-random-integers-into-range-min-max-without-branching",
			"excerpts": [
			  "As far as I know there is no algorithm that generates a random integer in [0, max] when max is not of the form 2^n-1 from 01 uniform generator ..."
			]
		  },
		  {
			"title": "Bit Hacks",
			"url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/cc6983c9ebd77c28e8ae85bc0e575360_MIT6_172F18_lec3.pdf",
			"excerpts": [
			  " You can access\nthem via compiler intrinsics, e.g., in GCC:\nint __builtin_popcount (unsigned int x);\nWarning: You may need to enable certain\ncompiler switches to access built-in functions,\nand your code may be less portable.\nExercise\nCompute the log base 2 of a power of 2 quickly\nusing a popcount instruction.\n© 2008–2018 by the MIT 6.172 Lecturers\n68\nFurther Readin",
			  "\nFurther Reading\nSean Eron Anderson, “Bit twiddling hacks,”\nhttp://graphics.stanford.edu/~seander/bithacks.h\ntml , 2009.\nDon",
			  "int __builtin_popcount (unsigned int x);",
			  "ttp://graphics.stanford.edu/~seander/bithacks.h\ntml",
			  "68",
			  "MIT OpenCourseWare",
			  "6.172 Performance Engineering of Software Systems",
			  "Fall 2018"
			]
		  },
		  {
			"title": ".net - Branchless min/max methods in c# - Stack Overflow",
			"url": "https://stackoverflow.com/questions/66605056/branchless-min-max-methods-in-c-sharp",
			"excerpts": [
			  "Just an FYI for anyone thinking of implementing some \"speedhack\" like this... The runtime and compiler are obviously already aware of optimizations like this and have already applied them for you. You don't need to reinvent the wheel. Quick benchmarks show only miniscule differences between the \"fast\" branchless method, naive `x > y ? x : y` , and `Math.Max()` ."
			]
		  },
		  {
			"title": ".net - Branchless min/max methods in c# - Stack Overflow",
			"url": "https://stackoverflow.com/a/66850544",
			"excerpts": [
			  "You can use bitwise and shift operators like this:\nThe `>>` operator is right shift and I use 31 for `Int` you can use 63 for `long` numbers.",
			  "If you're using the upcoming .NET 7, you can make `Min` and `Max` generic like this:",
			  "However, even with `AgressiveInlining` in IL this causes two calls to `ToInt32` so isn't really more efficient.",
			  "You can see that it doesn't produce any branching instructions: [SharpLab link](https://sharplab.io/:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAJgEYBYAKGIAYACY8gOgCUBXAOwwEt8YLAMIR8AB14AbGFADKMgG68wMXAG4a9BrIAW2KGIAy2YO258BG6pvJIGvHgwCyDgBQOMDbGnuPgASgYAXgA+BgBJADNXbAYAHgZgH29E/ysbOw9nbAR3RxSsgOCwqJj4xJ8krzSaGgBtACleDABxGC4ZZVcMAE8xGAhoj39/AF0MhgAVCMi4yZDXYAgISQZILgATZt4ILh9pjB12/aZiGElcGH8aAG8aAEhiAHYGAFUuXGxIwQBBDY3XLBIqdzpcfO9Pt8WD9cHElitKj0MDAFkC1rstnxdiMrABfWq0Wy+TwuLikPKeAp+QKhGbk2IJKopALpQmZRxOHLkrJUzxFWlRenlKpMmrWaiNZptDpQLq9fqDCkjcZsqZ0uYLeGrdaYnZ7NWHY5q4hnC5XW4PZ7ozbbXYMAD8DENXAYIBBZrxQA===)",
			  "You can see that it doesn't produce any branching instructions: [SharpLab link](https://sharplab.io/:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAJgEYBYAKGIAYACY8gOgCUBXAOwwEt8YLAMIR8AB14AbGFADKMgG68wMXAG4a9BrIAW2KGIAy2YO258BG6pvJIGvHgwCyDgBQOMDbGnuPgASgYAXgA+BgBJADNXbAYAHgZgH29E/ysbOw9nbAR3RxSsgOCwqJj4xJ8krzSaGgBtACleDABxGC4ZZVcMAE8xGAhoj39/AF0MhgAVCMi4yZDXYAgISQZILgATZt4ILh9pjB12/aZiGElcGH8aAG8aAEhiAHYGAFUuXGxIwQBBDY3XLBIqdzpcfO9Pt8WD9cHElitKj0MDAFkC1rstnxdiMrABfWq0Wy+TwuLikPKeAp+QKhGbk2IJKopALpQmZRxOHLkrJUzxFWlRenlKpMmrWaiNZptDpQLq9fqDCkjcZsqZ0uYLeGrdaYnZ7NWHY5q4hnC5XW4PZ7ozbbXYMAD8DENXAYIBBZrxQA===)"
			]
		  },
		  {
			"title": "c++ - Is there a branchless method to quickly find the min/max of two double-precision floating-point values? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/55109204/is-there-a-branchless-method-to-quickly-find-the-min-max-of-two-double-precision",
			"excerpts": [
			  "Is there a branchless method to quickly find the min/max of two double-precision floating-point values?",
			  "All 64-bit x86 processors support [SSE2](https://en.wikipedia.org/wiki/SSE2) ; it is required by the AMD64 extensions. Even most x86 processors without 64-bit support SSE2. It was released in 2000. You'd have to go back a long way to find a processor that didn't support SSE2. But what about if you did? Well, even there, [you get branchless code on most popular",
			  "The `fucomi` instruction performs a comparison, setting flags, and then the `fcmovnbe` instruction performs a conditional move, based on the value of those flags. This is all completely branchless, and relies on instructions introduced to the x86 ISA with the Pentium Pro back in 1995, supported on all x86 chips since the Pentium II.",
			  "I.\nThe only compiler that *won't* generate branchless code here is MSVC, because [it doesn't take advantage of the `FCMOVxx` instruction](https://stackoverflow.com/questions/13661285/generating-cmov-instructions-using-microsoft-compilers/41144749) . Instead, you get:\nNotice the branching `JP` instruction (jump if parity bit set). The `FCOM` instruction is used to do the comparison, which is part of the base x87 FPU instruction set. Unfortunately, this sets flags in the FPU status word, so in order to branch on those flags, they need to be extracted. That's the purpose of the `FNSTSW` instruction, which stores the x87 FPU status word to the general-purpose `AX` register (it could also store to memory, but…why?). The code then `TEST` s the appropriate bit, and branches accordingly to ensure that the correct value is returned. In addition to the branch, retrieving the FPU status word will also be relatively slow. This is why the Pentium Pro introduced the `FCOM` instructions.\nSect",
			  "However, it is *unlikely* that you would be able to improve upon the speed of any of this code by using bit-twiddling operations to determine min/max. There are two basic reasons:"
			]
		  },
		  {
			"title": "c - What is the instruction that gives branchless FP min and max on x86? - Stack Overflow",
			"url": "https://stackoverflow.com/questions/40196817/what-is-the-instruction-that-gives-branchless-fp-min-and-max-on-x86",
			"excerpts": [
			  "Since modern floating-point instruction sets can compute min and max without branches",
			  "I was referring to `minss` / `minsd` ",
			  "gcc` and `clang` will both optimize `(a < b) ? a : b` to `minss` / `minsd` , so I don't bother using intrinsics.",
			  "If you want to use `_mm_min_ss` / `_mm_min_ps` yourself, write code that lets the compiler make good asm even without -ffast-math.",
			  "*Don't try to use `_mm_min_ss` on scalar floats; the intrinsic is only available with `__m128` operands",
			  "So with only SSE2, and we've done a conditional MINPS in two extra instructions (ORPS and MOVAPS, unless loop unrolling allows the MOVAPS to disappear).",
			  "The alternative without SSE4.1 BLENDVPS is ANDPS/ANDNPS/ORPS to blend, plus an extra MOVAPS. ORPS is more efficient than BLENDVPS anyway (it's 2 uops on most CPUs).",
			  "Note that min of an array should use vectors, not scalar; preferably with multiple accumulators to hide FP latency."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/distance/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene, shaders and more."
			]
		  },
		  {
			"title": "computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/mandelbulb/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene, shaders and more."
			]
		  },
		  {
			"title": "Population Count - Chessprogramming wiki",
			"url": "https://www.chessprogramming.org/Population_Count",
			"excerpts": [
			  "[Count bits set in parallel a.k.a.\n ... \nSection Title: References\nCo",
			  "Ramanathan, [Intel](/Intel \"Intel\") , covers SSE4 and popcnt",
			  "[↑]() [__builtin_popcountll](https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html) [GCC](/Free_Software_Foundation \"Free Software Foundation\") Intrinsic\n[↑](",
			  "[Counting bits set](http://www-graphics.stanford.edu/~seander/bithacks.html) from [Bit Twiddling Hacks](http://graphics.stanford.edu/%7Eseander/bithacks.html) by [Sean Eron Anderson](http://graphics.stanford.edu/%7Eseander/)",
			  "[Addison-Wesley](https://en.wikipedia.org/wiki/Addison%E2%80%93Wesley)"
			]
		  },
		  {
			"title": "C Math Hacks : r/cprogramming",
			"url": "https://www.reddit.com/r/cprogramming/comments/1ao9q2y/c_math_hacks/",
			"excerpts": [
			  "Bit Twiddling Hacks by Sean Eron Anderson: https://graphics.stanford.edu/~seander/bithacks.html",
			  "The Art of Computer Programming vol 4 §7.1.",
			  "Note that some of the techniques you’ll see online are useful on older computers but not newer ones. J",
			  "Just beware that some of the really “clever” tricks are sometimes beat by more simple, straightforward techniques."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/",
			"excerpts": [
			  "Section Title: [Inigo Quilez](/)   ::     :: > Indices of useful functions > Renderer/En",
			  "[Avoiding trigonometry III](noatan)",
			  "[Avoiding trigonometry II](sincos)",
			  "[Avoiding trigonometry I](noacos)",
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, math, art, demoscene"
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/noatan/",
			"excerpts": [
			  "This is another example of avoiding trigonometry that came up the other day. I recommend reading [Pat I](../noacos) and [Part II](../sincos) of this miniseries where I propose that using trigonometric functions (sin, cos, tan) or their inverses (asin, acos, atan) in any geometry code or rendering system is always a code smell.",
			  "This time, we’ll look at a challenge that got sent my way when I claimed one can always avoid the rather expensive atan()/atan2() function. The problem is that of snapping a 2D unit vector to the closest 45 degree angle (N, NE, E, SE, S, SW, W, NW). The claim was that you do need to commute the angle of such a vector, round it to the closest 45 degree angle, then reconstruct the vector.",
			  ". Something like this perhaps:\nvec2 snap45( in vec2 v )\n{ const float r = 6.283185 / 8.0 ; // 45 degrees float a = atan (v.y,v.x); // vector to angle float s = r* round (a/r); // snap angle return vec2 ( cos (s), sin (s)); // angle to vector }",
			  "This seems reasonable, and it does work - on the right side you can see how as the vector v in white moves, the yellow vector snaps to the closest 45 degrees direction. The triangular sectors are the basins of attraction of each direction. All good. But my claim was that surely you wouldn’t need trigonometry to achieve the same behavior, and that if this code happened to be in a perf critical path, you’d want to do better."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/",
			"excerpts": [
			  "Section Title: [Inigo Quilez](/) :: :: > Learning\nContent:\n[articles Articles, tutorials, code, and presentations related to computer graphics, maths, fractals, etc.](articles) [video tutorials Video tutorials and live coding sessions (some more than a decade old!), on using maths to create images.](live) [shaders Code related to my articles, many undocumented algorithms and techniques, and most of my procedural artwork and mathematical paintings.](https://www.shadertoy.com/user/iq) [recreational math Often times I find myself playing with and exploring random math ideas or facts. Here are some of them.](maths) [code Code for demos, libraries and utilities that I use, that are not hosted in Shadertoy.](code)",
			  "Section Title: [Inigo Quilez](/) :: :: > Learning > Tools, Products and Projects\nContent:\n[shadertoy A website that I co-founded with Pol Jeremias where you can create, share and learn realtime computer graphics.](https://www.shadertoy.com) [graphtoy A tool for graphing and designing mathematical functions, useful during procedural programming.](https://graphtoy.com) [human shader A collaborative project where 2000 people completed the first ever human brain powered mathematical painting!](https://humanshader.com) [demoscene Archive of all the demoscene pieces I created from 1998 to 2021, mostly featuring realtime rendering and procedural content.](demoscene) [quill I'm the current developer of \"Quill\", the VR painting and hand animation tool responsible for a few award winning VR short films.](https://quill.art/) [project Neo A SDF based 2D and 3D design tool we built at Adobe.](https://projectneo.adobe.com/)"
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/articles/noacos/",
			"excerpts": [
			  "I think we should use less trigonometry in computer graphics. As my understanding of projections, reflections, and vector operations improved over time, I experienced a growing unease every time I saw trigonometry at the core of 3D algorithms inside rendering engines. These days I'm at a point where if I see some asin, acos, atan, sin, cos or tan in the middle of a 3D algorithm, I'll assume the code was written by an inexperienced programmer and that it needs review.",
			  "Now, don't get me wrong. Trigonometry **is** convenient and necessary for data input and for feeding the larger algorithm. What's wrong is when angles and trigonometry suddenly emerge deep in the internals of a 3D engine or algorithm out of nowhere. In other words, where the inputs and the outputs to an algorithm of function are vectors and geometry, and suddenly in the middle of the implementation trigonometry appears. Because, most of the time (if not all of the time) such a thing is unnecessarily complicated, error prone and overall unnecessary. It also gets in the way of elegance and truth, if that matters to you. Let me explain.",
			  "mat3x3 rotationAlign( const vec3 & d, const vec3 & z )\n{ const vec3 ax = cross ( z, d ); const float co = do t( z, d ); const float si = length ( ax ); const vec3 v = ax/si; const float ic = 1.0f - co; return mat3x3 ( v.x*v.x*ic + co, v.y*v.x*ic - si*v.z, v.z*v.x*ic + si*v.y,\nv.x*v.y*ic + si*v.z, v.y*v.y*ic + co, v.z*v.y*ic - si*v.x,\nv.x*v.z*ic - si*v.y, v.y*v.z*ic + si*v.x, v.z*v.z*ic + co );\n}\nGood, ",
			  "Good, no **normalize** (), no **acos** (), no **clamp** () anymore. We are definitely getting somewhere now. Let's keep going and also get rid of the length/square root operation by noticing that **si** 2 =1- **co** 2 (since sine and cosine lay on the unit circle) and by propagating the 1/si term into the matrix and getting some cancellations. What we get is:\nmat3x3 rotationAlign( const vec3 & d, const vec3 & z )\n{ const vec3 v = cross ( z, d ); const float c = dot ( z, d ); const float k = ( 1.0f -c)/( 1.0f -c*c); return mat3x3 ( v.x*v.x*k + c, v.y*v.x*k - v.z, v.z*v.x*k + v.y,\nv.x*v.y*k + v.z, v.y*v.y*k + c, v.z*v.y*k - v.x,\nv.x*v.z*K - v.y, v.y*v.z*k + v.x, v.z*v.z*k + c );\n}"
			]
		  },
		  {
			"title": "8088 Domination Post-Mortem, Conclusion",
			"url": "https://trixter.oldskool.org/2014/06/20/8088-domination-post-mortem-conclusion/",
			"excerpts": [
			  "The major factor in optimizing 8088 code for speed is minimizing memory accesses, because the CPU takes 4 cycles to read (or write) a byte — any ...Read more"
			]
		  },
		  {
			"title": "BitScan",
			"url": "https://www.chessprogramming.org/BitScan",
			"excerpts": [
			  "A 64-bit De Bruijn Sequence contains 64-overlapped unique 6-bit sequences, thus a circle of 64 bits, where five leading zeros overlap five hidden \"trailing\" ..."
			]
		  },
		  {
			"title": "Inigo Quilez :: computer graphics, maths, shaders, fractals, demoscene",
			"url": "https://iquilezles.org/blog/page_201409.php",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, demoscene, shaders and more."
			]
		  },
		  {
			"title": "SSE for CPU tracers",
			"url": "https://iquilezles.org/articles/sse/",
			"excerpts": [
			  "Tutorials and articles of Inigo Quilez on computer graphics, fractals, demoscene, shaders and more."
			]
		  },
		  {
			"title": "GitHub - jdbruijn/bit-twiddling-hacks: Implementation of Sean Anderson's Bit Twiddling Hacks",
			"url": "https://github.com/jdbruijn/bit-twiddling-hacks",
			"excerpts": [
			  "Documentation can be found in the [documentation page](http://vidavidorra.github.io/BitOperations/html/index.html) , and for the\nfunction macros and functions on the [BitOperations.h documentation page](http://vidavidorra.github.io/BitOperations/html/BitOperations_8h.html) .",
			  "For the functions that are based on Sean Eron Anderson's [Bit Twiddling Hacks](https://graphics.stanford.edu/~seander/bithacks.html) the arithmetics are explained on Joe Gibson's [markdown version](https://github.com/gibsjose/BitHacks/blob/master/BitHacks.md) of\nSean Eron Anderson's Bit Twiddling Hacks."
			]
		  },
		  {
			"title": "Bit Twiddling Hacks - HackMD",
			"url": "https://hackmd.io/@derailment/bit-twiddling-hacks",
			"excerpts": [
			  "Bit Twiddling Hacks"
			]
		  },
		  {
			"title": "LZ4 on the 8088: One small drop « Oldskooler Ramblings",
			"url": "https://trixter.oldskool.org/2013/02/09/lz4-on-the-8088-one-small-drop/",
			"excerpts": [
			  "I thought I had squeezed every drop of blood from the stone that is LZ4 decompression on the 8088, but with some help from Peter Ferrie and Terje Mathisen, we’ve managed to improve the decompression speed by another 1%.  1% may seem laughable, but believe me, it’s quite an accomplishment if you followed [my previous 3-part series on optimizing for the 8088](https://trixter.oldskool.org/2013/01/10/optimizing-for-the-8088-and-8086-cpu-part-1/) ."
			]
		  },
		  {
			"title": "Optimizing for the 8088 and 8086 CPU: Part 1 « Oldskooler Ramblings",
			"url": "https://trixter.oldskool.org/2013/01/10/optimizing-for-the-8088-and-8086-cpu-part-1/",
			"excerpts": [
			  "There is a small but slowly growing hobby around retroprogramming for old PCs and compatibles.",
			  "This hobby has existed for decades for other platforms, as evidenced by the active demoscenes on [each](http://www.pouet.net/prodlist.php?platform%5B%5D=Commodore+64&order=&x=23&y=4&page=1&order=) [retro](http://www.pouet.net/prodlist.php?platform%5B%5D=ZX+Spectrum&order=&x=31&y=7&page=1&order=) [platform](http://www.pouet.net/prodlist.php?platform%5B%5D=Atari+XL%2FXE&order=&x=28&y=7&page=1&order=) , but the IBM PC (and other 4.77MHz 8088 compatibles) has only recently started to gain that same sort of attention.",
			  "Section Title: [Oldskooler Ramblings](https://trixter.oldskool.org/) > Optimizing for the 8088 and 8086 CPU: Part 1",
			  "Before we begin, let me clarify that while I’m using “8088” throughout most of this text, what I am writing applies equally to the 8086 as well. The 8086 and 8088 are functionally identical, with the 8086 being slightly faster due to a having a 16-bit bus and a larger prefetch queue, both of which are covered later.",
			  "Contrary to what you might think about a CPU old enough to be Justin Bieber’s father, it is possible to wring acceptable speed out of an 8088 if you understand the situation most 8088s are forced into (the slow RAM of the IBM PC) and how to deal with it, as well as the CISC-like advantages the chip has.",
			  "By understanding both, it is possible to write assembly code that can run faster than the best 6502 or Z80 code at similar clock speeds."
			]
		  },
		  {
			"title": "Lifetime-Sensitive Modulo Scheduling in a Production Environment | IEEE Transactions on Computers",
			"url": "https://dl.acm.org/doi/10.1109/12.910814",
			"excerpts": [
			  "Swing Modulo Scheduling (SMS). It generates schedules that are near optimal in terms of initiation interval, register requirements, and stage count.",
			  "This paper presents a novel software pipelining approach, which is called Swing Modulo Scheduling (SMS).",
			  "SMS is compared with other heuristic methods, showing that it outperforms them in terms of the quality of the obtained schedules and compilation time.",
			  "Swing Modulo Scheduling is a heuristic approach that has a low computational cost.",
			  "This paper first describes the technique and evaluates it for the Perfect Club benchmark suite on a generic VLIW architecture.",
			  "To further explore the effectiveness of SMS, the experience of incorporating it into a production quality compiler for the Equator MAP1000 processor is described; implementation issues are discussed, as well as modifications and improvements to the original algorithm.",
			  "Finally, experimental results from using a set of industrial multimedia applications are presented."
			]
		  },
		  {
			"title": "17-18-vliw.pdf",
			"url": "https://www.cs.cmu.edu/afs/cs/academic/class/15740-f18/www/lectures/17-18-vliw.pdf",
			"excerpts": [
			  "Hyperblock: A single-entry, multiple-exit block with internal\ncontrol flow eliminated using predication (if-conversion)",
			  "\nHyperblock formation\n1. Block selection\n2. Tail duplication\n3. If-conversion\n"
			]
		  },
		  {
			"title": "TMS320C6000 Assembly Language Tools v 7.4 User's ...",
			"url": "http://www.ihsys.com/ohsspds/Documents/TI_TMS320C6000_Assembly_Language_Tools.pdf",
			"excerpts": [
			  "... Optimizing Compiler v7.4 User's Guide. Describes the TMS320C6000 C compiler and the assembly optimizer. This C compiler accepts ANSI standard C source code ...Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Programmer's Guide",
			"url": "http://bitsavers.informatik.uni-stuttgart.de/components/ti/TMS320xx/320C6000/SPRU198G_TMS320C6000_Programmers_Guide_200208.pdf",
			"excerpts": [
			  "he C64x provides double-word load and store instructions (LDDW and\nSTDW) which can access 64 bits of data at a time. Up to two double-word load\nor store instructions can be issued every cycle. This provides a peak band-\nwidth of 128 bits per cycle to on-chip memory"
			]
		  },
		  {
			"title": "Efficient formulation for optimal modulo schedulers",
			"url": "https://dl.acm.org/doi/pdf/10.1145/258915.258933",
			"excerpts": [
			  "We present the performance characteristics of the four modulo scheduling algorithms with the struc- tured formulation in Table 1. In addition to the number.Read more"
			]
		  },
		  {
			"title": "Fast enumeration-based modulo scheduling heuristic for VLIW architectures | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Fast-enumeration-based-modulo-scheduling-heuristic-Bahtat-Belkouch/34f94fb3997e68c55d4f71b8697b4747d015c71f",
			"excerpts": [
			  "IEEE Trans. Computers. 2001. TLDR. This paper first describes the Swing Modulo Scheduling technique and evaluates it for the Perfect Club benchmark suite on a ..."
			]
		  },
		  {
			"title": "TI 'C6x dispatch pipeline stage with the addition of ...",
			"url": "https://www.researchgate.net/figure/TI-C6x-dispatch-pipeline-stage-with-the-addition-of-the-Modulo-Schedule-Buffers-and_fig4_3940504",
			"excerpts": [
			  "This paper describes a new architectural mechanism, called the Modulo Schedule Buffers, that provides an elegant interface for the execution of modulo scheduled ...Read more"
			]
		  },
		  {
			"title": "Modulo schedule buffers | Proceedings of the 34th annual ACM/IEEE international symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.5555/563998.564018",
			"excerpts": [
			  "This paper describes a new architectural mechanism, called the Modulo Schedule Buffers, that provides an elegant interface for the execution of modulo scheduled ...Read more"
			]
		  },
		  {
			"title": "Software pipelining via rotating registers",
			"url": "https://cs.ijs.si/courses/processor/Chapter4/tsld159.htm",
			"excerpts": [
			  "Software pipelining via rotating registers. Software pipelining - improves performance by overlapping execution of different software loops - execute more ..."
			]
		  },
		  {
			"title": "A comparative study of modulo scheduling techniques",
			"url": "https://dl.acm.org/doi/pdf/10.1145/514191.514208",
			"excerpts": [
			  "**A comparative study of modulo scheduling techniques**",
			  "2.2.4*\n*Swing Modulo Scheduling*\nThe\nSwing\nMo\ndulo\nSc\nheduling\n(SMS)\nsc\nheme\nw\nas\npro-\np\nosed\nb\ny\nLlosa,\nGonz�\nalez,\nAyguad\n�\ne\nand\nV\nalero\n[\u00020].\nI",
			  "A comparative study of modulo scheduling techniques"
			]
		  },
		  {
			"title": "Register allocation for software pipelined loops",
			"url": "https://dl.acm.org/doi/pdf/10.1145/143103.143141",
			"excerpts": [
			  "Software pipelining is an important instruction scheduling\ntechnique for efficiently overlapping successive iterations of\nlooPs\nand executing\nthem\nin parallel.",
			  "Modulo\nscheduling\nis applicable\nto RISC,\nCISC,\nsuperscalar,\nsuperpipelined,\nand VLIW processors,\nand is useful whenever\na\nprocessor\nimplementation\nhas\nparallelism\neither\nby\nhaving",
			  "Permission",
			  "to",
			  "copy",
			  "without",
			  "fee"
			]
		  },
		  {
			"title": "Allocating rotating registers by scheduling | Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1145/2540708.2540738",
			"excerpts": [
			  "A rotating register file is an architectural support for software pipelining, where\nmany registers can be renamed at once when a rotating branch is executed.",
			  "L.-F. Chao, A. S. LaPaugh, and E. H.-M. Sha. Rotation scheduling: a loop pipelining algorithm."
			]
		  },
		  {
			"title": "Using Intel® Developer on Itanium® Architecture for ...",
			"url": "https://wwwuser.gwdguser.de/~parallel/parallelrechner/altix_documentation/Vtune_TP.pdf",
			"excerpts": [
			  "– Itanium Reference Manual for Software Developers. – Itanium-2 Reference Manual for Software Optimization. – Introduction to Micro-architectural Software ...Read more"
			]
		  },
		  {
			"title": "KeyStone Training KeyStone C66x CorePac Instruction Set ...",
			"url": "https://www.ti.com.cn/content/dam/videos/external-videos/en-us/1/3816841626001/3877481353001.mp4/subassets/instruction-set-architecture-presentation.pdf",
			"excerpts": [
			  "Datapaths of the .L and .S units have been increased from 32-bit to 64-bit. • Datapaths of the .M units have been increased from. 64-bit to 128-bit.Read more"
			]
		  },
		  {
			"title": "TMS320C6000 Optimizing C/C++ Compiler v8.3.x",
			"url": "https://www.ti.com/lit/ug/sprui04d/sprui04d.pdf",
			"excerpts": [
			  "**4.6.2 Software Pipelining Information**",
			  "The compiler embeds software pipelined loop information in the .asm file. This information is used to optimize\nC/C++ code or linear assembly code.",
			  "Example 4-1 illustrates the information that is\ngenerated for each loop.",
			  "**4.6.2.1 Software Pipelining Information Terms**",
			  "op unroll factor.** The number of times the loop was unrolled specifically to increase performance.",
			  "nown minimum trip count.** The minimum number of times the loop will be executed",
			  "own maximum trip count.** The maximum number of times the loop will be executed.",
			  "Known max trip count factor.** Factor that would always evenly divide the loops trip count. This information\ncan be used to possibly unroll the loo",
			  " dependency bound.** The distance of the largest loop carry path. A loop carry path occurs\nwhen one iteration of a loop writes a value that must be read in a future iteration. Instructions that are part of\nthe loop carry bound are marked with the ^ symbol.",
			  "nitiation interval** (ii). The number of cycles between the initiation of successive iterations of the loop. The\nsmaller the initiation interval, the fewer cycles it takes to execute a loop",
			  "nitiation interval** (ii). The number of cycles between the initiation of successive iterations of the loop. The\nsmaller the initiation interval, the fewer cycles it takes to execute a loop",
			  "The --debug_software_pipeline option adds additional information displaying the register usage at each cycle of\nthe loop kernel and displays the instruction ordering of a single iteration of the software pipelined loop.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute in\nparallel.",
			  "Software pipelining is a technique use to schedule from a loop so that multiple iterations of a loop execute in\nparallel."
			]
		  },
		  {
			"title": "Software Pipelining Irregular Loops On the TMS320C6000 VLIW ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/384198.384216",
			"excerpts": [
			  "*\nSoftware pipelining is a powerful compile-time transformation\nfor optimizing loops to aggressively exploit the many functional\nunits on today’s VLIW and superscalar architectures [2,5]. This\noptimization is essential for maximizing the performance of the\nTexas Instruments TMS320C6000 [7,8], a family of VLIW\nprocessors targeted at Digital Signal Processing (DSP)\napplications.",
			  "In contrast to what one might\nexpect, we found that the best heuristic for choosing between\nthe two techniques was to select register copying over\npredication whenever possible.",
			  "predication and a new technique called register copying to\ntransform irregular loops into loops that can be easily pipelined\nusing our existing infrastructure.",
			  "Such\nhardware is not currently available in the TMS320C6000\narchitecture family.",
			  "Although there are only a limited number of irregular loops in\nthe DSP domain, there are several important application areas\nwhere performance-critical loops are irregular.",
			  "However, we are not aware of previous research that exploits\nconditional moves to enable irregular loop pipelining.",
			  "However, we are not aware of previous research that exploits\nconditional moves to enable irregular loop pipelining.",
			  "\nRegister moves have been previously used in other contexts to\nfacilitate software pipelining in many other ways — perhaps the\nmost well known being an alternative to modulo variable\nexpansion [2] for preventing excessively long register lifetimes.",
			  "\nRegister moves have been previously used in other contexts to\nfacilitate software pipelining in many other ways — perhaps the\nmost well known being an alternative to modulo variable\nexpansion [2] for preventing excessively long register lifetimes.",
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "Our irregular\nloop software pipelining strategy provides significant speedups\nfor most of these irregular loops.",
			  "In this paper, we discuss our software-based preprocessing\ntechnique that allows compilers to software pipeline irregular\nloops on architectures with no special support beyond the ability\nto predicate (conditionally nullify) instructions using a small\nnumber of static registers.",
			  "More recently, researchers have extended software pipelining to\nthe more general class of *irregular* (WHILE) loops which\nterminate when an arbitrary condition occurs, possibly mid-\niteration [3,6,11].",
			  "Traditional software pipelining is only applicable\nto *regular* (FOR) loops. Loops of this type are each controlled\nby a regularly updated counter and terminate when the counter\nreaches a value that was established before loop execution\nbegan."
			]
		  },
		  {
			"title": "Accelerating control-flow intensive code in spatial hardware",
			"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-870.pdf",
			"excerpts": [
			  "Pointer Arithmetic and Memory Disambiguation",
			  "Overcoming the performance limitations due to explicit control-flow is the key issue\nthat needs to be addressed for custom hardware to become performance-competitive with\nconventional processors on sequential code [BAG05].",
			  "thesis, we found that when the resource utilization is already high – because of either high\ninstruction level parallelism in the design or as a result of loop unrolling – the control and\ninterconnect (multiplexing) costs of further loop unrolling or loop pipelining outweigh the\ngains achieved in performance. This is because frequently the longest combinational path\nin the circuit (the critical path length) increases so much that the input to output circuit\ndelay becomes worse”.",
			  "Together with the use of data-dependent control-flow, the implication of\nsuch pointer arithmetic is that the precise sequence of locations that a pointer points-to\nat runtime cannot be determined at compile-time, except in the most trivial cases (i.e.\nstatic points-to analysis is undecidable [Lan92]). Pointer arithmetic can therefore often\nobfuscate the data dependence between producers and consumers, leaving the potential\nfor a true or name dependence that can only be resolved at runtime, when the values of\npointer variables are known. This is the ‘unknown address problem’ [PMHS85].",
			  "Pointer arithmetic is an established feature of many imperative languages (especially C\nand C++)."
			]
		  },
		  {
			"title": "program demultiplexing: data-flow based speculative",
			"url": "https://ftp.cs.wisc.edu/sohi/theses/saisantosh.pdf",
			"excerpts": [
			  "This dissertation proposes Program Demultiplexing or PD, a speculative parallelization model that has two novel contributions. In PD, a speculative thread is ...Read more"
			]
		  },
		  {
			"title": "Speculative Trace Scheduling in VLIW Processors",
			"url": "https://www.researchgate.net/publication/2843206_Speculative_Trace_Scheduling_in_VLIW_Processors",
			"excerpts": [
			  "Traces are speculatively scheduled on the processor based on their probability of execution. We have developed a tool \"SpliTree\" to generate traces ..."
			]
		  },
		  {
			"title": "Predicated software pipelining technique for loops with conditions",
			"url": "https://www.researchgate.net/publication/3744185_Predicated_software_pipelining_technique_for_loops_with_conditions",
			"excerpts": [
			  "PDF | An effort to formalize the process of software pipelining loops with conditions is presented. A formal framework for scheduling such loops, based."
			]
		  },
		  {
			"title": "The Bene t of Predicated Execution for Software Pipelining",
			"url": "http://impact.crhc.illinois.edu/shared/papers/hicss-93-ppredicate.pdf",
			"excerpts": [
			  "Lam\nprop\noses\nmo\ndulo\nv\nariable\nexpansion\nand\nhierarc\nhical\nreduction\nfor\nhandling\nregister\nallo\ncation\nand\ncondi-\ntional\nconstructs\nresp\nectiv\nely\n[\n].",
			  "o\ndulo\nSc\nheduling\nIn\na\nmo\ndulo\nsc\nheduled\nsoft\nw\nare\npip\neline,\na\nlo\nop\nit-\neration\nis\ninitiated\nev\nery\nII\ncycles,\nwhere\nII\nis\nthe\nIni-\ntiation\nInterva"
			]
		  },
		  {
			"title": "Loop transformations for clustered VLIW architectures",
			"url": "https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=1180&context=etds",
			"excerpts": [
			  "The transformed loops exploit considerable parallelism and data locality, and therefore are amenable to software pipelining to generate high performance code.Read more"
			]
		  },
		  {
			"title": "A Clustered VLIW Architecture Based",
			"url": "https://era.ed.ac.uk/bitstream/1842/366/1/fernandes.PDF",
			"excerpts": [
			  "Software pipelining generally increases register pressure in VLIW machines. The register file required in such cases may compromise scalability, which has ...Read more"
			]
		  },
		  {
			"title": "phd-thesis-matai.pdf - Kastner Research Group",
			"url": "https://kastner.ucsd.edu/wp-content/uploads/2013/08/admin/phd-thesis-matai.pdf",
			"excerpts": [
			  "irregular kernels have properties\nsuch as indirect memory access, pointer chasing that are hard for modern high-level\nsynthesis tools to parallelize.",
			  "irregular kernels are hard to\nsynthesis with modern high-level synthesis tools than applications that purely contain\n89\nregular kerne",
			  "One solution to this problem is finding the irregular kernel\nbottlenecks and making them high-level synthesis friendly by restructuring the portion of\nthe kernel that causes problem.",
			  "It is often involves completely re-writing (for restructuring purposes) the whole kernel\nor application from scratch."
			]
		  },
		  {
			"title": "Alternative formats If you require this document in an alternative ...",
			"url": "https://purehost.bath.ac.uk/ws/portalfiles/portal/187945987/main_thises.pdf",
			"excerpts": [
			  "Rangan et al. (2004) introduced a new technique to utilize a decoupled software pipeline\nfor optimizing the performance of recursive data structures (RDS)(e.g., linked lists, trees and\ngraphs).",
			  "For this kind of structure difficulties have been encountered when trying to execute it\nin parallel, because the instructions of a given iteration of a loop depend on the pointer value\nthat is loaded from a previous iteration.",
			  "Therefore to address this problem, a decoupled software\npipeline has been used so as to avoid stalls that are happening with the long variable-latency\ninstruction in RDS loops.",
			  "It is clear that RDS loops consist of two parts. The first contains\nthe traversal code (critical path of execution) and the second represents the computation that\nshould be carried out on each node traversed by the first part."
			]
		  },
		  {
			"title": "(PDF) The Superblock: An Effective Technique for VLIW and Superscalar Compilation",
			"url": "https://www.researchgate.net/publication/220358960_The_Superblock_An_Effective_Technique_for_VLIW_and_Superscalar_Compilation",
			"excerpts": [
			  "A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware. However, ILP within basic blocks is extremely limited for control-intensive programs. We have developed a set of techniques for exploiting ILP across basic block boundaries. These techniques are based on a novel structure called thesuperblock. The superblock enables the optimizer and scheduler to extract more ILP along the important execution paths by systematically removing constraints due to the unimportant paths.",
			  "Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler. Superblock optimizations and scheduling are shown to be useful while taking into account a variety of architectural features."
			]
		  },
		  {
			"title": "Trace Scheduling",
			"url": "https://www.researchgate.net/publication/303672755_Trace_Scheduling",
			"excerpts": [
			  "A compiler for VLIW and superscalar processors must expose sufficient instruction-level parallelism (ILP) to effectively utilize the parallel hardware.",
			  "Superblock optimization and scheduling have been implemented in the IMPACT-I compiler. This implementation gives us a unique opportunity to fully understand the issues involved in incorporating these techniques into a real compiler."
			]
		  },
		  {
			"title": "Class 12 Software Pipelining - Modulo Scheduling",
			"url": "https://www.eecs.umich.edu/courses/eecs583/slides/Lecture12.pdf",
			"excerpts": [
			  "Iterative Modulo Scheduling: An Algorithm for Software Pipelining Loops”, B.\nRau, MICRO-27, 1994, pp. 63-74.\n",
			  " Rotating registers\n",
			  "Itanium-style hardware\nsupport, then remove it later\n»"
			]
		  },
		  {
			"title": "Monica S. Lam",
			"url": "https://scholar.google.com.hk/citations?user=4hS0jZ8AAAAJ&hl=th",
			"excerpts": [
			  "Software pipelining: An effective scheduling technique for VLIW machines. M Lam. Proceedings of the ACM SIGPLAN 1988 conference on Programming Language …, 1988."
			]
		  },
		  {
			"title": "A hierarchical parallelizing compiler for VLIW/MIMD machines",
			"url": "https://link.springer.com/chapter/10.1007/3-540-57502-2_39",
			"excerpts": [
			  "Monica S. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proceedings ACM SIGPLAN '91 Conference on ...Read more"
			]
		  },
		  {
			"title": "an effective scheduling technique for VLIW machines",
			"url": "https://semopenalex.org/work/W4232919122",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop ...Read more"
			]
		  },
		  {
			"title": "Iterative Modulo Scheduling - B. Ramakrishna Rau",
			"url": "https://courses.engr.illinois.edu/ece511/fa2005/papers/Rau.1994.MICRO.pdf",
			"excerpts": [
			  "The objective of modulo scheduling is to engineer a schedule for one iteration' of the loop such that when this same schedule is repeated at regular intervals, ...Read more"
			]
		  },
		  {
			"title": "A Programmable Memory Hierarchy for Prefetching Linked ...",
			"url": "https://users.cs.duke.edu/~alvy/papers/ishpc02.pdf",
			"excerpts": [
			  "Pointer dereferences are required to generate addresses for successive elements in a linked data structure (LDS). This is commonly called the pointer- chasing ...Read more"
			]
		  },
		  {
			"title": "Behavior Specialized Processors by Tony Nowatzki",
			"url": "https://research.cs.wisc.edu/vertical/papers/thesis/tony.pdf",
			"excerpts": [
			  "The region selection heuristic performs a bottom-up tree traversal, selecting code which has the highest dynamic to static instruction ratio. TDG Transform ...Read more"
			]
		  },
		  {
			"title": "Compiler-Hardware Co-Design in High-Level Synthesis",
			"url": "https://theses.gla.ac.uk/85229/2/2025szafarczykphd.pdf",
			"excerpts": [
			  "predictable memory addresses are not the only source of irregularity. Unpredictable con-\ntrol flow can cause similar problems. Consider the loop in Listing 1.2 for example.\n**Listing 1.2:** Another example of an irregular code (control-dependent reduction).\nfor ( int i = 0; i < N; ++i)\nif (condition)\nreduction += f(reduction);\nHere, we have a clear inter-iteration dependency on the reduction variable—assuming that\nthe f() function takes more than one clock cycle, the compiler cannot pipeline the loop with\nan II of 1. The static schedule needs to allocate enough cycles in the loop to calculate the\nf(reduction) value before the next iterations starts, because it might need the result of this\ncalculation. Crucially, this schedule is rigid and has to be repeated for every loop iteration.",
			  "The above problems are the same challenges that were burdening compiler writers for Very\nLarge Instruction Word (VLIW) machines a few decades ago [140, 86]. In VLIW machines, as\nin FSMD HLS, the goal of discovering ILP lies entirely on the shoulders of the compiler, with\nthe difference that in VLIW the compiler decides which instructions can execute together by\npacking them in a single instruction word, instead of merging FSM states. At a high level, the\nproblem of scheduling irregular code in VLIW and FSMD HLS is essentially the same. In both\ncases, the compiler has limited information about the dependencies and control flow in the\ncode and has to assume a worst-case scenario, sequentializing most of the execution.",
			  "Today, VLIW machines are mainly used in domains that contain regular codes [147], such as\nsignal processing or machine learning. Most general-purpose processors are not VLIW ma-\nchines; they are RISC machines that contain many hardware structures to enable dynamic,\nsuperscalar, out-of-order execution. Similarly, an efficient HLS of irregular codes calls for\nadditional hardware structures to guide circuit execution at runtime, at which time the data\ndependencies and control-flow become know",
			  "ynamic Dataflow (DDF) HLS forgoes static scheduling completely, leaving the job of schedul-\ning to *latency-insensitive handshaking logic* that determines the execution of operators at\nruntime [126, 74]. Latency-insensitive channels allow for communication between a sender\nand receiver without having to rely on their cycle behavior [87]. Each operator in a dataflow\ncircuit has a wrapper with handshaking logic that stalls the operator until all inputs have\n ... \nis different in that we decouple only address generating instructions and speculate only on\nmemory operatio",
			  "peculation has its roots in compilers for Very Large Instruction Word (VLIW) ma-\nchines. Instruction scheduling in HLS is very similar to VLIW scheduling (no hardware sup-\nport for speculation, static mapping to functional units, etc.), with many algorithms like\nmodulo-scheduling and *if* -conversion originally developed for VLIW directly applicable to\nHLS [196, 3, 178]. Most recently, predicated execution in the form of gated SSA was proposed\nfor HLS with speculation support [97]. The speculation support in this and other works re-\nquires costly recovery on mis-speculation [127, 220, 14, 154, 93, 231]. Efficiently squashing\nspeculative computation on the wrong paths in a spatial dataflow architecture is hard, be-\ncause the architectural state is distributed [32]. Our speculative DAE sidesteps this issue, not\nrequiring any recovery: we speculate early (run ahead) in the AGU, and later handle mis-\nspeculations in the CU by taking an appropriate path in its CFG.",
			  "Control Flow Handling in GPUs",
			  "trol-flow handling in GPUs is most commonly implemented via predication. The al-\ngorithms used to calculate predicate masks and re-convergence points bear a striking re-\nsemblance to our work [148]. The Single Instruction Multiple Threads (SIMT) stack approac",
			  "In VLIW machines, as\nin FSMD HLS, the goal of discovering ILP lies entirely on the shoulders of the compiler, with\nthe difference that in VLIW the compiler decides which instructions can execute together by\npacking them in a single instruction word, instead of merging FSM states.",
			  "ltimately, the problem of scheduling VLIW machines for general purpose code, which con-\ntains many of the irregularities that we highlight here, was recognized to be intractable",
			  "ltimately, the problem of scheduling VLIW machines for general purpose code, which con-\ntains many of the irregularities that we highlight here, was recognized to be intractable",
			  "Most recently, predicated execution in the form of gated SSA was proposed\nfor HLS with speculation support [97].",
			  "ynamic Dataflow (DDF) HLS forgoes static scheduling completely, leaving the job of schedul-\ning to *latency-insensitive handshaking logic* that determines the execution of operators at\nruntim",
			  "ntrol speculation has its roots in compilers for Very Large Instruction Word (VLIW) ma-\nchines.",
			  "nstruction scheduling in HLS is very similar to VLIW scheduling (no hardware sup-\nport for speculation, static mapping to functional units, etc.), with many algorithms like\nmodulo-scheduling and *if* -conversion originally developed for VLIW directly applicable to\nHL",
			  "The static schedule needs to allocate enough cycles in the loop to calculate the\nf(reduction) value before the next iterations starts, because it might need the result of this\ncalculation."
			]
		  },
		  {
			"title": "Separation Logic for High-Level Synthesis - Spiral - Imperial College ...",
			"url": "https://spiral.imperial.ac.uk/bitstreams/420906a2-445a-411c-881c-dba6edcba25e/download",
			"excerpts": [
			  "Inter-Iteration Dependencies and Pipelining",
			  "Apart from replication, acceleration of the manual RTL design is obtained from pipelining\nthe tree traversal. This corresponds to pipelining the loop nest in Listing 3 which must take\ntwo (potential) inter-iteration dependencies into account.",
			  "The first occurs between fetching\npointers to data from the stack and pushing new pointers onto the stack, which hinders\npipelining.",
			  "However, because there are two push statements and one fetch statement, the\nitems stored on the stack (pointers *u* and *Z* , *d* ) accumulate if the condition in Line 9 holds\nin several iterations.",
			  "Once there are multiple pointers on the stack, these do not cause\nany read-write dependencies between iterations and hence can be overlapped in pipelined\nexecution.",
			  "Listing 4 shows a transformation of the loop in Listing 3 to implement this\nschedule.",
			  "The transformation distributes the execution of the original loop body over two\n(pipelineable) inner loops which exchange data via a newly inserted queue.",
			  "etail and elaborate on their implications for e ffi cient HLS. To this end, we present a case\nstudy comparing the implementations of two algorithms for a compute-intensive machine\nlearning application ( *K* -means clustering). A",
			  "the latency is initially degraded\nby 26 . 6 ⇥ compared to the hand-crafted RTL implementation. We narrow this significant\nperformance gap and improve the former latency by 8 ⇥ with source code transformations\nthat partition and privatise data structures accessed through pointers to enable paral-\nlelisation and pipelining of the loop traversing the pointer-linked data structure."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads",
			  "In this paper, we propose an effective, *fully automatic* ap-\nproach to non-speculative thread extraction, called *Decou-*\n*pled Software Pipelinin",
			  "igure 1. A simple linked list traversal loop**\n**executed as DOACROSS and DS"
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining | Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture",
			"url": "https://dl.acm.org/doi/10.1109/MICRO.2005.13",
			"excerpts": [
			  "Automatic Thread Extraction with Decoupled Software Pipelining > Abstract",
			  "DSWP exploits the finegrained pipeline parallelism lurking in most applications to extract long-running, concurrently executing threads.",
			  "Use of the non-speculative and truly decoupled threads produced by DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing inter-core communication and per-core resource requirements.",
			  "Automatic Thread Extraction with Decoupled Software Pipelining > References",
			  "20} R. Rangan, N. Vachharajani, M. Vachharajani, and D. I. August. Decoupled software pipelining with the synchronization array. In *Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques* , pages 177-188, September 2004.\n",
			  "21} K. Rich and M. Farrens. Code partitioning in decoupled compilers. In *Proceedings of the 6th European Conference on Parallel Processing* , pages 1008-1017, Munich, Germany, September 2000.\n"
			]
		  },
		  {
			"title": "Improving Hash Join Performance through Prefetching",
			"url": "http://www.pdl.cmu.edu/ftp/Database/icde04.pdf",
			"excerpts": [
			  "**Challenges in Applying Prefetching to Hash Join.**",
			  "A\nnaive approach to prefetching for hash join might simply try\nto hide the latency within the processing of a single tuple.\nFor example, to improve hash table probing performance,\none might try to prefetch hash bucket headers, hash buck-\nets, build tuples, etc. Unfortunately, such an approach would\nhave little benefit because later memory references often de-\npend upon previous ones (via pointer dereferences). Existing\ntechniques for overcomingthis *pointer-chasing problem* [18]\nwill not work because the randomness of hashing makes it\nimpossible to predict the memory locations to be prefetched."
			]
		  }
		],
		"reasoning": "The executive summary outlines several advanced techniques for VLIW instruction scheduling, directly addressing the user's request for obscure and advanced methods. It emphasizes **Hyperblock Formation with Predicated Execution** as a key strategy for managing control flow, referencing the Itanium and Hexagon architectures as examples. This technique is crucial for handling data-dependent branches and creating larger scheduling regions. The summary also highlights **Enhanced Modulo Scheduling (EMS)** as a software-only alternative for irregular control flow, and **Decoupled Software Pipelining (DSWP)** for tackling memory latency in irregular workloads like tree traversals by separating access and execution stages. For long dependency chains, such as those found in hashing, **Multi-Stream Interleaving** is proposed to fill VLIW slots by processing independent data streams concurrently. The summary also points to **Bit-Slicing** as a specialized technique for cryptographic workloads, transforming them into massively parallel bitwise operations. Furthermore, it mentions the **DFAPacketizer** from Qualcomm Hexagon as a blueprint for managing heterogeneous functional units, and the value of **manual loop unrolling**, compiler pragmas like `MUST_ITERATE`, and alignment assertions (`_nassert`) for guiding schedulers. Finally, it suggests repurposing fault-tolerant VLIW research on **instruction replication** to speculatively execute code in idle slots, thereby hiding latency or eliminating inter-cluster communication bottlenecks, citing performance gains.",
		"confidence": "high"
	  },
	  {
		"field": "software_managed_memory_strategies.key_technique",
		"citations": [
		  {
			"title": "Optimizing DMA Data Transfers for Embedded Multi-Cores",
			"url": "http://www-verimag.imag.fr/~maler/Papers/thesis-selma.pdf",
			"excerpts": [
			  "Double buffering or more generally multi-buffering is a well known programming technique\nreferred to as *software pipelining* used to hide memory latency. The main idea is to overlap com-\nputations and data transfers and it is expressed at the software level.",
			  "This takes advantage from the fact that DMA can work in parallel with a processor. Pro-\ngram 2.4.2 uses *double buffering* for input blocks *B* *x* [0] , *B* *x* [1] and output blocks *B* *y* [0] , *B* *y* [1] . At\neach iteration before the processor starts working on the current buffer, it issues an *asynchronous*\nDMA call to fetch the next buffer. Therefore, the processor can work on a super block *j* residing\nin one buffer while the DMA brings *in parallel* the super block *j* + 1 to the other buffer. Note that\n*comp* ( *j* ) is used as a shorthand for the inner double loop in Program 2.4.1. In practice, events are\nassociated with each *dma_get* and *dma_put* command to ensure the synchronization between each\ndata transfer and its corresponding computation.",
			  "Hence, program 2.4.2 defines a *software pipeline* with 3 stages: input of super block ( *j* + 1) ,\ncomputation on super block *j* and output of super block ( *j* *−* 1) , see Figure 2.10. Reading the first\nblock and writing back the last block are, respectively, the *prologue* and *epilogue* of the pipeline.\nNote that Figure 2.10 only describes the obvious precedences between the computations and data\ntransfers but tells us nothing about their relative durations.",
			  "ile for general purpose multiprocessor architectures, the design choice of using caches is\nobvious, it is not the case anymore for MPSoCs. The main reason is that MPSoCs are resource\nconstrained systems and managing cache coherence is very expensive both in terms of area and\npower [LPB06]. Therefore Scratchpad Memories (SPMs) have been proposed as an alternative\nto caches where data (and sometimes code) transfers through the memory hierarchy are *explic-*\n*itly* managed by the software. Furthermore, unlike a general purpose processor where a standard\ncache-based memory hierarchy is employed to guarantee good average performance over a wide\nrange of applications, in MPSoCs, the overall memory organization has to be tailored in order\nto fit at best the needs of the target class of applications. Indeed, according to the memory ac-\ncess pattern of the target applications, the use of scratchpad memories can be more efficient t"
			]
		  },
		  {
			"title": "Automated Scratchpad Mapping and Allocation for Embedded ...",
			"url": "https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=4035&context=etd",
			"excerpts": [
			  "If the per-core on-chip memory or the share memory (MSMC) are programmat-\nically configured to behave partially or fully as a scratchpad memory (SPM) space,\nintegrated DMA controller allows data to be exchanged between on- and off-chip\nmemories in parallel to operations being performed on the DSP.",
			  "With EDMA, data accessed in a predictable, regular access pattern can be con-\ncurrently prefetched using EDMA to exchange data between scratchpad and DRAM."
			]
		  }
		],
		"reasoning": "Double buffering is explicitly described as a classic technique to hide memory latency by overlapping computation with memory transfers, typically by using two buffers and coordinating with a DMA engine to prefetch the next data block while the current data is being processed. This aligns directly with the described approach of using two scratchpad buffers (ping and pong) and DMA to prime the next phase while the CPU works on the current phase, producing a software-pipelined overlap between data movement and computation. The cited material also discusses the role of asynchronous DMA operations and overlapping of data transfers with computation to keep the processor busy, which is the core idea behind ping-pong buffering in a scratchpad-memory context. Additional excerpts describe the broader concept of software-pipelined data movement and stages where memory transfers and computation are overlapped, reinforcing how a two-buffer scheme can be integrated into a VLIW workflow to maximize utilization of heterogeneous units and minimize stall cycles. There is also mention of using DMAs explicitly to manage memory transfers in parallel with processing, which supports the practical implementation details of the described technique (two SPM buffers, DMA-driven data filling, and overlapping with CPU work). Finally, the EDMA/DMA-oriented discussions illustrate how explicit, controller-driven memory transfers can be orchestrated to sustain continuous computation, which underpins the efficacy of the ping-pong buffering approach in hiding latency in scratchpad-based systems.",
		"confidence": "high"
	  },
	  {
		"field": "software_managed_memory_strategies.core_concept",
		"citations": [
		  {
			"title": "Dynamic Allocation for Scratch-Pad Memory using Compile ...",
			"url": "https://terpconnect.umd.edu/~barua/udayakumaran-TECS-2005.pdf",
			"excerpts": [
			  "In this research we propose a highly predictable, low overhead and yet dynamic, memory allocation strategy\nfor embedded systems with scratch-pad memory. A *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache. It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  " *scratch-pad* is a fast compiler-managed SRAM memory\nthat replaces the hardware-managed cache",
			  "It is motivated by its better real-time guarantees vs cache and by\nits significantly lower overheads in energy consumption, area and overall runtime, even with a simple allocation\nscheme.",
			  "Scratch-pad allocation primarily methods are of two types. First, software-caching schemes emulate the work-\nings of a hardware cache in software. Instructions are inserted before each load/store to check the software-\nmaintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption and\nSRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of\nalgorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation\nschemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that\nnever changes at runtime cannot achieve the full locality benefits of a cache."
			]
		  },
		  {
			"title": "Compiler-Decided Dynamic Memory Allocation for Scratch- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-CASES-2003.pdf",
			"excerpts": [
			  "scratch-pad* is a fast compiler-managed\nSRAM memory that replaces the hardware-managed cache. It is\nmotivated by its better real-time guarantees vs cache and by its sig-\nnificantly lower overheads in energy consumption, area and overall\nruntime, even with a simple allocation scheme [4]"
			]
		  },
		  {
			"title": "Dynamic Management of Scratch-Pad Memory Space",
			"url": "https://www.researchgate.net/profile/Ismail-Kadayif-2/publication/3904523_Dynamic_management_of_scratch-pad_memory_space/links/0a85e53a977b05a602000000/Dynamic-management-of-scratch-pad-memory-space.pdf",
			"excerpts": [
			  "We propose a compiler-controlled dynamic\non-chip scratch-pad memory (SPM) management framework that uses\nboth loop and data transformations."
			]
		  },
		  {
			"title": "An Integrated Scratch-Pad Allocator for Affine and Non- ...",
			"url": "https://user.eng.umd.edu/~barua/udayakumaran-DATE-2006.pdf",
			"excerpts": [
			  "The prevalence of SPM in embedded systems is evident\nfrom the large variety of chips with SPM available today in\nthe market.( *e.g.,* [1,4]). Such memory is typically software\nmanaged. Software management, apart from avoiding ex-\ntra caching hardware, also allows for more predictable pro-\ngram execution than when caches are present. Studies have\nfurther shown other benefits of using SPM including reduc-\ntions in power consumption, memory latency, area and even\nsignificant runtime gains for embedded applications. Trends\nin embedded designs indicate that this dominance of SPM’s\nwill continue in the future [9]."
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques ...",
			"url": "https://inria.hal.science/inria-00170210v1/document",
			"excerpts": [
			  "[Udayakumaran and Barua, 2006] S. Udayakumaran and R. Barua.\nAn integrated\nscratch-pad allocator for affine and non-affine code. In DATE , pages 925–930, 2006.\n["
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/taco09.pdf",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embed-\nded system"
			]
		  },
		  {
			"title": "a compiler approach for scratchpad memory management",
			"url": "http://ieeexplore.ieee.org/document/1515604/",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "A compiler approach for scratchpad memory management",
			"url": "https://www.researchgate.net/publication/4178715_Memory_coloring_A_compiler_approach_for_scratchpad_memory_management",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "https://www.researchgate.net/publication/220170090_Compiler-Directed_Scratchpad_Memory_Management_via_Graph_Coloring",
			"excerpts": [
			  "graph coloring algorithm for scalars that is generalized as described in [Smith et al. 2004] to deal with interchangeable and aliased registers.Read more"
			]
		  },
		  {
			"title": "Compiler-directed scratchpad memory management via graph coloring | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1582710.1582711",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
			  "Section Title: Compiler-directed scratchpad memory management via graph coloring > Abstract",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM.",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file.",
			  "Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications."
			]
		  }
		],
		"reasoning": "The field value asserts that Scratchpad Memory is a small, fast, on-chip SRAM mapped into the processor’s address space but not managed by hardware caches, with explicit compiler- or programmer-controlled data movement between main memory and the SPM, delivering predictable, low-latency access and energy savings. The most direct support comes from excerpts that define scratchpad memory as a fast on-chip SRAM managed by software, emphasize predictability and lower energy compared to caches, and describe memory-management approaches that use compiler or runtime techniques to place data in the SPM and manage transfers. Specifically, the definition-style excerpts establish what SPM is and why it is preferred for predictability and energy efficiency. The discussions of an integrated scratch-pad allocator and memory-coloring give concrete mechanisms by which data can be moved between main memory and SPM under compiler control, which matches the core idea that compilers/programmers govern data placement to achieve predictability and energy benefits. Subsequent references to compiler-driven dynamic/d static memory management and graph-coloring approaches provide additional evidence that SPM management can be done without hardware caches, aligning with the claim of explicit control over data movement and timing. In short, multiple excerpts directly define SPM and explain software-driven allocation/movement strategies that realize predictable, low-latency memory access and energy benefits, supporting the field value. The combination of definition-focused sources and implementation-focused sources provides coherent, well-supported backing for the stated concept.",
		"confidence": "high"
	  },
	  {
		"field": "key_architectural_feature_recommendations.feature_name",
		"citations": [
		  {
			"title": "Intel® Itanium™ Architecture Software Developer's Manual",
			"url": "http://bebop.cs.berkeley.edu/resources/arch-manuals/itanium/itanium-asdm1-apparch-24531703s.pdf",
			"excerpts": [
			  "Register rotation renames registers by adding the register number to the value of a register rename\nbase (rrb) register"
			]
		  },
		  {
			"title": "Introduction to Software Pipelining in the IA-64 Architecture",
			"url": "https://www.ele.uva.es/~jesman/BigSeti/ftp/Microprocesadores/Intel/IA-64/Presentaciones/ia-64_notes.pdf",
			"excerpts": [
			  "The General registers, Floating Point (FP) registers, and the Predicate\nregisters have the ability to rotate. Conceptually, the data rotates or\nmoves from a register to the adjacent register. For example, if data is\nwritten to F32 on one iteration, the data will be in F33 on the next\niteration. This allows the instruction to reference a new “register” on\neach iteration.",
			  "Modulo Loop Scheduling is just one software pipelining technique.\nThere is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.\nThe architectural support makes it easy to create SWP loops. The\nresulting code has high performance and is compact.",
			  "rotating registers",
			  "Registers rotate for each loop iteration",
			  "General, predicate, FP registers rotate",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count registers.",
			  "There is direct support in the IA-64 architecture for modulo loop\nscheduling: rotating registers, predicates, special branch instructions,\nand loop count regist"
			]
		  },
		  {
			"title": "The Itanium processor, part 10: Register rotation",
			"url": "https://devblogs.microsoft.com/oldnewthing/20150807-00/?p=91151",
			"excerpts": [
			  "And there you have register rotation. It lets you compress the prologue, kernel, and epilogue of a pipelined loop into a single instruction ...Read more"
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "dware support for software pipelining is typically used to eliminate some or all of the code\nexpansion normally associated with this technique. One way to accomplish this is to provide\nsupport for modulo variable expansion so that the same effect can be achieved without multiple\ncopies of the loop code. Specifically, a mechanism must be provided for accessing different pieces\nof data using the same register name so that a single copy of the code can be used for each loop\niteration. In [Su90] a horrible architectural structure is proposed in which data is physically shifted\nbetween registers arranged in an array. In [Ugurdag93] an equally distasteful mechanism is described\nin which functional units are connected to a set of register files through large reconfigurable\ncrossbars. These are two unfortunate examples of what can happen when designers fail to realize\nthat “It’s the wires, stupid” [Knight97].\nA more practical solution found in both the Cydra 5 and the (ridiculously named) Itanium is\n**register rotation** ([Dehnert89], [Doshi99]). A certain set of registers are specified as rotating\nregisters. When an instruction references one of these registers, a hardware index is added to the\nregister number specified in the instruction (modulo the number of rotating registers). The index is\nincremented at the start of each new iteration. Thus, the same instruction will refer to different\nregisters on each cycl"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "The algorithm of modulo variable expansion is as follows.\nFirst, we identify those variables that are redefined at the\nbeginning of every iteration.\nNext, we pretend that every",
			  "Modulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. The small set of values can even reside in register\nfiles, cutting down on both the memory traffic and the latency\nof the computation.",
			  "odulo variable expansion takes\nadvantage of the flexibility of VLJW machines in scalar com-\nputation, and reduces the number of locations allocated to a\nvariable by reusing the same location in non-overlapping\niterations. "
			]
		  }
		],
		"reasoning": "The field value Rotating Registers is directly supported by passages that describe rotating (or rotating/register rotation) as a core mechanism to rename or rotate registers across iterations, enabling software pipelining on VLIW/IA-64 architectures. For instance, explanations of register rotation renaming registers by adding a base to the register number, and incrementing the index at the start of each new iteration, illustrate how rotating registers provide per-iteration distinct views of registers without code blow-up. Additional passages articulate that registers rotate conceptually, so data written to a given register in one iteration appears in the next iteration under a rotated name, which is a concrete instantiation of rotating registers. Several excerpts explicitly state that rotating registers are part of IA-64 tooling for modulo loop scheduling and software pipelining, including descriptions of rotating registers in general-purpose and FP/predicate spaces, and notes that IA-64 provides direct support for modulo loop scheduling via rotating registers and related features. Other cited items discuss the Itanium manual and developer notes emphasizing register rotation as a feature to compress prologue/kernel/epilogue in pipelined loops, which further substantiates the claim that rotating registers are a targeted architectural mechanism to improve VLIW/SWP throughput. Taken together, these excerpts directly support the notion that rotating registers are a recognized architectural feature used to facilitate software-pipelined scheduling on VLIW and IA-64 architectures, making them highly relevant to the field value. Some additional excerpts discuss the broader context of software pipelining, modulo variable expansion, and rotating register files as hardware support, which reinforces the connection but is one step removed from the core concept of rotating registers themselves. The strongest explicit connections are the ones that define or demonstrate rotation of registers across iterations, and the claim that IA-64 (Itanium) and similar VLIW architectures expose rotating registers as part of their architectural support for SWP.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.3",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine"
			]
		  }
		],
		"reasoning": "The finegrained field value is about Hierarchical Reduction, a software-pipelining technique that converts an entire control construct into a single pseudo-operation to enable scheduling on VLIW machines without reliance on predication hardware. The most relevant excerpt directly states that a hierarchical reduction scheme reduces entire control constructs to an object similar to an operation in a basic block, which is the core idea described in the field value. Additional excerpts reinforce this by noting that hierarchical reduction complements software-pipelining and yields consistent performance improvements, indicating its role as a foundational method for handling conditionals within VLIW scheduling. Other excerpts corroborate the same concept by describing hierarchical reduction as a device that enables innermost-loop software pipelining and as a complementary technique improving overall scheduling outcomes. Taken together, these excerpts substantiate the core claims: hierarchical reduction enables software-pipelined loops with conditionals, does not require specialized hardware, and serves as a foundational, portable technique that can precede or accompany more advanced methods like EMS. ",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.2",
		"citations": [
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  "The EMS algorithm consists of five steps:",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be "
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  }
		],
		"reasoning": "Enhanced Modulo Scheduling is presented as a software-only scheduling technique that fuses the strengths of if-conversion and hierarchical reduction to handle loops with conditional branches on VLIW processors without dedicated predication hardware. The core description states that EMS translates conditional constructs into straight-line code via if-conversion, then applies modulo scheduling to the resulting schedule, and finally regenerates the explicit control flow in the final schedule. This directly supports the finegrained value describing EMS as a software-only method that combines the benefits of if-conversion and hierarchical reduction, used for software-pipelining loops with conditionals on VLIW hardware lacking predicated-execution support. Several excerpts explicitly outline the EMS five-step process (applying if-conversion, generating the data dependence graph, modulo-scheduling the loop, applying modulo-variable expansion, and regenerating conditional branches), and others discuss its relative performance advantages over Hierarchical Reduction, including claims of about 18% improvement in certain studies. The cited material also emphasizes EMS as a low-cost, hardware-free approach that provides the scheduling advantages of predicated execution without hardware support, aligning precisely with the described field value. Taken together, these excerpts corroborate the described EMS technique, its workflow, and its primary use-case and benefits. The content also mentions predicated execution concepts and how EMS relates to hardware capabilities, reinforcing the stated context for EMS usage on VLIW machines without predication hardware.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.0",
		"citations": [
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more",
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  }
		],
		"reasoning": "The most relevant material directly defines and explains Decoupled Software Pipelining (DSWP): it describes how DSWP partitions a loop into long-running threads that execute in a pipelined fashion and communicate through a synchronization mechanism such as a synchronization array or inter-thread queues. This aligns with the finegrained field’s core principle of decoupling memory access from computation by creating producer–consumer stages that can run in a pipeline, thereby hiding memory latency. The references that elaborate CP and off-CP thread execution, the notion of automatic thread extraction, and how DSWP partitions loops into long-running threads that communicate through queues map directly to the described technique and its utility for loop-carried dependencies and irregular control flow. Extensions like PS-DSWP (Parallel-Stage DSWP) show combining pipeline parallelism with data parallelism, which strengthens the idea of interleaved pipelines to keep many slots occupied even when the critical path includes long dependency chains. Works discussing LLVM-based DSWP implementations demonstrate practical tooling for applying these ideas in real compilers, reinforcing the mechanism of splitting dependences and communicating results between stages. Additional material on Pipette highlights intra-core pipeline parallelism and queue-based latency hiding, which resonates with the same theme of decoupled stages and queues to mitigate latency. In total, the most directly supporting excerpts present the explicit DSWP concept, its queue-based inter-thread communication, SCC-based partitioning, and practical extensions that preserve the central idea of decoupling latency to maximize pipeline occupancy in irregular workloads.",
		"confidence": "high"
	  },
	  {
		"field": "software_managed_memory_strategies.dma_scheduling_pattern",
		"citations": [
		  {
			"title": "Optimizing DMA Data Transfers for Embedded Multi-Cores",
			"url": "http://www-verimag.imag.fr/~maler/Papers/thesis-selma.pdf",
			"excerpts": [
			  "Double buffering or more generally multi-buffering is a well known programming technique\nreferred to as *software pipelining* used to hide memory latency. The main idea is to overlap com-\nputations and data transfers and it is expressed at the software level.",
			  "This takes advantage from the fact that DMA can work in parallel with a processor. Pro-\ngram 2.4.2 uses *double buffering* for input blocks *B* *x* [0] , *B* *x* [1] and output blocks *B* *y* [0] , *B* *y* [1] . At\neach iteration before the processor starts working on the current buffer, it issues an *asynchronous*\nDMA call to fetch the next buffer. Therefore, the processor can work on a super block *j* residing\nin one buffer while the DMA brings *in parallel* the super block *j* + 1 to the other buffer. Note that\n*comp* ( *j* ) is used as a shorthand for the inner double loop in Program 2.4.1. In practice, events are\nassociated with each *dma_get* and *dma_put* command to ensure the synchronization between each\ndata transfer and its corresponding computation.",
			  "Hence, program 2.4.2 defines a *software pipeline* with 3 stages: input of super block ( *j* + 1) ,\ncomputation on super block *j* and output of super block ( *j* *−* 1) , see Figure 2.10. Reading the first\nblock and writing back the last block are, respectively, the *prologue* and *epilogue* of the pipeline.\nNote that Figure 2.10 only describes the obvious precedences between the computations and data\ntransfers but tells us nothing about their relative durations.",
			  "ile for general purpose multiprocessor architectures, the design choice of using caches is\nobvious, it is not the case anymore for MPSoCs. The main reason is that MPSoCs are resource\nconstrained systems and managing cache coherence is very expensive both in terms of area and\npower [LPB06]. Therefore Scratchpad Memories (SPMs) have been proposed as an alternative\nto caches where data (and sometimes code) transfers through the memory hierarchy are *explic-*\n*itly* managed by the software. Furthermore, unlike a general purpose processor where a standard\ncache-based memory hierarchy is employed to guarantee good average performance over a wide\nrange of applications, in MPSoCs, the overall memory organization has to be tailored in order\nto fit at best the needs of the target class of applications. Indeed, according to the memory ac-\ncess pattern of the target applications, the use of scratchpad memories can be more efficient t"
			]
		  },
		  {
			"title": "Automated Scratchpad Mapping and Allocation for Embedded ...",
			"url": "https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=4035&context=etd",
			"excerpts": [
			  "If the per-core on-chip memory or the share memory (MSMC) are programmat-\nically configured to behave partially or fully as a scratchpad memory (SPM) space,\nintegrated DMA controller allows data to be exchanged between on- and off-chip\nmemories in parallel to operations being performed on the DSP.",
			  "With EDMA, data accessed in a predictable, regular access pattern can be con-\ncurrently prefetched using EDMA to exchange data between scratchpad and DRAM.",
			  "Data accessed irregularly in a data-dependent pattern can be cached in a separate\nregion of scratchpad to take advantage of locality.",
			  "Direct memory access (DMA) is a feature of modern computers that allows certain\nhardware subsystems within the computer to access system memory independently\nof the central processing unit (CPU).",
			  "Without DMA, when the CPU is using pro-\ngrammed input/output, it is typically fully occupied for the entire duration of the\nread or write operation, and is thus unavailable to perform other work.",
			  "With DMA,\nthe CPU initiates the transfer, does other operations while the transfer is in progress,\nand receives an interrupt from the DMA controller when the operation is done."
			]
		  },
		  {
			"title": "Applications Using the TMS320C6000 Enhanced DMA",
			"url": "https://www.ti.com/lit/pdf/spra636",
			"excerpts": [
			  "Ping-Pong Buffering for McBSP Data",
			  "While the EDMA is transferring\ndata in to and out of the ping buffers, the CPU is manipulating the data in the pong buffers.",
			  "The ping–pong scheme for this example is shown in\nFigure 27.",
			  "The EDMA channels may be configured to access any location in the device’s memory map."
			]
		  },
		  {
			"title": "TMS320C6000 DSP Enhanced Direct Memory Access ( ...",
			"url": "https://www.ti.com/lit/pdf/spru234",
			"excerpts": [
			  "Each channel has two parameter sets, ping and pong.",
			  "Ping-pong buffering is a simple technique that allows the CPU activity to be\ndistanced from the EDMA activity. This means that there are multiple (usually\ntwo) sets of data buffers for all incoming and outgoing data streams.",
			  "The link address for the\nping entry is set to the PaRAM offset of the pong parameter set (Figure 1−27)."
			]
		  },
		  {
			"title": "ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing. | Request PDF",
			"url": "https://www.researchgate.net/publication/221569161_ROS-DMA_A_DMA_Double_Buffering_Method_for_Embedded_Image_Processing_with_Resource_Optimized_Slicing",
			"excerpts": [
			  "This paper presents our work on PfeLibÃ¢Â€Â”a high performance software library for image processing and computer vision algorithms for an embedded system. The main target platform for PfeLib is the TMS320C6000 series of digital signal processors (DSPs) from Texas instrume",
			  "ROS-DMA can yield substantially better performance compared to using L2 cache.",
			  "Section Title: ROS-DMA: A DMA Double Buffering Method for Embedded Image Processing with Resource Optimized Slicing."
			]
		  },
		  {
			"title": "Optimizing Data Distribution for Loops on Embedded ...",
			"url": "https://utd-ir.tdl.org/server/api/core/bitstreams/cb4d3e16-7abb-4de8-8fc4-77d727d707f1/content",
			"excerpts": [
			  "472–511, May 2006. [13] S. Udayakumaran and R. Barua, “Compiler-decided dy- namic memory allocation for scratch-pad based embed ...Read more"
			]
		  },
		  {
			"title": "(PDF) Dynamic management of scratch-pad memory space",
			"url": "https://www.academia.edu/78529842/Dynamic_management_of_scratch_pad_memory_space",
			"excerpts": [
			  "We propose a compiler-controlled dynamic on-chip scratch-pad memory (SPM) management framework that uses both loop and data transformations. Experimental ..."
			]
		  },
		  {
			"title": "Scratchpad Memory : A Design Alternative for Cache On-chip ...",
			"url": "https://robertdick.org/aeos/reading/banakar-scratchpad.pdf",
			"excerpts": [
			  "In this paper we address the problem of on-chip mem- ory selection for computationally intensive applications, by proposing scratch pad memory as an ...Read more"
			]
		  },
		  {
			"title": "A Survey of Scratch-Pad Memory Management Techniques for low-power and -energy",
			"url": "https://www.researchgate.net/publication/29649252_A_Survey_of_Scratch-Pad_Memory_Management_Techniques_for_low-power_and_-energy",
			"excerpts": [
			  "In the dynamic approach, program blocks can be transferred between SPM and the offchip memory during the application execution [12] . In addition to performance and energy consumption, SPM as an SRAM-based on-chip memory plays a major role in the reliability of embedded systems.",
			  "The main idea underlying the proposed mechanism includes four stages: 1) to use parity codes for error detection in the SPM, 2) to keep an address matching table in the main memory to store the address of the copy of SPM blocks in the main memory, in the case of dynamic SPM, 3) to allocate a specific segment of the main memory as an SPM backup, in the case of static SPM, and 4) to recover from errors using an interrupt service routine (ISR).",
			  "The memory subsystem accounts for a significant portion of the aggregate energy budget of contemporary embedded systems. Moreover, there exists a large potential for optimizing the energy consumption of the memory subsystem. Consequently, novel memories as well as novel algorithms for their efficient utilization are being designed.",
			  "An Optimal Memory Allocation Scheme for Scratch-Pad Based Embedded Systems",
			  "Assigning program and data objects to scratchpad for energy reduction",
			  "Dynamic overlay of scratchpad memory for energy minimization",
			  "Data partitioning for maximal scratchpad usage",
			  "The set of selected program parts are determined with an optimal algorithm using integer linear programming.",
			  "Scratchpad memory: design alternative for cache on-chip memory in embedded systems.",
			  "A minimal increase in code size is also reported.",
			  "ed which analyzes the application, partitions an array variable whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Res"
			]
		  },
		  {
			"title": "A Scratchpad Memory Allocation Scheme for Dataflow Models",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-104.pdf",
			"excerpts": [
			  "In [3], Banakar et al show that on average a scratchpad memory has 34% smaller area and 40% lower power consumption than a cache of the same ...Read more"
			]
		  },
		  {
			"title": "Compiler-Directed Scratch Pad Memory Optimization for ...",
			"url": "https://users.eecs.northwestern.edu/~choudhar/Publications/CompilerDirectedScratchPadMemoryOptimEmbeddedMultiprocessors.pdf",
			"excerpts": [
			  "Abstract—This paper presents a compiler strategy to optimize data accesses in regular array-intensive applications running on embedded multiprocessor ..."
			]
		  },
		  {
			"title": "Adaptive Scratch Pad Memory Management for Dynamic ...",
			"url": "https://www.engr.colostate.edu/~sudeep/wp-content/uploads/j10.pdf",
			"excerpts": [
			  "Guided by compiler analysis for generating scratch pad data layouts and hardware components for tracking dynamic memory accesses, the scratch pad data."
			]
		  },
		  {
			"title": "TMS320C64x EDMA Architecture",
			"url": "https://www.ti.com/lit/pdf/spra994",
			"excerpts": [
			  "• The CPU accesses memory in the L2 SRAM space. This access goes directly to L2 within the cache memory system; refer to TMS320C64x DSP Two Level Internal ...Read more"
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch- ...",
			"url": "https://scispace.com/pdf/compiler-decided-dynamic-memory-allocation-for-scratch-pad-1eaxs54tgy.pdf",
			"excerpts": [
			  "In static management of scratch-pad memory, the contents of the scratch-pad\nare decided at compile time itself. A simple way of doing this is based on programmer\nhints. Thus, the programmer provides annotations that tell if a variable is to be put\ninto the scratch-pad address space or not. In such a scheme, it is the programmer\nresponsibility to ensure that a particular variable fits completely inside the scratch-\npad address space. To make this process efficient, the selection strategy can be based\non compiler analysis of the application. Examples of such static allocation strategies\ninclude [12,13,73,77]. The advantage of a static scheme is that it does not involve\nany additional per-access overhead like that of software caching as no translation",
			  "Software caching [36,59] represents a dynamic allocation strategy. This class\nof methods emulate the behavior of a hardware cache in software. In particular, a\ntag consisting of the high-order bits of the address is stored for each cache line in\nsoftware. Before each load or a store, additional instructions are compiler-inserted\nto mask out the high-order bits of the address, access the tag, compare the tag with\nthe high-order bits and then branch conditionally to hit or miss code. On a hit, in\nthe hit code, using a mapping table the address is mapped to a new address in the\nscratch-pad where the data resides. Otherwise it is a cache miss and new data is\nbrought into the scratch-pad. At that point, the index tables are updated to reflect\nthe presence of this new data in the scratch-pad. Being a runtime strategy like\ncaching, the strategy can adapt to runtime conditions. This makes it particularly\nsuited for workloads whose runtime conditions continuously vary.",
			  "We propose a dynamic allocation methodology for global and stack data and\nprogram code that (i) accounts for changing program requirements at runtime (ii)\nhas no software-caching tags (iii) requires no run-time checks (iv) has extremely low\noverheads, and (v) yields 100% predictable memory access times.",
			  "data that is about to be accessed frequently is copied into the scratch-pad using\ncompiler-inserted code at fixed and infrequent points in the program. Earlier data\nis evicted if necessary.",
			  "When compared to an existing static allocation scheme,\nresults show that our scheme reduces runtime by up to 39.8% and energy by up to\n31.3% on average for our benchmarks, depending on the SRAM size used.",
			  "The actual\ngain depends on the SRAM size, but our results show that close to the maximum\nbenefit in run-time and energy is achieved for a substantial range of small SRAM\nsizes commonly found in embedded systems.",
			  "Our comparison with a direct mapped\ncache shows that our method performs roughly as well as a cached architecture in\nruntime and energy while delivering better real-time benefits.",
			  "Compiler-Decided Dynamic Memory Allocation for Scratch-Pad\nBased Embedded Systems"
			]
		  },
		  {
			"title": "Hexagon™ V73 HVX",
			"url": "https://docs.qualcomm.com/doc/80-N2040-54/80-N2040-54_REV_AB_Qualcomm_Hexagon_V73_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "HVX supports a local memory called vector TCM (VTCM) for scratch buffers and scatter/gather\noperations.",
			  "The size of the memory is implementation-defined. The size is discoverable from the\nconfiguration table.",
			  "VTCM needs normal virtual to physical translation just like other memory.",
			  "This memory has higher performance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer. It serves as the input or output\nof the scatter/gather instructions.",
			  "The following are advantages of using VTCM as the\nintermediate buffer:\n\nGuarantees no eviction (vs. L2 if the set is full)\n\nFaster than L2$ (does not have the overhead of cache management, like association)\n\nReduces L2$ pressure\n\nLower power than L2$\n\nSupports continuous read and write for every packet without contention",
			  ".9.4 L2FETCH**\nUse the L2FETCH instruction to prepopulate the L2 cache with data prior to using VMEM loads.\nL2FETCH is best performed in sizes less than 8 KB and issued at least several hundred cycles prior\nto using the data. If the L2FETCH instruction is issued too early, data can be evicted before use. In\ngeneral, prefetching and processing on image rows or tiles works best",
			  ".9.5 Access data contiguously**\nWhenever possible, arrange data in memory so that it is accessed contiguously. For example,\ninstead of repeatedly striding through memory, data might be first tiled, striped, or decimated to\n ... \nin L1, followed by scalar load instructions. Execute the DCFETCH anytime after the VMEM store,\nhowever, software should budget at least 30 cycles before issuing the scalar load instructi",
			  ".9.8 Avoid scatter/gather stalls**\nScatter and gather operations compete for memory and can result in long latency, therefore take\ncare to avoid stalls. The following techniques improve performance around scatter and gather:\n\nDistribute accesses across the intra-vector address range (lower address bits). Even\ndistribution across the least significant inter-vector address bits can also be beneficial. For\nV73, address bits [10:3] are important to avoid conflicts. Ideally this applies per vector\ninstruction, but distributing these accesses out between vector instructions can help absorb\nconflicts within a vector instruction.\n\nMinimize the density of scatter and gather instructions. Spread out these instructions in a\nlarger loop rather than concentrating them in a tight loop. The hardware can process a small\nnumber of these instructions in parallel. If it is difficult to spread these instructions out, limit\nbursts to four for a given thread (for "
			]
		  },
		  {
			"title": "Hexagon V79 HVX Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-61/memory.html",
			"excerpts": [
			  "HVX supports a local memory called vector tightly coupled memory\n(VTCM) for scratch buffers and scatter/gather operations.",
			  "The size of\nthe memory is implementation-defined.",
			  "VTCM needs normal virtual to physical\ntranslation just like other memory.",
			  "This memory has higher\nperformance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer.",
			  "It\nserves as the input or output of the scatter/gather instructions.",
			  "The\nfollowing are advantages of using VTCM as the intermediate buffer:",
			  "Guarantees no eviction (vs. L2 if the set is full)",
			  "Faster than L2$ (does not have the overhead of cache management, like\nassociation)",
			  "Reduces L2$ pressure",
			  "Lower power than L2$",
			  "Supports continuous read and write for every packet without\ncontention",
			  "In addition to HVX VMEM access, normal Hexagon memory access\ninstructions can access this memory."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies.",
			  "Decoupled software pipelining (DSWP) solves this by preferentially fetching and executing instructions from a program's critical path (CP). This allows DSWP to achieve high IPC with fast simple execution cores.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "Special microarchitectural support, called the\nsynchronization array (SA), provides low-latency inter-thread\nsynchronization and value communication and acts as a decoupling\nbuffer between the threads.",
			  "Decoupled execution ensures that stalls\nin one thread do not affect the other.",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Automatic Multithreading for DSWP: Unlike prior attempts at automatically multithreading sequential programs, DSWP does\nnot try to partition programs into totally independent threads.",
			  "We have a working compiler implementation that\nautomatically generates DSWP code."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization.",
			  " partitions loops\ninto long-running threads that communicate via inter-core queues.\nDSW",
			  "Parallel-Stage Decoupled Software Pipelining [16] (PS-DSWP)\nis an extension to combine pipeline parallelism with some stages\nexecuted in a DOALL, data-parallel fashion.",
			  "This paper presents the preliminary design of a new PS-DSWP method capable of handling arbitrary structured control flow, a slightly better ...Read more"
			]
		  },
		  {
			"title": "Phloem: Automatic Acceleration of Irregular Applications with ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2023.phloem.hpca.pdf",
			"excerpts": [
			  "Recent work has proposed hardware support for fine-grain pipeline parallelism, hiding long latencies by decoupling irregular applications into pipeline stages.Read more"
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation.",
			  "We evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP.",
			  "The base model used is a 6-core Itanium 2 CMP developed using\nthe Liberty Simulation Environment (LSE). The cores are validated\n(IPC and constituent error components accurate to within 6% of\nreal hardware for benchmarks measured [15]), and the details of\neach core in the model are given in Table 1. To this base model, a\nsynchronization array with 512 queues, each with 32 entries, was\nadded. The produce , consume\nand queue.set\ninstructions\nare also modeled in the pipeline of the Itanium 2 cores.",
			  "We have implemented PS-DSWP in the VELOCITY research\ncompiler [22]. The applications are first compiled into a low-level\nintermediate representation, and then transformed by a round of\nclassical optimizations. PS-DSWP was then automatically applied\nto the selected loops. Translation to Itanium 2 instructions is fol-\nlowed by another round of classical optimizations, scheduling, reg-\nister allocation, and a final scheduling phase.",
			  "t DSWP in combination\nwith speculation and parallel stages has the potential to unlock par-\nallelism among all the programs in the SPEC2000 integer bench-\nmark suite.",
			  "This paper presented a new non-speculative compiler transfor-\nmation called Parallel-Stage Decoupled Software Pipelining (PS-\nDSWP). PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformation. This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators.",
			  "For other programs, however, these tools yield poor performance because they invoke computation units for instructions sequentially, without exploiting parallelism."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages.",
			  "Decoupled software pipelining\n(DSWP) [ 35 ] uses a synchronization array to facilitate com-\nmunication between cores or the threads of a multithreaded\ncore. But DSWP focuses on pipelining a single loop across\ndi ff erent threads, which is too limiting for irregular applications.\nFor example, BFS uses a 3-level nested loop, with stages",
			  " Pipette’s ISA fea-\ntures control mechanisms that allow e ff ective decoupling under\nirregular control flow. ",
			  "es.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers.",
			  "By time-multiplexing stages on the same\ncore, Pipette avoids load imbalance and achieves high core IPC.",
			  " it uses the\nphysical register file to implement queues cheaply, avoiding the\nstorage costs of prior techniques. Sec",
			  "Pipette decouples threads within\nthe core using architecturally visible queues."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more",
			  "This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual ...Read more"
			]
		  },
		  {
			"title": "Quantifying the Semantic Gap Between Serial and Parallel ...",
			"url": "https://par.nsf.gov/servlets/purl/10337772",
			"excerpts": [
			  "This measures the TLP limit of both cyclic multithreading (CMT). (e.g., DoAcross [20], HELIX [3]) and pipeline multithreading. (PMT) (e.g., DSWP [5], PS-DSWP [8]) ...Read more"
			]
		  },
		  {
			"title": "From Sequential Programs to Concurrent Threads - Liberty ...",
			"url": "https://liberty.princeton.edu/Publications/cal05_dswp.pdf",
			"excerpts": [
			  "Ottoni, E. Raman, and D. I.. August, “Practical and accurate low-level pointer analysis,” in Proc. of the Int'l. Symp. on Code Generation and Optimization, 2005 ..."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Since technology scaling no longer provides previously seen rates of performance improvements, microprocessor designers have adopted a number of other techniques to improve processor performance at reasonable energy costs. An entire class of techniques improves performance by offloading computation intensive parts of programs from a general-purpose processor to customized accelerators [15], [17], [18], [24], [31], [34]. One approach to build these customized accelerators is to manually design dedicated circuit modules for each application using hardware description languages (HDLs) [24], [26]. While this approach often yields the best result, it requires significant non-recurring engineering costs to convert algorithms to HDL specifications. Another less labor-intensive approach is to use high-level synthesis (HLS) tools to generate customized accelerators from programs written in C or other high-level programming languages [4], [12], [13], [16], [30], [31], [32], [37].",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Hexagon™ V73",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-53_REV_AB_Qualcomm_Hexagon_V73_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "L2FETCH is nonblocking. After the instruction is initiated, the program continues to the next\ninstruction while the prefetching is performed in the background.",
			  "When data cache prefetching is enabled, the Hexagon processor observes patterns of data cache\nmisses and attempts to predict future misses based on recurring patterns of misses where the\naddresses are separated by a constant stride. If such patterns are found, the processor attempts\nto automatically prefetch future cache lines."
			]
		  },
		  {
			"title": "Hexagon™ V69",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-50_REV_AA_Qualcomm_Hexagon_V69_ProgrammerS_Reference_Manual.pdf",
			"excerpts": [
			  "The l2fetch instruction is non-blocking: it initiates a prefetch operation that is performed in the\nbackground by the prefetch engine while the thread continues to execute Hexagon processor\ninstructions."
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "within a fixed time after execution starts. But, when the sources are required varies.",
			  "Instructions that need more pipelining require early sources. Only HVX registers are Early",
			  "Source registers. Early source operands include:",
			  "Input to the multiplier. For example “V3.h = vmpyh(V2.h, V4.h)”. V2 and V4 are",
			  "multiplier inputs. For multiply instructions with accumulation, the accumulator is",
			  "not considered an early source multiplier input.",
			  "Input to shift/bit count instructions. Only the register that is being shifted or",
			  "counted is considered early source. Accumulators are not early sources.",
			  "Input to permute instructions. Only registers that are being permuted are",
			  "considered early source (not an acccumulator).",
			  "Unaligned store data is an early source.",
			  "If an early source register is produced in the previous vector packet, an interlock stall can",
			  "be incurred. Software should strive to schedule an intervening packet between the",
			  "producer and an early source consumer.",
			  "Vector Instructions",
			  "The following example shows various interlock cases:",
			  "V8 = VADD(V0,V0)",
			  "V0 = VADD(V8,V9)",
			  "// NO STALL",
			  "V1 = VMPY(V0,R0)",
			  "// STALL due to V0",
			  "V2 = VSUB(V2,V1)",
			  "// NO STALL on V1",
			  "V5:4 = VUNPACK(V2) // STALL due to V2",
			  "V2 = VADD(V0,V4)",
			  "// NO STALL on V4"
			]
		  },
		  {
			"title": "Hexagon V60 HVX Programmer's Reference Manual",
			"url": "https://docs.qualcomm.com/doc/80-N2040-30/80-N2040-30_REV_C_Hexagon_V60_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Certain instructions require Early Sources. Early source registers include:\n■\nInput to the multiplier. For example “ V3.h = vmpyh(V2.h, V4.h) ”. Here V2\nand V4 are multiplier inputs. For multiply instructions with accumulation, the\naccumulator is not considered an Early Source multiplier input.\n■\nInput to Shift/Bit Count instructions. For shifts, all vector sources are Early\nSource except for accumulators.\n■\nInput to Permute instructions. Only registers that are being permuted are\nconsidered Early Source (not Accumulator).\n■\nUnaligned Store Data is an Early Source.\nIf an Early Source register is produced in the previous vector packet, an interlock stall will\noccur. The software should try to schedule an intervening packet between the producer of\nan Early Source register. For example, the following shows various interlock cases:\nV8 = VADD(V0,V0)\nV0 = VADD(V8,V9) // NO STALL\nV1 = VMPY(V0,R0) // STALL due to V0\nV2 = VSUB(V2,V1) // NO STALL on V1\nV5:4 = VUNPACK(V2) // STALL due to V2\nV2 = VADD(V0,V4) // NO ST"
			]
		  },
		  {
			"title": "Qualcomm Corporate Template",
			"url": "https://llvm.org/devmtg/2017-02-04/Halide-for-Hexagon-DSP-with-Hexagon-Vector-eXtensions-HVX-using-LLVM.pdf",
			"excerpts": [
			  "Halide also provides a scheduling directive to prefetch data into the L2 cache.",
			  "For example, “ my_func.prefetch (y, 2)” will prefetch into the L2 cache,\n2 iterations worth of data needed in the ‘y’ loop.\n"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V73 Programmer's Reference Manual ...",
			"url": "https://device.report/m/9617176bfefffcbc1efdcc55512335e567e0d9a1c8a2813a1538de55ed99cf3b",
			"excerpts": [
			  "... instruction scheduling. 1.3 Technical ... 5.10.6.3 Software-based l2fetch The l2fetch instruction provides powerful L2 prefetching of data or instructions.Read more"
			]
		  },
		  {
			"title": "Advanced Scheduling Guide",
			"url": "https://documentation-be.hexagon.com/bundle/Advanced-Scheduling-Notes---2021.0/raw/resource/enus/Advanced%20Scheduling%20Notes%20-%202021.0.pdf?save_local=true",
			"excerpts": [
			  "Advanced Scheduling Guide. Master>Scheduling>Advanced Scheduling Rules. Run No: Create different run numbers to enable comparison of different settings and ...Read more"
			]
		  },
		  {
			"title": "Hexagon V68 Programmer s Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-46_REV_B_Hexagon_V68_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "The Hexagon processor includes the instruction dcfetch . This instruction queries the L1\ndata cache based on the address specified in the instruction:",
			  "If the address is present in the cache, no action is taken.",
			  "If the cache line for the address is missing, the processor attempts to fill the cache\nline from the next level of memory. The thread does not stall, but rather continues\nexecuting while the cache line fill occurs in the background."
			]
		  }
		],
		"reasoning": "The target field captures a standard pattern for software-managed memory where a DMA controller handles bulk transfers to a scratchpad memory, enabling the CPU to continue computations in parallel. Several excerpts explicitly discuss scratchpad-based memory management and the advantages of software-controlled data movement, which aligns with the need to overlap DMA transfers with VLIW execution and to hide memory latency in loops with dependencies. In particular, entries describing dynamic scratchpad allocation and software-managed DMA (including EDMA-like PaRAM chaining for continuous transfers) provide direct support for the described pattern: the CPU initiates a transfer, the DMA engine progresses in the background, and the core uses the scratchpad as a staging area to keep ALUs busy, with synchronization mechanisms (interrupts or polling) signaling completion. Other excerpts expand on related techniques such as double buffering, ping-pong buffering, and nonblocking or asynchronous DMA, which further reinforce how DMA-driven data movement can be interleaved with computation to maintain high utilization on a VLIW engine. The cited material also covers how scratchpad memory management (static/dynamic) affects predictability and performance, which is relevant when evaluating scheduling strategies that rely on predictable memory transfers and overlapping computation with data transfers. Overall, the most relevant portions describe concrete DMA-based data movement patterns (initiation, background transfer, and synchronization) and concrete mechanisms (PaRAM-style chaining, double buffering) that support the requested field value. ",
		"confidence": "medium"
	  },
	  {
		"field": "software_managed_memory_strategies.allocation_algorithms",
		"citations": [
		  {
			"title": "Compiler-directed scratchpad memory management via graph coloring | ACM Transactions on Architecture and Code Optimization",
			"url": "https://dl.acm.org/doi/10.1145/1582710.1582711",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
			  "Section Title: Compiler-directed scratchpad memory management via graph coloring > Abstract",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM.",
			  "Content:\nScratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file.",
			  "Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications."
			]
		  },
		  {
			"title": "Compiler-Directed Scratchpad Memory Management via ...",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/taco09.pdf",
			"excerpts": [
			  "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embed-\nded system",
			  "his paper introduces a general-purpose compiler approach, called memory coloring ,\nto assign static data aggregates such as arrays and structs in a program to an SPM.",
			  "The novelty\nof this approach lies in partitioning the SPM into a pseudo register file (with interchangeable\nand aliased registers), splitting the live ranges of data aggregates to create potential data transfer\nstatements between SPM and off-chip memory, and finally, adapting an existing graph coloring\nalgorithm for register allocation to assign the data aggregates to the pseudo register file.",
			  "ur\nexperimental results using a set of 10 C benchmarks from MediaBench and MiBench show that\nour methodology is capable of managing SPMs efficiently and effectively for large embedded appli-\ncations. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and\ncompared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator"
			]
		  },
		  {
			"title": "A Compiler Approach for Scratchpad Memory Management",
			"url": "http://www.cse.unsw.edu.au/~jingling/papers/pact05.pdf",
			"excerpts": [
			  "cratchpad memory (SPM), a fast software-managed on-*\n*chip SRAM, is now widely used in modern embedded pro-*\n*cessors. Compared to hardware-managed cache, it is more*\n*efficient in performance, power and area cost, and has the*\n*added advantage of better time predictability. This paper*\n*introduces a general-purpose compiler approach, called*\n*memory coloring, to efficiently allocating the arrays in a*\n*program to an SPM. The novelty of our approach lies in*\n*partitioning an SPM into a “register file”, splitting the live*\n*ranges of arrays to create potential data transfer statements*\n*between the SPM and off-chip memory, and finally, adapt-*\n*ing an existing graph-colouring algorithm for register al-*\n*location to assign the arrays in the program into the reg-*\n*ister fi",
			  " approach is efficient due to the practical*\n*efficiency of graph-colouring algorithms. We have imple-*\n*mented this work in SUIF and machSUIF. Preliminary re-*\n*sults over benchmarks show that our approach represents a*\n*promising solution to automatic SPM management.",
			  "A scratchpad memory (SPM) is a fast on-chip SRAM"
			]
		  },
		  {
			"title": "a compiler approach for scratchpad memory management",
			"url": "http://ieeexplore.ieee.org/document/1515604/",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "A compiler approach for scratchpad memory management",
			"url": "https://www.researchgate.net/publication/4178715_Memory_coloring_A_compiler_approach_for_scratchpad_memory_management",
			"excerpts": [
			  "This paper introduces a general-purpose compiler approach, called memory coloring, to efficiently allocating the arrays in a program to an SPM.Read more"
			]
		  },
		  {
			"title": "Compiler-decided dynamic memory allocation for scratch- ...",
			"url": "https://scispace.com/pdf/compiler-decided-dynamic-memory-allocation-for-scratch-pad-1eaxs54tgy.pdf",
			"excerpts": [
			  "In static management of scratch-pad memory, the contents of the scratch-pad\nare decided at compile time itself. A simple way of doing this is based on programmer\nhints. Thus, the programmer provides annotations that tell if a variable is to be put\ninto the scratch-pad address space or not. In such a scheme, it is the programmer\nresponsibility to ensure that a particular variable fits completely inside the scratch-\npad address space. To make this process efficient, the selection strategy can be based\non compiler analysis of the application. Examples of such static allocation strategies\ninclude [12,13,73,77]. The advantage of a static scheme is that it does not involve\nany additional per-access overhead like that of software caching as no translation",
			  "Software caching [36,59] represents a dynamic allocation strategy. This class\nof methods emulate the behavior of a hardware cache in software. In particular, a\ntag consisting of the high-order bits of the address is stored for each cache line in\nsoftware. Before each load or a store, additional instructions are compiler-inserted\nto mask out the high-order bits of the address, access the tag, compare the tag with\nthe high-order bits and then branch conditionally to hit or miss code. On a hit, in\nthe hit code, using a mapping table the address is mapped to a new address in the\nscratch-pad where the data resides. Otherwise it is a cache miss and new data is\nbrought into the scratch-pad. At that point, the index tables are updated to reflect\nthe presence of this new data in the scratch-pad. Being a runtime strategy like\ncaching, the strategy can adapt to runtime conditions. This makes it particularly\nsuited for workloads whose runtime conditions continuously vary.",
			  "We propose a dynamic allocation methodology for global and stack data and\nprogram code that (i) accounts for changing program requirements at runtime (ii)\nhas no software-caching tags (iii) requires no run-time checks (iv) has extremely low\noverheads, and (v) yields 100% predictable memory access times.",
			  "data that is about to be accessed frequently is copied into the scratch-pad using\ncompiler-inserted code at fixed and infrequent points in the program. Earlier data\nis evicted if necessary.",
			  "When compared to an existing static allocation scheme,\nresults show that our scheme reduces runtime by up to 39.8% and energy by up to\n31.3% on average for our benchmarks, depending on the SRAM size used.",
			  "The actual\ngain depends on the SRAM size, but our results show that close to the maximum\nbenefit in run-time and energy is achieved for a substantial range of small SRAM\nsizes commonly found in embedded systems.",
			  "Our comparison with a direct mapped\ncache shows that our method performs roughly as well as a cached architecture in\nruntime and energy while delivering better real-time benefits.",
			  "Compiler-Decided Dynamic Memory Allocation for Scratch-Pad\nBased Embedded Systems"
			]
		  },
		  {
			"title": "Hexagon™ V73 HVX",
			"url": "https://docs.qualcomm.com/doc/80-N2040-54/80-N2040-54_REV_AB_Qualcomm_Hexagon_V73_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "HVX supports a local memory called vector TCM (VTCM) for scratch buffers and scatter/gather\noperations.",
			  "The size of the memory is implementation-defined. The size is discoverable from the\nconfiguration table.",
			  "VTCM needs normal virtual to physical translation just like other memory.",
			  "This memory has higher performance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer. It serves as the input or output\nof the scatter/gather instructions.",
			  "The following are advantages of using VTCM as the\nintermediate buffer:\n\nGuarantees no eviction (vs. L2 if the set is full)\n\nFaster than L2$ (does not have the overhead of cache management, like association)\n\nReduces L2$ pressure\n\nLower power than L2$\n\nSupports continuous read and write for every packet without contention",
			  ".9.4 L2FETCH**\nUse the L2FETCH instruction to prepopulate the L2 cache with data prior to using VMEM loads.\nL2FETCH is best performed in sizes less than 8 KB and issued at least several hundred cycles prior\nto using the data. If the L2FETCH instruction is issued too early, data can be evicted before use. In\ngeneral, prefetching and processing on image rows or tiles works best",
			  ".9.5 Access data contiguously**\nWhenever possible, arrange data in memory so that it is accessed contiguously. For example,\ninstead of repeatedly striding through memory, data might be first tiled, striped, or decimated to\n ... \nin L1, followed by scalar load instructions. Execute the DCFETCH anytime after the VMEM store,\nhowever, software should budget at least 30 cycles before issuing the scalar load instructi",
			  ".9.8 Avoid scatter/gather stalls**\nScatter and gather operations compete for memory and can result in long latency, therefore take\ncare to avoid stalls. The following techniques improve performance around scatter and gather:\n\nDistribute accesses across the intra-vector address range (lower address bits). Even\ndistribution across the least significant inter-vector address bits can also be beneficial. For\nV73, address bits [10:3] are important to avoid conflicts. Ideally this applies per vector\ninstruction, but distributing these accesses out between vector instructions can help absorb\nconflicts within a vector instruction.\n\nMinimize the density of scatter and gather instructions. Spread out these instructions in a\nlarger loop rather than concentrating them in a tight loop. The hardware can process a small\nnumber of these instructions in parallel. If it is difficult to spread these instructions out, limit\nbursts to four for a given thread (for "
			]
		  },
		  {
			"title": "Hexagon V79 HVX Programmer Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-N2040-61/memory.html",
			"excerpts": [
			  "HVX supports a local memory called vector tightly coupled memory\n(VTCM) for scratch buffers and scatter/gather operations.",
			  "The size of\nthe memory is implementation-defined.",
			  "VTCM needs normal virtual to physical\ntranslation just like other memory.",
			  "This memory has higher\nperformance and lower power.",
			  "Use VTCM for intermediate vector data, or as a temporary buffer.",
			  "It\nserves as the input or output of the scatter/gather instructions.",
			  "The\nfollowing are advantages of using VTCM as the intermediate buffer:",
			  "Guarantees no eviction (vs. L2 if the set is full)",
			  "Faster than L2$ (does not have the overhead of cache management, like\nassociation)",
			  "Reduces L2$ pressure",
			  "Lower power than L2$",
			  "Supports continuous read and write for every packet without\ncontention",
			  "In addition to HVX VMEM access, normal Hexagon memory access\ninstructions can access this memory."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies.",
			  "Decoupled software pipelining (DSWP) solves this by preferentially fetching and executing instructions from a program's critical path (CP). This allows DSWP to achieve high IPC with fast simple execution cores.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "Special microarchitectural support, called the\nsynchronization array (SA), provides low-latency inter-thread\nsynchronization and value communication and acts as a decoupling\nbuffer between the threads.",
			  "Decoupled execution ensures that stalls\nin one thread do not affect the other.",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Automatic Multithreading for DSWP: Unlike prior attempts at automatically multithreading sequential programs, DSWP does\nnot try to partition programs into totally independent threads.",
			  "We have a working compiler implementation that\nautomatically generates DSWP code."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP.",
			  "These automatic thread partitioning methods free the programmer\nfrom manual parallelization.",
			  " partitions loops\ninto long-running threads that communicate via inter-core queues.\nDSW",
			  "Parallel-Stage Decoupled Software Pipelining [16] (PS-DSWP)\nis an extension to combine pipeline parallelism with some stages\nexecuted in a DOALL, data-parallel fashion.",
			  "This paper presents the preliminary design of a new PS-DSWP method capable of handling arbitrary structured control flow, a slightly better ...Read more"
			]
		  },
		  {
			"title": "Phloem: Automatic Acceleration of Irregular Applications with ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2023.phloem.hpca.pdf",
			"excerpts": [
			  "Recent work has proposed hardware support for fine-grain pipeline parallelism, hiding long latencies by decoupling irregular applications into pipeline stages.Read more"
			]
		  },
		  {
			"title": "Parallel-Stage Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/cgo08_psdswp.pdf",
			"excerpts": [
			  "This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation.",
			  "We evaluated PS-DSWP on a set of com-\nplex loops from general-purpose applications. PS-DSWP showed\nup to 155% (114% on average) speedup with up to 6 threads on this\nset of loops, and showed better scalability than DSWP.",
			  "The base model used is a 6-core Itanium 2 CMP developed using\nthe Liberty Simulation Environment (LSE). The cores are validated\n(IPC and constituent error components accurate to within 6% of\nreal hardware for benchmarks measured [15]), and the details of\neach core in the model are given in Table 1. To this base model, a\nsynchronization array with 512 queues, each with 32 entries, was\nadded. The produce , consume\nand queue.set\ninstructions\nare also modeled in the pipeline of the Itanium 2 cores.",
			  "We have implemented PS-DSWP in the VELOCITY research\ncompiler [22]. The applications are first compiled into a low-level\nintermediate representation, and then transformed by a round of\nclassical optimizations. PS-DSWP was then automatically applied\nto the selected loops. Translation to Itanium 2 instructions is fol-\nlowed by another round of classical optimizations, scheduling, reg-\nister allocation, and a final scheduling phase.",
			  "t DSWP in combination\nwith speculation and parallel stages has the potential to unlock par-\nallelism among all the programs in the SPEC2000 integer bench-\nmark suite.",
			  "This paper presented a new non-speculative compiler transfor-\nmation called Parallel-Stage Decoupled Software Pipelining (PS-\nDSWP). PS-DSWP combines the pipeline parallelism of DSWP [13,\n16] with iteration-level parallelism of DOALL [1] in a single trans-\nformation. This paper described in detail the algorithms and the\ncompiler implementation required to automatically apply the PS-\nDSWP transformation."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators.",
			  "For other programs, however, these tools yield poor performance because they invoke computation units for instructions sequentially, without exploiting parallelism."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages.",
			  "Decoupled software pipelining\n(DSWP) [ 35 ] uses a synchronization array to facilitate com-\nmunication between cores or the threads of a multithreaded\ncore. But DSWP focuses on pipelining a single loop across\ndi ff erent threads, which is too limiting for irregular applications.\nFor example, BFS uses a 3-level nested loop, with stages",
			  " Pipette’s ISA fea-\ntures control mechanisms that allow e ff ective decoupling under\nirregular control flow. ",
			  "es.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers.",
			  "By time-multiplexing stages on the same\ncore, Pipette avoids load imbalance and achieves high core IPC.",
			  " it uses the\nphysical register file to implement queues cheaply, avoiding the\nstorage costs of prior techniques. Sec",
			  "Pipette decouples threads within\nthe core using architecturally visible queues."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more",
			  "This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual ...Read more"
			]
		  },
		  {
			"title": "Quantifying the Semantic Gap Between Serial and Parallel ...",
			"url": "https://par.nsf.gov/servlets/purl/10337772",
			"excerpts": [
			  "This measures the TLP limit of both cyclic multithreading (CMT). (e.g., DoAcross [20], HELIX [3]) and pipeline multithreading. (PMT) (e.g., DSWP [5], PS-DSWP [8]) ...Read more"
			]
		  },
		  {
			"title": "From Sequential Programs to Concurrent Threads - Liberty ...",
			"url": "https://liberty.princeton.edu/Publications/cal05_dswp.pdf",
			"excerpts": [
			  "Ottoni, E. Raman, and D. I.. August, “Practical and accurate low-level pointer analysis,” in Proc. of the Int'l. Symp. on Code Generation and Optimization, 2005 ..."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Since technology scaling no longer provides previously seen rates of performance improvements, microprocessor designers have adopted a number of other techniques to improve processor performance at reasonable energy costs. An entire class of techniques improves performance by offloading computation intensive parts of programs from a general-purpose processor to customized accelerators [15], [17], [18], [24], [31], [34]. One approach to build these customized accelerators is to manually design dedicated circuit modules for each application using hardware description languages (HDLs) [24], [26]. While this approach often yields the best result, it requires significant non-recurring engineering costs to convert algorithms to HDL specifications. Another less labor-intensive approach is to use high-level synthesis (HLS) tools to generate customized accelerators from programs written in C or other high-level programming languages [4], [12], [13], [16], [30], [31], [32], [37].",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Hexagon™ V73",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-53_REV_AB_Qualcomm_Hexagon_V73_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "L2FETCH is nonblocking. After the instruction is initiated, the program continues to the next\ninstruction while the prefetching is performed in the background.",
			  "When data cache prefetching is enabled, the Hexagon processor observes patterns of data cache\nmisses and attempts to predict future misses based on recurring patterns of misses where the\naddresses are separated by a constant stride. If such patterns are found, the processor attempts\nto automatically prefetch future cache lines."
			]
		  },
		  {
			"title": "Hexagon™ V69",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-50_REV_AA_Qualcomm_Hexagon_V69_ProgrammerS_Reference_Manual.pdf",
			"excerpts": [
			  "The l2fetch instruction is non-blocking: it initiates a prefetch operation that is performed in the\nbackground by the prefetch engine while the thread continues to execute Hexagon processor\ninstructions."
			]
		  },
		  {
			"title": "Qualcomm Hexagon V66 HVX Programmer's Reference ...",
			"url": "https://docs.alexrp.com/hexagon/hexagon_v66_hvx.pdf",
			"excerpts": [
			  "within a fixed time after execution starts. But, when the sources are required varies.",
			  "Instructions that need more pipelining require early sources. Only HVX registers are Early",
			  "Source registers. Early source operands include:",
			  "Input to the multiplier. For example “V3.h = vmpyh(V2.h, V4.h)”. V2 and V4 are",
			  "multiplier inputs. For multiply instructions with accumulation, the accumulator is",
			  "not considered an early source multiplier input.",
			  "Input to shift/bit count instructions. Only the register that is being shifted or",
			  "counted is considered early source. Accumulators are not early sources.",
			  "Input to permute instructions. Only registers that are being permuted are",
			  "considered early source (not an acccumulator).",
			  "Unaligned store data is an early source.",
			  "If an early source register is produced in the previous vector packet, an interlock stall can",
			  "be incurred. Software should strive to schedule an intervening packet between the",
			  "producer and an early source consumer.",
			  "Vector Instructions",
			  "The following example shows various interlock cases:",
			  "V8 = VADD(V0,V0)",
			  "V0 = VADD(V8,V9)",
			  "// NO STALL",
			  "V1 = VMPY(V0,R0)",
			  "// STALL due to V0",
			  "V2 = VSUB(V2,V1)",
			  "// NO STALL on V1",
			  "V5:4 = VUNPACK(V2) // STALL due to V2",
			  "V2 = VADD(V0,V4)",
			  "// NO STALL on V4"
			]
		  },
		  {
			"title": "Hexagon V60 HVX Programmer's Reference Manual",
			"url": "https://docs.qualcomm.com/doc/80-N2040-30/80-N2040-30_REV_C_Hexagon_V60_HVX_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "Certain instructions require Early Sources. Early source registers include:\n■\nInput to the multiplier. For example “ V3.h = vmpyh(V2.h, V4.h) ”. Here V2\nand V4 are multiplier inputs. For multiply instructions with accumulation, the\naccumulator is not considered an Early Source multiplier input.\n■\nInput to Shift/Bit Count instructions. For shifts, all vector sources are Early\nSource except for accumulators.\n■\nInput to Permute instructions. Only registers that are being permuted are\nconsidered Early Source (not Accumulator).\n■\nUnaligned Store Data is an Early Source.\nIf an Early Source register is produced in the previous vector packet, an interlock stall will\noccur. The software should try to schedule an intervening packet between the producer of\nan Early Source register. For example, the following shows various interlock cases:\nV8 = VADD(V0,V0)\nV0 = VADD(V8,V9) // NO STALL\nV1 = VMPY(V0,R0) // STALL due to V0\nV2 = VSUB(V2,V1) // NO STALL on V1\nV5:4 = VUNPACK(V2) // STALL due to V2\nV2 = VADD(V0,V4) // NO ST"
			]
		  },
		  {
			"title": "Qualcomm Corporate Template",
			"url": "https://llvm.org/devmtg/2017-02-04/Halide-for-Hexagon-DSP-with-Hexagon-Vector-eXtensions-HVX-using-LLVM.pdf",
			"excerpts": [
			  "Halide also provides a scheduling directive to prefetch data into the L2 cache.",
			  "For example, “ my_func.prefetch (y, 2)” will prefetch into the L2 cache,\n2 iterations worth of data needed in the ‘y’ loop.\n"
			]
		  },
		  {
			"title": "Qualcomm Hexagon V73 Programmer's Reference Manual ...",
			"url": "https://device.report/m/9617176bfefffcbc1efdcc55512335e567e0d9a1c8a2813a1538de55ed99cf3b",
			"excerpts": [
			  "... instruction scheduling. 1.3 Technical ... 5.10.6.3 Software-based l2fetch The l2fetch instruction provides powerful L2 prefetching of data or instructions.Read more"
			]
		  },
		  {
			"title": "Advanced Scheduling Guide",
			"url": "https://documentation-be.hexagon.com/bundle/Advanced-Scheduling-Notes---2021.0/raw/resource/enus/Advanced%20Scheduling%20Notes%20-%202021.0.pdf?save_local=true",
			"excerpts": [
			  "Advanced Scheduling Guide. Master>Scheduling>Advanced Scheduling Rules. Run No: Create different run numbers to enable comparison of different settings and ...Read more"
			]
		  },
		  {
			"title": "Hexagon V68 Programmer s Reference Manual",
			"url": "https://docs.qualcomm.com/bundle/publicresource/80-N2040-46_REV_B_Hexagon_V68_Programmers_Reference_Manual.pdf",
			"excerpts": [
			  "The Hexagon processor includes the instruction dcfetch . This instruction queries the L1\ndata cache based on the address specified in the instruction:",
			  "If the address is present in the cache, no action is taken.",
			  "If the cache line for the address is missing, the processor attempts to fill the cache\nline from the next level of memory. The thread does not stall, but rather continues\nexecuting while the cache line fill occurs in the background."
			]
		  }
		],
		"reasoning": "The core field describes allocation_algorithms for software-managed memory (SPM) and enumerates a progression from Static Allocation to Dynamic Allocation, including explicit DMA-driven data transfers, Profile-Guided Allocation, and Memory Coloring as a graph-coloring approach applied to the SPM pseudo-register file. The strongest support comes from excerpts that directly introduce and elaborate Memory Coloring as a compiler technique to allocate static data in SPM by partitioning SPM into a pseudo-register file and applying graph-coloring to data aggregates, with experimental results indicating efficient management of SPMs in embedded contexts. This aligns with the field’s fourth item, Memory Coloring, and with the broader theme of SPM management via compiler strategies. Additional, highly relevant support comes from excerpts detailing Dynamic Memory Allocation for scratch-pad memory, including dynamic placement decisions, compiler-inserted data transfers (e.g., DMA-based movement), and the claim of predictability and low overhead for dynamic schemes. Those passages anchor the field’s second item (Dynamic Allocation with explicit data transfers) and its emphasis on predictability and runtime efficiency. Static Allocation is represented in excerpts discussing compiler-decided/static decisions about SPM contents, providing a foundation for the field’s first item. Profile-Guided Allocation is represented by references to using runtime or profile information to identify hot data and guide allocation, which matches the field’s third item. Taken together, these excerpts coherently support each of the four components listed in the field value, from static through dynamic and profile-guided approaches to the memory-coloring technique, mapping to a coherent view of SPM allocation strategies in modern embedded toolchains. The strongest, most direct supports are those that explicitly define Memory Coloring and its graph-coloring rationale, followed by those that articulate dynamic DMA-based strategies, and then static and profile-guided concepts, which appear as adjacent discussions in the corpus. ",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.1",
		"citations": [
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance.",
			  ". Experiments show\nthat for 5 kernels from different domains, CGPA gives a 3.3x ge-\nomean performance improvement at 20% average energy overhead\ncompared to non-parallelized hardware specialization results.\nIn",
			  "h hardware module with indepen-\ndent control that implements instructions from the original loop is\ncalled a *worker* . Each worker has its own independent control cir-\ncuit and dedicated memory ports to the ca",
			  "\nFigure 2 shows a logical view of coarse-grained pipelined acceler-\nators with a Sequential–Parallel–Sequential (S-P-S) pipeline. The\nnumber of stages for different applications is not fixed, and is de-\ntermined automatically for each application by the CGPA com-",
			  "Figure 2: A logical view of CGPA architecture within the dashed\nbox. Each grey box contains circuit modules customized for the\ntargeted loop and generated by CGPA compiler. In this figure, a\nSequential–Parallel–Sequential (S-P-S) 3-stage pipeline is shown."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "WP+, splits a loop into new loops with dependence pat-\nterns amenable to parallelization using techniques that were\noriginally either inapplicable or poorly-performing.",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "DSWP extracts at most one thread per SCC.",
			  "the throughput is lim-\nited by the slowest stage (Speedup = 1 / *T* slowest stage ",
			  "This paper demonstrates significant performance\ngains on a commodity 8-core multicore machine running a\nvariety of codes transformed with DSWP+.",
			  "However, as Figure 1(c) shows, the loop-carried\ndependence pattern in each stage is different.",
			  "By choosing suitable parallelization strategies for\nStages 2 and 3, the execution times of SCC B and SCC C can\nbe reduced to that of SCC AD .",
			  "These primitives are implemented in software using\ncache-aware, concurrent lock-free queues [8].",
			  "DSWP+ transformation works in conjunction with\nother loop parallelization techniques.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  },
		  {
			"title": "Speculative Parallelization Using Software Multi-threaded ...",
			"url": "https://llvm.org/pubs/2010-03-ASPLOS-SpeculativeParallelization.pdf",
			"excerpts": [
			  "o provide improved levels of coarse-grained loop-level paral-\nlelism, Bridges et al. [7] and Thies et al. [29] use pipeline trans-\nformations such as Decoupled Software Pipelining (DSWP) [21],\nenhanced with speculation and replication of stages without any\ncross-ite",
			  "DSWP executes\nparts of the loop body in parallel, with dependences fl owing uni-\ndirectionally in a pipelined fashion.",
			  "Figure 6 shows this execution model.",
			  "Loop parallelization is done as follows: The speculative part of\nthe loop is wrapped in an MTX, with each iteration split into mul-\ntiple subTXs (shown in Figure 6 as similarly shaded ovals). These\nsubTXs are executed by the stages of the Spec-PS-DSWP pipeline.",
			  "Spec-PS-DSWP relies on a\ncommit stage (Figure 6, Commit Stage). The functionality needed\nin the commit stage is already encapsulated by the commit unit’s\nfunctionality in the SMTX system described in the previous sec-\ntion.",
			  "Spec-DOALL\nimplementation may be improved by employing techniques such\nas speculative fi ssion [33]. However, this results in sequential ex-\necution of the dependence recurrences outside the Spec-DOALL\nregion; Spec-PS-DSWP with SMTX executes them in parallel us-\ning pipelining.",
			  "DOACROSS can be used to good effect in these domains [3].",
			  "DOALL partitions the iteration space into groups that are executed\nconcurrently with no inter-thread communication.",
			  "DOALL often\nresults in speedup that is proportional to the number of threads;\nhowever, it is inapplicable when the loop has cross-iteration de-\npendences.",
			  "SWP parti-\ntions the loop body across the execution threads, and each thread is\nresponsible for all iterations of its piece of the loop body",
			  "As Fig-\nure 2(a) shows, ignoring the pipeline fi ll time, both DOACROSS\nand DSWP yield a speedup of 2x using 2 threads at steady state.",
			  "when the inter-core latency is increased to 2 cycles as in Fig-\nure 2(b), the speedup with DOACROSS reduces to 1.33x whereas\nthe speedup with DSWP remains 2x at steady state.",
			  "he primary contribution of this paper is the introduction of\nthe notion of a Software Multi-threaded Transaction (SMTX) to\nthe literature. SMTXs are ef fi ciently executed by a novel software\nruntime system that uses memory versioning. The SMTX system\nmaintains speculative state and non-speculative state separately\nusing process-separation, with non-speculative state owned by a\n ..."
			]
		  },
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "In addition to Outrider, Decoupled software pipelining. (DSWP) [35] uses a synchronization array to facilitate com- munication between cores or the threads of a ...Read more",
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more",
			  "tion uses the physical register file to im-\nplement queues at very low cost, putting otherwise-idle registers\nto use. ",
			  "Pipette\noutperforms data-parallel implementations of several challenging\nirregular applications by gmean 1.9 Ã (and up to 3.9 Ã ).",
			  "e Pipette BFS version uses pipeline parallelism\nto hide long-latency accesses e ff ectively.",
			  "Pipette achieves high performance by splitting the program\nacross each long-latency indirection , resulting in four stages",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "Pipette\nintroduces architectural support for pipeline parallelism within\nthe threads of a multithreaded core.",
			  "s.\nQueues hide latency very e ff ectively when they allow producer\nstages to run far ahead of consumers. ",
			  "Decoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism | Request PDF",
			"url": "https://www.researchgate.net/publication/346864128_Pipette_Improving_Core_Utilization_on_Irregular_Applications_through_Intra-Core_Pipeline_Parallelism",
			"excerpts": [
			  "Increasing core utilization to gain performance has been explored by pipelining and helper threading techniques [31]. Recently, Pipette [19] added ..."
			]
		  },
		  {
			"title": "Accelerating Irregular Applications with Pipeline Parallelism",
			"url": "https://people.csail.mit.edu/qmn/papers/nguyen_phd_thesis_2022.pdf",
			"excerpts": [
			  "We explore a different and more effective approach to improve uti- lization in simultaneous multithreading (SMT) cores: exploiting pipeline."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://ieeexplore.ieee.org/document/9251856/",
			"excerpts": [
			  "Instruction latencies are so large that even SMT cores running multiple data-parallel threads suffer poor utilization.We find that irregular applications have ...Read more"
			]
		  },
		  {
			"title": "Designing and Realizing Scalable Latency Tolerance for ...",
			"url": "https://parallel.princeton.edu/papers/isca22-marcelo.pdf",
			"excerpts": [
			  "DAE slices a program into two ... Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism.Read more"
			]
		  },
		  {
			"title": "Aggressive Pipelining of Irregular Applications on ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/3140659.3080228",
			"excerpts": [
			  "On spatial architectures such as FPGAs, dataflow graph can be implemented as a hardware pipeline (Figure 6 (d)) by properly embedding the graph.Read more"
			]
		  },
		  {
			"title": "Compiler Aspects of Hardware Accelerators - Computer Science",
			"url": "https://cseweb.ucsd.edu/~xil174/output0.pdf",
			"excerpts": [
			  "The soft processor handles irregular code like any other CPU-based architectures while the co-processor accelerates the more regular code. The compilation ...Read more"
			]
		  },
		  {
			"title": "(PDF) Efficient Mapping of Irregular C++ Applications to ...",
			"url": "https://www.researchgate.net/publication/262163399_Efficient_Mapping_of_Irregular_C_Applications_to_Integrated_GPUs",
			"excerpts": [
			  "We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with minimal changes. This ...Read more"
			]
		  },
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  "n the approxi-\nmately equivalent performance of these two techniques, fu-\nture designers can determine the more cost-effective hard-\nware implementatio",
			  "The EMS algorithm consists of five steps:",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  "*like Hierarchical Reduction, the code must be regenerated**\n**by inserting conditional branches after modulo scheduling.*",
			  "MS), combines\nthe benefits of the previous Modulo Scheduling techniques,\nIf-conversion with Predicated Execution and Hierarchical\nReduction, to remove the limitations of each technique",
			  "f-\nconversion eliminates the need for prescheduling the paths\nof the conditional construct, which is essential to Hierar-\nchical Reductio",
			  "ll three Modulo Scheduling techniques and GURPR*\nhave been implemented in a prototype compiler.",
			  "r exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reductio"
			]
		  },
		  {
			"title": "Enhanced modulo scheduling for loops with conditional branches.",
			"url": "https://puma.ub.uni-stuttgart.de/bibtex/196dcbc0b79dd30a7db4d46cb8950c83c",
			"excerpts": [
			  "Enhanced modulo scheduling for loops with conditional branches. ... N. Warter, G. Haab, K. Subramanian, and J. Bockhaus."
			]
		  },
		  {
			"title": "An Implementation of Swing Modulo Scheduling with ...",
			"url": "https://llvm.org/pubs/2005-06-17-LattnerMSThesis.pdf",
			"excerpts": [
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.\nE",
			  "\nEMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.\n",
			  "\nIf-conversion is performed to transform the loop body into straight line predicated code using\nthe RK algorithm [32].\n",
			  "While If-conversion does allow loops with conditional statements to be software pipelined, the\ndownside is that both execution path’s resources must be summed when determining the resource\nconstraints for the loop. This can lead to a pipelined loop that does not have an optimal II.\n3",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,. Modulo Scheduling the loop, applying modulo variable expansion, ...Read more",
			  "Enhanced Modulo Scheduling [41] (EMS) is another Modulo Scheduling technique to modulo sched-\nule loops with conditional branches by translating them into straight line code. If-conversion and\nHierarchical Reduction both place restrictions on the scheduling of instructions that may pre-\nvent Modulo Scheduling from achieving an optimal II. EMS attempts to avoid these problems by\ncombining the best of both algorithms. It uses If-conversion, without special hardware support, to\neliminate prescheduling conditional constructs. It uses the regeneration techniques like Hierarchical\nReduction to insert conditional statements back after Modulo Scheduling.",
			  "EMS consists of five basic steps: Applying If-conversion, generating the data dependence graph,\nModulo Scheduling the loop, applying modulo variable expansion, and finally regenerating the\nexplicit control structure of the code by inserting conditional branches.",
			  "The best type of hardware support is one that provides Predicated Execution , such as IA64 [20].\nPredicated Execution allows instructions to be nullified during their execution in the pipeline. So if\nan instruction is speculatively moved above a branch and the branch is taken, even if the instruction\nis already in the processor’s pipeline, the instruction can be nullified",
			  "If-conversion replaces conditional branches with a compare instruction that sets a flag. Instruc-\ntions that were dependent upon the conditional branch are now instructions that only execute if\nthe flag is set. If-conversion typically requires hardware support, but some algorithms [41] have\nmade slight modifications to avoid this. Hardware support such as predicated execution, common\non VLIW and EPIC architectures (such as IA64) set a conditional flag per instruction and allow\ninstructions to execute only when the conditional flag is tru",
			  "The second main contribution of this thesis is to extend the Swing Modulo Scheduling algorithm\nto handle more than single basic block loops. We have modified the algorithm to handle superblock\nloops (single-entry, multiple-exit, multiple basic block loops). These extensions allow the algorithm\nto be applied at various stages of compilation:\n• Statically: Just as the original Swing Modulo Scheduling algorithm was done statically, the\nextensions simply expand the number of loops acceptable for this transformation.\n• Offline: Using profile information, hot paths (subsets of the loop in the form of single entry,\nmultiple exit loops), can be recognized and be transformed by our extensions.\n• Runtime: Similar to the profile driven optimization, hot paths are found during program\n .."
			]
		  },
		  {
			"title": "Effective Cluster Assignment for Modulo Scheduling",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-98-modulo.pdf",
			"excerpts": [
			  "Llosa, et al., 'Swing Modulo Scheduling: A lifetime-. Sensitive Approach,' IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques,.Read more"
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. ",
			  "An empirical study of the importance of an architectural support, referred to as predicted execution, for the effectiveness of software pipelining is presented to help future microprocessor designers to determine whether predicated execution support is worthwhile given their own estimation of the increased hardware cost.",
			  "An Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction or If-conversion with Predicated Execution. Expand",
			  "A multiple-II modulo scheduling technique is proposed for architectures with predicated execution that maintains the benefits of a local scheduling based approach while reducing the effective II. Expand",
			  "This dissertation shows that Modulo Scheduling with Predicated Execution has better performance and lower code expansion than Modulo scheduling with ICTs on processors without special hardware support. Expand",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "Loops with conditional branches have multiple execution paths which are difficult to software pipeline. The modulo scheduling technique for software pipelining addresses this problem by converting loops with conditional branches into straight-line code before scheduling.",
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle.",
			  "However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "m](/author/M.-Lam/39682108)\nPublished in SIGP 1 June 1988\nComputer Science\nTLDR\nThis paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. Expan",
			  "lic Array Optimizing Compiler](/paper/A-Systolic-Array-Optimizing-Compiler-Lam/4b463c767e23a011e6da6f6ea4c8536449893f55)\n[M. Lam](/author/M.-Lam/39682108)\nComputer Science, Engineering\n1989\nTLDR\nThis thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics. Expan",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines",
			  "This thesis shows that software pipelining is a practical and efficient code scheduling technique for highly parallel and pipelined processors, and extends the previous scheduling algorithm and introduced a new optimization called modulo variable expansion, showing that near-optimal results can be obtained using software heuristics.",
			  "\n[### A compilation technique for software pipelining of loops with conditional jumps](/paper/A-compilation-technique-for-software-pipelining-of-Ebcioglu/c7351dc5f5fd24019da946e204796883c60317d3)\n[K. Ebcioglu](/author/K.-Ebcioglu/3310105)\nComputer Science\nMICRO 20\n1987\nWe describe a compilation algorithm for efficient software pipelining of general inner loops, where the number of iterations and the time taken by each iteration may be unpredictable, due to … Expand",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme ...",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper describes the scheduling algorithms, highlights the modifications required to use profile information, and explains the hardware and compiler support for dealing with hazards that arise from aggressive use of profile information."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine,\nthus allowing us to achieve similar performanc",
			  " software pipelining, itera-\ntions of a loop in a source program are continuously initiated\nat constant intervals without having to wait for preceding\niterations to complete.",
			  "this paper proposes a hierarchical\nreduction\nscheme whereby entire control constructs are reduced to an\nobject similar to an operation in a basic block. Scheduling\ntechniques previously defined for basic blocks can be applied\nacross basic blocks.",
			  "is paper extends previous results of software pipelining\nin two ways: First, this paper shows that by using an im-\nproved algorithm, near-optimal performance can be obtained\nwithout\nspecialized hardware.",
			  "cond, we propose a\nhierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block.",
			  "The Warp array is a linear array of VLIW\nprocessors, each capable of a peak computation rate of 10\nmillion floating-point operations per second (10 MFLGPS).",
			  " techniques proposed have been validated by an im-\nplementation of a compiler for Warp, a systolic array consist-\ning of 10 VLIW processors",
			  "Software pipelining, as addressed here, is the problem of\nscheduling the operations within an iteration, such that the\niterations can be pipelined to yield optimal throughput,",
			  "This paper shows that software pipelining is a practical,\nefficient, and general technique for scheduling the parallelism\nin a VLIW machine.",
			  "he hierarchical reduction technique is\nto make software pipelining applicable to all innermost loops,\nincluding those containing conditional statements.\n",
			  "With this scheme, all innermost loops, including\nthose containing conditional statements, can be software\npipelined.",
			  "archical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtained.",
			  "Software Pipelining:\nAn Effective Scheduling Technique for VLIW Machines",
			  "In\nsoftware pipelining, iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete.",
			  "This paper extends previous results of software pipelining\nin two ways: First, this paper shows that by using an im-\nproved algorithm, near-optimal performance can be obtained\nwithout\nspecialized hardwar",
			  "Second, we propose a\nhierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block. With this scheme, all innermost loops, including\nthose containing conditional statements, can be software\npipelined.",
			  "ierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtain",
			  "The thesis of this paper is that software pipelining [24,25,30] is a viable alternative technique for scheduling VLIW processors. In software pipelining, itera ...",
			  "Software pipelining is more attractive than source code\nunrolling for two reasons. First, software pipelining offers the\npossibility of achieving optimal throughput.",
			  "The second reason, a practical concern, is perhaps\nmore important. In trace scheduling, the performance almost\nalways improves as more iterations are unrolled.",
			  " As the degree of unrolling increases, so do the\nproblem size and the fmal code size.",
			  "Therefore, the compilation time is unaffected.",
			  "urthermore, unlike source unrolling, there is an optimal de-\ngree of unrolling for each schedule, and can easily be deter-\nmined when the schedule is comple",
			  "relies primarily on source code unrolling. At the end of each\niteration in the original source is an exit out of the loop; the\nmajor trace is constructed by assuming that the exits off the\nloop are not taken.",
			  "Software pipelining, as addressed here, is the problem of scheduling the operations within an iteration, such that the iterations can be pipelined to yield ...Read more",
			  "\nThis paper shows that software pipelining is an effective\nand viable scheduling technique for VLIW processors. ",
			  " In\nsoftware pipelining, iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete. ",
			  " The advantage of software pipelining is that\noptimal performance\ncan be achieved with compact object\ncode.\n",
			  "\nThe techniques proposed have been validated by an im-\nplementation of a compiler for Warp, a systolic array consist-\ning of 10 VLIW processors.",
			  "t software pipelining is a practical,\nefficient, and general technique for scheduling the parallelism\nin a VLIW machine.",
			  "We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsion.",
			  "ion. The latter implements part of the functionality of the\nspecialized hardware proposed in the polycyclic machine",
			  "In software pipelining, itera-\ntions of a loop in a source program are continuously initiated\nat constant intervals without having to wait for preceding\niterations to complete.",
			  "This compiler has been used for\ndeveloping a large number of applications in the areas of",
			  "hierarchical reduction scheme whereby entire control con-\nstructs are reduced to an object similar to an operation in a\nbasic block.",
			  "The advantage of software\npipelining is that optimal performance can be achieved with\ncompact object code.",
			  "iterations of a loop in the source program\nare continuously initiated at constant intervals, before the\npreceding iterations complete.",
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine",
			  "e have extended previous results of\nsoftware pipelining in two ways. First, we show that near-\noptimal results can often be obtained for loops containing\nboth intra- and inter-iteration data dependency, using software\nheuristics. We have improved the scheduling heuristics and\nintroduced a new optimization called modulo variable expan-\nsio"
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines - SUIF",
			"url": "https://suif.stanford.edu/papers/lam03-sp.pdf",
			"excerpts": [
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures.\n ... \nand Glaeser developed the first compiler with software pipelining\nfor the polycyclic architecture, which had a novel crossbar whose\ncross-points provided a programmable form of delay to support\nsoftware pipelining[7].\nIntegrated floating-point adder and multiplier chips were first in-\ntroduced by Weitek Inc. in 1984. This led to a flurry of activities in\ndeveloping “high-performance” numerical engines, among which\nwere the commercial Multiflow Trace computer and the CMU Warp\nsystolic array. The latter research project provided the context in\nwhich this paper was written. The availability of integrated floating-\npoint chips was, of course, just a milestone along the road to single-\nchip ILP processors. Today, all modern general-purpose machines\nemploy ILP and instruction scheduling is needed in all optimizing\ncompilers.\n**2.**\n**CONTRIBUTIONS OF THE PAPER**\nAt the time the paper was written, trace scheduling[5] was con-\nsidered to be the technique of choice for scheduling VLIW (Very\nLong Instruction Word) machines. This paper establishes software\npipelining as a useful static scheduling technique for VLIW pro-\ncessors without requiring specialized architectural support.\nThis paper has three major results. First, instead of relying on\nspecialized hardware support like a polycyclic interconnect, this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a smal",
			  "The basic idea behind software pipelining was first developed by. Patel and Davidson for scheduling hardware pipe-lines. As instruction- level parallelism made ...Read more",
			  "RETROSPECTIVE:\n**Software Pipelining:**\n**An Effective Scheduling Technique for VLIW Machines**\nMonica S. Lam\nComputer Systems Laboratory\nStanford University\n**ABSTRACT**\nThe basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures",
			  "The basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling.",
			  "In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines",
			  "The most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support.",
			  "Our understanding of software pipelining\nsubsequently deepened with the work of many others.",
			  "And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.",
			  "software pipelining is effective on VLIW machines without compli-\ncated hardware support. O",
			  "At the time the paper was written, trace scheduling[5] was con-\nsidered to be the technique of choice for scheduling VLIW (Very\nLong Instruction Word) machines. This paper establishes software\npipelining as a useful static scheduling technique for VLIW pro-\ncessors without requiring specialized architectural support.",
			  " this\npaper shows that the same effect can be achieved by using mod-\nulo variable expansion and unrolling the generated code a small\nnumber of times. Se",
			  "The main difference is that tasks in their pipelines are\nindependent of each other, whereas iterations in a loop may have\ndependences between them.\nThe",
			  ". Software pipelining is applicable\nonly to codes with predictable behavior like numerical applications;\nas such, it only expands the number of instructions in innermost\nloops slightly. O",
			  ". Software pipelining has been used in proces-\nsors from HP, IBM, Intel, SGI, Sun and Texas Instrument, none of\nwhich has any specialized software pipelining support.",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without compli-.",
			  "**Software Pipelining:**",
			  "**An Effective Scheduling Technique for VLIW Machines**",
			  "Monica S. Lam",
			  "Computer Systems Laboratory",
			  "Stanford University",
			  "**ABSTRACT**",
			  "The basic idea behind software pipelining was first developed by\nPatel and Davidson for scheduling hardware pipe-lines. As instruction-\nlevel parallelism made its way into general-purpose computing, it\nbecame necessary to automate scheduling. How and whether in-\nstructions can be scheduled statically have major ramifications on\nthe design of computer architectures. Rau and Glaeser were the first\nto use software pipelining in a compiler for a machine with spe-\ncialized hardware designed to support software pipelining. In the\nmeantime, trace scheduling was touted to be the scheduling tech-\nnique of choice for VLIW (Very Long Instruction Word) machines.\nThe most important contribution from this paper is to show that\nsoftware pipelining is effective on VLIW machines without compli-\ncated hardware support. Our understanding of software pipelining\nsubsequently deepened with the work of many others. And today,\nsoftware pipelining is used in all advanced compilers for machines\nwith instruction-level parallelism, none of which, except the Intel\nItanium, relies on any specialized support for software pipelining.\n**1.**\n**BACKGROUND**\nThe story of software pipelining cannot be told without also telling\nthe story of instruction-level parallelism (ILP) in computer archi-\ntectures"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations.",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines > Abstract",
			  "Section Title: Software pipelining: an effective scheduling technique for VLIW machines > References",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "‪Monica S. Lam‬ - ‪Google Scholar‬",
			"url": "https://scholar.google.com/citations?user=4hS0jZ8AAAAJ&hl=en",
			"excerpts": [
			  "Software pipelining: An effective scheduling technique for VLIW machines",
			  "Lam",
			  "Software pipelining: An effective scheduling technique for VLIW machines. M Lam. Proceedings of the ACM SIGPLAN 1988 conference on Programming Language …, 1988.Read more"
			]
		  },
		  {
			"title": "(PDF) Modulo scheduling without overlapped lifetimes",
			"url": "https://www.academia.edu/125636329/Modulo_scheduling_without_overlapped_lifetimes",
			"excerpts": [
			  "M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. In Proc. of the ACM SIGPLAN '88 Conference on Programming Language Design and ...Read more"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for ...",
			"url": "https://dl.acm.org/doi/10.1145/989393.989420",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in ...Read more",
			  "The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without complicated hardware support. Our ...Read more"
			]
		  },
		  {
			"title": "Software pipelining, VLIW, EPIC, instruction-set support",
			"url": "https://www.doc.ic.ac.uk/~phjk/AdvancedCompArchitecture/Lectures/pdfs/Ch06-StaticSchedulingForILP.pdf",
			"excerpts": [
			  "Monica S. Lam, Robert P. Wilson. 19th ISCA, May 1992, pages 19-21. Limits of ... To help trace scheduling and software pipelining, the Itanium.Read more",
			  "IA-64",
			  "EPIC = “2nd generation VLIW”?",
			  "VLIW: Very Large Instruction Word"
			]
		  },
		  {
			"title": "Modulo Scheduling with Reduced Register Pressure",
			"url": "https://upcommons.upc.edu/bitstreams/9de3a477-f5f8-4307-88c5-16ab35f9e150/download",
			"excerpts": [
			  "Lam, “Software Pipelining: An Effective Scheduling Tech-\nnique for VLIW Machines,” *Proc. SIGPLAN ’88 Conf. Programming*\n*Language Design, and Implementation* , pp. 318-328, June 1988."
			]
		  },
		  {
			"title": "Compile-time ILP extraction Modulo Scheduling",
			"url": "https://courses.e-ce.uth.gr/CE658/lectures/Lecture05_ModuloScheduling.pdf",
			"excerpts": [
			  "Static scheduling – Explicit loop unrolling",
			  "Static scheduling – Explicit loop unrolling",
			  "Static scheduling – Explicit loop unrolling",
			  "(software pipelining)",
			  "Modulo scheduling* is an algorithm for software\npipelinin",
			  "All loop iterations have a common schedule\nModulo scheduling requires hardware support\nThe number of cycles between scheduling\nsuccessive loop iterations is called iteration\ninterval (II)\nWe want to minimize II",
			  "Initiation Interval\nDetermine lower bound on II (MII)\nAttempt to generate a schedule at that II\nThere are two constraints on the MII\nResource constraints\nRecurrence constraints",
			  "uter Architecture\n1\n**CE 658**\n**Advanced Computer**\n**Architecture**\n**Fall 2018**\n**Compile-time ILP extraction**\n**Modulo Scheduling**\n**Nikos Bellas**\n**Computer and Communications Engineering Department**\n**University of Thessaly**\nParallel Computer Architecture\n2\n**Readings for this lecture**\n**Hardware and Software for VLIW and EPIC**\nH&P, Appendix G in v.4\nH&P, Chapter 4 in v.3\n**S** **cheduling techniques**\nB. Rau, et. al *“Code Generation Schema for Modulo Scheduled Loops”,*\nMicro 24, December 1992\nCT Hwuang, et. al *“PLS: A Scheduler for Pipeline Synthesis”, IEEE*\n*Transactions on CAD,* September 1993\nJosep Llosa, et. al. *“Swing Modulo Scheduling: A Life* *-Time Sensitive*\n*Approach,* ” , PACT 1996\nParallel Computer Architecture\n3\nStatic scheduling – Explicit loop unrolling\nSchedule code for a superscalar processor considering the\nfollowing latencies:\nMemory instr: 2 cycles\nFP instr: 3 cycles\nBranch : 1 cycle\nParallel Computer Architecture\n4\nStatic scheduling – Explicit loop unrolling\nFirst attempt: straightforward scheduling\n5 instructions in 9 cycles (IPC = 0.56)\n11% scheduling efficiency (5 instructions in 9*5=45 slots)\nOnly 2 FP registers used ($f0, $f4)\nParallel Computer Architecture\n5\nStatic scheduling – Explicit loop unrolling\nMuch better with implicit loop unrolling (7 times) and scheduling\n(software pipelining)\n23 instructions in 9 cycles (IPC = 2.56)\n51% scheduling efficiency\n14 FP registers used (register pressure)\nSchedule that eliminates all stalls\nParallel Computer Architecture\n6\nDynamic vs Static Scheduling\nSuperscalar processors decide on-the-fly how\nmany and which instructions to issue\nComplex hardware is used to check for\ndependencies and schedule operations\nThe window of instructions that are checked\neach time is limited by the hardware resources\n•\nAll checking has to be done at real time\n•\nMinimal compiler assistance required\nParallel Computer Architecture\n7\nDynamic vs Static Scheduling\nStatically scheduled processors rely on the\ncompiler to achieve good performance\nMain objective of the compiler is to reduce\npotential data hazards\nThe processor still has to stall in case of\nhazards\nThe extreme case of static scheduling is VLIW\nParallel Computer Architecture\n8\nVLIW technology\nDue to the growing complexity to scale the dynamic\nscheduling techniques, the industry started to re-\nexamine explicit encoding of multiple operations (mid\n90's)\nRely on compiler technology to form the instructions in\nan issue packet and schedule all of them\nsimultaneously\n•\nVLIW – Very Long Instruction Word\n•\nMulti-operation instructions (64-128 bits or more)\nThe hardware does not check for dependencies\nno dependencies among operations in the same issue\nword\nParallel Computer Architecture\n9\nVLIW approach\nOnly sensible for wide-issue processors (> 4 slots)\nMultiple operations packed in one instruction\nEach operation slot corresponds to a specific FU\nConstant operation latencies specified\nParallelism within an instruction.\nBasic compiler techniques are the same for VLIW or statically\nscheduled processors\nParallel Computer Architecture\n10\nVLIW approach\nThe compiler:\nschedules for maximum throughput\nAs many useful operations as possible within an\ninstruction\nWith the help of profiling feedback\nModulo scheduling, trace scheduling, speculation,\npredication, superblocks, hyperblocks ........\nGuarantees intra-instruction parallelism\nSchedules to avoid data hazards\npure VLIW engines have no interlocks !\nParallel Computer Architecture\n11\nVLIW approach\nVLIW (at least the original forms) has several short-comings\nthat precluded it from becoming mainstream:\n•\nVLIW instruction sets are not backward compatible between implementations.\nWhen wider implementations (more execution units) are built, the instruction set\nfor the wider machines is not backward compatible with older, narrower\nimplementations.\n•\nLoad responses from a memory hierarchy which includes CPU caches and\nDRAM do not have a deterministic delay. This makes static scheduling of load\ninstructions by the compiler very difficult and inefficient.\n•\nCode explosion with all these compiler techniques\nJosh Fisher. Very Long Instruction Word architectures and the ELI-512 .\n*Proceedings of the 10th annual international symposium on Computer architecture* .\n1983. New York, NY\nThis work resulted in a company called Multiflow.\nParallel Computer Architecture\n12\nModulo Scheduling\nA Class of Methods for Software Pipelining\nLoops\nParallel Computer Architecture\n13\nSoftware Pipelining\nA technique for scheduling instructions to exploit\nILP in inner loops\nMany programs spend most of their time there\nParallelism between loop iterations\nThe technique is based on overlapping multiple\nloop iterations and executing them in parallel\nSimilar to multiple instruction execution in hardware\npipelining\nIt attempts to achieve the benefits of loop unrolling\nwithout performing explicit loop unrolling\nParallel Computer Architecture\n14\nModulo scheduling\n*Modulo scheduling* is an algorithm for software\npipelining\nAll loop iterations have a common schedule\nModulo scheduling requires hardware support\nThe number of cycles between scheduling\nsuccessive loop iterations is called iteration\ninterval (II)\nWe want to minimiz"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete.",
			  "The advantage of software pipelining is that optimal performance can be achieved with compact object code.",
			  "First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined.",
			  "It also diminishes the start-up cost of loops with small number of iterations.",
			  "The techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors.",
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining ...",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Chapter 4 – VLIW",
			"url": "https://www.ece.ucdavis.edu/~akella/270W05/chapter4.ppt.pdf",
			"excerpts": [
			  "• **SW is symbolic so does not increase the code**",
			  "• **Does not increase register pressure, however**",
			  "*\n**register management is tricky - lifetimes of**",
			  "\n**registers values can be long**\n",
			  "• **Loop unrolling and SW pipelining attack different**",
			  "**problems so in general they can be combined**",
			  " **LU - reduces loop overhead I.e. branch and**\n",
			  "\n**counter update**\n",
			  " **SWP - reduces the time when the loop is not**\n",
			  "*\n**running at its peak**",
			  "Software pipelining is different - it involves reorganizing a loop with ... • This helps both hw based scheduling and SW pipelining and global ..."
			]
		  },
		  {
			"title": "Intel® IA-64 Architecture Software Developer's Manual",
			"url": "https://portal.cs.umbc.edu/help/architecture/24531701.pdf",
			"excerpts": [
			  "IA-64 provides extensive support for software-pipelined loops, including register rotation, special\nloop branches, and application registers. When combined with predication and support for\nspeculation, these features help to reduce code expansion, path length, and branch mispredictions\nfor loops that can be software pipelined.",
			  "The beginning of this chapter reviews basic loop terminology and instructions, and describes the\nproblems that arise when optimizing loops in the absence of architectural support. The IA-64\nspecific loop support features are then introduced. The remainder of this chapter describes the\nprogramming and optimization of various type of loops using the IA-64 features."
			]
		  },
		  {
			"title": "Developing High-Performance Applications for HP-UX on IA-64",
			"url": "http://www.3kranger.com/Interact/Features/0101devIA64.shtm",
			"excerpts": [
			  "The IA-64 architecture provides special branches, along with rotating registers (including predicates), which allow the compiler to generate software pipelined ...Read more"
			]
		  },
		  {
			"title": "Software pipelining - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Software_pipelining",
			"excerpts": [
			  "Lam showed that special hardware is unnecessary for effective modulo scheduling. Her technique, *modulo variable expansion* is widely used in practice.",
			  "The trick is to replicate the body of the loop after it has been scheduled, allowing different registers to be used for different values of the same variable when they have to be live at the same time.",
			  "ca_Lam \"Monica Lam\") presents an elegant solution to this problem in her thesis, *A Systolic Array Optimizing Compiler* (1989) ( [ISBN](/wiki/ISBN_(identifier) \"ISBN (identifier)\") [0-89838-300-5](/wiki/Special:BookSources/0-89838-300-5 \"Special:BookSources/0-89838-300-5\") ). She calls it *modulo variable expansion* . "
			]
		  },
		  {
			"title": "The most prominent example of VLIW processors was ...",
			"url": "https://news.ycombinator.com/item?id=38676884",
			"excerpts": [
			  "The most prominent example of VLIW processors was the Itanic, er, Itanium. It, er, didn't work out well. Hence Itanic. Their premise was that the compiler ...Read more"
			]
		  },
		  {
			"title": "Microarchitectural Techniques for Instruction-driven Specialization ...",
			"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-210.pdf",
			"excerpts": [
			  "The most ambitious attempt to apply VLIW to general-purpose workloads was Intel’s\nItanium, based on the Explicitly Parallel Instruction Computing (EPIC) model and the IA-\n64 instruction set",
			  "EPIC was an evolution of the VLIW concept—drawing on ideas from the\nearlier Cydrome and TRACE architectures—while adding ISA-level features to maximize\ngeneral-purpose ILP and software compatibility."
			]
		  },
		  {
			"title": "Field-testing IMPACT EPIC Research Results in Itanium 2.",
			"url": "http://impact.crhc.illinois.edu/shared/papers/field2004.pdf",
			"excerpts": [
			  "red-\nication, explicit control speculation, data speculation, and\nmodulo scheduling aids",
			  "rformance in control-intensive, general-purpose pro-\ngrams, therefore, depends heavily on the compiler’s ILP-\nexposing transformations, which IA-64 supports with pred-\nication, explicit control speculation, data speculation, and\nmodulo scheduling ai"
			]
		  }
		],
		"reasoning": "The target fine-grained field value identifies CGPA as a coarse-grained, HLS-enabled extension of the DSWP paradigm, specifically designed to generate hardware accelerators from loop bodies with irregular behavior. The most directly relevant excerpts define CGPA as an HLS framework that partitions individual loops into separate pipelined stages and generates buffer-connected hardware modules, enabling coarse-grained parallelism and selective replication of code sections to manage hardware cost. This aligns with the field value’s description of PS-DSWP-inspired partitioning adapted for hardware accelerator design, focusing on selective replication of code regions to minimize area and interconnect. Supporting material also discusses CGPA’s motivation to tolerate irregular memory accesses and to harness parallelism at a coarse granularity, which matches the use case of loops with irregular memory patterns. Additional context from related works on decoupled software pipelining, PS-DSWP, and pipeline parallelism further reinforces how CGPA fits into the broader family of ILP-exposing transformations that aim to balance throughput with hardware cost. Taken together, the excerpts provide coherent, corroborating evidence that CGPA is the central technique described in the fine-grained field value, including its core principle of selective replication and coarse-grained pipeline structuring for hardware accelerators.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.3.primary_use_case",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine"
			]
		  }
		],
		"reasoning": "The finegrained field value describes a foundational method for applying software pipelining to loops with conditional statements on any VLIW machine, without relying on predication hardware support. Content from the excerpts directly supports this: multiple discussions present software pipelining as an effective technique for VLIW machines, with additional emphasis on enabling broad applicability across innermost loops and conditional statements. One excerpt introduces a hierarchical reduction scheme that reduces entire control constructs to an object akin to a basic-block operation, which directly sustains consistent performance improvements and effectively handles conditionals within software-pipelined loops. This addresses the core challenge of conditional branches in VLIW contexts by restructuring control flow to fit the pipelining model, thereby aligning with a foundational, hardware-agnostic approach. Other excerpts reiterate the same theme by explaining near-optimal software-pipelined performance without specialized hardware and emphasizing the integration of hierarchical reduction with software pipelining to extend applicability to complex control flow. Taken together, these excerpts coherently support the idea of a foundational, broadly applicable software-pipelining method for loops with conditionals on VLIW machines, independent of predication capabilities.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.2.core_principle",
		"citations": [
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "The EMS algorithm consists of five steps:",
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "Modulo schedule the loop body.",
			  "Generate the data dependence graph.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  }
		],
		"reasoning": "The most directly relevant excerpt explicitly mentions converting a loop body using if-conversion to predicated code, which is the first step described in the field value. Following that, the five-step EMS algorithm outline aligns with the field description of applying modulo scheduling after the predicated transformation and then regenerating explicit control flow. Other excerpts discuss EMS performance benefits, the notion of predicated execution, and the data-path/pipeline aspects that arise from modulo scheduling under EMS, which corroborate the claim of combining if-conversion with hierarchical reduction and modulo scheduling. The remaining excerpts provide corroborating details about EMS’ relative performance, implementation notes, and comparisons to other scheduling approaches, reinforcing the EMS-centric narrative without introducing conflicting techniques. Overall, the connections are coherent and consistently support the EMS narrative and its three-part workflow (if-conversion to predicated straight-line code, modulo scheduling, regeneration of control flow).",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.3.core_principle",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine"
			]
		  }
		],
		"reasoning": "The finegrained field value describes a software-only technique that enables software pipelining of loops with conditionals by recursively reducing an entire control construct into a single pseudo-operation. The excerpts collectively articulate this exact idea: hierarchical reduction is presented as a scheme whereby whole control constructs, including conditional branches, are reduced to an object comparable to a basic block operation, which in turn facilitates software pipelining without hardware specialization. Specifically, one excerpt states that hierarchical reduction allows innermost loops, including those with conditional statements, to be software pipelined and that it diminishes the startup cost of loops with few iterations. This directly supports the notion of turning a conditional control flow into a unified pseudo-operation with a resource usage pattern that represents the union of all internal paths and whose length corresponds to the maximum path length. Additional excerpts reinforce that the technique is a central mechanism in software pipelining for VLIW architectures, describing hierarchical reduction as a method that complements software pipelining and yields consistent performance improvements. Collectively, these points map closely to the field value: a software-only reduction that creates a single pseudo-operation encapsulating all path resources and maximum path length, enabling effective pipelining across conditionals. The excerpts that emphasize the hierarchical reduction concept and its impact on loop-pipelining without hardware-specific features provide direct alignment with the finegrained field value.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.3.method_name",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine"
			]
		  }
		],
		"reasoning": "The target field value is Hierarchical Reduction. Several excerpts explicitly name and describe hierarchical reduction as a scheme used in VLIW software pipelining. One excerpt states that hierarchical reduction reduces entire control constructs to an object similar to an operation in a basic block, which directly defines the technique and its core idea. Another excerpt notes that this scheme permits all innermost loops, including those with conditional statements, to be software pipelined, illustrating practical impact. Additional excerpts reiterate that hierarchical reduction complements software pipelining, reinforcing its role as a distinct technique within VLIW scheduling strategies. The presence of the term itself across multiple excerpts, along with concrete descriptions of how the technique operates and its benefits (e.g., reducing startup costs, enabling pipelining of complex control structures), directly supports the field value. The discussed context aligns with the research and niche discussions around advanced VLIW scheduling methods and the hierarchical reduction approach, making the connection solid and explicit.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.2.method_name",
		"citations": [
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "The EMS algorithm consists of five steps:",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct"
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  }
		],
		"reasoning": "The phrase Enhanced Modulo Scheduling (EMS) refers to a software-pipelining technique designed to schedule loops in a way that minimizes the initiation interval and handles loops with conditional branches or irregular control flow. Several excerpts clearly articulate EMS as a technique for scheduling loops to achieve better performance on VLIW processors, including descriptions of EMS as a five-step process and its emphasis on converting loop bodies into straight-line predicated code, as well as on creating and using the data dependence graph to modulo-schedule the loop body. This set of excerpts directly supports the notion that EMS is a structured scheduling method aimed at generating a software pipeline with improved throughput for loop-intensive code. Additional excerpts describe how EMS handles branches via if-conversion and predicated execution, illustrating the mechanisms by which EMS deals with control-flow complexity and conditional execution, which are integral to EMS in practice. Some excerpts also compare EMS performance against alternative scheduling strategies (e.g., hierarchical reduction) and discuss scenarios where EMS offers measurable improvements, thereby reinforcing the relevance of EMS as a core technique in advanced VLIW scheduling. Together, the excerpts connect the field value to EMS by (a) defining EMS, (b) detailing its steps and components (e.g., data dependence graph, modulo scheduling of the loop body, predication or if-conversion), and (c) situating EMS in performance comparisons and practical implementations on VLIW-like architectures.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.0.primary_use_case",
		"citations": [
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more",
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  }
		],
		"reasoning": "The most directly relevant information centers on decoupled software pipelining (DSWP) as a technique to schedule loops with long variable latencies and irregular control flow, such as recursive or pointer-chasing traversals. Excerpts describing how DSWP splits work into CP and off-CP threads and pipelines them to hide latency directly address the challenge of loop-carried dependencies that hinder traditional scheduling. Excerpt emphasizes that static scheduling falls short for variable latencies and that DSWP enables long-running threads to execute concurrently, which aligns with the goal of interleaving independent computations to fill parallel slots when a critical path (like a hash or traversal) becomes a bottleneck. Additional excerpts outline the methodology of DSWP in general loops, and its LLVM-based implementations, illustrating practical pathways for extracting thread-level parallelism and inserting synchronization between split threads to maintain data dependence correctness. The surrounding discussions of multicore and thread-based execution models provide context for applying DSWP to heterogeneous VLIW-like pipelines, where multiple functional units and per-cycle constraints require careful scheduling of independent work to keep all slots busy despite irregular access patterns. The corroborating material on advancing DSWP concepts (e.g., parallel-stage variants) reinforces the idea that breaking work into stages or threads can mitigate latency and irregular control flow, which is precisely the type of technique sought for the described research problem.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.3.key_benefit",
		"citations": [
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | ACM SIGPLAN Notices",
			"url": "https://dl.acm.org/doi/abs/10.1145/960116.54022",
			"excerpts": [
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware. Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the start-up cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.",
			  "Second, we propose a *hierarchical reduction* scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block.",
			  "This paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, near-optimal performance can be obtained without specialized hardware."
			]
		  },
		  {
			"title": "An Effective Scheduling Technique for VLIW Machines",
			"url": "https://suif.stanford.edu/papers/lam-sp.pdf",
			"excerpts": [
			  "Hierarchical reduction comple-\nments the software pipelining technique, permitting a consis-\ntent performance improvement be obtaine"
			]
		  }
		],
		"reasoning": "The requested fine-grained field value asserts that software pipelining can be applied to all innermost loops, not just straight-line code, using a purely software-based approach, and notes portability with occasional suboptimality compared to advanced techniques like EMS. Excerpts explicitly discuss software pipelining enhancements and introduce hierarchical reduction as a means to reduce entire control constructs to an operation in a basic block, enabling software pipelining of innermost loops, including those with conditional statements, and also highlight that this approach improves startup costs and delivers near-optimal performance without specialized hardware. These statements directly support the claimed capability to extend software pipelining to innermost loops (beyond straight-line code) and characterize the trade-offs (portability and potential suboptimality relative to more advanced techniques). The connections are clear: hierarchical reduction and related software-pipelining strategies are the mechanisms by which innermost loops are pipelined in a purely software-driven manner, providing portable benefits while acknowledging that more advanced techniques may yield better schedules.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.0.core_principle",
		"citations": [
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized.",
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  }
		],
		"reasoning": "The target field value describes automatically partitioning a sequential loop into multiple pipeline stages that run as separate threads, with explicit producer-consumer style communication between stages. It also mentions that partitioning is typically based on identifying strongly connected components in the loop dependence graph to break cyclic dependencies. The most directly supporting material shows that decoupled software pipelining splits a loop into multiple threads that communicate as needed to maintain synchronization, often inserting dependence communication between threads. Additional sources describe static partitioning of loops into critical-path and off-critical-path threads, which aligns with the CP/off-CP thread model and the concept of staged parallelism. Other excerpts discuss extracting thread-level parallelism from loops and the use of producer-consumer style queues or dataflow-like synchronization to hide latencies, which corroborates the idea of pipeline stages communicating to utilize available slots. Together, these excerpts substantiate the core ideas of partitioning a loop into stages/threads with inter-stage communication, and they provide context for how such scheduling techniques are conceived and implemented in DS planning, though they do not all explicitly mention the SCC-based partitioning heuristic referenced in the field value.",
		"confidence": "low"
	  },
	  {
		"field": "advanced_software_pipelining_methods.0.method_name",
		"citations": [
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism.",
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more",
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized."
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP."
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  }
		],
		"reasoning": "The target field value is a specific software-pipelining technique called Decoupled Software Pipelining (DSWP). Excerpts that explicitly name and describe DSWP directly substantiate the field value, such as discussions of how DSWP splits programs into critical-path and off-critical-path threads to expose instruction-level parallelism and its use in mitigating long variable-latency stalls. Other excerpts describe DSWP in various contexts (e.g., in LLVM pipelines, general loop extraction, and discussions of related variants like Parallel-Stage DSWP), which reinforces the precise identification of DSWP as the core technique and situates it within broader scheduling research and practice. Excerpts that discuss DSWP’s motivation, such as handling irregular latencies in loop-carried dependencies and recursive data structures, provide direct support for why DSWP is relevant to challenging scheduling problems on VLIW-like architectures. Together, these excerpts establish a coherent view that DSWP is a foundational, well-studied approach to decoupling and overlapping work across threads to improve ILP and tolerate latency, which aligns with the stated field value. Quoted concepts include: the notion of automatic thread partitioning to expose long-running threads, the idea of splitting loops into CP and off-CP threads that execute concurrently in a pipelined fashion, and the description of DSWP as a method to extract and execute multiple threads to improve utilization of parallelism, including references to LLVM implementations and extensions like PS-DSWP.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.0.key_benefit",
		"citations": [
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://microarch.org/micro53/papers/738300a596.pdf",
			"excerpts": [
			  "A pipeline-\nparallel program is structured as a series of feed-forward\npipeline stages, with each stage executing on a separate thread.\nDecoupling stages with queues hides latency by allowing\nproducer stages to run far ahead of consumer stages."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Research/DSWP/",
			"excerpts": [
			  "Section Title: Decoupled Software Pipelining\nContent:\nCompiler and microarchitectural techniques have been largely successful in improving program\nperformance by exposing instruction-level parallelism (ILP). However, performance is still far\nfrom ideal in applications that experience long variable latency stalls (e.g. recursive data structure (RDS) traversals). Static scheduling techniques including software pipelining cannot schedule optimally for variable latencies.",
			  "How does DSWP work? It works by statically splitting programs into critical path (CP) and off-critical path (off-CP)\nthreads that run concurrently on thread-parallel architectures like\nSMT or CMP",
			  "CP and off-CP threads\nexecute concurrently in a pipelined fashion resulting in staged\nparallelism."
			]
		  },
		  {
			"title": "Automatic Thread Extraction with Decoupled Software Pipelining",
			"url": "https://liberty.princeton.edu/Publications/micro05_dswp.pdf",
			"excerpts": [
			  " Software Pipelining* (DSWP). DSWP exploits the fine-\ngrained *pipeline parallelism* lurking in most applications\nto extract long-running, concurrently executing threads"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining in LLVM",
			"url": "https://www.cs.cmu.edu/~fuyaoz/courses/15745/report.pdf",
			"excerpts": [
			  "Decoupled software pipelining [5] presents an easy way to automatically ex-\ntract thread-level parallelism for general loops in any program. The compiler\ndoes this by examining the dependences of the loops in a given program,\nsplitting the instructions of those loops into multiple smaller loops that\nexecute in independent threads, and inserting dependence communication\nwhere necessary between these threads so that they remain synchronized.",
			  "DSWP can be described as a series of steps that first accumulate information about the program, then use that information to the extract the TLP from the ...Read more"
			]
		  },
		  {
			"title": "Decoupled software pipelining creates parallelization ...",
			"url": "https://dl.acm.org/doi/10.1145/1772954.1772973",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that ...Read more"
			]
		  },
		  {
			"title": "Advances in Parallel-Stage Decoupled Software Pipelining ...",
			"url": "https://minesparis-psl.hal.science/hal-00744090/file/A-462.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is an automatic thread\npartitioning method which could partition a sequential program\nto run on multiple cores, and Parallel-Stage DSWP (PS-DSWP)\nexposes data parallelism into task pipelines extracted by DSWP."
			]
		  }
		],
		"reasoning": "The fine-grained field value describes a technique where memory latency is hidden by decoupling the memory access portion of a loop from the compute portion, so the access thread can run ahead and fill data queues, preventing compute units from stalling. The strongest excerpt explicitly discusses Pipette's approach to improving core utilization by decoupling pipeline stages and hiding memory latency through producer-consumer separation, which directly embodies the requested mechanism. A second excerpt discusses the broader challenge of long variable latencies in loops and how decoupled software pipelining aims to expose instruction-level parallelism despite irregular latencies, reaffirming the core idea of decoupling to mitigate latency. Additional excerpts describe the general concept of decoupled software pipelining as a method for automatically extracting parallelism by splitting loops into multiple threads and enabling staged or thread-level parallelism, which provides the methodological backdrop for applying the latency-hiding decoupling technique in VLIW contexts. Further entries elaborate about how DS-Pipelining can be described as a series of steps to extract thread-level parallelism and how it operates in compiler passes, which supports the practical application of the latency-hiding strategy in real compilers. Collectively, these excerpts support the central claim that decoupling memory access from computation (and allowing memory-bound stages to progress independently) is a viable strategy to convert memory latency into pipeline depth and improve utilization of heterogeneous or fixed-function execution resources in VLIW-like architectures.",
		"confidence": "medium"
	  },
	  {
		"field": "advanced_software_pipelining_methods.2.primary_use_case",
		"citations": [
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ob",
			  "n this paper we present an Enhanced Modulo Schedul-**\n**ing (EMS) technique that schedules loops with conditional**\n**branches in such a way that the minimum II can be ",
			  "The EMS algorithm consists of five steps:",
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  "Generate the data dependence graph.",
			  "Modulo schedule the loop body.",
			  "*Generate the software pipeline stages using modulo**\n**variable expansion to rename overlapping register life-",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution.",
			  "In this paper we present an Enhanced Modulo Scheduling (EMS) technique that can achieve a lower minimum Initiation Interval than modulo scheduling techniques that rely on either Hierarchical Reduction of If-conversion with Predicated Execution.",
			  "These three modulo scheduling techniques have been implemented in a prototype compiler."
			]
		  }
		],
		"reasoning": "The most directly relevant piece states that EMS uses If-conversion with no additional hardware support for conditional execution, which precisely matches the scenario of software-pipelining loops with conditionals on a VLIW without predication hardware. Additional excerpts describe EMS handling of loops with conditional branches and the general workflow of generating dependence graphs and modular scheduling stages, which supports the applicability of EMS to such a constraint. Some excerpts discuss If-conversion and predicated execution, or claim performance comparisons under different hardware branch capabilities; those provide contextual contrasts but do not undermine the core match to the no-predication-hardware scenario. Collectively, the cited passages establish EMS as a viable technique for scheduling loops with branches on VLIW without predicated hardware, including the notion of converting branches or conditions into forms suitable for a static, predication-free pipeline. The content also outlines typical EMS steps and the emphasis on exploiting software-pipeline stages within a fixed-initiation-interval framework, which reinforces the match to the stated field value.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.2.key_benefit",
		"citations": [
		  {
			"title": "Enhanced Modulo Scheduling for Loops with ...",
			"url": "https://dl.acm.org/doi/pdf/10.1145/144965.145796",
			"excerpts": [
			  "For exist-\ning architectures which support one branch per cycle, EMS\nperforms approximately 18% better than Modulo Schedul-\ning with Hierarchical Reduct",
			  " EMS uses If-conversion with no addi-**\n**tional hardware support for conditional execution",
			  "y If-conversion to convert the loop body into**\n**straight-line predicated code."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "We also show that If-conversion with Predicated Execution outperforms EMS assuming one branch per cycle. However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  }
		],
		"reasoning": "The field value asserts that EMS achieves better performance (about 18% better) than a simpler technique like Hierarchical Reduction by avoiding prescheduling of conditional constructs and using a more optimal resource model, effectively delivering the benefits of if-conversion without extra hardware cost. The supporting excerpts provide the following connections: one excerpt states that EMS performs approximately 18% better than Modulo Scheduling with Hierarchical Reduction, directly matching the performance gain claim. Another excerpt reinforces that EMS can achieve this improvement with no additional hardware support for conditional execution, supporting the hardware-cost-free aspect. Additional excerpts discuss the role of if-conversion and predicated execution in the context of EMS, illustrating how EMS embodies the benefits of predication and conditional handling without imposing hardware overhead, which underpins the field value’s rationale of avoiding hardware costs while retaining the advantages of advanced scheduling techniques. Together, these excerpts corroborate the claim of a sizable performance gain over hierarchical approaches, explain the cost-free conditional handling, and situate EMS within the framework of if-conversion/predicated execution that enables the observed benefits.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.1.method_name",
		"citations": [
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "CGPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency",
			  "CGPA also utilizes hidden data-level parallelism within the\npipelined stages to achieve high performance."
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://ieeexplore.ieee.org/document/6881405/",
			"excerpts": [
			  "Compared to the sequential method, CGPA shows speedups of 3.0x-3.8x for 5 kernels from programs in different domains."
			]
		  }
		],
		"reasoning": "Coarse-Grained Pipelined Accelerators (CGPA) is introduced as an HLS framework that uses coarse-grained pipeline parallelism to generate efficient hardware accelerators for loops from unannotated C/C++ programs. This directly supports the field value by naming the technique and describing its core purpose: producing accelerators through pipelined loop execution. Additional excerpts explain that CGPA automatically partitions loops into separate stages and creates buffer-connected hardware modules, which elaborates on how CGPA achieves coarse-grained pipelining and overlaps the execution of different iterations. Further excerpts discuss how CGPA enables pipelining with variable memory latency and tolerates irregular access patterns, underlining its relevance to real-world scheduling on VLIW-like backends. Supplementary excerpts extend the CGPA discussion by reiterating that CGPA produces independent workers (pipeline stages) with their own control circuits and memory ports, clarifying the modular, stage-structured nature of the CGPA approach and its practical performance benefits. Collectively, these excerpts map directly to the notion of CGPA as a coarse-grained, pipeline-based accelerator generation technique, including its decomposition of loops into pipelined stages, the automatic partitioning into stages, and the resulting hardware module generation and performance improvements.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.1.key_benefit",
		"citations": [
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops.",
			  " partitions loops\ninto long-running threads that communicate in a pipelined\nmanner via inter-core queues. Thi",
			  "WP+, splits a loop into new loops with dependence pat-\nterns amenable to parallelization using techniques that were\noriginally either inapplicable or poorly-performing.",
			  "This work recognizes that\nDSWP can also be an *enabling transformation* for other\nloop parallelization techniques.",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences.",
			  "par-\nallelizing each stage of the DSWP+ pipeline using (poten-\ntially) different techniques, not only is the benefit of DSWP\nincreased, but the applicability *and* performance of other\nparallelization techniques are enhanced",
			  "Second, DSWP finds the loop recurrences, instructions\nparticipating in a dependence cycle. DSWP groups de-\npendence cycles into strongly-connected components\n(SCCs) that form an acyclic grap",
			  "his paper evalu-\nates DSWP+ as an enabling framework for other transfor-\nmations by applying it in conjunction with DOALL, LO-\nCALWRITE, and SpecDOALL to individual stages of the\npipe",
			  "Finally, DSWP inserts *produce* and *consume* operations\nto transmit data values in case of data dependences and\nbranch conditions for control dependences",
			  "SWP+ not only extracts the parts without loop-carried de-\npendences, but also extracts parts with dependence patterns\nthat are amenable to parallelization techniques other than\n(speculative) DOALL.",
			  "Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate ...Read more"
			]
		  }
		],
		"reasoning": "The target field value describes a systematic framework that applies DSWP-like parallelism and explicitly avoids duplicating heavyweight sections containing loads and multiplies across parallel stages. Excerpts describing Decoupled Software Pipelining (DSWP) explain that it partitions loops into long-running threads that communicate via queues, enabling pipelined execution without duplicating large, heavyweight regions. This directly supports the idea of interleaving independent computations to fill parallel slots while minimizing hardware area and power by avoiding replication of heavy load/multiply-dominated code in multiple stages. Further, explicit notes about producing and consuming operations within DSWP show how data is transferred between stages without duplicating work, which aligns with the goal of avoiding duplication of heavyweight regions. The PS-DSWP entries describe a non-speculative extension that enables even broader parallelization while preserving latency-hiding benefits, reinforcing the notion of a framework that systematically interleaves work to saturate the pipeline. Entries discussing DSWP+ and its conjunction with other loop parallelization techniques indicate that this approach is intended to be combined with complementary strategies, which fits the idea of a coherent framework rather than ad-hoc tricks. Collectively, these excerpts support the notion of a structured method (DSWP/DSWP+-family) to interleave independent computations, hide latencies, and minimize hardware duplication of heavy sections in a VLIW-like setting.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.1.core_principle",
		"citations": [
		  {
			"title": "(PDF) Parallel-stage decoupled software pipelining",
			"url": "https://www.researchgate.net/publication/220799100_Parallel-stage_decoupled_software_pipelining",
			"excerpts": [
			  "The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the ...Read more",
			  "This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP ...Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://people.csail.mit.edu/sanchez/papers/2020.pipette.micro.pdf",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Pipette: Improving Core Utilization on Irregular Applications ...",
			"url": "https://dspace.mit.edu/bitstream/handle/1721.1/143866/2020.pipette.micro.pdf?sequence=2&isAllowed=y",
			"excerpts": [
			  "By exploiting pipeline parallelism within a multithreaded core,. Pipette hides latencies more effectively than the same number of data-parallel threads.Read more"
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops."
			]
		  },
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency"
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "Compared to the sequential method, CGPA shows speedups of 3.0x--3.8x for 5 kernels from programs in different domains.",
			  "GPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations.",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators."
			]
		  }
		],
		"reasoning": "The requested field value centers on extending decoupled software pipelining principles to high-level synthesis for hardware accelerators, with a focus on coarse-grained pipeline parallelism and selective replication to manage hardware cost and communication overhead. The most directly supportive material discusses Parallel-stage Decoupled Software Pipelining (PS-DSWP) as an evolution of DSWP that partitions loops into stages and enables pipelined, parallel execution across those stages, a natural fit for generating coarse-grained accelerator pipelines in an HLS flow. Descriptions of PS-DSWP emphasize emitting multiple pipeline stages with produce/consume style synchronization, which aligns with creating coarse-grained parallelism while controlling replication and inter-stage communication. Adjacent content on DSWP describes the core idea of decoupling loop stages and enabling communication through queues, which underpins the replication/partitioning strategy. Additional highly relevant material covers CGPA (Coarse-Grained Pipelined Accelerators), which explicitly talks about automatically partitioning loops into pipelined stages and building buffer-connected modules, thereby providing concrete HLS-oriented techniques for achieving coarse-grained pipeline parallelism in accelerator designs. The combination of PS-DSWP and CGPA sources provides a coherent set of methods for extending software-pipelining ideas toward hardware-accelerator generation with coarse granularity and controlled replication, matching the finegrained field value. Less central but supportive material discusses how decoupled pipelining concepts have been extended and applied in other domains (such as worker modules and decoupled pipelines) and general background on software pipelining and modulo scheduling, which reinforces the foundational relevance of the approach, though it is not as targeted as the PS-DSWP and CGPA discussions.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.2",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "ted by branches.\nAn alternate approach is to attempt to eliminate branches altogether. **Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates",
			  " paper we survey some common compiler and architectural techniques for increasing\nprogram ILP and making more effective use of the available hardware resources. We begin by\ndiscussing **Trace Scheduling** ([Fisher81], [Fisher83]) in section 2. In trace scheduling, compilation\nproceeds by selecting a likely path of execution (called a trace). This trace is compacted by\nperforming code motion without regard to basic block boundaries. To preserve correct execution,\noff-trace compensation code must be inserted when instructions are moved past a split or",
			  "**Predicated Execution and Enhanced Modulo Scheduling**",
			  "**\nOne of the objections to hierarchical reduction of conditional constructs is its rigidity. Since\nconditionals are scheduled separately without any regard to the resource requirements of the rest of\nthe loop, the resulting reduced nodes may have complex resource usages that create artificial\nconflicts and may prevent a schedule from being found. In [Warter93a] another approach is\nsuggested wherein conditional branches are replaced by predicated execution, as described in section\n"
			]
		  },
		  {
			"title": "Program decision logic optimization using predication and control ...",
			"url": "https://liberty.princeton.edu/Publications/ieeepro89_decision.pdf",
			"excerpts": [
			  "paths are selected for inclusion, the code after if-conversion con- sists of only a single block of predicated instructions, a hyper- block [9], as shown in ..."
			]
		  },
		  {
			"title": "A Framework for Balancing Control Flow and Predication",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-97-framework.pdf",
			"excerpts": [
			  "hyperblock* is a structure created to facilitate optimization\nand scheduling for predicated architectures [14]. A hyperblock is\na set of predicated basic blocks in which control may only enter\nfrom the top, but may exit from one or more locations. Hyper-\nblocks are formed by applying tail duplication and if-conversion\nover a set of carefully selected paths. Inclusion of a path into\na hyperblock is done by considering its profitability. The prof-\nitability is determined by four pieces of information: resource\nutilization, dependence height, hazard presence, and execution\nfrequency",
			  "A hyperblock\nwhich contains multiple paths will not complete until all of its\nconstituent paths have completed. Therefore, the overall height\nof the hyperblock is the maximum of all the original paths' de-\npendence height",
			  "he scheduler with partial reverse if-conversion operates by\nidentifying the paths composing a hyperblock. Paths which are\nprofitable to overlap remain unchanged. Conversely, a path that\ninteracts poorly with the other paths is removed from the hyper-\nblock. In particular, the partial reverse if-converter decides to\neject certain paths, or parts of paths, to enhance the schedule",
			  "The first phase of the compilation framework is to aggres-\nsively perform hyperblock formation.\nThe hyperblock former\ndoes not need to exactly compute what paths, or parts of paths,\nwill fit in the available resources and be completely compatible\nwith each other. Instead, it forms hyperblocks which are larger\nthan the target architecture can handl",
			  "tial reverse if-converter was able to adjust the\namount of if-conversion to match the available resources to effi-\nciently utilize the processor. As a result, the execution time for\nthe *update* function is reduced to 16.8 million cycles with partial\nreverse if-conversion, a 52% performance improvement over the\nsuperblock code.",
			  " compilation frame-\nwork, shown in Figure 4, consists of two phases of predicate ma-\nnipulation surrounding classical, predicate specific, and ILP op-\ntimizations. The first predicate manipulation phase, hyperblock\nformation, has been addressed thoroughly in [14]. The second\npredicate manipulation phase, adjustment of hyperblocks during\nscheduling, is proposed in this work and has been termed *partial*\n*reverse if-conve"
			]
		  },
		  {
			"title": "\nAn If-Conversion Algorithm Based on Predication Execution",
			"url": "https://scialert.net/fulltext/?doi=itj.2010.984.988",
			"excerpts": [
			  "A hyperblock is a structure created to facilitate optimization and scheduling for predicated architectures.",
			  "A hyperblock is a set of predicated basic blocks in which control may only enter from the top, but may exit from one or more locations.",
			  "Hyperblocks are formed by applying tail duplication and if-conversion over a set of carefully selected paths.",
			  "Inclusion of a path into a hyperblock is done by considering its profitability.",
			  "The profitability is determined by the following factors: resource utilization, invalid instructions, branch instructions inherent instruction dependence, branch execution frequency, the balance of branch execution and so on.",
			  "Not only can if-conversion eliminate the branch and provide continuous instruction stream, it can also extend the length of basic block and improve the effective parallelism of code."
			]
		  },
		  {
			"title": "the-partial-reverse-if-conversion-framework-for-balancing- ...",
			"url": "https://scispace.com/pdf/the-partial-reverse-if-conversion-framework-for-balancing-3439qyi908.pdf",
			"excerpts": [
			  "14] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, R. A. Bringmann, and W. W. Hwu, “Effective compiler support for\npredicated execution using the hyperblock,” in *Proceedings of the 25th International Symposium on Microarchitecture* ,\npp. 45–54, December 1992",
			  "[15] S. A. Mahlke, R. E. Hank, J. McCormick, D. I. August, and W. W. Hwu, “A comparison of full and partial predicated\nexecution support for ILP processors,” in *Proceedings of the 22th International Symposium on Computer Architecture* ,\npp. 138–150, June 1995"
			]
		  },
		  {
			"title": "Predicate Analysis and If-Conversion in an Itanium Link- ...",
			"url": "https://www2.cs.arizona.edu/~debray/Publications/ifcvt.pdf",
			"excerpts": [
			  "he Itanium contains multiple functional units and uses pro-\ngrammer specified instruction-level parallelism. Moreover, every\ninstruction is *predicated* : It specifies a one-bit predicate register,\nand if the value of that register is true (1), then the instruction is\nexecuted; otherwise, the instruction usually has no effect",
			  " order to make effective use of the capabilities of such archi-\ntectures, a compiler must selectively eliminate conditional branches\nin favor of predicated instructions that are conditionally executed.",
			  "process, known as *if-conversion* , must be carried out judi-\nciously: if it is too aggressive, it leads to contention for system\nresources and a concomitant degradation in performance; if it is not\naggressive enough, it results in insufficient instruction-level paral-\nlelism, which also leads to a loss in performanc"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "Predication allows for a significant increase in basic\nblock size.",
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "Predicated instructions with different predicates can be mixed with each other and with unconditional code, allowing better [instruction scheduling](/wiki/Instruction_scheduling \"Instruction scheduling\") and so even better performance.",
			  "Elimination of unnecessary branch instructions can make the execution of necessary branches, such as those that make up loops, faster by lessening the load on [branch prediction](/wiki/Branch_prediction \"Branch prediction\") mechanisms.",
			  "Elimination of the cost of a branch misprediction which can be high on deeply pipelined architectures.",
			  "Other detriments are the following: [[ 3 ]]()\nPredication complicates the hardware by adding levels of [logic](/wiki/Control_unit \"Control unit\") to critical [paths](/wiki/Datapath \"Datapath\") and potentially degrades clock speed.",
			  "A predicated block includes cycles for all operations, so shorter [paths](/wiki/Control-flow_graph \"Control-flow graph\") may take longer and be penalized.",
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true."
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://pages.cs.wisc.edu/~fischer/cs701/meta-optimization.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, the IM-\nPACT predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock. A hyperblock is a predi-\ncated single-entry, multiple-exit regi",
			  "Though predication effectively exposes ILP, simply pred-\nicating everything will diminish performance by saturating\nmachine resources with useless instruction",
			  " predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [1",
			  "h frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively affect performance.",
			  "Path ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels o",
			  "r of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc-\ntion in a hyperblock executes, long paths effectively\ndelay the time to completion of short paths. The cost\nof misprediction is relatively high for short paths.",
			  "mber of branches in path** :\nPaths of control\nthrough several branches have a greater chance of mis-\npredicting. Therefore, it may be worthwhile to predi-\ncate such paths. On the other hand, including several\nsuch paths may produce large hyperblocks that satu-\nrate resou",
			  "ompiler optimization considerations** : Paths that\ncontain hazard conditions ( *i.e.* , pointer dereferences\nand procedure calls) limit the effectiveness of many\ncompiler optimizations. In the presence of hazards, a\ncompiler must make conservative assump",
			  "chine-specific considerations** : A heuristic should\naccount for machine characteristics. For instance, the\nbranch delay penalty is a decisive factor",
			  "learly, there is much to consider when designing a heuris-\ntic for hyperblock selection",
			  "MPACT compiler begins by transforming the code\nso that it is more amenable to hyperblock formation [15].\nIMPACT’s algorithm then identifies acyclic paths of con-\ntrol that are suitable for hyperblock inclusio",
			  "A pri-\nority function— which is the critical calculation in the predi-\ncation decision process— assigns a value to each of the paths\nbased on characteristics such as the ones just described [15].\nSome of these characteristics come from runtime profiling.\nIMPACT uses the priority function shown below:\n*h* *i* =\n�\n0 *.* 25\n:\nif *path* *i* contains a hazard.\n1\n:\nif *path* *i* is hazard free.\n*d ratio* *i* =\n*dep height* *i*\nmax *j* =1 *→* *N* *dep height* *j*\n*o ratio* *i* =\n*num ops* *i*\nmax *j* =1 *→* *N* *num ops* *j*\n*priority* *i* = *exec ratio* *i* *·* *h* *i* *·* (2 *.* 1 *−* *d ratio* *i* *"
			]
		  },
		  {
			"title": "Optimize Options (Using the GNU Compiler Collection (GCC))",
			"url": "https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html",
			"excerpts": [
			  "RTL if-conversion tries to remove conditional branches around a block and",
			  "replace them with conditionally executed instructions.",
			  "These parameters\ngive the maximum permissible cost for the sequence that would be generated\nby if-conversion depending on whether the branch is statically determined\nto be predictable or not.",
			  "lso use other heuristics to decide whether if-conversion is likely to be\nprofitable.",
			  "`tracer-max-code-growth`",
			  "Stop tail duplication once code growth has reached given percentage.",
			  "he maximum number of instructions in a block which should be\nconsidered for if-conversion. ",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "Set to 0\nif either vectorization ( -ftree-vectorize ) or if-conversion\n( -ftree-loop-if-convert ) is disabled."
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region.",
			  ". The processor\nnullifies all instructions in the incorrect path. I",
			  "redication can effectively expose ILP, simply predi-\ncating everything will diminish performance by saturating machine resources with useless\ninstructions",
			  "f p2 is true all of the time for instance, the instructions guarded by p3 con-\nsume valuable execution slots"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.eecg.utoronto.ca/~steffan/carg/readings/metaopt_talk.pdf",
			"excerpts": [
			  "***Hyperblock Formation***\nUses If-Conversion to convert control dependences into data\ndependences.\nSingle entry multiple exit block with predicated instructions.",
			  "There is a limit to the effectiveness of predicating instructions\nand forming large hyperblocks.",
			  "Path predictability - unpredictable branches",
			  "Path frequency - frequent paths",
			  "**Page 13**\n***Hyperblock Formation (2)***\nNumber of instructions in path - shorter paths\nNumber of branches in path - more branches but not too many\nCompiler optimization considerations (e.g. exceptions)\nMachine-specific considerations (e.g. branch delay penalty)",
			  "Experiment performed with Trimaran's IMPACT compiler\napproximating the Intel Itanium architecture (EPIC)."
			]
		  },
		  {
			"title": "(PDF) A novel heuristic for selection of hyperblock in If-conversion",
			"url": "https://www.researchgate.net/publication/261459597_A_novel_heuristic_for_selection_of_hyperblock_in_If-conversion",
			"excerpts": [
			  "Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow.",
			  "Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions.",
			  "In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied.",
			  "On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution.",
			  "This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time",
			  "A hyperblock [10] is predicted region of code that contains straight-line sequence of instruction with a single entry point and possibly multiple exit points.",
			  "The navigation bandwidth of prediction mechanism depends upon the degree of ILP. It can be increased by increasing control flow prediction in procedural languages at compile time.",
			  "The main idea behind this concept is to use a step beyond the prediction of common branch and permitting the architecture to have the information about the CFG (Control Flow Graph) components of the program to have better branch decision for ILP.",
			  "on the ItaniumTM processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ].",
			  "We therefore implement the first formally verified implementation of a general if-conversion algorithm, with support for heuristics for branch prediction."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Most of these\nschemes group sequential basic blocks into larger entities, thereby\nreducing control dependencies, increasing the fetch rate and allow-\ning more opportunities for optimizatio",
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "imilar to\nsuperblocks, successive sets of instructions are combined and op-\ntimized to form a single hyperblock, however, hyperblocks may\ninclude multiple paths if the bias cannot be determined",
			  "predicated execution using hyper-\nblocks [1"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "THE INTEL IA-64 COMPILER CODE GENERATOR",
			"url": "https://www.cs.virginia.edu/~skadron/cs854_uproc_survey/spring_2001/cs854/m5044.pdf",
			"excerpts": [
			  "Predication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features."
			]
		  }
		],
		"reasoning": "The provided excerpts describe Hyperblock Formation as a technique that facilitates optimization and scheduling for predicated architectures. It is formed by merging disjoint paths of execution into a single predicated hyperblock, which is a single-entry, multiple-exit region. This process involves tail duplication and if-conversion over selected paths, with profitability determined by factors like resource utilization and execution frequency. The goal is to eliminate branches, create larger basic blocks, and expose more instruction-level parallelism (ILP) to the scheduler. The excerpts also mention that hyperblocks can contain multiple paths and that their overall height is the maximum of the original paths' dependence height. Some excerpts also discuss the trade-offs, such as potential increases in code size and the need for careful balancing of control flow and predication to avoid saturating machine resources with useless instructions.",
		"confidence": "high"
	  },
	  {
		"field": "advanced_software_pipelining_methods.1.primary_use_case",
		"citations": [
		  {
			"title": "CGPA: Coarse-Grained Pipelined Accelerators",
			"url": "https://dl.acm.org/doi/pdf/10.1145/2593069.2593105",
			"excerpts": [
			  "oarse-Grained\nPipelined Accelerators (CGPA), an HLS framework which uses\ncoarse-grained pipeline parallelism to generate efficient hardware\naccelerators for loops from unannotated C/C++ programs.",
			  "\nleverages two distinct insights to improve efficiency and applica-\nbility for HLS. First, complex loop bodies with irregular memory\naccesses and imperfect loops usually contain coarse-grained code\nsections performing different tasks. HLS tools should separate and\nmodularize these tasks to build an efficient system. Second, these\ncomplex loop bodies usually contain sections which are paralleliz-\nable. Coarse-grained decoupled pipelining techniques [28, 27] can\nexploit the presence of these parallelizable sections to enable a type\nof parallelism not exploited by existing HLS tools.",
			  "GPA automatically partitions individual loops into separate\npipelined stages and generates buffer-connected hardware modules\nfor these stages. Pipelining enables the overlapping of execution\nof an earlier iteration of the loop with a later iteration and also\nallows the synthesized hardware to tolerate variable memory la-\ntency"
			]
		  },
		  {
			"title": "CGPA | Proceedings of the 51st Annual Design Automation Conference",
			"url": "https://dl.acm.org/doi/10.1145/2593069.2593105",
			"excerpts": [
			  "o address this problem, this paper proposes Coarse-Grained Pipelined Accelerators (CGPA), an HLS framework that utilizes coarsegrained pipeline parallelism techniques to synthesize efficient specialized accelerator modules from irregular C/C++ programs without requiring any annotations. Comp",
			  "High-level synthesis (HLS) tools dramatically reduce the nonrecurring engineering cost of creating specialized hardware accelerators."
			]
		  },
		  {
			"title": "Decoupled Software Pipelining Creates Parallelization ...",
			"url": "https://liberty.princeton.edu/Publications/cgo10_dswp_plus.pdf",
			"excerpts": [
			  "Decoupled Software Pipelining (DSWP) is one approach to\nautomatically extract threads from loops."
			]
		  },
		  {
			"title": "Software pipelining: an effective scheduling technique for VLIW machines | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Software-pipelining%3A-an-effective-scheduling-for-Lam/d23aca9204f8ed7a040aa15e30ed90528c755771",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Effective Compiler Support For Predicated Execution Using The Hyperblock | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Effective-Compiler-Support-For-Predicated-Execution-Mahlke-Lin/c1fd38df2d5ca1f81694396cc88b190447193d4e",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "[PDF] Enhanced Modulo Scheduling For Loops With Conditional Branches | Semantic Scholar",
			"url": "https://www.semanticscholar.org/paper/Enhanced-Modulo-Scheduling-For-Loops-With-Branches-Warter-Haab/6a4039574e71246946e04245efa89fe174d48885",
			"excerpts": [
			  "This paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors, and proposes a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block."
			]
		  },
		  {
			"title": "HyperLink Enhanced Modulo Scheduling for Loops ...",
			"url": "http://impact.crhc.illinois.edu/paper_details.aspx?paper_id=148",
			"excerpts": [
			  "Loops with conditional branches have multiple execution paths which are difficult to software pipeline. The modulo scheduling technique for software pipelining addresses this problem by converting loops with conditional branches into straight-line code before scheduling.",
			  "We show that for existing architectures which support one branch per cycle, EMS performs approximately 18% better than Hierarchical Reduction.",
			  "However, with hardware support for multiple branches per cycle, EMS should perform as well as or better than If-conversion with Predicated Execution."
			]
		  },
		  {
			"title": "Software pipelining - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Software_pipelining",
			"excerpts": [
			  "The trick is to replicate the body of the loop after it has been scheduled, allowing different registers to be used for different values of the same variable when they have to be live at the same time."
			]
		  }
		],
		"reasoning": "Direct discussions of creating hardware accelerators from loop bodies with irregular memory patterns are found in literature describing coarse-grained pipelined accelerators (CGPA) that partition loops into pipelined stages and generate buffer-connected hardware modules for those stages. This directly aligns with designing pipelined hardware accelerators for loops with irregular memory accesses, as the approach targets exploiting parallelizable sections within complex loop bodies and tolerating memory latency via pipelined hardware modules. The emphasis on separating tasks within irregular loop bodies and enabling a form of decoupled or coarse-grained pipelining provides concrete methods to realize hardware accelerators that accommodate irregular memory behavior while aiming for efficient resource usage. Additional excerpts describe how CGPA automatically partitions loops into stages and enables overlapping execution between iterations, which is essential for hiding memory latency and improving hardware utilization in the presence of irregular access patterns. Further, discussions about tolerating variable memory latency and decoupled software pipelining extend these ideas by showing how to structure hardware and software interactions to keep multiple hardware resources busy even when memory access patterns are irregular, which is central to achieving high resource efficiency in accelerators. The inclusion of high-level synthesis (HLS) framing and notes about buffering and hidden data-level parallelism reinforces the practical angle of turning irregular loop bodies into hardware blocks that work efficiently together. Other excerpts elaborate on advanced scheduling techniques like decoupled software pipelining (DSWP and PS-DSWP) and enhanced modulo scheduling (EMS) in VLIW contexts. These provide additional strategies for interleaving independent work with dependent paths, managing control flow with predication, and reducing startup/throughput penalties in hardware-accelerated pipelines, which are valuable for maintaining high utilization when some paths are latency-bound or irregular. A set of excerpts discussing predicated execution, predication-aware scheduling (as used in IA-64-like architectures), and reduced control constructs (hierarchical reduction) complement the accelerator design narrative by offering architectural levers to minimize code expansion and hazards while keeping hardware utilization high. Finally, references to systematic treatments of software pipelining for VLIW machines (Lam 1988 and successors) provide foundational justification for using software-pipelining concepts to structure accelerator pipelines, especially when loops exhibit irregular behavior and still require high-throughput execution. Overall, the strongest support comes from content that explicitly ties loop-to-hardware translation (CGPA-like approach) to irregular memory access handling and resource-conscious accelerator design, with supplementary value added by DSWP/EMS predication and latency-hiding techniques that further improve hardware occupation under irregular patterns.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.2.primary_mechanism",
		"citations": [
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "ted by branches.\nAn alternate approach is to attempt to eliminate branches altogether. **Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates",
			  "**Predicated Execution and Enhanced Modulo Scheduling**"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true."
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism."
			]
		  }
		],
		"reasoning": "The field value asserts that the main mechanism is if-conversion using predicated execution, which merges multiple important execution paths into one predicated region by converting conditional branches into predicated instructions, effectively transforming control dependencies into data dependencies. The most directly supportive excerpts explain predicated execution as a mechanism to remove branches by making instructions conditional on predicates, and they explicitly tie predication to combining control-flow paths into a single scheduling region. Additional excerpts discuss predication and if-conversion in the context of hyperblocks, where a predicated block aggregates multiple paths and becomes a single scheduling unit, which aligns with the described mechanism of merging paths into a larger region. Other excerpts describe how hyperblocks are formed and how prediction manipulation around hyperblocks facilitates ILP, including the idea that conditional branches can be replaced by predicated execution to improve scheduling and resource utilization. Together, these excerpts substantiate the claim that if-conversion and predicated execution are the primary mechanism for combining multiple execution paths into a predicated, larger region, transforming control dependencies into data dependencies and enabling more effective VLIW scheduling. ",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.2.technique_name",
		"citations": [
		  {
			"title": "A Framework for Balancing Control Flow and Predication",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-97-framework.pdf",
			"excerpts": [
			  "hyperblock* is a structure created to facilitate optimization\nand scheduling for predicated architectures [14]. A hyperblock is\na set of predicated basic blocks in which control may only enter\nfrom the top, but may exit from one or more locations. Hyper-\nblocks are formed by applying tail duplication and if-conversion\nover a set of carefully selected paths. Inclusion of a path into\na hyperblock is done by considering its profitability. The prof-\nitability is determined by four pieces of information: resource\nutilization, dependence height, hazard presence, and execution\nfrequency"
			]
		  },
		  {
			"title": "\nAn If-Conversion Algorithm Based on Predication Execution",
			"url": "https://scialert.net/fulltext/?doi=itj.2010.984.988",
			"excerpts": [
			  "A hyperblock is a structure created to facilitate optimization and scheduling for predicated architectures.",
			  "A hyperblock is a set of predicated basic blocks in which control may only enter from the top, but may exit from one or more locations.",
			  "Hyperblocks are formed by applying tail duplication and if-conversion over a set of carefully selected paths."
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.eecg.utoronto.ca/~steffan/carg/readings/metaopt_talk.pdf",
			"excerpts": [
			  "**Page 13**\n***Hyperblock Formation (2)***\nNumber of instructions in path - shorter paths\nNumber of branches in path - more branches but not too many\nCompiler optimization considerations (e.g. exceptions)\nMachine-specific considerations (e.g. branch delay penalty)",
			  "Experiment performed with Trimaran's IMPACT compiler\napproximating the Intel Itanium architecture (EPIC)."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ]."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  }
		],
		"reasoning": "Hyperblock formation is described as a predicated block structure that aggregates multiple paths into a single-entry, multiple-exit region to improve ILP and scheduling on predicated architectures. The most direct support comes from a definition of a hyperblock as a set of predicated basic blocks enabling optimization and scheduling for predicated architectures, and the statement that a hyperblock is created to facilitate optimization and scheduling for predicated architectures. Building on that, other excerpts elaborate that hyperblocks are formed through techniques like tail duplication and if-conversion, and that their profitability depends on resource usage, hazard presence, and path execution characteristics. Additional excerpts discuss how predication and hyperblocks enable longer, predicated regions, how they interact with trace or hyperblock-based scheduling, and how the scheduler benefits from larger predicated regions since they expose ILP opportunities. Several sources also discuss that hyperblock formation is a step in compiler pipelines (e.g., IMPACT/MAPT-style frameworks) and is tied to predication techniques that transform control dependencies into data dependencies. Taken together, these excerpts consistently support the notion that Hyperblock Formation is a core technique for control-flow optimization in VLIW/predicated architectures, particularly when dealing with multiple possible execution paths and the need to fill parallel slots with predicated instructions.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.2.key_trade_offs",
		"citations": [
		  {
			"title": "Predicate Analysis and If-Conversion in an Itanium Link- ...",
			"url": "https://www2.cs.arizona.edu/~debray/Publications/ifcvt.pdf",
			"excerpts": [
			  "he Itanium contains multiple functional units and uses pro-\ngrammer specified instruction-level parallelism. Moreover, every\ninstruction is *predicated* : It specifies a one-bit predicate register,\nand if the value of that register is true (1), then the instruction is\nexecuted; otherwise, the instruction usually has no effect"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true.",
			  "Predicated instructions with different predicates can be mixed with each other and with unconditional code, allowing better [instruction scheduling](/wiki/Instruction_scheduling \"Instruction scheduling\") and so even better performance.",
			  "Elimination of unnecessary branch instructions can make the execution of necessary branches, such as those that make up loops, faster by lessening the load on [branch prediction](/wiki/Branch_prediction \"Branch prediction\") mechanisms.",
			  "Elimination of the cost of a branch misprediction which can be high on deeply pipelined architectures."
			]
		  },
		  {
			"title": "VLIW compilation techniques",
			"url": "http://www.ai.mit.edu/projects/aries/Documents/vliw.pdf",
			"excerpts": [
			  "ted by branches.\nAn alternate approach is to attempt to eliminate branches altogether. **Predicated Execution**\n([Hsu86], [Mahlke92a], [Mahlke95]) is a mechanism which permits some branches to be removed by\nallowing execution of instructions to be conditional on the values of hardware defined predicates"
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "Predication allows for a significant increase in basic\nblock size.",
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for"
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region."
			]
		  },
		  {
			"title": "(PDF) A novel heuristic for selection of hyperblock in If-conversion",
			"url": "https://www.researchgate.net/publication/261459597_A_novel_heuristic_for_selection_of_hyperblock_in_If-conversion",
			"excerpts": [
			  "Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow.",
			  "Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions.",
			  "In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied.",
			  "On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution.",
			  "This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time",
			  "A hyperblock [10] is predicted region of code that contains straight-line sequence of instruction with a single entry point and possibly multiple exit points.",
			  "The navigation bandwidth of prediction mechanism depends upon the degree of ILP. It can be increased by increasing control flow prediction in procedural languages at compile time.",
			  "The main idea behind this concept is to use a step beyond the prediction of common branch and permitting the architecture to have the information about the CFG (Control Flow Graph) components of the program to have better branch decision for ILP.",
			  "on the ItaniumTM processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported."
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ].",
			  "We therefore implement the first formally verified implementation of a general if-conversion algorithm, with support for heuristics for branch prediction."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Most of these\nschemes group sequential basic blocks into larger entities, thereby\nreducing control dependencies, increasing the fetch rate and allow-\ning more opportunities for optimizatio",
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "imilar to\nsuperblocks, successive sets of instructions are combined and op-\ntimized to form a single hyperblock, however, hyperblocks may\ninclude multiple paths if the bias cannot be determined",
			  "predicated execution using hyper-\nblocks [1"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "THE INTEL IA-64 COMPILER CODE GENERATOR",
			"url": "https://www.cs.virginia.edu/~skadron/cs854_uproc_survey/spring_2001/cs854/m5044.pdf",
			"excerpts": [
			  "Predication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features."
			]
		  }
		],
		"reasoning": "The field value asserts that explicit hardware support for predicated execution, including a set of predicate registers, is fundamental, and that this predication can increase predicate pressure while eliminating branch mispredictions but consuming slots and power. The most directly supportive statements describe predication as a feature of certain architectures (such as Itanium) where every instruction is predicated, relying on predicate registers to guard execution. They also explicitly note that while predication can remove branches and improve ILP, it can also lead to resource contention if overused. These points align with the claim that explicit hardware support improves portability only with predicate infrastructure, but increases predicate pressure and can consume execution resources. Related discussions about hyperblocks show how predication is used to form large predicated regions to expose ILP, which further contextualizes the trade-offs between larger predicated regions (better parallelism) and resource/power costs or increased path length. Other excerpts discuss predication in general (predicated execution enables removing conditional branches, Itanium-style predication, and the impact of predication on branch prediction), which corroborates the core idea that predication changes the resource/power/latency landscape of VLIW schedules rather than simply eliminating branches. Additional sources discuss compiler heuristics for deciding when to apply if-conversion and predication, highlighting the practical aspect of when predication pays off given resource constraints, hazard presence, and path lengths. All together, these excerpts support the field value’s core claims: explicit hardware predication exists, predicate registers enable it, it increases predicate pressure, and while it removes mispredictions, it consumes slots and power. Some excerpts also provide practical scheduling frameworks (hyperblocks, predication framing) that illustrate how to balance these trade-offs in real VLIW schedules.",
		"confidence": "high"
	  },
	  {
		"field": "control_flow_optimization_techniques.2.description",
		"citations": [
		  {
			"title": "A Framework for Balancing Control Flow and Predication",
			"url": "http://impact.crhc.illinois.edu/shared/papers/micro-97-framework.pdf",
			"excerpts": [
			  "hyperblock* is a structure created to facilitate optimization\nand scheduling for predicated architectures [14]. A hyperblock is\na set of predicated basic blocks in which control may only enter\nfrom the top, but may exit from one or more locations. Hyper-\nblocks are formed by applying tail duplication and if-conversion\nover a set of carefully selected paths. Inclusion of a path into\na hyperblock is done by considering its profitability. The prof-\nitability is determined by four pieces of information: resource\nutilization, dependence height, hazard presence, and execution\nfrequency"
			]
		  },
		  {
			"title": "\nAn If-Conversion Algorithm Based on Predication Execution",
			"url": "https://scialert.net/fulltext/?doi=itj.2010.984.988",
			"excerpts": [
			  "A hyperblock is a set of predicated basic blocks in which control may only enter from the top, but may exit from one or more locations."
			]
		  },
		  {
			"title": "the-partial-reverse-if-conversion-framework-for-balancing- ...",
			"url": "https://scispace.com/pdf/the-partial-reverse-if-conversion-framework-for-balancing-3439qyi908.pdf",
			"excerpts": [
			  "14] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, R. A. Bringmann, and W. W. Hwu, “Effective compiler support for\npredicated execution using the hyperblock,” in *Proceedings of the 25th International Symposium on Microarchitecture* ,\npp. 45–54, December 1992",
			  "[15] S. A. Mahlke, R. E. Hank, J. McCormick, D. I. August, and W. W. Hwu, “A comparison of full and partial predicated\nexecution support for ILP processors,” in *Proceedings of the 22th International Symposium on Computer Architecture* ,\npp. 138–150, June 1995"
			]
		  },
		  {
			"title": "Hyperblock Scheduling for Verified High-Level Synthesis | Proceedings of the ACM on Programming Languages",
			"url": "https://dl.acm.org/doi/10.1145/3656455",
			"excerpts": [
			  "If-conversion is helpful because larger blocks can give the scheduler more opportunities to find parallelism.",
			  "If-conversion introduces the predicated instructions that form hyperblocks.",
			  "The pointed-to block is left unchanged in case it is still pointed to by other blocks; as such, this transformation performs *tail duplication* [ [13]() ].",
			  "We therefore implement the first formally verified implementation of a general if-conversion algorithm, with support for heuristics for branch prediction."
			]
		  },
		  {
			"title": "(PDF) A novel heuristic for selection of hyperblock in If-conversion",
			"url": "https://www.researchgate.net/publication/261459597_A_novel_heuristic_for_selection_of_hyperblock_in_If-conversion",
			"excerpts": [
			  "Predicated execution is a promising architectural feature for exploiting instruction-level parallelism in the presence of control flow.",
			  "A hyperblock [10] is predicted region of code that contains straight-line sequence of instruction with a single entry point and possibly multiple exit points.",
			  "The main idea behind this concept is to use a step beyond the prediction of common branch and permitting the architecture to have the information about the CFG (Control Flow Graph) components of the program to have better branch decision for ILP.",
			  "Compiling for predicated execution involves converting program control flow into conditional, or predicated, instructions.",
			  "In order to effectively apply if-conversion, one must address two major issues: what should be if-converted and when the if-conversion should be applied.",
			  "On the other hand the final code generated for a processor with predicated execution requires a delicate balance between control flow and predication to achieve efficient execution.",
			  "This paper presents an effective compilation framework that allows the compiler to maximize the benefits of predication as a compiler representation while delaying the final balancing of control flow and predication to schedule time",
			  "The navigation bandwidth of prediction mechanism depends upon the degree of ILP. It can be increased by increasing control flow prediction in procedural languages at compile time.",
			  "on the ItaniumTM processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported."
			]
		  },
		  {
			"title": "Predicate Analysis and If-Conversion in an Itanium Link- ...",
			"url": "https://www2.cs.arizona.edu/~debray/Publications/ifcvt.pdf",
			"excerpts": [
			  " order to make effective use of the capabilities of such archi-\ntectures, a compiler must selectively eliminate conditional branches\nin favor of predicated instructions that are conditionally executed."
			]
		  },
		  {
			"title": "The Effects of Predicated Execution on Branch Prediction",
			"url": "https://www.princeton.edu/~rblee/ELE572Papers/the-effects-of-predicated.pdf",
			"excerpts": [
			  "Predication allows for a significant increase in basic\nblock size.",
			  "Predication can remove\nforward branch instructions by translating the instructions\nfollowing the branch into *predicate* for"
			]
		  },
		  {
			"title": "Predication (computer architecture) - Wikipedia",
			"url": "https://en.wikipedia.org/wiki/Predication_(computer_architecture)",
			"excerpts": [
			  "Predicated instructions with different predicates can be mixed with each other and with unconditional code, allowing better [instruction scheduling](/wiki/Instruction_scheduling \"Instruction scheduling\") and so even better performance.",
			  "Elimination of unnecessary branch instructions can make the execution of necessary branches, such as those that make up loops, faster by lessening the load on [branch prediction](/wiki/Branch_prediction \"Branch prediction\") mechanisms.",
			  "Elimination of the cost of a branch misprediction which can be high on deeply pipelined architectures.",
			  "Other detriments are the following: [[ 3 ]]()\nPredication complicates the hardware by adding levels of [logic](/wiki/Control_unit \"Control unit\") to critical [paths](/wiki/Datapath \"Datapath\") and potentially degrades clock speed.",
			  "A predicated block includes cycles for all operations, so shorter [paths](/wiki/Control-flow_graph \"Control-flow graph\") may take longer and be penalized.",
			  "Intel\") [IA-64](/wiki/IA-64 \"IA-64\") architecture, most instructions are predicated. The predicates are stored in 64 special-purpose predicate [registers](/wiki/Processor_register \"Processor register\") ; and one of the predicate registers is always true so that *unpredicated* instructions are simply instructions predicated with the value true."
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://pages.cs.wisc.edu/~fischer/cs701/meta-optimization.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, the IM-\nPACT predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock. A hyperblock is a predi-\ncated single-entry, multiple-exit regi",
			  "Though predication effectively exposes ILP, simply pred-\nicating everything will diminish performance by saturating\nmachine resources with useless instruction",
			  " predictability** : Predictable branches incur no\nmisprediction penalties, and thus, should probably re-\nmain unpredicated. Combining multiple paths of ex-\necution into a single predicated region uses precious\nmachine resources [1",
			  "h frequency** :\nInfrequently executed paths are\nprobably not worth predicating.\nIncluding the path\nin a hyperblock would consume resources, and could\nnegatively affect performance.",
			  "Path ILP** : If a path’s level of parallelism is low, it may\nbe worthwhile to predicate the path. In other words, if\na path does not fully use machine resources, combining\nit with another sequential path probably will not di-\nminish performance. Because predicated instructions\ndo not need to know the value of their guarding pred-\nicate until late in the pipeline, a processor can sustain\nhigh levels o",
			  "r of instructions in path** : Long paths use\nup machine resources, and if predicated, will likely\nslow execution. This is especially true when long paths\nare combined with short paths. Since every instruc-\ntion in a hyperblock executes, long paths effectively\ndelay the time to completion of short paths. The cost\nof misprediction is relatively high for short paths.",
			  "mber of branches in path** :\nPaths of control\nthrough several branches have a greater chance of mis-\npredicting. Therefore, it may be worthwhile to predi-\ncate such paths. On the other hand, including several\nsuch paths may produce large hyperblocks that satu-\nrate resou",
			  "ompiler optimization considerations** : Paths that\ncontain hazard conditions ( *i.e.* , pointer dereferences\nand procedure calls) limit the effectiveness of many\ncompiler optimizations. In the presence of hazards, a\ncompiler must make conservative assump",
			  "chine-specific considerations** : A heuristic should\naccount for machine characteristics. For instance, the\nbranch delay penalty is a decisive factor",
			  "learly, there is much to consider when designing a heuris-\ntic for hyperblock selection",
			  "MPACT compiler begins by transforming the code\nso that it is more amenable to hyperblock formation [15].\nIMPACT’s algorithm then identifies acyclic paths of con-\ntrol that are suitable for hyperblock inclusio",
			  "A pri-\nority function— which is the critical calculation in the predi-\ncation decision process— assigns a value to each of the paths\nbased on characteristics such as the ones just described [15].\nSome of these characteristics come from runtime profiling.\nIMPACT uses the priority function shown below:\n*h* *i* =\n�\n0 *.* 25\n:\nif *path* *i* contains a hazard.\n1\n:\nif *path* *i* is hazard free.\n*d ratio* *i* =\n*dep height* *i*\nmax *j* =1 *→* *N* *dep height* *j*\n*o ratio* *i* =\n*num ops* *i*\nmax *j* =1 *→* *N* *num ops* *j*\n*priority* *i* = *exec ratio* *i* *·* *h* *i* *·* (2 *.* 1 *−* *d ratio* *i* *"
			]
		  },
		  {
			"title": "Optimize Options (Using the GNU Compiler Collection (GCC))",
			"url": "https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html",
			"excerpts": [
			  "RTL if-conversion tries to remove conditional branches around a block and",
			  "replace them with conditionally executed instructions.",
			  "These parameters\ngive the maximum permissible cost for the sequence that would be generated\nby if-conversion depending on whether the branch is statically determined\nto be predictable or not.",
			  "lso use other heuristics to decide whether if-conversion is likely to be\nprofitable.",
			  "`tracer-max-code-growth`",
			  "Stop tail duplication once code growth has reached given percentage.",
			  "he maximum number of instructions in a block which should be\nconsidered for if-conversion. ",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "The compiler will try to\nprovide a reasonable default for this parameter using the BRANCH_COST\ntarget macro.",
			  "Set to 0\nif either vectorization ( -ftree-vectorize ) or if-conversion\n( -ftree-loop-if-convert ) is disabled."
			]
		  },
		  {
			"title": "Automating the Construction of Compiler Heuristics Using ...",
			"url": "https://commit.csail.mit.edu/papers/2006/stephenson_phdthesis.pdf",
			"excerpts": [
			  "ng a process dubbed if-conversion, a predicating compiler merges disjoint paths of execu-\ntion into a predicated hyperblock [67]. A hyperblock is a predicated single-entry, multiple-\nexit region.",
			  ". The processor\nnullifies all instructions in the incorrect path. I",
			  "redication can effectively expose ILP, simply predi-\ncating everything will diminish performance by saturating machine resources with useless\ninstructions",
			  "f p2 is true all of the time for instance, the instructions guarded by p3 con-\nsume valuable execution slots"
			]
		  },
		  {
			"title": "Meta Optimization: Improving Compiler Heuristics with ...",
			"url": "https://www.eecg.utoronto.ca/~steffan/carg/readings/metaopt_talk.pdf",
			"excerpts": [
			  "***Hyperblock Formation***\nUses If-Conversion to convert control dependences into data\ndependences.\nSingle entry multiple exit block with predicated instructions.",
			  "There is a limit to the effectiveness of predicating instructions\nand forming large hyperblocks.",
			  "Path predictability - unpredictable branches",
			  "Path frequency - frequent paths",
			  "**Page 13**\n***Hyperblock Formation (2)***\nNumber of instructions in path - shorter paths\nNumber of branches in path - more branches but not too many\nCompiler optimization considerations (e.g. exceptions)\nMachine-specific considerations (e.g. branch delay penalty)",
			  "Experiment performed with Trimaran's IMPACT compiler\napproximating the Intel Itanium architecture (EPIC)."
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://iccd.et.tudelft.nl/Proceedings/2004/22310466.pdf",
			"excerpts": [
			  "Most of these\nschemes group sequential basic blocks into larger entities, thereby\nreducing control dependencies, increasing the fetch rate and allow-\ning more opportunities for optimizatio",
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "imilar to\nsuperblocks, successive sets of instructions are combined and op-\ntimized to form a single hyperblock, however, hyperblocks may\ninclude multiple paths if the bias cannot be determined",
			  "predicated execution using hyper-\nblocks [1"
			]
		  },
		  {
			"title": "Compiler-Based Frame Formation for Static Optimization",
			"url": "https://personal.utdallas.edu/~gxm112130/papers/iccd04.pdf",
			"excerpts": [
			  "Superblocks comprise a block of instructions with single entry\nand multiple exits [10, 13].",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* ",
			  "Control dependencies in hyperblocks [17] are converted into\ndata dependencies through the process of *if-conversion* "
			]
		  },
		  {
			"title": "THE INTEL IA-64 COMPILER CODE GENERATOR",
			"url": "https://www.cs.virginia.edu/~skadron/cs854_uproc_survey/spring_2001/cs854/m5044.pdf",
			"excerpts": [
			  "Predication, or conditional execution of an\ninstruction based on a predicate, is one of the\nkey IA-64 architectural features."
			]
		  }
		],
		"reasoning": "The hyperblock is described as a predicated single-entry, multiple-exit region that aggregates control-flow paths into one scheduling unit. Explicit definitions state that a hyperblock is a structure created to facilitate optimization and scheduling for predicated architectures, and that it consists of predicated basic blocks with a single entry point and multiple exit points. This directly supports the field value’s characterization of a hyperblock as a single-entry, multiple-exit sequence of predicated instructions intended to expose a large amount of instruction-level parallelism across multiple control paths for the scheduler. Several excerpts reinforce this with precise definitions, including the notion that a hyperblock comprises predicated basic blocks where control may only enter from the top and exit through several points, and that forming hyperblocks often involves tail duplication and if-conversion. The role of predication in enabling ILP by removing or masking branches and allowing a continuous, larger instruction stream is also described, further tying predicated regions to improved scheduling in VLIW-like architectures. Additional sources discuss how to form, select, and balance hyperblocks in practice, including heuristics for profitability, resource usage, dependence height, hazard presence, and execution frequency, which corroborate the practical considerations for constructing such regions to maximize parallelism when loop-carried dependencies exist. Overall, the strongest support comes from explicit hyperblock definitions and the framing of hyperblocks as predicated, single-entry/multiple-exit regions designed to expose wide scheduler parallelism across multiple control paths.",
		"confidence": "high"
	  }
	],
	"outputSchema": {
	  "type": "object",
	  "properties": {
		"executive_summary": {
		  "type": "string",
		  "description": "A high-level synthesis of the most effective and obscure VLIW scheduling techniques applicable to the user's custom architecture, summarizing the key strategies for handling irregular memory access, long dependency chains, and maximizing slot utilization."
		},
		"vliw_scheduling_playbook": {
		  "type": "string",
		  "description": "A practical, step-by-step decision tree and playbook for scheduling code on the target VLIW. This guide covers the entire process, from initial loop analysis and MII calculation to selecting strategies for control flow, memory access, and register pressure, providing a direct, actionable answer to the user's optimization problem."
		},
		"control_flow_optimization_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique_name": {
				"type": "string",
				"description": "The name of the control flow optimization technique."
			  },
			  "description": {
				"type": "string",
				"description": "A summary of how the technique works to manage control flow."
			  },
			  "primary_mechanism": {
				"type": "string",
				"description": "The core mechanism used by the technique, e.g., if-conversion, tail duplication."
			  },
			  "key_trade_offs": {
				"type": "string",
				"description": "The main trade-offs associated with using this technique, such as code size bloat or hardware requirements."
			  }
			},
			"required": [
			  "technique_name",
			  "description",
			  "primary_mechanism",
			  "key_trade_offs"
			],
			"additionalProperties": false
		  },
		  "description": "Detailed explanations of techniques for managing data-dependent control flow to reduce branch overhead and increase ILP. This includes trace scheduling, superblock scheduling, hyperblock formation, and predicated execution (if-conversion), along with their respective trade-offs regarding code size, predicate pressure, and scheduling complexity."
		},
		"advanced_software_pipelining_methods": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "method_name": {
				"type": "string",
				"description": "The name of the advanced software pipelining method."
			  },
			  "core_principle": {
				"type": "string",
				"description": "The fundamental principle behind the method."
			  },
			  "primary_use_case": {
				"type": "string",
				"description": "The specific type of loop or problem this method is best suited to solve."
			  },
			  "key_benefit": {
				"type": "string",
				"description": "The main advantage of using this method over simpler techniques."
			  }
			},
			"required": [
			  "method_name",
			  "core_principle",
			  "primary_use_case",
			  "key_benefit"
			],
			"additionalProperties": false
		  },
		  "description": "An in-depth look at advanced software pipelining techniques beyond basic modulo scheduling. This covers methods like Decoupled Software Pipelining (DSWP) for breaking loop-carried dependencies, Coarse-Grained Pipelined Accelerators (CGPA), and Enhanced Modulo Scheduling (EMS) for handling loops with conditional branches."
		},
		"memory_latency_hiding_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique_name": {
				"type": "string",
				"description": "The name of the memory latency hiding technique or prefetcher."
			  },
			  "mechanism_summary": {
				"type": "string",
				"description": "A brief explanation of how the technique works."
			  },
			  "implementation_type": {
				"type": "string",
				"description": "The type of implementation, e.g., Hardware-assisted, Software-only, Hardware-Software Co-design."
			  },
			  "reported_performance": {
				"type": "string",
				"description": "Quantifiable performance gains or speedups reported in research."
			  }
			},
			"required": [
			  "technique_name",
			  "mechanism_summary",
			  "implementation_type",
			  "reported_performance"
			],
			"additionalProperties": false
		  },
		  "description": "A collection of strategies specifically designed to hide memory latency in the presence of irregular access patterns like pointer-chasing. This includes hardware-assisted data speculation (e.g., Itanium's ALAT), dynamic hazard resolution, and various software and hardware prefetching mechanisms (e.g., IMP, Prodigy, data structure fetchers)."
		},
		"long_dependency_chain_strategies": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "strategy_name": {
				"type": "string",
				"description": "The name of the strategy for handling long dependency chains."
			  },
			  "description": {
				"type": "string",
				"description": "How the strategy works to break or parallelize a sequential workload."
			  },
			  "primary_application": {
				"type": "string",
				"description": "The typical application or problem domain for this strategy, e.g., hash functions, cryptography."
			  },
			  "impact_on_vliw_units": {
				"type": "string",
				"description": "How this strategy helps utilize the specific functional units of the VLIW."
			  }
			},
			"required": [
			  "strategy_name",
			  "description",
			  "primary_application",
			  "impact_on_vliw_units"
			],
			"additionalProperties": false
		  },
		  "description": "Specific techniques for restructuring or parallelizing workloads with long, sequential dependency chains, such as hash functions. This includes multi-stream interleaving (processing multiple independent data streams concurrently), bit-slicing to convert scalar problems into parallel bitwise operations, and algebraic transformations."
		},
		"register_pressure_management_techniques": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "technique_name": {
				"type": "string",
				"description": "The name of the technique for managing register pressure."
			  },
			  "type": {
				"type": "string",
				"description": "The category of the technique, e.g., Software-only compiler optimization, Hardware support, Scheduling Heuristic."
			  },
			  "mechanism": {
				"type": "string",
				"description": "How the technique works to reduce the number of live registers."
			  },
			  "main_trade_off": {
				"type": "string",
				"description": "The primary cost or trade-off, such as increased code size or required architectural support."
			  }
			},
			"required": [
			  "technique_name",
			  "type",
			  "mechanism",
			  "main_trade_off"
			],
			"additionalProperties": false
		  },
		  "description": "A critical analysis of methods for managing the high register pressure induced by aggressive software pipelining. This covers software-only approaches like Modulo Variable Expansion (MVE) and live-range splitting, hardware support like rotating registers, and advanced scheduling heuristics like Swing Modulo Scheduling (SMS)."
		},
		"software_managed_memory_strategies": {
		  "type": "object",
		  "properties": {
			"core_concept": {
			  "type": "string",
			  "description": "The fundamental idea of using Scratchpad Memory (SPM)."
			},
			"key_technique": {
			  "type": "string",
			  "description": "The primary technique used to hide latency, such as double-buffering."
			},
			"allocation_algorithms": {
			  "type": "string",
			  "description": "Methods used to decide which data to place in the SPM (e.g., static, dynamic, profile-guided)."
			},
			"dma_scheduling_pattern": {
			  "type": "string",
			  "description": "The pattern for using a DMA engine to overlap data transfers with computation."
			}
		  },
		  "required": [
			"core_concept",
			"key_technique",
			"allocation_algorithms",
			"dma_scheduling_pattern"
		  ],
		  "additionalProperties": false
		},
		"instruction_packing_algorithms_and_heuristics": {
		  "type": "object",
		  "properties": {
			"problem_analogy": {
			  "type": "string",
			  "description": "The classic computer science problem that instruction packing is analogous to."
			},
			"exact_methods": {
			  "type": "string",
			  "description": "Optimal but computationally expensive methods like ILP and Constraint Programming."
			},
			"heuristic_methods": {
			  "type": "string",
			  "description": "Fast but suboptimal methods used in production compilers, such as list scheduling."
			},
			"compile_time_trade_off": {
			  "type": "string",
			  "description": "The fundamental trade-off between the time it takes to compile the code and the runtime performance of the result."
			}
		  },
		  "required": [
			"problem_analogy",
			"exact_methods",
			"heuristic_methods",
			"compile_time_trade_off"
		  ],
		  "additionalProperties": false
		},
		"platform_specific_compiler_insights": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "platform_name": {
				"type": "string",
				"description": "The name of the VLIW platform, e.g., TI C6x DSP, Intel Itanium."
			  },
			  "key_architectural_feature": {
				"type": "string",
				"description": "The most notable architectural feature or compiler strategy of the platform."
			  },
			  "specific_techniques": {
				"type": "string",
				"description": "Examples of specific pragmas, instructions, or compiler passes from that platform."
			  },
			  "relevance_to_custom_vliw": {
				"type": "string",
				"description": "How the insights from this platform can be applied to the user's custom architecture."
			  }
			},
			"required": [
			  "platform_name",
			  "key_architectural_feature",
			  "specific_techniques",
			  "relevance_to_custom_vliw"
			],
			"additionalProperties": false
		  },
		  "description": "A collection of obscure and advanced compiler techniques and architectural insights from specific VLIW platforms. This will include details on TI C6x DSP pragmas (MUST_ITERATE) and diagnostics, Qualcomm Hexagon's two-stage scheduler and DFAPacketizer, Intel Itanium's EPIC features (bundling, speculation), and Transmeta Crusoe's dynamic code morphing."
		},
		"manual_and_niche_optimizations": {
		  "type": "array",
		  "items": {
			"type": "object",
			"properties": {
			  "optimization_name": {
				"type": "string",
				"description": "The name of the niche optimization technique."
			  },
			  "source_community": {
				"type": "string",
				"description": "The community where this trick originated, e.g., Demoscene, DSP Engineers."
			  },
			  "description": {
				"type": "string",
				"description": "A summary of how the optimization works."
			  },
			  "vliw_application": {
				"type": "string",
				"description": "How the technique can be used to better utilize VLIW functional units."
			  }
			},
			"required": [
			  "optimization_name",
			  "source_community",
			  "description",
			  "vliw_application"
			],
			"additionalProperties": false
		  },
		  "description": "A curated list of non-obvious optimization tricks sourced from experienced DSP engineers, the demoscene, and competitive programming communities. This covers techniques like SIMD Within A Register (SWAR), branchless arithmetic transforms, latency-aware instruction pairing, and manual loop unrolling to outperform compiler heuristics."
		},
		"machine_learning_in_scheduling": {
		  "type": "object",
		  "properties": {
			"primary_approach": {
			  "type": "string",
			  "description": "The main machine learning paradigm being applied, e.g., Deep Reinforcement Learning."
			},
			"mechanism": {
			  "type": "string",
			  "description": "How the ML model is used to make scheduling decisions."
			},
			"key_features": {
			  "type": "string",
			  "description": "The types of program and architectural features the model uses as input."
			},
			"safeguards": {
			  "type": "string",
			  "description": "How negative side effects like increased register pressure are managed."
			}
		  },
		  "required": [
			"primary_approach",
			"mechanism",
			"key_features",
			"safeguards"
		  ],
		  "additionalProperties": false
		},
		"repurposed_fault_tolerance_techniques": {
		  "type": "object",
		  "properties": {
			"core_idea": {
			  "type": "string",
			  "description": "The central concept of repurposing fault-tolerance research for performance."
			},
			"original_technique": {
			  "type": "string",
			  "description": "The fault-tolerance mechanism being adapted, e.g., dynamic instruction replication."
			},
			"performance_application": {
			  "type": "string",
			  "description": "The specific performance problem this repurposed technique solves, e.g., hiding inter-cluster communication latency."
			},
			"justification": {
			  "type": "string",
			  "description": "Why this approach is viable, typically citing the high under-utilization of functional units in wide-issue machines."
			}
		  },
		  "required": [
			"core_idea",
			"original_technique",
			"performance_application",
			"justification"
		  ],
		  "additionalProperties": false
		},
		"empirical_case_studies_summary": {
		  "type": "object",
		  "properties": {
			"platform": {
			  "type": "string",
			  "description": "The VLIW-like processor the case study is based on."
			},
			"workload": {
			  "type": "string",
			  "description": "The type of benchmark or workload analyzed."
			},
			"key_finding": {
			  "type": "string",
			  "description": "The main takeaway or quantified result, such as achieved IPC or effectiveness of a technique."
			},
			"implication": {
			  "type": "string",
			  "description": "What this finding implies for the user's custom architecture."
			}
		  },
		  "required": [
			"platform",
			"workload",
			"key_finding",
			"implication"
		  ],
		  "additionalProperties": false
		},
		"key_architectural_feature_recommendations": {
		  "type": "object",
		  "properties": {
			"feature_name": {
			  "type": "string",
			  "description": "The name of the recommended architectural feature."
			},
			"description": {
			  "type": "string",
			  "description": "What the feature is and how it works."
			},
			"benefit_for_vliw": {
			  "type": "string",
			  "description": "The specific benefit this feature provides for VLIW scheduling and performance."
			},
			"relevance_to_user_problem": {
			  "type": "string",
			  "description": "How this feature directly addresses the user's challenges with irregular memory and loop-carried dependencies."
			}
		  },
		  "required": [
			"feature_name",
			"description",
			"benefit_for_vliw",
			"relevance_to_user_problem"
		  ],
		  "additionalProperties": false
		}
	  },
	  "required": [
		"executive_summary",
		"vliw_scheduling_playbook",
		"control_flow_optimization_techniques",
		"advanced_software_pipelining_methods",
		"memory_latency_hiding_techniques",
		"long_dependency_chain_strategies",
		"register_pressure_management_techniques",
		"software_managed_memory_strategies",
		"instruction_packing_algorithms_and_heuristics",
		"platform_specific_compiler_insights",
		"manual_and_niche_optimizations",
		"machine_learning_in_scheduling",
		"repurposed_fault_tolerance_techniques",
		"empirical_case_studies_summary",
		"key_architectural_feature_recommendations"
	  ],
	  "additionalProperties": false
	}
  }